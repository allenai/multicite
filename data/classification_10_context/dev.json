[
 {
  "id": "02521fd9721c264ee05315dec9b31d_0",
  "x": "In low-resource conditions using only 10 hours of labeled data, we achieve Word Error Rates (WER) of 10.2% and 23.5% on the standard test \"clean\" and \"other\" benchmarks of the Librispeech dataset, which is almost on bar with previously published work that uses 10 times more labeled data. Moreover, compared to previous work that uses two models in tandem <cite>(Baevski et al., 2019b)</cite> , by using one model for both BERT pre-trainining and fine-tuning, our model provides an average relative WER reduction of 9%. 1 ---------------------------------- **INTRODUCTION** Representation learning has been an active research area for more than 30 years (Hinton et al., 1986) , with the goal of learning high level representations which separates different explanatory factors of the phenomena represented by the input data (LeCun et al., 2015; Bengio et al., 2013) . Disentangled representations provide models with exponentially higher ability to generalize, using little amount of labels, to new conditions by combining multiple sources of variations. 1 We will open source the code for our models. Building Automatic Speech Recognition (ASR) systems, for example, requires a large volume of training data to represent different factors contributing to the creation of speech signals, e.g. background noise, recording channel, speaker identity, accent, emotional state, topic under discussion, and the language used in communication. The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_1",
  "x": "Representation learning has been an active research area for more than 30 years (Hinton et al., 1986) , with the goal of learning high level representations which separates different explanatory factors of the phenomena represented by the input data (LeCun et al., 2015; Bengio et al., 2013) . Disentangled representations provide models with exponentially higher ability to generalize, using little amount of labels, to new conditions by combining multiple sources of variations. 1 We will open source the code for our models. Building Automatic Speech Recognition (ASR) systems, for example, requires a large volume of training data to represent different factors contributing to the creation of speech signals, e.g. background noise, recording channel, speaker identity, accent, emotional state, topic under discussion, and the language used in communication. The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> . Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\".",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_2",
  "x": "The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> . Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\". To understand the nature of their learned representations, we train models using the continuous and the discrete BERT approaches on spectral features, e.g. Mel-frequency cepstral coefficients (MFCC), as well as on pre-trained Wav2vec features . These comparisons provide insights on how complementary the acoustically motivated contrastive loss function is to the other masked language model one. The unsupervised and semi-supervised ASR approaches is in need for test suites like the unified downstream tasks available for language representation models (Devlin et al., 2018) . Lscher et al., 2019) evaluated semi-supervised self-labeling WER performance on the standard test \"clean\" and test \"other\" benchmarks of the Librispeech dataset (Panayotov et al., 2015) when using only 100 hour subset as labeled data.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_3",
  "x": "The model is based on two convolutional neural networks where the encoder f : X \u2192 Z produces a representation z i for each time step i at a rate of 100 Hz and the aggregator g : Z \u2192 C combines multiple encoder time steps into a new representation c i for each time step i. Given c i , the model is trained to distinguish a sample z i+k that is k steps in the future from distractor samplesz drawn from a distribution p n , by minimizing the contrastive loss for steps k = 1, . . . , K: where T is the sequence length, \u03c3(x) = 1/(1 + exp(\u2212x)), and where \u03c3(z i+k h k (c i )) is the probability of z i+k being the true sample. A step-specific affine transformation h k (c i ) = W k c i + b k is applied to c i (van den Oord et al., 2018) . The loss L = K k=1 L k is optimized by summing (1) over different step sizes. The learned high level features produced by the context network c i are shown to be better acoustic representations for speech recognition compared to standard spectral features. ---------------------------------- **VQ-WAV2VEC** vq-wav2vec <cite>(Baevski et al., 2019b)</cite> learns vector quantized (VQ) representations of audio data using a future time-step prediction task. Similar to wav2vec, there is a convolutional encoder and decoder networks f : X \u2192 Z and g :\u1e90 \u2192 C for feature extraction and aggregation. However, in between them there is a quantization module q : Z \u2192\u1e90 to build discrete representations which are input to the aggregator.",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_4",
  "x": "The quantization module replaces the original representation z by\u1e91 = e i from a fixed size codebook e \u2208 R V \u00d7d which contains V representations of size d. ---------------------------------- **APPROACH** ---------------------------------- **DISCRETE BERT** Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step.",
  "y": "extends"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_5",
  "x": "**DISCRETE BERT** Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ). ---------------------------------- **CONTINUOUS BERT** A masked language modeling task cannot be performed with continuous inputs and outputs, as there are no targets to predict in place of the masked tokens.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_6",
  "x": "Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ). ---------------------------------- **CONTINUOUS BERT** A masked language modeling task cannot be performed with continuous inputs and outputs, as there are no targets to predict in place of the masked tokens. Instead of reconstructing the input as in (van den Oord et al., 2017), we classify the masked positive example among a set of negatives.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_7",
  "x": "All of our experiments are performed by pretraining on 960 hours of Librispeech (Panayotov et al., 2015) training set, fine-tuning on labeled 10 hours and 1 hour sets sampled equally from the two conditions of the training set, and evaluating on the standard dev and test splits. ---------------------------------- **MODELS** ---------------------------------- **QUANTIZED INPUTS TRAINING** We first train the vq-wav2vec quantization model following the gumbel-softmax recipe described in <cite>(Baevski et al., 2019b)</cite> . After training this model For quantizing MFCC and log-mel filterbanks we first compute dense features using the scripts from the Kaldi (Povey) toolkit. We then compute 13.5k K-Means centroids, to match the number of unique tokens produced by the vq-wav2vec model, using 8 32GB Volta GPUs. To fit into GPU memory, we subsample 50% of MFCC features and 25% of FBANK features from the training set before running the K-Means algorithm. The model we use for the masked language modeling task is a standard BERT model with 12 layers, model dimension 768, inner dimension (FFN) 3072 and 12 attention heads (Devlin et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_8",
  "x": "The learning rate is warmed up over the first 10,000 updates to a peak value of 1 \u00d7 10 \u22125 , and then linearly decayed over a total of 250k updates. We train on 128 GPUs with a batch size of 3072 tokens per GPU giving a total batch size of 393k tokens (Ott et al., 2018) . Each token represents 10ms of audio data. To mask the input sequence, we follow <cite>(Baevski et al., 2019b)</cite> and randomly sample p = 0.05 of all tokens to be a starting index, without replacement, and mask M = 10 consecutive tokens from every sampled index; spans may overlap. ---------------------------------- **CONTINUOUS INPUTS TRAINING** For training on dense features, we use a model similar to a standard BERT model with the same parameterization as the one used for quantized input training, but we use the wav2vec, MFCC or FBANK inputs directly. We add 128 relative positional embeddings at every multi-head attention block as formulated in (Dai et al., 2019) instead of fixed positional embeddings to ease handling longer examples. We train this model on only 8 GPUs, with a batch size of 9600 inputs per GPU resulting in a total batch size of 76,800. We find that increasing the number of GPUs (which increases the effective batch size) does not lead to better results with this particular setup.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_9",
  "x": "We use the weights found by these sweeps to evaluate and report results for all other splits. The sweeps are run with beam size of 250, while the final decoding is done with beam size of 1500. The quantized BERT models have a limit of 2048 source tokens due to their use of fixed positional embeddings. During training we discard longer examples and during evaluation we discard randomly chosen tokens from each example until they are at most 2048 tokens long. We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in<cite> Baevski et al. (2019b)</cite> . The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features. The discrete BERT training augments the wav2vec features with a higher level of representation that captures the sequential structure of the full utterance through the masked language modeling loss.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_10",
  "x": "The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features. The discrete BERT training augments the wav2vec features with a higher level of representation that captures the sequential structure of the full utterance through the masked language modeling loss. On the other hand, the continuous BERT training, given its contrastive InforNCE loss, can be viewed as another level of acoustic representations that captures longer range regularities. ---------------------------------- **RESULTS** Using the MFCC and FBANK as inputs to the continuous and discrete BERT models provide insights on the synergies of different levels of acoustic and language model representations. Similar to the observations in (Mohamed et al., 2012) , the FBANK features are more friendly to unsupervised local acoustic representation learning methods like continuous BERT, leading to consistent gains compared to MFCC features for both 10h and 1h sets. model plays the role of a language model and input wav2vec features learns high level acoustic representations, in the very low-resource condition of 1h fine-tuning, the average relative improvement between quantized FBANK and Wav2vec inputs is larger in the \"clean\" subsets -55%, which require better local acoustic representations, compared to 45% WER reduction for the noisy \"other\" subsets that rely more on the global language modeling capabilities.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_11",
  "x": "We believe the reason is due to the complementary nature of the discrete BERT language modelling loss and the wav2vec acoustically motivated pre-training, as opposed to the relatively redundant acoustic pre-training losses of the continious BERT and wav2vec. In the 1h fine-tuning case, however, better local acoustic features provide more gains in the \"clean\" subsets compared to the \"other\" ones, following the same trend of the quantized FBANK and wav2vec features under the same conditions. Table 2 shows the competitive performance of the discrete BERT approach compared to previously published work which is fine-tuned on more than 10 times the labeled data. ---------------------------------- **ABLATIONS** To understand the value of self-supervision in our setup, Table 3 shows WERs for both continuous and discrete input features fine-tuned from random weights, without BERT pre-training, using (Table  4 ). Adding a second layer of representation more than halved the WER, with more gains observed in the \"clean\" subset as also observed in 4.4. ---------------------------------- **DISCUSSION AND RELATED WORK** The the success of BERT (Devlin et al., 2018) and Word2Vec (Mikolov et al., 2013) for NLP tasks motivated more research on self-supervised approaches for acoustic word embedding and unsupervised acoustic feature representation (Bengio and Heigold; Levin et al.; Chung et al., b; He et al.; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> , either by predicting masked discrete or continuous input, or by contrastive prediction of neighboring or similarly sounding segments using distant supervision or proximity in the audio signal as an indication of similarity.",
  "y": "background"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_0",
  "x": "Another major breakthrough in the domain was the introduction of Long-Short Term Memory (LSTM) architecture [7] which improvised over the RNN by introducing a context cell which stores the prior relevant information. The vanilla VQA model [1] used a combination of VGGNet [3] and LSTM [7] . This model has been revised over the years, employing newer architectures and mathematical formulations. Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] . Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention Networks [12] and Pythia v1.0 [13] . Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model <cite>[14]</cite> . Finally, we discuss the observations and future directions. ---------------------------------- **II. DATASETS**",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_1",
  "x": "It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images. [20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential Modules [30] , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA Challenge 2018 1 . The architecture is similar to Teney et al. <cite>[14]</cite> with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models. Differential Networks [20] : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN [22] . The differential modules [30] are used to refine the features in both text and images. GRU [31] is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. 4.",
  "y": "similarities"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_2",
  "x": "As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model <cite>[14]</cite> . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_3",
  "x": "The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance. ---------------------------------- **V. CONCLUSION** The Visual Question Answering has recently witnessed a great interest and development by the group of researchers and scientists from all around the world. The recent trends are observed in the area of developing more and more real life looking datasets by incorporating the real world type questions and answers. The recent trends are also seen in the area of development of sophisticated deep learning models by better utilizing the visual cues as well as textual cues by different means. The performance of even best model is still lagging and around 60-70% only.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_0",
  "x": "It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1). Previous models would have to learn these relations only from the annotated training data, which is much too sparse for learning all possible relations between all events, states or claims. The recently proposed BERT model (Devlin et al., 2019) takes a promising step towards addressing this problem: the BERT representations are trained using a language modelling and, crucially, a \"next sentence prediction\" task, where the model is presented with the actual next sentence vs. a different sentence and needs to select the original next sentence. We believe it is a good fit for discourse relation recognition, since the task allows the model to represent what a typical next sentence would look like. In this paper, we show that a BERT-based model outperforms the current state of the art by 8% points in 11-way implicit discourse relation classification on PDTB. We also show that after pre- trained with small size cross-domain data, the model can be easily transferred to a new domain: it achieves around 16% accuracy gain on BioDRB compared to state of the art model.",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_1",
  "x": "Recent models exploit such similarity relations between the two arguments, as well as simpler surface features that occur in one relational argument and correlate with specific coherence relations (e.g., the presence of negation, temporal expressions etc. may give hints as to what coherence relation may be present, see Park and Cardie (2012) ; Asr and Demberg (2015)). However, relations between arguments are often a lot more diverse than simple contrasts that can be captured through antonyms, and may rely on world knowledge (Kishimoto et al., 2018) . It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1). Previous models would have to learn these relations only from the annotated training data, which is much too sparse for learning all possible relations between all events, states or claims. The recently proposed BERT model (Devlin et al., 2019) takes a promising step towards addressing this problem: the BERT representations are trained using a language modelling and, crucially, a \"next sentence prediction\" task, where the model is presented with the actual next sentence vs. a different sentence and needs to select the original next sentence. We believe it is a good fit for discourse relation recognition, since the task allows the model to represent what a typical next sentence would look like.",
  "y": "motivation"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_2",
  "x": "It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in<cite> Bai and Zhao (2018)</cite> , we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009) , which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015) , which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 0-22, as promoted in . We also follow the standard in the literature to formulate the task as an 11-way classification task. Results are presented in Table 1 . We evaluated three versions of the BERT-based model. All of our BERT models use the pre-trained representations and are fine-tuned on the PDTB training data. The version marked as \"BERT\" does not do any additional pre-training. BERT+WSJ in addition performs further pre-training on the 1 https://github.com/google-research/ bert#pre-trained-models parts of the Wall Street Journal corpus that do not have discourse relation annotation. The model version \"BERT+WJS w/o NSP\" also performs pre-training on the WSJ corpus, but only uses the Masked Language Modelling task, not the Next Sentence Prediction task in the pre-training.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_3",
  "x": "In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> . What's more, the BERT model outperformed on cross validation by around 8%, with significance of p<0.01. Significance test was performed by estimating variance of the model from the performance on different folds in cross-validation (paired t-test). For the Lin and Ji evaluations, we estimated variance due to random initialization by running them 5 times and calculating the likelihood that the state-of-the-art model result would come from that distribution. ---------------------------------- **EVALUATION ON BIODRB**",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_4",
  "x": "We compared the results with four state-of-theart systems: Cai and Zhao (2017) proposed a model that takes a step towards calculating discourse expectations by using attention over an encoding of the first argument, to generate the representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse relation arguments. Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> . What's more, the BERT model outperformed on cross validation by around 8%, with significance of p<0.01.",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_5",
  "x": "The BioDRB contains a lot of professional words / phrases that are extremely hard to model. In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD** Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90 BioBERT Table 2 : Accuracy (%) on BioDRB level 2 relations with different settings. Cross-Domain means trained on PDTB and tested on BioDRB.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_6",
  "x": "The biomedical domain is very different from the WSJ or the data on which the BERT model was trained. The BioDRB contains a lot of professional words / phrases that are extremely hard to model. In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD** Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90 BioBERT Table 2 : Accuracy (%) on BioDRB level 2 relations with different settings.",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_8",
  "x": "In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD** Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90 BioBERT Table 2 : Accuracy (%) on BioDRB level 2 relations with different settings. Cross-Domain means trained on PDTB and tested on BioDRB. For the In-Domain setting, we used 5-fold cross-validation and report average accuracy.",
  "y": "differences"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_0",
  "x": "Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20].",
  "y": "background"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_1",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space.",
  "y": "future_work motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_2",
  "x": "The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "motivation future_work"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_3",
  "x": "Additional prospects for future practical research lines will also be discussed in this comment. The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_4",
  "x": "Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20].",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_0",
  "x": "Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually factor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010) . Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b) , second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010 ) models. Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a ; * The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model 1 based on these previous works.",
  "y": "background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_1",
  "x": "Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP. The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i.e., PP \u2192 IN NP. This algorithm is more complex than the approximate decoding algorithm of<cite> Huang (2008)</cite> . However, its efficiency heavily depends on the size of the parse forest it has to handle. Forest pruning (Char- Child ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_2",
  "x": "Some back-off structural features are used for smoothing, which cannot be presented due to limited space. With only lexical features in a part, this parsing model backs off to a first-order one similar to those in the previous works. Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. ---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_3",
  "x": "---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP \u2192 NP VP in Figure 1 , the algorithm will extract child features from its children NP \u2192 DT QP and VP \u2192 VBN PP. The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i.e., PP \u2192 IN NP. This algorithm is more complex than the approximate decoding algorithm of<cite> Huang (2008)</cite> .",
  "y": "differences"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_5",
  "x": "**EXPERIMENT** Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007) , as listed in Table 2 . For parser combination, we follow the setting of Fossum and Knight (2009) , using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002) . A factor \u03bb is introduced to balance the two models. It is tuned on a development set using the gold sec- (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54<cite> Huang (2008)</cite> 91.69 43.5 Combination Fossum and Knight (2009) 92. 4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (Kiefer, 1953) . The parameters \u03b8 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and<cite> Huang (2008)</cite> . The performance of our first-and higher-order parsing models on all sentences of the two test sets is presented in Table 3 , where \u03bb indicates a tuned balance factor.",
  "y": "uses"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_0",
  "x": "These laws have been generalized to account for more general patterns [6, 7, 8, 9, 10, 11, 12, 13] . Mapping a text into a time series can also reveal hidden structural patterns. Methods applied to investigate text structure at word level include investigations on probability distributions [14] , correlations [15, 16, 17, 18, 19] , and networks properties [20, 21, 22, 23] . Another way to investigate text structure is by the analysis of sentence lengths. Typically, each sentence carries a full message and transmits an idea in contrast with an isolated word. Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures. Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] .",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_1",
  "x": "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed. Other possible variations are the removal of stop words and the lemmatization [21] . In a similar way, the number of characters and variations related to lemmatization and stop words removal could also be considered at sentence level. However, to choose between words or characters to measure a sentence may lead to doubts. For instance, the Japanese language has three alphabets and the number of characters may differ for the same word [25] . In the present work, we investigated the robustness of distinct measures of sentence lengths.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_2",
  "x": "However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in<cite> [28]</cite> asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words. In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic. ---------------------------------- **SIMILARITIES BETWEEN DISTRIBUTIONS** Next, we investigated the distribution of sentence lengths within each book, for each measure of sentence length considered here. This test exploits the distance \u03ba between two empirical cumulated distributions C 1 and C 2 : where sup is the supremum function; the smaller the \u03ba the greater the similarity between the distributions. Typically, the number of characters is greater than the number of words (Figs. 1b and 1c) . Thus, to make a fair comparison, the all time series were normalized by their mean values prior to the KS test. Figure 3a and 3b illustrate some results for the book The Brothers Karamazov.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_3",
  "x": "It is expected that h * = 0.5. Using the The Brothers Karamazov as example again, Fig. 4a and 4b illustrate, respectively, F (m) and the differences between the Hurst exponents for all six series. We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * \u223c 0.5, implying in long-range correlations. All the series from the other books reflects this behavior (h \u223c 0.75). This result is consistent with the multifractal analysis performed in<cite> [28]</cite> . In addition, the values for the difference between Hurst exponents (\u2206h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another. Also, the standard deviation for h within each book was close to 0.001. Lastly, no correlations were found between the number of sentences in a text and the Hurst exponent. ---------------------------------- **CONCLUSIONS**",
  "y": "similarities"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_0",
  "x": "This principle has already been used to explain the origins of other linguistic laws: Zipf's law of abbreviation, namely, the frequency of more frequent words to be shorter [3,<cite> 4]</cite> , and Menzerath's law, the tendency of a larger linguistic construct to be made of smaller components [5] . Our argument combines two constraints for compression: (1) non-singular coding, i.e. any two different words should not be represented by the same string of letters or phonemes, and (2) unique decipherability, i.e. given a continuous sequence of letters or phonemes, there should be only one way of segmenting it into words [6] . The former is needed to reduce the cost of retrieving the original meaning. The latter is required to reduce the cost of determining word boundaries. Thus both constraints on compression and compression itself, are realistic cognitive pressures that are vital to fight against the now-or-never bottleneck of linguistic processing [7] . Suppose that words are coded using an alphabet of N letters (or phonemes) and that p i and l i are the probability and the length of the i-th most probable word. On the one hand, optimal uniquely decipherable coding gives [6] l where \u2308..\u2309 is the ceiling function. Thus On the other hand, optimal non-singular coding gives<cite> [4]</cite> for N > 1. When N and i are sufficiently large, we have",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_2",
  "x": "Combining Eqs. 3 and 5, one obtains and finally Zipf's law (Eq. 1) with \u03b1 = 1. By presenting this derivation we are not taking for granted that real language use is fully optimal with regard to any of the coding schemes mentioned above. Instead, our point is that it is not surprising that languages tend toward Zipf's law given the convenience of both kinds of compression for easy and fast communication [7] . Our derivation of Zipf's law is reminiscent of Mandelbrot's derivation based on random typing [8] , that is defined by three parameters, N (the alphabet size), p s (the probability of hitting the space) and l 0 (the minimum word length). The last parameter has been introduced to accommodate other variants of Mandelbrot's model [9] . Mandelbrot's derivation assumes that typing at random determines the probability of a word, which has two key implications. First, a relationship between the length of a word and its probability<cite> [4]</cite> l = a log p + b, where a and b are constants (a < 0) defined on the parameters of the model as and",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_3",
  "x": "First, it departs from realistic cognitive pressures [7] . Second, random typing is based exclusively on random choices but its parameters cannot be set at random: indeed, a precise tuning of the parameters is needed to mimic Zipf's law with \u03b1 = 1 [8] . In contrast, our argument only requires N to be large enough. Third, its assumptions are far reaching: compression allows one to shed light on the origins of three linguistic laws at the same time: Zipf's law for word frequencies, Zipf's law of abbreviation and Menzerath's law with the unifying principle of compression [3, <cite>4,</cite> 5] . There are many ways of explaining the origins of power-law-like distributions such as Zipf's law for word frequencies [12] but compression appears to be as the only one that can lead to a compact theory of statistical laws of language. Although uniquely decipherable codes are a subset of non-singular codes, it is tempting to think that both optimal non-singular coding and optimal uniquely decipherable coding cannot be satisfied to a large extend simultaneously. However, random typing is an example of how both constraints can be met approximately. We suggest that human languages are additional examples of a different nature. The two forms of optimality can coexist to some degree because the need for unique decipherability is alleviated by statistical cues that humans use to segment the linguistic input [13] . Here we have only sketched a new path to derive power-law-like distributions of ranks from efficiency considerations.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_0",
  "x": "The traditional clustering measure of F-Score (Zhao et al., 2005 ) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense. The outcome of this process might be more helpful for systems that produce a large number of clusters. In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_1",
  "x": "The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense. The outcome of this process might be more helpful for systems that produce a large number of clusters. In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies. Subsequently, we present the use of V-measure <cite>(Rosenberg and Hirschberg, 2007)</cite> as an evaluation measure that can overcome the current limitations of F-Score.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_2",
  "x": "Subsequently, we present the use of V-measure <cite>(Rosenberg and Hirschberg, 2007)</cite> as an evaluation measure that can overcome the current limitations of F-Score. Finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of WSD performance. The proposed evaluation setting will be applied in the SemEval-2010 WSI task. ---------------------------------- **SEMEVAL-2007 WSI EVALUATION SETTING** The SemEval-2007 WSI task (Agirre and Soroa, 2007) evaluates WSI systems on 35 nouns and 65 verbs. The corpus consists of texts of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006) . For each target word tw, the task consists of firstly identifying the senses of tw (e.g. as clusters of target word instances, cooccurring words, etc.), and secondly tagging the instances of the target word using the automatically induced clusters. In the next sections, we describe and review the two evaluation schemes. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_3",
  "x": "The recall of class gs i with respect to cluster c j is defined as the number of their common instances divided by the total sense size, i.e. R(gs i , c j ) = a ij a i . The F-Score of gs i with respect to c j , F (gs i , c j ), is then defined as the harmonic mean of P (gs i , c j ) and R(gs i , c j ). The F-Score of class gs i , F (gs i ), is the maximum F (gs i , c j ) value attained at any cluster. Finally, the F-Score of the entire clustering solution is defined as the weighted average of the F-Scores of each GS sense (Formula 1), where q is the number of GS senses and N is the total number of target word ings 1 gs 2 gs 3 cl 1 500 100 100 cl 2 100 500 100 cl 3 100 100 500 stances. If the clustering is identical to the original classes in the datasets, F-Score will be equal to one. In the example of Table 1 , F-Score is equal to 0.714. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Homogeneity refers to the degree that each cluster consists of data points, which primarily belong to a single GS class. On the other hand, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster. A perfect homogeneity would result in a precision equal to 1, while a perfect completeness would result in a recall equal to 1.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_4",
  "x": "However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions. In fact, the clustering shown in Table 3 should have a better homogeneity than the clustering shown in Table 1 , since intuitively speaking each cluster contains fewer classes. Moreover, the second clustering should also have a better completeness, since each GS class contains fewer clusters. An additional instance of the matching problem manifests itself, when F-Score fails to evaluate the quality of smaller clusters. For example, if we add in Table 3 one more cluster (cl 4 ), which only tags 50 additional instances of gs 1 , then we will be able to observe that this cluster will not be matched to any of the GS senses, since cl 1 is matched to gs 1 . Although F-Score will decrease since the recall of gs 1 will decrease, the evaluation setting ignores the perfect homogeneity of this small cluster. ---------------------------------- **SWSI RESULTS & DISCUSSION**",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_5",
  "x": "Table 2 shows the unsupervised and supervised performance of systems participating in SWSI. As far as the baselines is concerned, the 1c1w baseline groups all instances of a target word into a single cluster, while the 1c1inst creates a new cluster for each instance of a target word. Note that the 1c1w baseline is equivalent to the MFS in the supervised evaluation. As it can be observed, a system with low entropy (high purity) does not necessarily achieve high F-Score. This is due to the fact that entropy and purity only measure the homogeneity of a clustering solution. For that reason, the 1c1inst baseline achieves a perfect entropy and purity, although its clustering solution is far from ideal. On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution. However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions.",
  "y": "differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_6",
  "x": "For example, high supervised recall also means high purity and low entropy as in I2R, but not vice versa as in UOY. UOY produces a large number of clean clusters, in effect suffering from an unreliable mapping of clusters to senses due to the lack of adequate training data. Moreover, an additional supervised evaluation of WSI methods using a different dataset split resulted in a different ranking, in which all of the systems outperformed the MFS baseline (Agirre and Soroa, 2007) . This result indicates that the supervised evaluation might not provide a reliable estimation of WSD performance, particularly in the case where the mapping relies on a single dataset split. 3 SemEval-2010 WSI evaluation setting 3.1 Unsupervised evaluation using V-measure Let us assume that the dataset of a target word tw comprises of N instances (data points). These data points are divided into two partitions, i.e. a set of automatically generated clusters C = {c j |j = 1 . . . n} and a set of gold standard classes GS = {gs i |gs = 1 . . . m}. Moreover, let a ij be the number of data points, which are members of class gs i and elements of cluster c j . V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Recall that homogeneity refers to the degree that each cluster consists of data points which primarily belong to a single GS class.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_7",
  "x": "**FORMULAS 2 AND 3 DEFINE H(GS) AND H(GS|C).** When there is only a single class (H(GS) = 0), any clustering would produce a perfectly homogeneous solution. In the worst case, the class distribution within each cluster is equal to the overall class distribution (H(GS|C) = H(GS)), i.e. clustering provides no new information. Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1\u2212 H(GS|C) H(GS) in any other case <cite>(Rosenberg and Hirschberg, 2007)</cite> . Symmetrically to homogeneity, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster. To evaluate completeness, V-measure examines the distribution of cluster assignments within each class. The conditional entropy of the cluster given the class distribution, H(C|GS), quantifies the remaining entropy (uncertainty) of the cluster given that the class distribution is known. Consequently, when H(C|GS) is 0, we have the perfectly complete solution, since all the data points of a class belong to the same cluster. Therefore, symmetrically to homogeneity, the completeness c of a clustering solution is 1 if there is only a single cluster (H(C) = 0), and 1 \u2212 ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_8",
  "x": "Therefore, symmetrically to homogeneity, the completeness c of a clustering solution is 1 if there is only a single cluster (H(C) = 0), and 1 \u2212 ---------------------------------- **H(C|GS) H(C)** in any other case. In the worst case, completeness will be equal to 0, particularly when H(C|GS) is maximal and equal to H(C). This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . Formulas 4 and 5 define H(C) and H(C|GS). Finally h and c can be combined and produce V-measure, which is the harmonic mean of homogeneity and completeness. Returning to our clustering example in Table 1 , its V-measure is equal to 0.275. In section 2.3, we also presented an additional clustering (Table 3) , which had the same F-Score as the clustering in Table 1, despite the fact that it intuitively had a better completeness and homogeneity.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_9",
  "x": "---------------------------------- **V-MEASURE RESULTS & DISCUSSION** In Table 4 , we also observe that the 1c1inst baseline achieves a high performance. In nouns only I2R is able to outperform this baseline, while in verbs the 1c1inst baseline achieves the highest result. By the definition of homogeneity (section 3.1), this baseline is perfectly homogeneous, since each cluster contains one instance of a single sense. However, its completeness is not 0, as one might intuitively expect. This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . This worst solution is not equivalent to the 1c1inst, hence completeness of 1c1inst is greater than 0. Additionally, completeness of this baseline benefits from the fact that around 18% of GS senses have only one instance in the test set. Note however, that on average this baseline achieves a lower completeness than most of the systems.",
  "y": "similarities"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_0",
  "x": "Some authors have suggested an alternative computational model called Object-Action Complexes (OACs) [9] , which links low-level sensorimotor knowledge with high-level symbolic reasoning hierarchically in autonomous robots. In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction. These studies indicate that perceptual input can be linked with the human action system for predicting future outcomes of actions, i. e., the effect of actions, particularly when the person possesses concrete personal experience of the actions being observed in others [16, 17] . This has also been exploited under the deep learning paradigm [18] , by using a Multiple Timescales Recurrent Neural Network (MTRNN) to have an artificial simulated agent infer human intention from joint information about object affordances and human actions. One difference between this line of research and ours is that we use real, noisy data acquired from robots and sensors to test our models, rather than virtual simulations. ----------------------------------",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_1",
  "x": "A large and growing body of research is directed towards having robots learn new cognitive skills, or improving their capabilities, by interacting autonomously with their surrounding environment. In particular, robots operating in an unstructured scenario may understand available opportunities conditioned on their body, perception and sensorimotor experiences: the intersection of these elements gives rise to object affordances (action possibilities), as they are called in psychology [6] . The usefulness of affordances in cognitive robotics is in the fact that they capture essential properties of environment objects in terms of the actions that a robot is able to perform with them [7, 8] . Some authors have suggested an alternative computational model called Object-Action Complexes (OACs) [9] , which links low-level sensorimotor knowledge with high-level symbolic reasoning hierarchically in autonomous robots. In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction. These studies indicate that perceptual input can be linked with the human action system for predicting future outcomes of actions, i. e., the effect of actions, particularly when the person possesses concrete personal experience of the actions being observed in others [16, 17] .",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_2",
  "x": "In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction. These studies indicate that perceptual input can be linked with the human action system for predicting future outcomes of actions, i. e., the effect of actions, particularly when the person possesses concrete personal experience of the actions being observed in others [16, 17] . This has also been exploited under the deep learning paradigm [18] , by using a Multiple Timescales Recurrent Neural Network (MTRNN) to have an artificial simulated agent infer human intention from joint information about object affordances and human actions. One difference between this line of research and ours is that we use real, noisy data acquired from robots and sensors to test our models, rather than virtual simulations. ---------------------------------- **PROPOSED APPROACH** In this paper, we combine (1) the robot affordance model of <cite>[10]</cite> , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_3",
  "x": "In this paper, we combine (1) the robot affordance model of <cite>[10]</cite> , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements. We consider three manipulative gestures corresponding to physical actions performed by agent(s) onto objects on a table (see Fig. 1 ): grasp, tap, and touch. We reason on the effects of these actions onto the objects of the world, and on the co-occurring verbal description of the experiments. In the complete framework, we will use Bayesian Networks (BNs), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in Fig. 2 . One of the advantages of using BNs is that their expressive power allows the marginalization over any set of variables given any other set of variables. Our main contribution is that of extending <cite>[10]</cite> by relaxing the assumption that the action is known during the learning phase. This assumption is acceptable when the robot learns through self-exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another (human) agent. We estimate the action performed by a human user during a human-robot collaborative task, by employing statistical inference methods and Hidden Markov Models (HMMs). This provides two advantages. First, we can infer the executed action during training.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_4",
  "x": "**BAYESIAN NETWORK FOR AFFORDANCE-WORDS MODELING** Following the method adopted in <cite>[10]</cite> , we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it. The world behavior is defined by random variables describing: the actions A, defined over the set A = {ai}, object properties F , over F = {fi}, and effects E, over E = {ei}. We denote X = {A, F, E} the state of the world as experienced by the robot. The verbal descriptions are denoted by the set of words W = {wi}. Consequently, the relationships between words and concepts are expressed by the joint probability distribution p(X, W ) of actions, object features, effects, and words in the spoken utterance. The symbolic variables and their discrete values are listed in Table 1 . In addition to the symbolic variables, the model also includes word variables, describing Figure 3 : Structure of the HMMs used for human gesture recognition, adapted from [4] . In this work, we consider three independent, multiple-state HMMs, each of them trained to recognize one of the considered manipulation gestures. the probability of each word co-occurring in the verbal description associated to a robot experiment in the environment. This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> . As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable A assumes a deterministic value.",
  "y": "similarities uses"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_5",
  "x": "The symbolic variables and their discrete values are listed in Table 1 . In addition to the symbolic variables, the model also includes word variables, describing Figure 3 : Structure of the HMMs used for human gesture recognition, adapted from [4] . In this work, we consider three independent, multiple-state HMMs, each of them trained to recognize one of the considered manipulation gestures. the probability of each word co-occurring in the verbal description associated to a robot experiment in the environment. This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> . As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable A assumes a deterministic value. This assumption is relaxed in the present study, by extending the model to the observation of external (human) agents as explained below. ---------------------------------- **HIDDEN MARKOV MODELS FOR GESTURE RECOGNITION** As for the gesture recognition HMMs, we use the models that we previously trained in [4] for spotting the manipulationrelated gestures under consideration.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_6",
  "x": "As for the gesture recognition HMMs, we use the models that we previously trained in [4] for spotting the manipulationrelated gestures under consideration. Our input features are the 3D coordinates of the tracked human hand: the coordinates are obtained with a commodity depth sensor, then transformed to be centered on the person torso (to be invariant to the distance of the user from the sensor) and normalized to account for variability in amplitude (to be invariant to wide/emphatic vs narrow/subtle executions of the same gesture class). The gesture recognition models are represented in Fig. 3 , and correspond to the Gesture HMMs block in Fig. 2 . The HMM for one gesture is defined by a set of (hidden) discrete states S = {s1, . . . , sQ} which model the temporal phases comprising the dynamic execution of the gesture, and by a set of parameters \u03bb = {A, B, \u03a0}, where A = {aij} is the transition probability matrix, aij is the transition probability from state si at time t to state sj at time t + 1, B = {fi} is the set of Q observation probability functions (one per state i) with continuous mixtures of Gaussian values, and \u03a0 is the initial probability distribution for the states. At recognition (testing) time, we obtain likelihood scores of a new gesture being classified with the common Forward- Backward inference algorithm. In Sec. 3.3, we discuss different ways in which the output information of the gesture recognizer can be combined with the Bayesian Network of words and affordances. ---------------------------------- **COMBINING THE BN WITH GESTURE HMMS** In this study we wish to generalize the model of <cite>[10]</cite> by observing external (human) agents, as shown in Fig. 1 . For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs.",
  "y": "extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_7",
  "x": "This corresponds to the Gesture HMMs block in Fig. 2 . The Affordance-Words Bayesian Network (BN) model and the Gestures HMMs may be combined in different ways [19] : 1. the Gesture HMMs may provide a hard decision on the action performed by the human (i. e., considering only the top result) to the BN, 2. the Gesture HMMs may provide a posterior distribution (i. e., soft decision) to the BN, 3. if the task is to infer the action, the posterior from the Gesture HMMs and the one from the BN may be combined as follows, assuming that they provide independent information: In the experimental section, we will show that what the robot has learned subjectively or alone (by self-exploration, knowing the action identity as a prior <cite>[10]</cite> ), can subsequently be used when observing a new agent (human), provided that the actions can be estimated with Gesture HMMs as in [4] . ---------------------------------- **EXPERIMENTAL RESULTS** We present preliminary examples of two types of results: predictions over the effects of actions onto environment objects, and predictions over the associated word descriptions in the presence or absence of an action prior. In this section, we assume that the Gesture HMMs provide the discrete value of the recognized action performed by a human agent (i. e., we enforce a hard decision over the observed action, referring to the possible combination strategies listed in Sec. 3.3). ----------------------------------",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_0",
  "x": "**INTRODUCTION** Apart from its application to machine translation, the encoder-decoder or sequence-to-sequence (seq2seq) paradigm has been successfully applied to monolingual text-to-text tasks including simplification <cite>(Nisioi et al., 2017)</cite> , paraphrasing (Mallinson et al., 2017) , style transfer (Jhamtani et al., 2017) , sarcasm interpretation (Peled and Reichart, 2017) , automated lyric annotation (Sterckx et al., 2017) and dialogue systems (Serban et al., 2016) . A sequence of input tokens is encoded to a series of hidden states using an encoder network and decoded to a target domain by a decoder network. During decoding, an attention mechanism is used to indicate which are the relevant input tokens at each step. This attention component is computed as an intermediate part of the model, and is trained jointly with the rest of the model. Alongside being crucial for effective translation, attention -while not necessarily correlated with human attention -brings interpretability to seq2seq models by visualizing how individual input elements contribute to the model's decisions. Attention values typically match up well with word alignments used in traditional statistical machine translation, obtained with tools such as GIZA++ (Och and Ney, 2000) or fast-align (Dyer et al., 2013) . Therefore, several works have included prior alignments from dedicated alignment software such as GIZA++ or fast-align (Alkhouli et al., 2016; Mi et al., 2016; Liu et al., 2016) . In particular, Mi et al. (2016) showed that the distance between the attention-infused alignments and the ones learned by an independent alignment model can be added to the networks' training objective, resulting in improved translation and alignment quality. Further, Gulcehre et al. (2017) demonstrated that this alignment between given input sentence and generated output can be planned ahead as part of a seq2seq model: their model makes a plan of future alignments using an alignment-plan matrix and decides when to follow this plan by learning a separate commitment vector.",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_1",
  "x": "**PRIOR ATTENTION FOR TEXT SIMPLIFICATION** While our model is essentially task-agnostic, we demonstrate prior attention for the task of sentence simplification. The goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning. It has been suggested that sentence simplification can be defined by three major types of operations: splitting, deletion, and paraphrasing (Shardlow, 2014) . We hypothesize that these operations occur at varying frequencies in the training data. We adopt our model in an attempt to capture these operations into attention matrices and the latent vector space, and thus control the form and degree of simplification through sampling from that space. We train on the Wikilarge collection used by Zhu (2010) . Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) . These are complex sentences paired with simplifications provided by Amazon Mechanical Turk workers and provide a more reliable evaluation of the task.",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_2",
  "x": "Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) . These are complex sentences paired with simplifications provided by Amazon Mechanical Turk workers and provide a more reliable evaluation of the task. ---------------------------------- **HYPERPARAMETERS AND OPTIMIZATION** We extend the OpenNMT (Klein et al., 2017) framework with functions for attention generation and release our code as a submodule. We use a similar archi- Table 2 : Quantitative evaluation of existing baselines from previous work and seq2seq with prior attention from the CVAE when choosing an optimal z sample for BLEU scores. tecture as Zhu et al. (2010) and<cite> Nisioi et al. (2017)</cite> : 2 layers of stacked unidirectional LSTMs with bi-linear global attention as proposed by Luong et al. (2015) , with hidden states of 512 dimensions. The vocabulary is reduced to the 50,000 most frequent tokens and embedded in a shared 500-dimensional space. We train using SGD with batches of 64 samples for 13 epochs after which the autoencoder is trained by translating sequences from training data.",
  "y": "uses"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_3",
  "x": "Next to the target-to-source length ratio, we apply automated measures commonly used to evaluate simplification systems (Woodsend and Lapata, 2011; Zhang and Lapata, 2017) : BLEU, SARI (Xu et al., 2016) , FKGL 1 (Kincaid, 1975) . Automated evaluation metrics for matrices originating from samples from different regions of latent codes are shown in Figure 2b . Inclusion of an attention mechanism was instrumental to match existing baselines. Our standard seq2seq model with attention, without prior attention, obtains a score of 89.92 BLEU points, which is close to scores obtained by similar models used in existing 1 Fleish-Kincaid Grade Level index. work on neural text simplification (Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . In Table 2 , we compare our seq2seq model with attention and without prior attention. A value for BLEU of 90.14 is found for z =[\u22122,0] which was tuned on a development set. For the same z value, a SARI value of 38.30 was reached. For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments.",
  "y": "similarities"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_4",
  "x": "Inclusion of an attention mechanism was instrumental to match existing baselines. Our standard seq2seq model with attention, without prior attention, obtains a score of 89.92 BLEU points, which is close to scores obtained by similar models used in existing 1 Fleish-Kincaid Grade Level index. work on neural text simplification (Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . In Table 2 , we compare our seq2seq model with attention and without prior attention. A value for BLEU of 90.14 is found for z =[\u22122,0] which was tuned on a development set. For the same z value, a SARI value of 38.30 was reached. For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments. For increasing values of z 1 , attention becomes more vertical and focused on single encoder states. This type of attention gives more control to the language model, as exemplified by output samples shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_0",
  "x": "In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an error analysis on our system's output. ---------------------------------- **INTRODUCTION** This work aims to learn strategies for textual response generation in a multimodal conversation directly from data. Conversational AI has great potential for online retail: It greatly enhances user experience and in turn directly affects user retention (Chai et al., 2000) , especially if the interaction is multi-modal in nature. So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_1",
  "x": "This work aims to learn strategies for textual response generation in a multimodal conversation directly from data. Conversational AI has great potential for online retail: It greatly enhances user experience and in turn directly affects user retention (Chai et al., 2000) , especially if the interaction is multi-modal in nature. So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_2",
  "x": "So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_3",
  "x": "So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_4",
  "x": "So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task. Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_5",
  "x": "Then these are combined together and passed through a linear layer l img to get the aggregated image representation for one turn of context, denoted by h are subsequently concatenated and passed as input to the context RNN. h cxt N , the final hidden state of the context RNN, acts as the initial hidden state of the decoder RNN. Finally, output is generated by passing h dec n,m through an affine transformation followed by a softmax activation. The model is trained using cross entropy on next-word prediction. During generation, the decoder conditions on the previous output token. <cite>Saha et al. (2017)</cite> propose a similar baseline model for the <cite>MMD</cite> dataset, extending HREDs to include the visual modality. However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance. While computationally leaner, this approach ultimately loses the objective of capturing multimodality over the context of multiple images and text. In contrast, we combine all the image representations in the utterance using a linear layer. We argue that modelling all images is necessary to answer questions that address previous agent responses.",
  "y": "similarities"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_6",
  "x": "During generation, the decoder conditions on the previous output token. <cite>Saha et al. (2017)</cite> propose a similar baseline model for the <cite>MMD</cite> dataset, extending HREDs to include the visual modality. However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance. While computationally leaner, this approach ultimately loses the objective of capturing multimodality over the context of multiple images and text. In contrast, we combine all the image representations in the utterance using a linear layer. We argue that modelling all images is necessary to answer questions that address previous agent responses. For example in Figure 3 , when the user asks \"what about the 4th image?\", it is impossible to give a correct response without reasoning over all images in the previous response. In the following, we empirically show that our extension leads to better results in terms of text-based similarity measures, as well as quality of generated dialogues. Example contexts for a given system utterance; note the difference in our approach from <cite>Saha et al. (2017)</cite> when extracting the training data from the original chat logs. For simplicity, in this illustration we consider a context size of 2 previous utterances.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_7",
  "x": "<cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ---------------------------------- **IMPLEMENTATION** We use the PyTorch 1 framework (Paszke et al., 2017) for our implementation. 2 We used 512 as the word embedding size as well as hidden dimension for all the RNNs using GRUs (Cho et al., 2014) with tied embeddings for the (bidirectional) encoder and decoder. The decoder uses Luong-style attention mechanism (Luong et al., 2015) with input feeding. We trained our model with the Adam optimizer (Kingma and Ba, 2015) , with a learning rate of 0.0004 and clipping gradient norm over 5. We perform early stopping by monitoring validation loss.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_8",
  "x": "<cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ---------------------------------- **IMPLEMENTATION** We use the PyTorch 1 framework (Paszke et al., 2017) for our implementation. 2 We used 512 as the word embedding size as well as hidden dimension for all the RNNs using GRUs (Cho et al., 2014) with tied embeddings for the (bidirectional) encoder and decoder. The decoder uses Luong-style attention mechanism (Luong et al., 2015) with input feeding. We trained our model with the Adam optimizer (Kingma and Ba, 2015) , with a learning rate of 0.0004 and clipping gradient norm over 5. We perform early stopping by monitoring validation loss.",
  "y": "motivation differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_9",
  "x": "We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts. 4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 . 5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points. 6 This can be primarily attributed to the way we created the input for our model from raw chat logs, as well as incorporating more information during decoding via attention.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_10",
  "x": "5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points. 6 This can be primarily attributed to the way we created the input for our model from raw chat logs, as well as incorporating more information during decoding via attention. Figure 4 provides example output utterances using M-HRED-attn with a context size of 5. Our model is able to accurately map the response to previous textual context turns as shown in (a) and (c). In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_11",
  "x": "Figure 4 provides example output utterances using M-HRED-attn with a context size of 5. Our model is able to accurately map the response to previous textual context turns as shown in (a) and (c). In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) .",
  "y": "motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_12",
  "x": "In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) . Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to en-code the image representations when he have multiple similar-looking images, e.g., black hats in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_13",
  "x": "In (c), it is able to capture that the user is asking about the style in the 1st and 2nd image. (d) shows an example where our model is able to relate that the corresponding product is 'jeans' from visual features, while it is not able to model finegrained details like in (b) that the style is 'casual fit' but resorts to 'woven'. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) . Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to en-code the image representations when he have multiple similar-looking images, e.g., black hats in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_14",
  "x": "We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts. 4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 . 5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points. 6 This can be primarily attributed to the way we created the input for our model from raw chat logs, as well as incorporating more information during decoding via attention.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_15",
  "x": "If there is no image in the utterance, we consider a 0 4096 vector to form the image context. In this work, we focus only on the textual response of the agent. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_16",
  "x": "If there is no image in the utterance, we consider a 0 4096 vector to form the image context. In this work, we focus only on the textual response of the agent. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_0",
  "x": "is study is signi cant as it presents one of the initial stance detection data sets proposed so far and the rst one for Turkish language, to the best of our knowledge. e data set and the evaluation results of the corresponding SVM-based approaches will form plausible baselines for the comparison of future studies on stance detection. ---------------------------------- **INTRODUCTION** Stance detection (also called stance identi cation or stance classication) is one of the considerably recent research topics in natural language processing (NLP). It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} <cite>[12]</cite> . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIDEWAYS'17, Prague, Czech Republic Stance detection is usually considered as a subtask of sentiment analysis (opinion mining) [13] topic in NLP.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_1",
  "x": "Stance detection (also called stance identi cation or stance classication) is one of the considerably recent research topics in natural language processing (NLP). It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} <cite>[12]</cite> . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIDEWAYS'17, Prague, Czech Republic Stance detection is usually considered as a subtask of sentiment analysis (opinion mining) [13] topic in NLP. Both are mostly performed on social media texts, particularly on tweets, hence both are important components of social media analysis. Nevertheless, in sentiment analysis, the sentiment of the author of a piece of text usually as Positive, Negative, and Neutral is explored while in stance detection, the stance of the author of the text for a particular target (an entity, event, etc.) either explicitly or implicitly referred to in the text is considered. Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems <cite>[12]</cite> . Previous work on stance detection include [16] where a stance classi er based on sentiment and arguing features is proposed in addition to an arguing lexicon automatically compiled.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_2",
  "x": "For instance, they nd that sequence models like HMMs perform be er at stance detection when compared with non-sequence models like Naive Bayes (NB) [7] . In another related study [10] , the authors conclude that topic-independent features can be exploited for disagreement detection in on-line dialogues. e employed features include agreement, cue words, denial, hedges, duration, polarity, and punctuation [10] . Stance detection on a corpus of student essays is considered in [5] . A er using linguistically-motivated feature sets together with multivalued NB and SVM as the learning models, the authors conclude that they outperform two baseline approaches [5] . In [4] , the author claims that Wikipedia can be used to determine stances about controversial topics based on their previous work regarding controversy extraction on the Web. Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_3",
  "x": "Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing. e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available. e domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_4",
  "x": "Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available. e domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included. We also provide the evaluation results of SVM classi ers (for each target) on this data set using unigram, bigram, and hashtag features. To the best of our knowledge, the current study is the rst one to target at stance detection in Turkish tweets. Together with the provided annotated data set and the corresponding evaluations with the aforementioned SVM classi ers which can be used as baseline systems, our study will hopefully help increase social media analysis studies on Turkish content. e rest of the paper is organized as follows: In Section 2, we describe our tweet data set annotated with the target and stance information. Section 3 includes the details of our SVM-based stance classi ers and their evaluation results with discussions. Section 4 includes future research topics based on the current study, and nally Section 5 concludes the paper with a summary.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_5",
  "x": "Additionally, to the best of our knowledge, it is also signi cant for being the rst stance-annotated data set including sports-related tweets, as previous stance detection data sets mostly include on-line texts on political/ethical issues. ---------------------------------- **STANCE DETECTION EXPERIMENTS USING SVM CLASSIFIERS** It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as <cite>[12]</cite> . In order to be used as a baseline and reference system for further studies on stance detection in Turkish tweets, we have trained two SVM classi ers (one for each target) using unigrams as features. Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets. e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] . We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_6",
  "x": "Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets. e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] . We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. is outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_7",
  "x": "e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. is outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_8",
  "x": "e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] . We have used the SVM implementation available in the Weka data mining application [6] where this particular implementation employs the SMO algorithm [14] to train a classi er with a linear kernel. e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. is outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_9",
  "x": "We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed. Still, we can conclude that unigram-based features lead to superior results compared to the results obtained using bigrams as features, based on our experiments on our data set. Yet, ngram-based features may be employed on the extended versions of the data set to verify this conclusion within the course of future work. With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams. e corresponding evaluation results of the SVM classi ers using unigrams together the existence of hashtags as features are provided in Table 2 . When the results given in Table 2 are compared with the results in Table 1 , a slight decrease in F-Measure (0.5%) for Target-1 is observed, while the overall F-Measure value for Target-2 has increased by 1.8%. Although we could not derive sound conclusions mainly due to the relatively small size of our data set, the increase in the performance of the SVM classi er Target-2 is an encouraging evidence for the exploitation of hashtags in a stance detection system. We leave other ways of exploiting hashtags for stance detection as a future work.",
  "y": "uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_10",
  "x": "Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed. Still, we can conclude that unigram-based features lead to superior results compared to the results obtained using bigrams as features, based on our experiments on our data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_11",
  "x": "**FUTURE PROSPECTS** Future work based on the current study includes the following: \u2022 e presented stance-annotated data set for Turkish has been created by one annotator only (the author of this study), yet, the data set should be er be revised and extended through crowdsourcing facilities. When employing such a procedure, other stance classes like Neither can be considered as well. e procedure will improve the quality the data set as well as the quality of prospective systems to be trained and tested on it. \u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set. \u2022 Lastly, the SVM classi ers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages (such as English) for comparison purposes. ----------------------------------",
  "y": "future_work uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_12",
  "x": "Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set. \u2022 Lastly, the SVM classi ers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages (such as English) for comparison purposes. ---------------------------------- **CONCLUSION** Stance detection is a considerably new research area in natural language processing and is considered within the scope of the wellstudied topic of sentiment analysis. It is the detection of stance within text towards a target which may be explicitly speci ed in the text or not. In this study, we present a stance-annotated tweet data set in Turkish where the targets of the annotated stances are two popular sports clubs in Turkey. e corresponding annotations are made publicly-available for research purposes. To the best of our knowledge, this is the rst stance detection data set for the Turkish language and also the rst sports-related stanceannotated data set.",
  "y": "future_work"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_0",
  "x": "Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3] . We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite>), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known.",
  "y": "uses"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_1",
  "x": "We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3] . ---------------------------------- **POSTERIOR EVALUATION DISTRIBUTION OF SUBSETS** We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite> ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_0",
  "x": "**ABSTRACT** Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data. However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling <cite>(Bowman et al., 2016)</cite> . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality. ---------------------------------- **INTRODUCTION** Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) .",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_1",
  "x": "However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context. They also introduced a loss clipping strategy in order to make the model more robust. Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_2",
  "x": "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality. ---------------------------------- **INTRODUCTION** Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_3",
  "x": "Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss <cite>(Bowman et al., 2016</cite>; , or resort to designing more sophisticated model structures (Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_4",
  "x": "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 . ---------------------------------- **METHODOLOGY** ---------------------------------- **BACKGROUND OF VAE** A variational autoencoder (VAE) is a deep generative model, which combines variational inference with deep learning.",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_5",
  "x": "The the second term is the KL divergence of the approximate posterior from prior, i.e., a regularisation pushing the learned posterior to be as close to the prior as possible. ---------------------------------- **VARIATIONAL AUTOENDODER WITH HOLISTIC REGULARISATION** In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing. Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder. However, one should note that our architecture can be readily applied to other types of RNN such as GRU. For each time stamp t (see Figure 1b) , we concatenate the hidden state h t and the cell state c t of the encoder.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_6",
  "x": "Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder. However, one should note that our architecture can be readily applied to other types of RNN such as GRU. For each time stamp t (see Figure 1b) , we concatenate the hidden state h t and the cell state c t of the encoder. The concatenation (i.e., [h t ; c t ]) is then fed into two linear transformation layers for estimating \u00b5 t and \u03c3 2 t , which are parameters of a normal distribution corresponding to the concatenation of h t and c t . Let Q \u03c6t (z t |x) = N (z t |\u00b5 t , \u03c3 2 t ), we wish Q \u03c6t (z t |x) to be close to a prior P (z t ), which is a standard Gaussian. Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q \u03c6t and P (z t )) will contribute to the overall KL loss of the ELBO. By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form KL(Q \u03c6t (z t |x) P (z t )). ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works <cite>(Bowman et al., 2016</cite>; .",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_7",
  "x": "**DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews. The statistics of these two datasets are summarised in Table 1 . ---------------------------------- **IMPLEMENTATION DETAILS** For the PTB dataset, we used the train-test split following <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018) . For the E2E dataset, we used the train-test split from the original dataset (Novikova et al., 2017) and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings (Mikolov et al., 2013) . We set the dimension of the hidden layers of both encoder and decoder to 256.",
  "y": "background uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_8",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** ---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews. The statistics of these two datasets are summarised in Table 1 . ---------------------------------- **IMPLEMENTATION DETAILS** For the PTB dataset, we used the train-test split following <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018) .",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_9",
  "x": "KL annealing is used to tackled the latent variable collapse issue <cite>(Bowman et al., 2016)</cite> ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (Yang et al., 2017) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance. Table 2 shows the language modelling results of our approach and the baselines. We report negative log likelihood (NLL), KL loss, and perplexity (PPL) on the test set. As expected, all the models have a higher KL loss in the inputless setting than the standard setting, as z is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E). For instance, when comparing with the strongest baseline vMF-VAE in the standard setting, our model reduces NLL from 96 to 79 and PPL from 98 to 43 in PTB, respectively. In the inputless setting, our performance gain is even higher, i.e., NLL reduced from 117 to 85 and PPL from 262 to 54.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_10",
  "x": "A similar pattern can be observed for the E2E dataset. These observations suggest that our approach can learn a better generative model for data. Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 . These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing <cite>(Bowman et al., 2016)</cite> , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss). Although both VAE-CNN and vMF-VAE outperform VAE-LSTM-base by a large margin in terms of reconstruction loss as shown in Figure 2 , one should also notice that these two models actually overfit the training data, as their performance on the test set is much worse (cf. Table 2 ). In contrast to the baselines which mitigate the KL collapse issue by carefully engineering the weight between the reconstruction loss and KL loss or choosing a different choice of prior, we provide a simple and elegant solution through holistic KL regularisation, which can effectively mitigate the KL collapse issue and achieve a better reconstruction error in both training and testing.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_0",
  "x": "While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in <cite>(Conneau et al., 2017)</cite> . We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models. We show that by adding such constraints, superior sentence embeddings can be achieved. We compare our method with the original implementation and show improvements in several tasks. ---------------------------------- **INTRODUCTION** Word embeddings are considered one of the key building blocks in natural language processing and are widely used for various applications (Mikolov et al., 2013; Pennington et al., 2014) . While word representations has been successfully used, representing the more complicated and nuanced nature of the next element in the hierarchy -a full sentence -is still considered a challenge. Once trained, universal sentence representations can be used as an out-of-the-box tool for solving various NLP and computer vision problems. Even though their importance is unquestionable, it seems that current results are still far from satisfactory.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_1",
  "x": "The challenge is learning a mapping T : {s i } n i=1 \u2192 F that manages to capture the semantics of each s i . While sentence embedding are not always used in similarity probing, we find this formulation useful as the similarity assumption is implicitly made when training classifiers on top of the embeddings in downstream tasks. Sentences embedding methods were mostly trained in an unsupervised setting. In (Le and Mikolov, 2014 ) the ParagraphVector model was proposed which is trained to predict words in the document. SkipThought (Kiros et al., 2015) vectors rely on the continuity of text to train an encoder-decoder model that tries to reconstruct the surrounding sentences of a given passage. In Sequential Denoising Autoencoders (SDAE) (Hill et al., 2016) high-dimensional input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted version. FastSent (Hill et al., 2016) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (Klein et al., 2015) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_2",
  "x": "In (Klein et al., 2015) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work. AE Reg and LM Reg refers to the Auto-Encoder and Language-Model regularization terms described in 2.1 and Combined refers to optimizing with both terms. Bi-AE Reg and Bi-LM Reg refers to the bi-directional Auto-Encoder and bi-directional Language-Model regularization terms described in 2.2. As evident from the results, adding simple unsupervised regularization terms improves the results of the model on almost all the evaluated tasks. sentence embedding methods and compare them on various benchmarks. The SNLI dataset is composed of 570K pairs of sentences with a label depicting the relationship between them, which can be either 'neutral', 'contradiction' or 'entailment'. The authors show that by leveraging the dataset, state-of-the-art representations can be obtained which are universal and general enough for solving various NLP tasks.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_3",
  "x": "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (Mikolov et al., 2013; Pennington et al., 2014) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_4",
  "x": "We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (Mikolov et al., 2013; Pennington et al., 2014) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
  "y": "differences"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_5",
  "x": "We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (Mikolov et al., 2013; Pennington et al., 2014) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_6",
  "x": "---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (Mikolov et al., 2013; Pennington et al., 2014) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling). The original model of <cite>(Conneau et al., 2017)</cite> was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 \u2212s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_7",
  "x": "Similarly to regularization terms described in 2.1, we devise variants of (1) and (2) which take into account the bi-directional architecture of the model. Here, we add two linear transformation layers: \u2190 \u2212 H \u2192 W on top of the forward LSTM and backward LSTM, respectively, and denote their output as { \u2212 \u2192 w t } and { \u2190 \u2212 w t }, respectively, where t = 1, . . . , T . Now, equations (1) and (2) are re-written as: We call the second regularization term in (3) a bi-directional auto-encoder regularization and in (4) a bi-directional language model regularization term. Again, \u03bb 1 and \u03bb 2 are hyper-parameters controlling the amount of regularization and were set to 0.5 in our experiments. ---------------------------------- **EXPERIMENTS** Following <cite>(Conneau et al., 2017)</cite> we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -Pang and Lee (2005) , SST -Socher et al. (2013) ), question-type (TREC -Li and Roth (2002) ), product reviews (CR - Hu and Liu (2004) ), subjectivity/objectivity (SUBJ - Pang and Lee (2005) ) and opinion polarity (MPQA -Wiebe et al. (2005) ). We also tested our approach on semantic textual similarity (STS 14 - Agirre et al. (2014) ), paraphrase detection (MRPC - Dolan et al. (2004) ), entailment and semantic relatedness tasks (SICK-R and SICK-E - Marelli et al. (2014) ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_10",
  "x": "---------------------------------- **CONCLUSIONS** In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings. Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations <cite>(Conneau et al., 2017)</cite> . However, every supervised learning tasks is prone to overfit. In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks. We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models. We note that the added regularization terms do come at the price of increasing the model size by ld parameters (where d and l are the dimensions of the word embedding and the LSTM hidden state, respectively) due to the added linear transformation (see 2.1). However, as evident from our results, this does not hinder the model performance, even though we did not increase the amount of training data. Moreover, since those term are unsupervised in nature, it is possible to pre-train the model on unlabeled data and then finetune it on the SNLI dataset.",
  "y": "motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_0",
  "x": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; . Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling of all possible relation paths, while also representing both relation types and nodes on the paths. We evaluated our approach on two datasets.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_1",
  "x": "Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015) . In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; . Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity.",
  "y": "motivation background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_2",
  "x": "---------------------------------- **PRIOR APPROACHES** The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) . Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path \u03c0. The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> . For this model, the embedding of a length-n path \u03a6 \u03c0 \u2208 R d\u00d7d is defined as the matrix product of the sequence of relation matrices for the relations in \u03c0. For the BILINEAR-DIAG model, all the matrices are diagonal and the computation reduces to coordinate-wise product of vectors in R d . In Guu et al. (2015) , information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a compositional regularizer for the learned node and relation embeddings. A more limited version of the same method was simultaneously proposed in Garcia-Duran et al. (2015) .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_3",
  "x": "3 Relation-path-aware Models We first review two existing methods using vector space relation paths modeling for KB completion in \u00a73.1. We then introduce our new algorithm that can efficiently take into account all relation paths between two nodes as features and simultaneously model intermediate nodes on relation paths in \u00a73.2. We present a detailed theoretical comparison of the efficiency of these three types of methods in \u00a73.3. ---------------------------------- **PRIOR APPROACHES** The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) . Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path \u03c0. The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_4",
  "x": "The test time and memory requirements of this method are the same as these of BILINEAR-DIAG, which is a substantial advantage over other methods, if evaluation-time efficiency is important. ---------------------------------- **PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_5",
  "x": "This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb . For each considered triple, however, we need to compute the sum of representations of path features that are active for the triple. We estimate the average number of active paths per node pair as T Ne 2 . Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_6",
  "x": "**PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb . For each considered triple, however, we need to compute the sum of representations of path features that are active for the triple. We estimate the average number of active paths per node pair as T Ne 2 .",
  "y": "differences uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_7",
  "x": "Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).** Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients. The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_8",
  "x": "The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb . For each considered triple, however, we need to compute the sum of representations of path features that are active for the triple. We estimate the average number of active paths per node pair as T Ne 2 . Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_9",
  "x": "Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L . The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively. Table 1 : KB completion results on NCI-PID test: comparison of our compositional learning approach (ALL-PATHS+NODES) with baseline systems. d is the embedding dimension; sampled paths occurring less than c times were pruned in PRUNED-PATHS.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_10",
  "x": "Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients. The memory required to store path representation sums F l (s, t) is O dLN e 2 in the worst case. Denote E = E kb + E txt . The time to compute these sums is O dE(1 + l=2...L (l \u2212 1)N e ) . After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L . The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_11",
  "x": "The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively. Table 1 : KB completion results on NCI-PID test: comparison of our compositional learning approach (ALL-PATHS+NODES) with baseline systems. d is the embedding dimension; sampled paths occurring less than c times were pruned in PRUNED-PATHS. ---------------------------------- **EXPERIMENTS** Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in<cite> (Guu et al., 2015)</cite> versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models?",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_12",
  "x": "Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in<cite> (Guu et al., 2015)</cite> versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models? (iii) Does modeling intermediate path nodes improve the accuracy of KB completion? Datasets We used two datasets for evaluation: NCI-PID and WordNet. For the first set of experiments, we used the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) as our knowledge base, which was created by editors from the Nature Publishing Groups, in collaboration with the National Cancer Institute. It contains a collection of highquality gene regulatory networks (also referred to as pathways). The original networks are in the form of hypergraphs, where nodes could be complex gene products (e.g., \"protein complex\" with multiple proteins bound together) and regulations could have multiple inputs and outputs. Following the convention of most network modeling approaches, we simplified the hypergraphs into binary regulations between genes (e.g., GRB2 positive reg MAPK3), which yields a graph with 2774 genes and 14323 triples. The triples are then split into train, dev, and test sets, of size 10224, 1315, 2784, respectively. We identified genes belonging to the same family via the common letter prefix in their names, which adds 1936 triples to training. As a second dataset, we used a WordNet KB with the same train, dev, and test splits as<cite> Guu et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_13",
  "x": "6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES additionally models nodes in the paths. PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_14",
  "x": "We used the gene mentions identified by Literome (Poon et al., 2014) , and considered sentences with co-occurring gene pairs from NCI-PID. We defined textual relations using the fully lexicalized dependency paths between two gene mentions, as proposed in Riedel et al. (2013) . Additionally, we define trigger-mapped dependency paths, where only important \"trigger\" words are lexicalized and the rest of the words are replaced with a wild-card character X. A set of 333 words often associated with regulation events in Literome (e.g. induce, inhibit, reduce, suppress) were used as trigger words. To avoid introducing too much noise, we only included textual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions. 6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES additionally models nodes in the paths. PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_15",
  "x": "The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results. After preliminary experimentation, we fixed \u03b2 to 0.1. Models using KB and textual relations were initialized from models using KB relations only 7 . Model training was stopped when the development set MAP did not improve for 40 iterations; the parameters with the best MAP on the development set were selected as output.",
  "y": "extends"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_16",
  "x": "So there is little difference between the heaviest pruned version (c=1000) and the lightest (c=1). When textual relations are included, the cutoff matters more, although the difference was small as many rarer textual relations were already filtered beforehand. In either case, the accuracy difference between ALL-PATHS and PRUNED-PATHS is small, and ALL-PATHS mainly gains in efficiency. However, when nodes are modeled, the compositional learning approach gains in accuracy as well, especially when text is jointly embedded. ---------------------------------- **NCI-PID RESULTS** Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices.",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_17",
  "x": "---------------------------------- **NCI-PID RESULTS** Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_18",
  "x": "11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ---------------------------------- **WORDNET RESULTS** The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_19",
  "x": "Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_20",
  "x": "11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ---------------------------------- **WORDNET RESULTS** The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5.",
  "y": "similarities"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_21",
  "x": "**WORDNET RESULTS** The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5. As can be seen, lower count cutoff performed better for paths up to length 3, but we could not run the method with path lengths up to 5 and count cutoff of 1, due to excessive memory requirements (more than 248GB). When using count cutoff of 10, paths up to length 5 performed worse than paths up to length 3. This performance degradation could be avoided with 12 We ran the trained model distributed by<cite> Guu et al. (2015)</cite> and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5. Due to the discrepancy, we report the original results from the authors' paper which lack MAP values instead. a staged training regiment where models with shorter paths are first trained and used to initialize models using longer paths. The performance of the ALL-PATHS method can be seen for maximum paths up to lengths 3 and 5, and with or without using features on intermediate path nodes. 13 As shown in Table 2 , longer paths were useful, and features on intermediate nodes were also beneficial. We tested the significance of the differences between several pairs of models and found that nodes led to significant improvement (p < .002) for paths of length up to 3, but not for the setting with longer paths.",
  "y": "uses"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_0",
  "x": "Most approaches utilize complex features to re-estimate the tree structures of a given sentence [1, 2, 3] . Unfortunately, sizes of treebanks are generally small and insufficient, which results in a common problem of data sparseness. Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8,</cite> 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_1",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8</cite>, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_2",
  "x": "100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [<cite>8</cite>] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees. There are three main steps in our rescoring approach. The first step is to have the parser to produce n-best parse trees with their structural scores. For each parsed tree including words, part-of-speech (PoS) and semantic role labels. Second, we extract word-to-word associations (or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective) from large amounts of auto-parsed data and adopt word2vecf [13] to train dependency-based word embeddings. The last step is to build a structural rescoring method to find the best tree structure from the n-best candidates. We conduct experiments on the standard data sets of the Chinese Treebank.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_0",
  "x": "**ABSTRACT** We consider the problem of making machine translation more robust to character-level variation at the source side, such as typos. Existing methods achieve greater coverage by applying subword models such as byte-pair encoding (BPE) and character-level encoders, but these methods are highly sensitive to spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural noise, as captured by frequent corrections in Wikipedia edit logs, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution. ---------------------------------- **INTRODUCTION** Machine translation systems are generally trained on clean data, without spelling errors. Yet machine translation may be used in settings in which robustness to such errors is critical: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018) , and interactive settings in which users must enter text on a mobile device. Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) .",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_1",
  "x": "Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques. Drawing inspiration from dropout (Srivastava et al., 2014) and noise-based regularization methods, we explore the space of random noising methods at training time, and evaluate performance on both clean text and text corrupted by natural noise based on real spelling mistakes on Wikipedia (Max and Wisniewski, 2010 ). We find that by feeding our translation models a balanced diet of several types of synthetic noise at training time -random character deletions, insertions, substitutions, and swapsit is possible to obtain substantial improvements on such naturally noisy data, with minimal impact on the performance on clean data, and without accessing the test noise data or even its distribution. We demonstrate that our method substantially improves the robustness of a transformer-based machine translation model with CNN character encoders to spelling errors across multiple input languages (German, French, and Czech) . Of the different noise types we use at training, we find that random character deletions are particularly useful, followed by character insertions. However, noisy training does not seem to improve translations of social media text, as indicated by performance on the recently-introduced MTNT dataset of Reddit posts (Michel and Neubig, 2018) . This finding aligns with previous work arguing that the distinctive feature of social media text is not noise or orthographical errors, but rather, variation in writing style and vocabulary (Eisenstein, 2013) .",
  "y": "background motivation"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_2",
  "x": "Typical character-level encoders are based on models such as convolutional neural networks (Kim et al., 2016) , which learn to match filters against specific character n-grams. When these n-grams are disrupted by orthographical noise, the resulting encoding may be radically different from the encoding of a \"clean\" version of the same text. <cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length. In Section 3.2, we show that they are indeed the primary contributors in improving our model's robustness to natural noise. During training, we used a balanced diet of all four noise types by sampling the noise, for each to-ken, from a multinomial distribution of 60% clean (no noise) and 10% probability for each type of noise. The noise was added dynamically, allowing for different mutations of the same example over different epochs. 2",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_3",
  "x": "We focus on orthographical noise, which is noise at the character level, affecting the spelling of individual terms. Orthographical noise is obviously problematic for machine translation systems that operate on token-level embeddings, because noised terms are likely to be out-ofvocabulary, even when pre-segmented into subwords using techniques such as byte-pair encoding (Sennrich et al., 2015) . A more subtle issue is that orthographical noise can also pose problems for character-level encoding models. Typical character-level encoders are based on models such as convolutional neural networks (Kim et al., 2016) , which learn to match filters against specific character n-grams. When these n-grams are disrupted by orthographical noise, the resulting encoding may be radically different from the encoding of a \"clean\" version of the same text. <cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length. In Section 3.2, we show that they are indeed the primary contributors in improving our model's robustness to natural noise.",
  "y": "motivation background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_4",
  "x": "Model We used a transformer-based machine translation model (Vaswani et al., 2017) with the CNN-based character encoder of Kim et al. (2016) on the source encoder. The model was implemented in Fairseq. 3 Hyperparameters We followed the base configuration of the transformer (Vaswani et al., 2017) , with 6 encoder and decoder layers of 512 model dimension, 2048 hidden dimensions, and 8 attention heads per layer. Our character embeddings had 256 dimensions and the character CNN's filters followed the specifications of Kim et al. (2016) . We optimized the model with Adam and used the inverse square-root learning rate schedule typically used for transformers, but with a peek learning rate of 0.001. Each batch contained a maximum of 8,000 tokens. We used a dropout rate of 0.2. We used beam search for generating the translations (5 beams), and computed BLEU scores to measure performance on the test 3 https://github.com/pytorch/fairseq set. Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_5",
  "x": "3 Hyperparameters We followed the base configuration of the transformer (Vaswani et al., 2017) , with 6 encoder and decoder layers of 512 model dimension, 2048 hidden dimensions, and 8 attention heads per layer. Our character embeddings had 256 dimensions and the character CNN's filters followed the specifications of Kim et al. (2016) . We optimized the model with Adam and used the inverse square-root learning rate schedule typically used for transformers, but with a peek learning rate of 0.001. Each batch contained a maximum of 8,000 tokens. We used a dropout rate of 0.2. We used beam search for generating the translations (5 beams), and computed BLEU scores to measure performance on the test 3 https://github.com/pytorch/fairseq set. Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise. Moreover, the negative effects of training on synthetic noise seem to be limited to both negative and positive fluctuations that are smaller than 1 BLEU point.",
  "y": "differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_6",
  "x": "Table 3 shows the model's performance on the German-to-English dataset when training with various mixtures of noise. We find that deletion is by far the most effective synthetic noise in preparing our model for natural orthographical errors, followed by insertion. The French and Czech datasets exhibit the same trend. We conjecture that the importance of deletion and insertion is that they distort the typical distances between characters, requiring the CNN character encoder to become more invariant to unexpected character movements. The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while <cite>previous work</cite> that trained only on substitutions and swaps was not able to do so <cite>(Belinkov and Bisk, 2018)</cite> . ---------------------------------- **TRANSLATING SOCIAL MEDIA TEXT** We also apply our synthetic noise training procedure to translation of social media, using the recently-released MTNT dataset of Reddit posts (Michel and Neubig, 2018) , focusing on the English-French translation pair. Note that no noise was inserted into the test data in this case; the only source of noise is the non-standard spellings inherent to the dataset. As shown in Table 4 , noised training has minimal impact on performance.",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_7",
  "x": "Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We also conducted preliminary experiments with similar noise-invariant models, but found that training a CNN with synthetic noise to work better.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_8",
  "x": "The use of noise to improve robustness in machine learning has a long history (e.g., Holmstrom and Koistinen, 1992; Wager et al., 2013) , with early work by Bishop (1995) demonstrating a connection between additive noise and regularization. To achieve robustness to orthographical errors, we require noise that operates on the sequence of characters. Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token.",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_9",
  "x": "Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We also conducted preliminary experiments with similar noise-invariant models, but found that training a CNN with synthetic noise to work better. ----------------------------------",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_10",
  "x": "Heigold et al. (2017) demonstrated that synthetic noising operations such as random swaps and replacements can significantly degrade performance when inserted at test time; they also show that some robustness can be obtained by inserting the same synthetic noise at training time. Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We also conducted preliminary experiments with similar noise-invariant models, but found that training a CNN with synthetic noise to work better.",
  "y": "background motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_0",
  "x": "Thus any judgements of the similarity of two composed phrases may be confounded by the degree to which those phrases are compositional. In this paper, we use a compound noun compositionality dataset (<cite>Reddy et al., 2011</cite>) to investigate the extent to which the underlying definition of context has an effect on a model's ability to support composition. We compare the Anchored Packed Tree (APT) model (Weir et al., 2016) , where composition is an integral part of the distributional model, with the commonly employed approach of applying na\u00efve compositional operations to state-of-the-art distributional representations. Consider the occurrence of the word student in the sentence \"The recently graduated student folded the dry clothes.\" Different distributional representations leverage the context, e.g., the fact that the target word student has occurred in the context folded, in different ways. Table 1 illustrates the contextual features which might be generated for student given different definitions of context. The most commonly used definition of context, in both traditional count-based representations and in more recent distributed embeddings, is proximity, i.e., the contextual features of a word occurrence are all those words which occur within a certain context window around the occurrence. However, contextual features may also be defined in terms of dependency relations. For example, in a dependency parse of the sentence we would expect to see a direct-object relation from folded to student. Contextual features based on dependency relations may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010) . Pad\u00f3 and Lapata (2007) proposed using dependency paths to define untyped contextual features; here any word in the context which has a dependency path to the target is considered a contextual feature.",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_1",
  "x": "For example, to carry out the composition of student with folded in the example sentence, it is necessary to align the representations. This can be done by offsetting all of the features of student by its dependency relation (NSUBJ) with folded. Intuitively we are viewing the representation of student from the perspective of actions (i.e., verbs) which are likely to be carried out by students. This view can be straightforwardly composed with the representation of folded because the representations are aligned i.e., they have features of the same type (e.g., DOBJ). ---------------------------------- **COMPOSITIONALITY OF COMPOUND NOUNS** Compositionality detection (<cite>Reddy et al., 2011</cite>) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. <cite>Reddy et al. (2011)</cite> introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase spelling bee is deemed to have high literalness in its use of the first constituent, low literalness in its use of the second constituent and a medium level of literalness with respect to the whole phrase.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_2",
  "x": "**COMPOSITIONALITY OF COMPOUND NOUNS** Compositionality detection (<cite>Reddy et al., 2011</cite>) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. <cite>Reddy et al. (2011)</cite> introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase spelling bee is deemed to have high literalness in its use of the first constituent, low literalness in its use of the second constituent and a medium level of literalness with respect to the whole phrase. Assuming the distributional hypothesis (Harris, 1954) , the observed co-occurrences of compositional target phrases are highly likely to have occurred with one or both of the constituents independently. On the other hand, the observed cooccurrences of non-compositional target phrases are much less likely to have occurred with either of the constituents independently. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_3",
  "x": "<cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus. ---------------------------------- **UNTYPED CONTEXTUAL FEATURES**",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_4",
  "x": "Assuming the distributional hypothesis (Harris, 1954) , the observed co-occurrences of compositional target phrases are highly likely to have occurred with one or both of the constituents independently. On the other hand, the observed cooccurrences of non-compositional target phrases are much less likely to have occurred with either of the constituents independently. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) .",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_5",
  "x": "<cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus. ---------------------------------- **UNTYPED CONTEXTUAL FEATURES**",
  "y": "motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_6",
  "x": "---------------------------------- **TYPED CONTEXTUAL FEATURES** For each word and compound phrase, elementary APT representations were constructed using the method and recommended settings of Weir et al. (2016) . For efficiency, we did not consider paths of length 3 or more. In relation to the construction of the elementary APTs, the most obvious parameter is the nature of the weight associated with each feature. We consider both the use of probabilities 2 and positive pointwise mutual information (PPMI) 1 Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the <cite>Reddy et al. (2011)</cite> evaluation task when trained on the BNC. However, these results were still significantly lower than those reported by <cite>Reddy et al. (2011)</cite> using the larger ukWaC corpus. 2 referred to as normalised counts by Weir et al. (2016) values.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_7",
  "x": "Following <cite>Reddy et al. (2011)</cite> , when using the UNI operation, we experiment with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A 2 is the APT associated with the head of the phrase and A \u03b4 1 is the properly aligned APT associated with the modifier where \u03b4 is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as: (1) We have also considered composition without alignment of the modifier's APT, i.e, using A 1 : In general, one would expect there to be little overlap between APTs which have not been properly aligned. However, in the case where \u03b4 is the NMOD relation, i.e., the internal relation in the vast majority of the compound phrases, both modifier and head are nouns and therefore there may well be considerable overlap between their unaligned dependency features. In order to examine the contribution of both the aligned and unaligned APTs in the composition process, we used a hybrid method where the composed representation is defined as: Table 2 : Average \u03c1 using neural word embeddings In the case where representations consist of APT weights which are probabilities, PPMI is estimated after composition. Therefore we refer to this as compose-first (CF) in contrast to composesecond (CS) where composition is carried out after PPMI calculations. In both cases, the cosine measure is applied to vectors made up PPMI values in order to calculate the similarity of the observed and composed representations. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_8",
  "x": "Boldface is used to indicate the best performing configuration of parameters for a particular model. Table 2 summarises results for different parameter settings for the neural word embeddings. Looking at the results in Table 2 , we see that the cbow model significantly outperforms the skip-gram model. Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation. Table 3 : Average \u03c1 using APT representations. APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space. Smoothing the PPMI calculation with a value of \u03b1 = 0.75 generally has a further small positive effect. On average, the results when probabilities are composed and PPMI is calculated as part of the similarity calculation (CF) are slightly higher than the results when PPMI weights are composed (CS) .",
  "y": "differences"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_9",
  "x": "We used a sufficiently large number of repetitions that errors are all small (\u2264 0.0015) and thus any difference observed which is greater than 0.005 is statistically significant at the 95% level. Boldface is used to indicate the best performing configuration of parameters for a particular model. Table 2 summarises results for different parameter settings for the neural word embeddings. Looking at the results in Table 2 , we see that the cbow model significantly outperforms the skip-gram model. Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation. Table 3 : Average \u03c1 using APT representations. APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space. Smoothing the PPMI calculation with a value of \u03b1 = 0.75 generally has a further small positive effect.",
  "y": "differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_0",
  "x": "**INTRODUCTION** Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like Fast-Text (Bojanowski et al., 2017) , and, more recently, pretrained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT<cite> (Devlin et al., 2019)</cite> . In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance<cite> (Devlin et al., 2019)</cite> . The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_1",
  "x": "This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance<cite> (Devlin et al., 2019)</cite> . The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size. As an illustration of the issues mentioned above, the multilingual BERT which Basque shares with other 103 languages is based on Wikipedia corpora, where English amounts to 2.5 thousand million tokens whereas for Basque it contains around 35 million tokens. Our corpus uses, in addition to the Basque Wikipedia, corpora crawled from news outlets (191M tokens) . Another important issue is subword tokenization. Thus, for some common Basque words such as etxerantz (to the house) or medikuarenera (to the doctor), the subword tokenization generated by the monolingual BERT we trained will substantially differ from the output produced by the multilingual BERT: mBERT: Et #xer #ant #z ours: Etxera #ntz mBERT: Medi #kua #rene #ra ours: Mediku #aren #era More specifically, mBERT's subwords tend to be shorter and less interpretable, while our subwords are closer to linguistically interpretable strings, like mediku (doctor) aren ('s) and era (to the). Furthermore, most of the time the released models have been thoroughly tested only in English.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_2",
  "x": "Another important issue is subword tokenization. Thus, for some common Basque words such as etxerantz (to the house) or medikuarenera (to the doctor), the subword tokenization generated by the monolingual BERT we trained will substantially differ from the output produced by the multilingual BERT: mBERT: Et #xer #ant #z ours: Etxera #ntz mBERT: Medi #kua #rene #ra ours: Mediku #aren #era More specifically, mBERT's subwords tend to be shorter and less interpretable, while our subwords are closer to linguistically interpretable strings, like mediku (doctor) aren ('s) and era (to the). Furthermore, most of the time the released models have been thoroughly tested only in English. Alternatively, multilingual versions have been tested in transfer learning scenarios for other languages, where they have not been compared to monolingual versions<cite> (Devlin et al., 2019)</cite> . The goal of this paper is to compare publicly available models for Basque with analogous models which have been trained with a larger, better quality corpus. This has been possible for the FastText and Flair models. In the case of BERT, only the multilingual version for 104 languages is available for Basque. We focus on four downstream NLP tasks, namely, topic classification, sentiment classification, Partof-Speech (POS) tagging and Named Entity Recognition (NER). The main contribution of our work is threefold: (1) We show that, in the case of Basque, publicly available models can be outperformed by analogous models trained with appropriate corpora.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_3",
  "x": "Fast-Text (Bojanowski et al., 2017) proposes an improvement over those models, consisting on embedding subword units, thereby attempting to introduce morphological information. Rich morphology languages such as Basque should especially profit from such word representations. FastText distributes embeddings for more than 150 languages trained on Common Crawl and Wikipedia. In this paper we build FastText embeddings using a carefully collected corpus in Basque and show that it performs better than the officially distributed embeddings in all NLP we tested, which stresses the importance of a following a carefully designed method when building and collecting the corpus. The aforementioned methods generate static word embeddings, that is, they provide a unique vector-based representation for a given word independently of the context in which the word occurs. Thus, if we consider the Basque word banku 3 , static word embedding approaches will calculate one vector irrespective of the fact that the same word banku may convey different senses when used in different contexts, namely, \"financial institution\",\"bench\", \"supply or stock\", among others. In order to address this problem, contextual word embeddings are proposed; the idea is to be able to generate different word representations according to the context in which the word appears. Examples of such contextual representations are ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018) , which are built upon LSTM-based architectures and trained as language models. More recently,<cite> Devlin et al. (2019)</cite> introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages.",
  "y": "motivation"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_4",
  "x": "Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik et al., 2018) , outperforming other well-known approaches such as BERT and ELMO <cite>(Devlin et al., 2019</cite>; Peters et al., 2018) . In any case, Flair is of interest to us because they distribute their own Basque pre-trained embedding models obtained from a corpus of 36M tokens (combining OPUS and Wikipedia). Flair-BMC models: We train our own Flair embeddings using the BMC corpus with the following parameters: Hidden size 2048, sequence length of 250, and a mini-batch size of 100. The rest of the parameters are left in their default setting. Training was done for 5 epochs over the full training corpus. The training of each model took 48h on a Nvidia Titan V GPU. Flair Embeddings: Flair's embeddings model words as sequences of characters. Moreover, the vector-based representation of a word will depend on its surrounding context. More specifically, to generate word embeddings they feed sentences as sequences of characters into a character-level Long short-term memory (LSTM) model which at each point in the sequence is trained to predict the next character. Given a sentence, a forward LSTM language model processes the sequence from the beginning of the sentence to the last character of the word we are modeling extracting its output hidden state.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_5",
  "x": "They reported significant improvements for NER by using this new version of Flair embeddings. Note that this does not affect to the way the Flair pre-trained embedding models are calculated. The pooling operation is involved in the process of using such pre-trained models in order to obtain word representations for a given task such as NER or POS using the Flair system. Flair System: For sequence labeling tasks, the calculated character-based embeddings are passed into a BiLSTM-CRF system based on the architecture proposed by (Huang et al., 2015) . For text classification tasks, the computed Flair embeddings are fed into a BILSTM 6 to produce a document level embedding which is then used in a linear layer to make the class prediction. Although for best results they recommend to stack their own Flair embeddings with additional static embeddings such as FastText, in this paper our objective is to compare the official pretrained Flair embeddings for Basque with our own Flair-BMC embeddings. ---------------------------------- **BERT LANGUAGE MODELS** We have trained a BERT<cite> (Devlin et al., 2019)</cite> model for Basque Language using the BMC corpus motivated by the rather low representation this language has in the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure.",
  "y": "uses"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_6",
  "x": "The main differences between our model and the original implementation are the corpus used for the pre-training, the algorithm for sub-word vocabulary creation and the usage of a different masking strategy that is not available for the BERT BASE model yet. Sub-word vocabulary We create a cased sub-word vocabulary containing 50,000 tokens using the unigram language model based sub-word segmentation algorithm proposed by Kudo (2018) . We do not use the same algorithm as BERT because the WordPiece (Wu et al., 2016) implementation they originally used is not publicly available. We have increased the vocabulary size from 30,000 sub-word units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage percentage to 99.95. Model Architecture In the same way as the original BERT architecture proposed by<cite> Devlin et al. (2019)</cite> our model is composed by stacked layers of Transformer encoders (Vaswani et al., 2017) . Our approach follows the BERT BASE configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. Pre-training objective Following BERT original implementation, we train our model on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. Even if the necessity of the NSP task has been questioned by some recent works (Yang et al., 2019; Liu et al., 2019; Lample and Conneau, 2019) we have decided to keep it as in the original paper to allow for head-to-head comparison.",
  "y": "extends differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_7",
  "x": "An upgraded version of BERT LARGE 7 has proven that WWM has substantial benefits in comparison with previous masking that was done after the sub-word tokenization. Pre-training procedure Similar to<cite> (Devlin et al., 2019)</cite> we use Adam with learning rate of 1e \u2212 4, \u03b2 1 = 0.9, \u03b2 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 0000 steps, and linear decay of the learning rate. The dropout probability is fixed to 0.1 on all the layers. As the attentions are quadratic to the sequence length, making longer sequences much more expensive, we pre-train the model with sequence length of 128 for 90% of the steps and sequence length of 512 for 10% of the steps. In total we train for 1, 000, 000 steps and a batch size of 256. The first 90% steps are trained using Cloud v2 TPUs and for the rest of the steps we use Cloud v3 TPUs 8 . ---------------------------------- **EVALUATION AND RESULTS** We conduct an extensive evaluation on four well known NLP tasks: Topic Classification, Sentiment Classification, 7 https://github.com/google-research/bert 8 https://cloud.google.com/tpu/pricing Part-of-Speech (POS) tagging and Named Entity Recognition (NER). The datasets used for each task are described in their respective sections.",
  "y": "similarities"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_8",
  "x": "To train the Flair system we use the parameters specified in (Akbik et al., 2019) for Pooled Contextual Embeddings. Flair is tuned on the development data using the test only for the final evaluation. We do not use the development set for training. For comparison between BERT models we fine-tune on the training data provided for each of the four tasks with both the official multilingual BERT<cite> (Devlin et al., 2019)</cite> model and with our BERTeus model (trained as described in Section 3.3.). Every reported result for every system is the average of five randomly initialized runs. The POS and NER experiments using mBERT and BERTeus are performed using the transformers library (Wolf et al., 2019) where it is recommended to remove the seed for random initialization. ---------------------------------- **TOPIC CLASSIFICATION** For the task of topic classification a dataset containing 12k news headlines (brief article descriptions) was compiled from the Basque weekly newspaper Argia 9 . News are classified uniquely according to twelve thematic categories.",
  "y": "similarities uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_0",
  "x": "later works proposed partially-or purely-convolutional CTC models [8] [9] [10] [11] and convolution-heavy encoder-decoder models [16] for ASR. However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR.",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_1",
  "x": "Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR. In Section 3, we validate SAN-CTC on the Wall Street Journal and LibriSpeech datasets by outperforming existing CTC models and most encoder-decoder models in character error rates (CERs), with fewer parameters or less training time. Finally, we train our models with different label alphabets (character, phoneme, subword), use WFST decoding to give word error rates (WERs), and examine the learned attention heads for insights. ---------------------------------- **MODEL ARCHITECTURES FOR CTC AND ASR** Consider an input sequence of T feature vectors, viewed as a matrix X \u2208 R T \u00d7d fr . Let L denote the (finite) label alphabet, and denote the output sequence as y = (y1, . . . , yU ) \u2208 L U .",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_2",
  "x": "However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_3",
  "x": "While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position. Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_4",
  "x": "While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position. Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_5",
  "x": "Operations per layer ---------------------------------- **SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on [22] . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, <cite>[27]</cite> argue their LAS self-attention heads are differentiated phoneme detectors. Further inductive biases like filter widths and causality could be expressed through time-restricted self-attention [26] and directed self-attention [25] , respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_6",
  "x": "---------------------------------- **DOWNSAMPLING** In speech, the input length T of frames can be many times larger than the output length U , in contrast to the roughly word-to-word setting of machine translation. This is especially prohibitive for self-attention in terms of memory: recall that an attention matrix of dimension \u2208 R T \u00d7T is created, giving the T 2 factor in Table 1 . A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one <cite>[27]</cite> . Note that CTC will still require U \u2264 T /k, however. ---------------------------------- **POSITION**",
  "y": "uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_7",
  "x": "Note that CTC will still require U \u2264 T /k, however. ---------------------------------- **POSITION** Self-attention is inherently content-based [22] , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define for position t. We consider three approaches: content-only [21] , which forgoes position encodings; additive [19] , which takes demb = dh and adds the encoding to the embedding; and concatenative, where one takes demb = 40 and concatenates it to the embedding. The latter was found necessary for self-attentional LAS <cite>[27]</cite> , as additive encodings did not give convergence. However, the monotonicity of CTC is a further positional inductive bias, which may enable the success of content-only and additive encodings. ---------------------------------- **EXPERIMENTS**",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_8",
  "x": "We also evaluate design choices in Table 4 . Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_9",
  "x": "For phoneme training, our labels come from the CMU pronunciation lexicon (Table 3 ). These models train in one day (Tesla V100), comparable to the Speech Transformer [19] ; however, SAN-CTC gives further benefits at inference time as token predictions are generated in parallel. We also evaluate design choices in Table 4 . Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_10",
  "x": "In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC. This also suggests possibly using fewer heads, directed self-attention [25] , and restricted contexts for faster training (Table 1) . Phoneme labels gave a sharp backward-attending head and more diffuse heads. We believe this to be a symptom of English characters being more contextdependent than phonemes (for example, emitting 'tt' requires looking ahead, as '-' must occur between two runs of 't' tokens). ---------------------------------- **LIBRISPEECH** We give the first large-scale demonstration of a fully self-attentional ASR model using the LibriSpeech ASR corpus [42] , an English corpus produced from audio books giving 960 hours of training",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_11",
  "x": "Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC. This also suggests possibly using fewer heads, directed self-attention [25] , and restricted contexts for faster training (Table 1) . Phoneme labels gave a sharp backward-attending head and more diffuse heads. We believe this to be a symptom of English characters being more contextdependent than phonemes (for example, emitting 'tt' requires looking ahead, as '-' must occur between two runs of 't' tokens).",
  "y": "differences similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_0",
  "x": "---------------------------------- **INTRODUCTION** Automatic essay scoring (AES) is the task of assigning grades to essays written in an educational setting, using a computer-based system with natural language processing capabilities. The aim of designing such systems is to reduce the involvement of human graders as far as possible. AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; <cite>Dong and Zhang, 2016</cite>; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task <cite>(Dong and Zhang, 2016)</cite> . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES. To the best of our knowledge, string kernels have never been used for this task. As string kernels are a simple approach that relies solely on character n-grams as features, it is fairly obvious that such an approach will not to cover several aspects (e.g.: semantics, discourse) required for the AES task.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_1",
  "x": "AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; <cite>Dong and Zhang, 2016</cite>; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task <cite>(Dong and Zhang, 2016)</cite> . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES. To the best of our knowledge, string kernels have never been used for this task. As string kernels are a simple approach that relies solely on character n-grams as features, it is fairly obvious that such an approach will not to cover several aspects (e.g.: semantics, discourse) required for the AES task. To solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag-of-super-wordembeddings (BOSWE) . To our knowledge, this is the first successful attempt to combine string kernels and word embeddings. We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_2",
  "x": "---------------------------------- **EXPERIMENTS** Data set. To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle. The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_3",
  "x": "Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_4",
  "x": "The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_5",
  "x": "The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_6",
  "x": "For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines.",
  "y": "uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_7",
  "x": "Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_8",
  "x": "All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_9",
  "x": "As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) . For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases. We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_10",
  "x": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results. Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts. The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_11",
  "x": "We combine HISK and BOSWE in the dual form by summing up the two corresponding matrices. For the learning phase, we employ the dual implementation of \u03bd-SVR available in LibSVM (Chang and Lin, 2011) . We set its regularization parameter to c = 10 3 and \u03bd = 10 \u22121 in all our experiments. In-domain results. The results for the in-domain automatic essay scoring task are presented in Table 2. In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_12",
  "x": "The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings. We tried to obtain a document embedding by averaging the word vectors for each document.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_13",
  "x": "We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings. We tried to obtain a document embedding by averaging the word vectors for each document. We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_14",
  "x": "It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings. We tried to obtain a document embedding by averaging the word vectors for each document. We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_15",
  "x": "We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it. In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_16",
  "x": "The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it. In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by . ---------------------------------- **CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_0",
  "x": "Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> . The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image. Similar to these recent methods, we also take a deep learning approach to referring expression generation and comprehension. However, while they use a generic model for object context -CNN features for the entire image containing the target object -we take a more focused approach to encode object comparisons. These object comparisons are critical for producing an unambiguous referring expression since one must consider visual characteristics of similar objects during generation in order to select the most distinct aspects for description.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_1",
  "x": "Recent approaches have pushed this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> . The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image. Similar to these recent methods, we also take a deep learning approach to referring expression generation and comprehension. However, while they use a generic model for object context -CNN features for the entire image containing the target object -we take a more focused approach to encode object comparisons.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_2",
  "x": "Three recent approaches for referring expression generation <cite>[26]</cite> and comprehension [14, 33] also take a deep learning approach. However, we add visual object comparisons and tie together language generation for multiple objects. Referring expression generation has been studied for many years [40, 22, 30] in linguistics and natural language processing. These works were limited by data collection and insufficient computer vision algorithms. Together Amazon Mechanical Turk and CNNs have somewhat mitigated these limitations, allowing us to revisit these ideas on large-scale datasets. We still use such work to motivate the architecture of our pipeline. For instance, Mitchell and Jordan et al [30, 16] show the importance of using attributes, Funakoshi et al [8] show the importance of relative relations between objects in the same perceptual group, and Kelleher et al [20] show the importance of spatial relationships. These provide motivation for our modeling choices: when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group. The REG datasets of the past were sometimes limited to using computer generated images [38] , or relatively small collections of natural objects [29, 28, 7] . Recently, a large-scale referring expression dataset was collected by Kazemzadeh et al [19] featuring natural objects in the real world.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_3",
  "x": "Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19,<cite> 26]</cite> . The availability of large-scale referring expression datasets allows us to train deep learning models. Additionally, our analysis of these datasets motivates our incorporation of visual comparisons between same-type objects, and the need to tie together choices for referring expression generation between objects. ---------------------------------- **MODELS** We implement several model variations for referring expression generation and comprehension. The first set of models are recent state of the art deep learning approaches from Mao et al <cite>[26]</cite> . We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2). Finally, we explore methods to jointly produce an entire set of referring expressions for all depicted objects of the same category (Sec 3.3).",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_4",
  "x": "The availability of large-scale referring expression datasets allows us to train deep learning models. Additionally, our analysis of these datasets motivates our incorporation of visual comparisons between same-type objects, and the need to tie together choices for referring expression generation between objects. ---------------------------------- **MODELS** We implement several model variations for referring expression generation and comprehension. The first set of models are recent state of the art deep learning approaches from Mao et al <cite>[26]</cite> . We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2). Finally, we explore methods to jointly produce an entire set of referring expressions for all depicted objects of the same category (Sec 3.3). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_5",
  "x": "For comparison, we implement both the baseline and strong model of Mao et al <cite>[26]</cite> . Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation. In particular, object and context are modeled as features from a CNN trained to recognize 1,000 object categories [36] from ImageNet [34] . Specifically, the visual representation is composed of: -Target object representation, o i . The object is modeled as features extracted from the VGG-fc7 layer by forwarding its bounding box through the network. -Global context representation, g i . Context is modeled as features extracted from the VGG-fc7 layer for the entire image. -Location/size representation, l i , for the target object. Location and size are modeled as a 5-d vector encoding the x and y locations of the top left and bottom right corners of the target object bounding box, as well as the bounding box size with respect to the image, i.e., l i = [",
  "y": "motivation uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_6",
  "x": "Context is modeled as features extracted from the VGG-fc7 layer for the entire image. -Location/size representation, l i , for the target object. Location and size are modeled as a 5-d vector encoding the x and y locations of the top left and bottom right corners of the target object bounding box, as well as the bounding box size with respect to the image, i.e., l i = [ Language generation is handled by a long short-term memory network (LSTM) [10] where inputs are the above visual features and the network is trained to generate natural language referring expressions. In Mao et al's baseline <cite>[26]</cite> , the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features. In addition, they also propose a stronger model that uses maximum mutual information (MMI) training to consider whether a listener would interpret a referring expression unambiguously. They impose this by penalizing the model if a generated referring expression could also be generated by some other object within the image. We implement both their original model and MMI model in our experiments. We subsequently refer to these two models as Baseline and MMI, respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_7",
  "x": "In summary, our final visual representation for a target object is: where o i , g i , l i are the target object, global context, and location/size features from the baseline model, \u03b4v i and \u03b4l i encodes visual appearance difference and location difference. W m and b m project the concatenation of the five types of features to be the final representation. ---------------------------------- **JOINT LANGUAGE GENERATION** For the referring expression generation task, rather than generating sentences for each object in an image separately [15] <cite>[26]</cite>, we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image. This makes sense intuitively -when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture. This can be observed in the fact that the expressions people generate for objects in an image tend to share similar patterns of expression. If you say \"the man on the left\" for one object then you tend to say \"the man on the right\" for the other object. We would like our algorithms to mimic these behaviors.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_8",
  "x": "As visual comparison, we aggregate the difference of hidden outputs to push away ambiguous information. . There, n is the the number of other objects of the same type. The hidden difference is jointly embedded with the target object's hidden output, and forwarded to the softmax layer for predicting the word. ---------------------------------- **DATA** We make use of 3 referring expression datasets in our work, all collected on top of the Microsoft COCO image collection [24] . One dataset, RefCOCOg <cite>[26]</cite> is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19] . In the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments . ---------------------------------- **DATASETS & ANALYSIS**",
  "y": "background uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_9",
  "x": "Both RefCOCO and RefCOCO+ contain an average of 3.9 sametype objects per image, while RefCOCOg contains an average of 1.63 sametype objects per image. The large number of same-type objects per image in RefCOCO and RefCOCO+ suggests that incorporating visual comparisons to same-type objecs will be useful. Dataset Splits: There are two types of splits of the data into train/test sets: a per-object split and a people-vs-objects split. The first type is per-object split. In this split, the dataset is divided by randomly partitioning objects into training and testing sets. This means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach <cite>[26]</cite> . The second type is people-vs-objects splits. One thing we observe from analyzing the datasets is that about half of the referred objects are people. Therefore, we create a split for RefCOCO and RefCOCO+ datasets that evaluates images containing multiple people (testA) vs images containing multiple instances of all other objects (testB).",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_10",
  "x": "We first perform some experiments to analyze the use of context in referring expressions (Sec 5.1). Given these findings, we then perform experiments evaluating the usefulness of our proposed visual and language innovations on the comprehension (Sec 5.2) and generation tasks (Sec 5.3). In experiments for the referring expression comprehension task, we use the same evaluation as Mao et al <cite>[26]</cite> , namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box. If the IOU is larger than 0.5 we count it as a true positive. Otherwise, we count it as a false positive. We average this score over all images. For the referring expression generation task we use automatic evaluation metrics, BLEU, ROUGE, and METEOR developed for evaluating machine translation results, commonly used to evaluate language generation results [41, 18, 5, 27, 39, 23] . We further perform human evaluations, and propose a new metric evaluating the duplicate rate of generated expressions. For both tasks, we compare our models with \"Baseline\" and \"MMI\" <cite>[26]</cite> . Specifically, Table 1 .",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_12",
  "x": "We also perform an ablation study, evaluating the combinations. ---------------------------------- **ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly. This may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task. Given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object. We found this to improve performance, suggesting room for improving the visual context feature.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_13",
  "x": "Specifically, Table 1 . Expression Comprehension accuracies on RefCOCO and RefCOCO+ of the Baseline model with differenct context source. Scale n indicates the size of the cropped window centered by the target object. we denote \"visdif\" as our visual comparison model, and \"tie\" as the LSTM tying model. We also perform an ablation study, evaluating the combinations. ---------------------------------- **ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_14",
  "x": "Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare the performance of \"visdif\" model without visual comparison, and visual comparison between different-category objects, between all objects, and between same-type objects. ---------------------------------- **REFERRING EXPRESSION COMPREHENSION** We evaluate performance on the referring expression comprehension task on RefCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+, we evaluate on the two subsets of people (testA) and all other objects (testB). For RefCOCOg, we evaluate on the per-object split as previous work <cite>[26]</cite> . Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO. Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_15",
  "x": "Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare the performance of \"visdif\" model without visual comparison, and visual comparison between different-category objects, between all objects, and between same-type objects. ---------------------------------- **REFERRING EXPRESSION COMPREHENSION** We evaluate performance on the referring expression comprehension task on RefCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+, we evaluate on the two subsets of people (testA) and all other objects (testB). For RefCOCOg, we evaluate on the per-object split as previous work <cite>[26]</cite> . Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO. Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_16",
  "x": "In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by <cite>[26]</cite> , which were trained uisng Multibox [4] . Results on shown in the bottom half of Table 2 . Although all comprehension accuracies drop due to imperfect detections, the improvements of our models over Baseline and MMI are still observed. One weakness of our automatic system is that it highly depends on detection performance, especially for general objects (testB). However, considering our detector was trained on MSCOCO validation only, we believe such weakness may be alleviated with more training data and stronger detection techniques, e.g., [12] Table 2 . Referring Expression comprehension results on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Rows of \"method(det)\" are the results of automatic system built on Fast-RCNN [9] and Multibox [4] detections. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_0",
  "x": "A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works<cite> (Rubin et al., 2016</cite>; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try to model the interaction of sentences within the document. We observed a pattern in the way sentences cluster in different kind of news articles. Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other. On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 . We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_1",
  "x": "On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1 . We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method. ---------------------------------- **RELATED WORK** Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_2",
  "x": "**RELATED WORK** Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_3",
  "x": "In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method. ---------------------------------- **RELATED WORK** Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_4",
  "x": "McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model. Recent works by Yang et al. (2017) ; De Sarkar et al. (2018) show that sophisticated neural models can be used for satirical news detection. To the best of our knowledge, none of the previous works represent individual documents as graphs where the nodes represent the sentences for performing clas-sification using a graph neural network. ---------------------------------- **RASHKIN ET AL. (2017) EXTENDS**",
  "y": "differences"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_5",
  "x": "Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model. Recent works by Yang et al. (2017) ; De Sarkar et al. (2018) show that sophisticated neural models can be used for satirical news detection. To the best of our knowledge, none of the previous works represent individual documents as graphs where the nodes represent the sentences for performing clas-sification using a graph neural network. ---------------------------------- **RASHKIN ET AL. (2017) EXTENDS** ---------------------------------- **DATASET AND BASELINE** We use SLN: Satirical and Legitimate News Database<cite> (Rubin et al., 2016)</cite> , RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_6",
  "x": "We train the models for a maximum of 10 epochs and use Adam optimizer with learning rate 0.001. For all the models, we use max-pool for pooling, which is followed by a fully connected projection layer with output nodes equal to the number of classes for classification. ---------------------------------- **EXPERIMENTAL SETTING** We conduct experiments across various settings and datasets. We report macro-averaged scores in Table 3 : 4-way classification results for different models. We only report F1-score following the SoTA paper. similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper<cite> (Rubin et al., 2016)</cite> reports a 10fold cross validation number on SLN. We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set.",
  "y": "differences"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_0",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency 2 increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_1",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency 2 increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_2",
  "x": "The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics. Our concern for units of measurement is not a simple matter of precision but one of great theoretical importance: if the length of a dependency is measured in units of word length (e.g., syllables or phonemes) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. Therefore, pressure to reduce dependency lengths implies pressure for compression [25, 26] , linking a principle of word order with a principle that operates (nonexclusively) on individual words. An understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case. 3 Acknowledgments ---------------------------------- **** Liu et al's reflections on the term dependency length minimization [1] may look anecdotal but they are not. By the turn of the 20th century, we put forward a \"Euclidean distance minimization\" hypothesis for the distance between syntactically linked words and various word order phenomena [2, 3] 1 .",
  "y": "background uses"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_3",
  "x": "For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs).",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_4",
  "x": "A less flashy contribution of [6] has been promoting the need of controlling for sentence length (as a predictor of dependency length in their mixed-effects regression model) in research on dependency length minimization, an important methodological issue [15] that was addressed early [2] but neglected in subsequent research (e.g., [16, 17, 18] ). Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_5",
  "x": "A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading. Suppose that one wishes to compare the cost of two orderings of the same sentence. The observation that the processing cost of a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it, and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so-called complexity profile (e.g., [22] ), rendering fair comparison impractical. The problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [23] and worsens when the sentences being compared differ not only in order but also in content. Another challenge is the precision of dependency length that is typically measured in words. Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics. Our concern for units of measurement is not a simple matter of precision but one of great theoretical importance: if the length of a dependency is measured in units of word length (e.g., syllables or phonemes) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words.",
  "y": "background uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_0",
  "x": "---------------------------------- **INTRODUCTION** There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010</cite>; Post and Gildea, 2009) . DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: \u2022 A natural recurring structure here would be the structure \"[ N P the [ N N president]]\", yet it occurs not at all in the data. TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975) . TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for \"splicing in\" of syntactic fragments within trees.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_1",
  "x": "From a linguistic point of view: Deeper auxiliary trees can help model large patterns of insertion and potential correlations between lexical items that extend over multiple levels of tree. Combining left and right auxiliary trees can help model modifiers of the same node from left and right (combination of adjectives and relative clauses for instance). Simultaneous insertion allows us to deal with multiple independent modifiers for the same constituent (for example, a series of adjectives). From a practical point of view, we show that an induced TIG provides modeling performance superior to TSG and comparable with TIG 0 . However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures. ---------------------------------- **PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_2",
  "x": "**PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees. 3 Therefore, we have an exchangeable process for generating right auxiliary trees as for initial trees in TSG. We must define three distinct base distributions for initial trees, left auxiliary trees, and right auxiliary trees. P init 0 generates an initial tree with root label c by sampling CFG rules fromP and making a binary decision at every node generated whether to leave it as a frontier node or further expand (with probability \u03b2 c ) (Cohn et al., 2009) . Similarly, our P right 0 generates a right auxiliary tree with root label c by first making a binary decision whether to generate an immediate foot or not (with probability \u03b3 right c ), and then sampling an appropriate CFG rule 3 We use right insertions for illustration; the symmetric analog applies to left insertions.",
  "y": "extends"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_3",
  "x": "In any derivation, for every initial tree node labelled c (except for frontier nodes) we determine whether or not there are insertions at this node by sampling a Bernoulli(\u00b5 left c ) distributed left insertion variable and a Bernoulli(\u00b5 right c ) distributed right insertion variable. For left auxiliary trees, we treat the nodes that are not along the spine of the auxiliary tree the same way we treat initial tree nodes, however for nodes that are along the spine (including root nodes, excluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetrically for right insertions). ---------------------------------- **INFERENCE** Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_4",
  "x": "**INFERENCE** Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002;<cite> Cohn and Blunsom, 2010)</cite> . Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_5",
  "x": "All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ). Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees. As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 depends mostly on TSG rules. If we look at the most likely derivations for the test data, TIG 0 assigns 663 insertions (351 left insertions) in the parsing of entire Section 23, meanwhile TIG assigns 3924 (2100 left insertions).",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_6",
  "x": "---------------------------------- **EVALUATION RESULTS** We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
  "y": "similarities uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_0",
  "x": "Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset. ---------------------------------- **INTRODUCTION** Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. The first is the advent of deep learning techniques (Goodfellow et al., 2016) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries<cite> (Hill et al., 2015</cite>; Hermann et al., 2015) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work<cite> (Hill et al., 2015)</cite> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_1",
  "x": "Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work<cite> (Hill et al., 2015)</cite> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text (Hermann et al., 2015) . In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. The missing word is assumed to appear in the document. Encouraged by the recent success of deep learning attention architectures (Bahdanau et al., 2015; Sukhbaatar et al., 2015) , we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks. The model first reads the document and the query using a recurrent neural network (Goodfellow et al., 2016) .",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_2",
  "x": "The model first reads the document and the query using a recurrent neural network (Goodfellow et al., 2016) . Then, it deploys an iterative inference process to uncover the inferential links that exist between the missing query word, the query, and the document. This phase involves a novel alternating attention mechanism; it first attends to some parts of the query, then finds their corresponding matches by attending to the document. The result of this alternating search is fed back into the iterative inference process to seed the next search step. This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document. After a fixed number of iterations, the model uses a summary of its inference process to predict the answer. This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models<cite> (Hill et al., 2015</cite>; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) . It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks.",
  "y": "differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_4",
  "x": "**TASK DESCRIPTION** One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books. The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token. The dataset is divided into four subsets depending on the type of the word replaced. The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by<cite> (Hill et al., 2015)</cite> . The CNN 2 corpus was generated from news articles available through the CNN website.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_5",
  "x": "The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by<cite> (Hill et al., 2015)</cite> . The CNN 2 corpus was generated from news articles available through the CNN website. The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements. Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token. For both datasets, the training and evaluation data consist of tuples (Q, D, A, a), where Q is the query (represented as a sequence of words), D is the document, A is the set of possible answers, and a \u2208 A is ---------------------------------- **ALTERNATING ITERATIVE ATTENTION** Our model is represented in Fig. 1 . Its workflow has three steps.",
  "y": "motivation"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_6",
  "x": "The inference is modelled by an additional recurrent GRU network. The recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer. In particular, at each time step: (1) it performs an attentive read on the query encodings, resulting in a query glimpse, q t , and (2) given the current query glimpse, it extracts a conditional document glimpse, d t , representing the parts of the document that are relevant to the current query glimpse. In turn, both attentive reads are conditioned on the previous hidden state of the inference GRU s t\u22121 , summarizing the information that has been gathered from the query and the document up to time t. The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process. Query Attentive Read Given the query encodings {q i }, we formulate a query glimpse q t at timestep t by: where q i, t are the query attention weights and A q \u2208 R 2h\u00d7s , where s is the dimensionality of the inference GRU state, and a q \u2208 R 2h . The attention we use here is similar to the formulation used in<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) , but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in (Luong et al., 2015) . Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t\u22121 .",
  "y": "differences similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_8",
  "x": "Table 2 reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from<cite> (Hill et al., 2015)</cite> and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016) . ---------------------------------- **CBT** Main result Our model (line 7) sets a new stateof-the-art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader (line 5). This performance gap is only partially reflected on the CBT-NE dataset. We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set, which sits on par with the best baseline. In CBT-NE, the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun. We found that approximatively 27.5% of validation examples and 29.6% of test examples contain an answer that has never been predicted in the training set. These numbers are considerably lower for the CBT-CN, for which only 2.5% and 4.6% of validation and test examples respectively contain an answer that has not been previously seen.",
  "y": "uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_10",
  "x": "In our case, the memory is represented by the set of document and query contextual encodings. Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016;<cite> Hill et al., 2015)</cite> , which were also applied to question answering. The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by (Kadlec et al., 2016) , which in turn was based on the earlier Pointer Networks of (Vinyals et al., 2015) . However, differently from our work, (Kadlec et al., 2016) perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks. To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models. In our model, the repeated, tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer, and then to focus on the parts of the document that are most salient to the currently-attended query components. A similar attempt in attending different components of the query may be found in (Hermann et al., 2015) . In that model, the document is processed once for each query word. This can be computationally intractable for large documents, since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times. In contrast, our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps.",
  "y": "similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_11",
  "x": "In our model, the repeated, tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer, and then to focus on the parts of the document that are most salient to the currently-attended query components. A similar attempt in attending different components of the query may be found in (Hermann et al., 2015) . In that model, the document is processed once for each query word. This can be computationally intractable for large documents, since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times. In contrast, our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps. The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models. Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015;<cite> Hill et al., 2015)</cite> . In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately. Moreover, we substitute the simple linear update with a GRU network.",
  "y": "similarities"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_0",
  "x": "**INTRODUCTION** Automatic metrics, such as BLEU (Papineni et al., 2002) , are widely used in machine translation (MT) as a substitute for human evaluation. Such metrics commonly take the form of an automatic comparison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) . Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice.",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_1",
  "x": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) . Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice. Within MT, approximate randomization for the purpose of statistical testing is also less common. Riezler and Maxwell (2005) provide a comparison of approximate randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011) . Our contribution in this paper is to revisit statistical significance tests in MT -namely, bootstrap resampling, paired bootstrap resampling and approximate randomization -and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs.",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_2",
  "x": "A better comparison of p-values would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is. The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the experiments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007) . We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, \u03c4 B , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1 , and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling<cite> (Koehn, 2004)</cite> , which is widely used in MT evaluation. We carry out evaluation over a range of MT systems, not only including pairs of systems that perform equally well, but also pairs of systems for which one system performs marginally better than the other. This enables evaluation of not only Type I error, but the overall accuracy of the tests. We carry out a large-scale human evaluation of all WMT 2012 shared task participating systems for two language pairs, and collect sufficient human judgments to facilitate statistical significance tests. This human evaluation data then provides a gold-standard against which to compare randomized tests.",
  "y": "motivation differences"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_3",
  "x": "Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993) . The test statistic is taken as the difference in scores of the two systems, S X \u2212 S Y , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (X b , Y b ) of a bootstrapped test set<cite> (Koehn, 2004)</cite> , constructed by sampling with replacement from the original test set translations. The bootstrap distribution S boot of the test statistic is estimated by calculating the value of the pseudo-statistic The null hypothesis distribution S H 0 can be estimated from S boot by applying the shift method (Noreen, 1989 ), which assumes that S H 0 has the same shape but a different mean than S boot . Thus, S boot is transformed into S H 0 by subtracting the mean bootstrap statistic from every value in S boot . Once this shift-to-zero has taken place, the null hypothesis is rejected if the probability of observing a more extreme value than the actual statistic is lower than a predetermined p-value \u03b1, which is typically set to 0.05. In other words, the score difference is significant at level 1 \u2212 \u03b1. Figure 3 provides a one-sided implementation of bootstrap resampling, where H 0 is that the score of System X is less than or equal to the score of Figure 5 includes a typical example of bootstrap resampling applied to BLEU, for a pair of systems for which differences in scores are significant, while Figure 6 shows the same for ME-TEOR but for a pair of systems with no significant difference in scores. ---------------------------------- **APPROXIMATE RANDOMIZATION**",
  "y": "background uses"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_0",
  "x": "In this paper, we introduce QVEC-CCA, which simultaneously addresses both problems, while preserving major strengths of QVEC. 1 ---------------------------------- **QVEC AND QVEC-CCA** We introduce QVEC-CCA-an intrinsic evaluation measure of the quality of word embeddings. Our method is a modification of QVEC-an evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource <cite>(Tsvetkov et al., 2015)</cite> . We review QVEC, and then describe QVEC-CCA. ---------------------------------- **QVEC.** The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource.",
  "y": "extends"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_1",
  "x": "To evaluate the semantic content of word vectors,<cite> Tsvetkov et al. (2015)</cite> exploit supersense annotations in a WordNetannotated corpus-SemCor (Miller et al., 1993 Table 2 : Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. \u2022 We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013) ; their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a) ; GloVe vectors (Pennington et al., 2014) ; Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990) ; and retrofitted GloVe and LSA vectors . \u2022 We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC-CCA, and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353) , MEN dataset (Bruni et al., 2012) , and SimLex-999 dataset (Hill et al., 2014, SimLex) . 3 \u2022 In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti) ; and the metaphor detection (Tsvetkov et al., 2014, Metaphor) . \u2022 Finally, we compute the Pearson's correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. We extend the setup of<cite> Tsvetkov et al. (2015)</cite> with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016) , and the second is dependency parsing (Parse), using the stack-LSTM model of . ----------------------------------",
  "y": "background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_2",
  "x": "\u2022 Finally, we compute the Pearson's correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. We extend the setup of<cite> Tsvetkov et al. (2015)</cite> with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016) , and the second is dependency parsing (Parse), using the stack-LSTM model of . ---------------------------------- **RESULTS.** To test the efficiency of QVEC-CCA in capturing the semantic content of word vectors, we evaluate how well the scores correspond to the 3 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014) . scores of word vector models on semantic benchmarks. QVEC and QVEC-CCA employ the semantic supersense-dimension vectors described in \u00a73. In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC-CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_0",
  "x": "The advantage of such an effort is that the resulting tool provides non-intrusive and cost-effective means to detect and warn at-risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy. 2. Using this model, we introduce a novel semi-automated process that converts the decision nodes in the RF model into natural language questions.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_1",
  "x": "In the United States, the estimated cost of T2DM rose to $245 billion in 2012 (Association, 2013 ). Yet, 90% of these individuals at high risk are not aware of it (Li et al., 2013) . Our long-term goal is to develop tools that automatically classify overweight individuals (hence at risk for T2DM 1 ) 1 In the CDC diabetes questionnaire available at http://www.cdc.gov/diabetes/prevention/ pdf/prediabetestest.pdf, overweight BMI contributes more than half of the points associated with diabetes risk. using solely public social media information. The advantage of such an effort is that the resulting tool provides non-intrusive and cost-effective means to detect and warn at-risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are:",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_2",
  "x": "1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy. 2. Using this model, we introduce a novel semi-automated process that converts the decision nodes in the RF model into natural language questions. We then use these questions to implement a quiz that mimics a 20-questions-like game. The quiz aims to detect if the person taking it is overweight or not based on indirect questions related to food or use of food-related words. To our knowledge, we are the first to use a semiautomatically generated quiz for data acquisition. 3. We demonstrate that this quiz serves as a non-intrusive and engaging data collection process for individuals 2 . The survey was posted online and evaluated with 945 participants, of whom 926 voluntarily provided supplemental data, such as information necessary to compute the Body Mass Index (BMI), demographics, and Twitter handle, demonstrating excellent engagement. The randomforest model backing the survey agreed with self-reported BMI in 78.7% of cases.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_3",
  "x": "Social media, especially Twitter, has been recently utilized as a popular source of data for public health monitoring, such as tracking diseases (Ginsberg et al., 2009; YomTov et al., 2014; Nascimento et al., 2014; Greene et al., 2011; Chew and Eysenbach, 2010) , mining drug-related adverse events (Bian et al., 2012) , predicting postpartum psychological changes in new mothers (De Choudhury et al., 2013) , and detecting life satisfaction (Schwartz et al., 2013) and obesity (Chunara et al., 2013; Cohen-Cole and Fletcher, 2008; Fernandez-Luque et al., 2011) . We focus our attention on the language of food on social media to identify overweight communities and individuals. In the last couple of years, several variants of this problem have been considered<cite> (Fried et al., 2014</cite>; Abbar et al., 2015; Culotta, 2014; Ardehaly and Culotta, 2015) . food-related tweets and use it to predict several population characteristics, namely diabetes rate, overweight rate and political tendency. Generally, they use state-level populations, e.g., one of their classification tasks is to label whether a state is more overweight than the national median. Overweight rate is the percentage of adults whose Body Mass Index (BMI) is larger than a normal range defined by NIH. The classification task is to label whether a state is more overweight than the national median. Individuals' tweets are localized at state level as a single instance to train several classifier models, and the performance of models is evaluated using leave-one-out cross-validation. Importantly,<cite> Fried et al. (2014)</cite> train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health. Abbar et al. (2015) also used aggregated information for predicting obesity and diabetes statistics.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_4",
  "x": "Importantly,<cite> Fried et al. (2014)</cite> train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health. Abbar et al. (2015) also used aggregated information for predicting obesity and diabetes statistics. They considered energy intake based on caloric values in food mentioned on social media, demographic variables, and social networks. This paper begins to address individual predictions, based on the simplifying assumption that all individuals can be labeled based on the known label of their home county, e.g., all individuals in an overweight county are overweight, which is less than ideal. In contrast, our work collects actual individual information through the survey derived from community information. Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010) . These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work.",
  "y": "motivation"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_5",
  "x": "In contrast, our work collects actual individual information through the survey derived from community information. Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010) . These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work. ---------------------------------- **METHOD** Fried et al. (2014) showed that states and large cities generate a considerable number of food-related tweets, which can be used to infer important information about the respective community, such as overweight status or diabetes risk. In an initial experiment, we tested this classifier on the identification of overweight individuals.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_6",
  "x": "---------------------------------- **DATA COLLECTION** Figure 1: Architecture of the semi-automatic approach for quiz generation from social media data. ---------------------------------- **AN INTERPRETABLE MODEL FOR COMMUNITY CLASSIFICATION** Our main data-collection idea is to use a playful 20-questions-like survey, automatically generated from a community-based model, which can be widely deployed to acquire training data on individuals. Our approach is summarized in Figure 1 . The first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions. To this end, we started with the same settings as<cite> Fried et al. (2014)</cite> : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner. Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_7",
  "x": "To this end, we started with the same settings as<cite> Fried et al. (2014)</cite> : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner. Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state. For each state, we extracted features based on unigram (i.e., single) words and hashtags from all the above tweets localized to the corresponding state. To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates<cite> (Fried et al., 2014</cite> Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data. motivation for this decision was interpretability: as shown below, decision trees can be easily converted into a series of if . . . then . . . else . . . statements, which form the building blocks of the quiz. To minimize the number of questions, we trained a random forest with 7 trees with maximum depth of 3, and we ignored tokens that appear fewer than 3 times in the training data. These parameter values were selected to make the quiz of reasonable length. We aimed at 20 questions, as in the popular \"20 questions\" game, in which one player must guess what object the other is thinking of by asking 20 or fewer yes-or-no questions. Further tuning confirmed that a small number of shallow trees are most effective in accurately partitioning the state-level data.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_8",
  "x": "**QUIZ** We next manually converted all decision statements in the random forest classifier into natural language questions. The main assumption behind this process is that language use parallels actual behavior, e.g., a person who talks about fruit on social media will also eat fruit in real life. This allowed us to produce more intuitive questions, such as How often do you eat fruit? for the top node in Figure 2, Figure 2 . ---------------------------------- **MODEL ACCURACY** Majority baseline 50.89 SVM<cite> (Fried et al., 2014)</cite> 80.39 RF (food + hashtags) 82.35 Discretized RF (food + hashtags) 78.43 Table 2 : Random forest (RF) classifier performance on state-level data relative to majority baseline and<cite> Fried et al. (2014)</cite> 's best classifier. We include two versions of our classifier: the first keeps numeric features (e.g., word counts) as is, whereas the second discretizes numeric features to three bins. of How often do you mention \"fruit\" in your tweets? Table 1 shows the questions and corresponding answers we used for the three left-most decision nodes in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_9",
  "x": "However, when taking the quiz, each individual participant answered between 12 and 24 questions, depending on their answers and the corresponding traversal of the decision trees. This quiz serves to gather training data, which will be used in future work to train a supervised model for the identification of individuals at risk. To our knowledge, this approach is a novel strategy for quiz generation, and it serves as an important stepping-stone toward our goal of building individualized public health tools driven by social media. With respect to data retention, we collect (with the permission of the participants) the following additional data to be used for future research: height, weight, sex, location, age, and social media handles for Twitter, Instagram, and Facebook. We only downloaded public posts using these handles. This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%. As discussed earlier, this discretization step was necessary to create intelligible Likert-scaled questions (Likert, 1932) .",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_10",
  "x": "However, when taking the quiz, each individual participant answered between 12 and 24 questions, depending on their answers and the corresponding traversal of the decision trees. This quiz serves to gather training data, which will be used in future work to train a supervised model for the identification of individuals at risk. To our knowledge, this approach is a novel strategy for quiz generation, and it serves as an important stepping-stone toward our goal of building individualized public health tools driven by social media. With respect to data retention, we collect (with the permission of the participants) the following additional data to be used for future research: height, weight, sex, location, age, and social media handles for Twitter, Instagram, and Facebook. We only downloaded public posts using these handles. This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%. As discussed earlier, this discretization step was necessary to create intelligible Likert-scaled questions (Likert, 1932) .",
  "y": "differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_0",
  "x": "However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> . Section 5 presents an analysis of the results of the shared task, and, finally, Section 6 concludes this paper presenting avenues for future work. ---------------------------------- **RELATED WORK**",
  "y": "similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_1",
  "x": "Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> . Section 5 presents an analysis of the results of the shared task, and, finally, Section 6 concludes this paper presenting avenues for future work. ---------------------------------- **RELATED WORK** Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language.",
  "y": "extends differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_2",
  "x": "Given the multitude of terms and definitions used in the literature, recent studies have investigated common aspects of the abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018) . However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> . Section 5 presents an analysis of the results of the shared task, and, finally, Section 6 concludes this paper presenting avenues for future work. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_3",
  "x": "Hate speech identification: It is perhaps the most widespread abusive language detection subtask. There have been several studies published on this sub-task such as Kwok and Wang (2013) and Djuric et al. (2015) who build a binary classifier to distinguish between 'clean' comments and comments containing hate speech and profanity. More recently, presented the hate speech detection dataset containing over 24,000 English tweets labeled as non offensive, hate speech, and profanity. Offensive language: The GermEval 2 (Wiegand et al., 2018) shared task focused on Offensive language identification in German tweets. A dataset of over 8,500 annotated tweets was provided for a course-grained binary classification task in which systems were trained to discriminate between offensive and non-offensive tweets and a second task where the organizers broke down the offensive class into three classes: profanity, insult, and abuse. Toxic comments: The Toxic Comment Classification Challenge was an open competition at Kaggle which provided participants with comments from Wikipedia labeled in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this. Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks. ---------------------------------- **TASK DESCRIPTION AND EVALUATION**",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_4",
  "x": "A dataset of over 8,500 annotated tweets was provided for a course-grained binary classification task in which systems were trained to discriminate between offensive and non-offensive tweets and a second task where the organizers broke down the offensive class into three classes: profanity, insult, and abuse. Toxic comments: The Toxic Comment Classification Challenge was an open competition at Kaggle which provided participants with comments from Wikipedia labeled in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this. Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks. ---------------------------------- **TASK DESCRIPTION AND EVALUATION** The training and testing material used for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, built for this task. OLID was annotated using a hierarchical three-level annotation model introduced in <cite>Zampieri et al. (2019)</cite> . We use the annotation of each of the three layers in OLID to each sub-task in OffensEval as follows: ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_5",
  "x": "\u2022 Other (OTH): The target of these offensive posts does not belong to any of the previous two categories (e.g. an organization, a situation, an event, or an issue). ---------------------------------- **TASK EVALUATION** Given the strong imbalance between the number of instances in each class across the three tasks, we used the macro-averaged F1-score as the official evaluation metric for all tasks. This metric weights precision and recall equally, and calculates the F1-score for each class independently. The values are then averaged, giving equal weight to all classes, regardless of the number of samples. ---------------------------------- **DATA** In this Section we summarize OLID, the dataset used for this task. A detailed description of the data collection process and annotation is presented in <cite>Zampieri et al. (2019)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_6",
  "x": "We also include the models and baselines provided in Zampieri et. al. (2019) in bold. chine learning models such as SVM and Random forest. 7 ---------------------------------- **CONCLUSION** In this paper, we presented the results of SemEval-2016 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval). In OffensEval we used OLID<cite> (Zampieri et al., 2019)</cite> , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers. message (sub-task B); and 3) what is the target of the offensive (sub-task C). OLID is publicly available to the research community.",
  "y": "similarities uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_0",
  "x": "**INTRODUCTION** Machine translation (MT) is the automatic translation of text from one human language to another. Statistical MT accomplishes this through a probabilistic model of the translation process. In phrase-based statistical MT (PSMT), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_1",
  "x": "Machine translation (MT) is the automatic translation of text from one human language to another. Statistical MT accomplishes this through a probabilistic model of the translation process. In phrase-based statistical MT (PSMT), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this work, we want to see whether syntax-related features can help choose between original and reordered sentence translations in PSMT.",
  "y": "motivation background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_2",
  "x": "We use as our starting point the PSMT system Moses . In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT ( \u00a73). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2). We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance. 2 Background and Related Work 2.1 Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_3",
  "x": "We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance. 2 Background and Related Work 2.1 Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001) , Quirk et al. (2005) ) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Manning, 2010) . One area where this lack of syntactic information is felt is the distortion model of the PSMT system. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved).",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_4",
  "x": "Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved). Without syntactic information here, PSMT systems lack a principled way to manage long-distance movements, leading to difficulty in language pairs where this is needed, such as English and Japanese or German. ---------------------------------- **REORDERING-AS-PREPROCESSING** The reordering-as-preprocessing approach addresses the PSMT reordering difficulty by removing word order differences prior to translation. This is done with a preprocessing step where the input sentence is parsed and a reordered alternative created on the basis of the resulting parse tree. Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation. The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system.",
  "y": "extends"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_5",
  "x": "Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation. The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system. Many other reordering-as-preprocessing systems exist. Xia and McCord (2004) present a system for French-English translation that, instead of using hand-crafted reordering rules, automatically learns reordering patterns from the corpus. Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ----------------------------------",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_6",
  "x": "The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system. Many other reordering-as-preprocessing systems exist. Xia and McCord (2004) present a system for French-English translation that, instead of using hand-crafted reordering rules, automatically learns reordering patterns from the corpus. Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_7",
  "x": "---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering. Our work has some similarities to that of Zwarts and Dras (2008) but uses the log-linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation. This idea of using linguistic features to improve statistical MT has appeared in a number of recent papers. Chiang et al. (2009) demonstrate an improvement in hierarchical PSMT and syntaxbased (string-to-tree) statistical MT through the addition of features pinpointing possible errors in the translation, for example the number of occurrences of a particular grammar production rule, or non-terminal in a rule. Xiong et al. (2010) derive features from the Link Grammar parser, in combination with word posterior probabilities, to detect MT errors (in order to subsequently improve translation quality). Unlike Chiang et al. (2009) , we work with PSMT and use features that consider the parse tree as a whole or aspects of the reordering process itself.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_8",
  "x": "Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering. Our work has some similarities to that of Zwarts and Dras (2008) but uses the log-linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation.",
  "y": "differences background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_9",
  "x": "Our work also differs from these lattice-based systems in that we preserve and incorporate the standard distortion model of the PSMT system in a way that the above systems cannot. Where the lattice-based systems aim to overcome the weaknesses of the distortion model by replacing it with the lattice-creating reordering process, we view the two components as complementary. This has an interesting consequence, which we introduce in \u00a73.1 and discuss further in \u00a75. Similar to our work and the work in the previous section, Ge (2010) includes features to assess reordering options, based on the structure of the resulting tree, for example which nonterminal appears as the first child and the size of jump required to reach the nonterminal used as the next child. In addition to the difference with the distortion model mentioned above, our work differs in that Ge (2010) focuses on finding the best reordering using syntactic features plus a few surface and POS-tag features as a way of \"guarding against parsing errors\", whereas we also look at using features to represent confidence in a parse. 3 Dual-Path PSMT In this paper, we develop a dual-path PSMT system. \u00a73.1 introduces the lattice input format, by which we provide the system with two variants of the input sentence: the original and the reordered alternative produced by the preprocessing step. \u00a73.2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives. Our system is built upon the PSMT system Moses . For reordering, we use the Berkeley parser (Petrov et al., 2006 ) and the rules given by <cite>Collins et al. (2005)</cite> , but any reordering preprocessing step could equally be used.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_10",
  "x": "Further extending the feature set would require a different optimisation procedure, such as MIRA, as discussed by Arun and Koehn (2007) . ---------------------------------- **EXPERIMENTS** ---------------------------------- **MODELS AND EVALUATION** Our baseline PSMT system is Moses , repository revision 3590. 4 We run all of our experiments using the Moses Experiment Management System; configuration files and scripts to reproduce our experiments are available online. 5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1).",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_11",
  "x": "**MODELS AND EVALUATION** Our baseline PSMT system is Moses , repository revision 3590. 4 We run all of our experiments using the Moses Experiment Management System; configuration files and scripts to reproduce our experiments are available online. 5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1). The reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above. We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths. confidence features (+FEATURES).",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_12",
  "x": "The reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above. We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths. confidence features (+FEATURES). We do not explore different subsets of the features here. For evaluation we use the standard BLEU metric (Papineni et al., 2002) , which measures n-gram overlap between the candidate translation and the given reference translation. ---------------------------------- **APPROXIMATE ORACLE** To get an idea of a rough upper bound, we implement the approximate oracle outlined in Zwarts and Dras (2008) . For every sentence, the approximate oracle compares the outputs of MOSES and REORDER with the reference translation and chooses the output that it expects will contribute to a higher BLEU score overall.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_13",
  "x": "For data we use the corpora provided for the 2010 Workshop on Statistical Machine Translation 7 translation task. The number of sentence pairs in each corpus are given in Table 2 . We trained 5-gram language models with SRILM (Stolcke, 2002) using the three language model files listed in Table 3 : BLEU scores for every system the last containing the remainder. One language model was produced for each file or subfile, giving a total of ten models. The final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus. Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_14",
  "x": "However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle. ---------------------------------- **RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_15",
  "x": "However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle. ---------------------------------- **RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified.",
  "y": "background uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_16",
  "x": "It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle. ---------------------------------- **RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified. It is especially interesting to note that the reordered system's translations are preferred by the oracle more often even though its overall performance is lower. Turning now to the results of our systems, we see that simply giving the system the option of both reordered and non-reordered versions of the sentence (LATTICE) produces an improved translation performance overall. While the addition of our confidence features (+FEATURES) leaves the performance roughly unchanged, the gap between LATTICE and the approximate oracle implies that this is due to the choice of features, and that a feature set may yet be found that will improve performance over the plain LATTICE system.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_17",
  "x": "Given our dual-path system, an obvious question is whether a multi-path extension is possible. Since the disjoint vocabularies inhibit compression of the lattice, extending the input to a multipath lattice is likely to rapidly encounter efficiency issues. However, some possibilities exist. One option would be to include paths that represent different reorderings of the sentence based on the same parse. For example, the original sentence could be compared with reorderings created by different reordering-as-preprocessing approaches. In this instance, it would be advisable to use a different vocabulary (by using a different token prefix) for each path, as each reordering is likely to require a different lexicalised distortion model. In the case where these reordered alternatives are all possible combinations of parts of one reordering process, our system approaches the work described in \u00a72.4, and in fact those systems will probably be more suitable as the preprocessing takes over the role of the PSMT distortion model. Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_18",
  "x": "In the case where these reordered alternatives are all possible combinations of parts of one reordering process, our system approaches the work described in \u00a72.4, and in fact those systems will probably be more suitable as the preprocessing takes over the role of the PSMT distortion model. Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements. The next major phase of this work is to extend and explore the feature space. This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system. ---------------------------------- **CONCLUSION** We adapt the lattice input to the PSMT system Moses to create a system that can simultaneously translate a sentence and its reordered variant produced by a syntactically-informed preprocessing step.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_19",
  "x": "This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system. ---------------------------------- **CONCLUSION** We adapt the lattice input to the PSMT system Moses to create a system that can simultaneously translate a sentence and its reordered variant produced by a syntactically-informed preprocessing step. We find that providing the system with this choice results in improved translation performance, achieving a BLEU score of 21.39, 0.62 higher than the baseline. We then augment the translation model of our system with a number of features to express our confidence in the reordering. While these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically-informed reordering. While our reordering step is a reimplementation of the <cite>Collins et al. (2005)</cite> system, contrary to their findings we do not see an improvement using the reordering step alone. This provides evidence against the idea that reordering improves translation performance absolutely.",
  "y": "differences uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_0",
  "x": "Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by<cite> Nuhn et al. (2013)</cite> . We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. ---------------------------------- **INTRODUCTION** Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_1",
  "x": "Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011;<cite> Nuhn et al., 2013)</cite> . Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014) . Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010) . However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time. ---------------------------------- **DECIPHERMENT MODEL** We use the notation from<cite> Nuhn et al. (2013)</cite> . Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i \u2208 V f and e i \u2208 V e respectively. The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_2",
  "x": "We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time. ---------------------------------- **DECIPHERMENT MODEL** We use the notation from<cite> Nuhn et al. (2013)</cite> . Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i \u2208 V f and e i \u2208 V e respectively. The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence. The substitutions are represented by a function \u03c6 : V f \u2192 V e such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function \u03c6 which does not have every \u03c6(f ) fixed is called a partial cipher function (Corlett and Penn, 2010) . The number of f s that are fixed in \u03c6 is given by its cardinality. \u03c6 is called an extension of \u03c6, if f is fixed in \u03c6 such that \u03b4(\u03c6 (f ), \u03c6(f )) yields true \u2200f \u2208 V f which are already fixed in \u03c6 where \u03b4 is Kronecker delta.",
  "y": "uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_3",
  "x": "The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence. The substitutions are represented by a function \u03c6 : V f \u2192 V e such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function \u03c6 which does not have every \u03c6(f ) fixed is called a partial cipher function (Corlett and Penn, 2010) . The number of f s that are fixed in \u03c6 is given by its cardinality. \u03c6 is called an extension of \u03c6, if f is fixed in \u03c6 such that \u03b4(\u03c6 (f ), \u03c6(f )) yields true \u2200f \u2208 V f which are already fixed in \u03c6 where \u03b4 is Kronecker delta. Decipherment is then the task of finding the \u03c6 for which the probability of the deciphered text is maximized. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm<cite> (Nuhn et al., 2013)</cite> which incrementally finds the most likely substitutions using the language model scores as the ranking. ---------------------------------- **NEURAL LANGUAGE MODEL**",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_4",
  "x": "The advantage of a neural LM is that it can be used to score the entire candidate plaintext for a hypothesized partial decipherment. In this work, we use a state of the art byte (character) level neural LM using a multiplicative LSTM (Radford et al., 2017) . Consider a sequence S = w 1 , w 2 , w 3 , ..., w N . The LM score of S is SCORE(S): P (S) = P (w 1 , w 2 , w 3 , ..., w N ) ---------------------------------- **BEAM SEARCH** Algorithm 1 is the beam search algorithm<cite> (Nuhn et al., 2013</cite> (Nuhn et al., , 2014 Hs. ADD((\u2205,0)) 5: while",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_5",
  "x": "We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers. We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect. The character NLM uses a single layer multiplicative LSTM (mLSTM) (Radford et al., 2017) with 4096 units. The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates. States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given subsequence. In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer 2 . ---------------------------------- **1:1 SUBSTITUTION CIPHERS** In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) ,<cite> Nuhn et al. (2013)</cite> and Hauer et al. (2014) . The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.",
  "y": "similarities uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_6",
  "x": "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) ,<cite> Nuhn et al. (2013)</cite> and Hauer et al. (2014) . The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution. ---------------------------------- **I H A V E D E P O S I T E D I N T H E C O U N T Y O F B E D F O R D A B O U T F O U R M I L E S F R O M B U F O R D S I N A N E X C A V A T I O N O R V A U L T S I X F E E T B E L O W T H E S U R F A C E O F T H E G R O U N D T H E F O L L O W I N G A R T I C L E S B E L O N G I N G J O I N T L Y T O T H E P A** ---------------------------------- **AN EASY CIPHER: ZODIAC-408** Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms. Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm<cite> (Nuhn et al., 2013)</cite> with beam size of 10M with a 6-gram LM which gives an SER of 2%. The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_7",
  "x": "---------------------------------- **RELATED WORK** Automatic decipherment for substitution ciphers started with dictionary attacks (Hart, 1994; Jakobsen, 1995; Olson, 2007) . Ravi and Knight (2008) frame the decipherment problem as an integer linear programming (ILP) problem. Knight et al. (2006) use an HMM-based EM algorithm for solving a variety of decipherment problems. Ravi and Knight (2011) extend the HMM-based EM approach with a Bayesian approach, and report the first automatic decipherment of the Zodiac-408 cipher. Berg-Kirkpatrick and Klein (2013) show that a large number of random restarts can help the EM approach. Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers. Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm. Nuhn et al. (2014) present various improvements to the beam search algorithm in<cite> Nuhn et al. (2013)</cite> including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_8",
  "x": "Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher. However, this approach needs supervision compared to our approach which uses a pre-trained neural LM. Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext. They apply this approach to shift ciphers (including Vigen\u00e8re ciphers). Their approach cannot be extended to homophonic ciphers and full message neural LMs as in our work. ---------------------------------- **CONCLUSION** This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from<cite> Nuhn et al. (2013</cite>; and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
  "y": "extends differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_0",
  "x": "We provide tasks for translation, language modeling, and classification. Optimizers update the model parameters based on the gradients. We provide wrappers around most PyTorch optimizers and an implementation of Adafactor (Shazeer and Stern, 2018) , which is a memory-efficient variant of Adam. Learning Rate Schedulers update the learning rate over the course of training. We provide several popular schedulers, e.g., the inverse square-root scheduler from <cite>Vaswani et al. (2017)</cite> and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) . Reproducibility and forward compatibility. FAIRSEQ includes features designed to improve reproducibility and forward compatibility. For example, checkpoints contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed. FAIRSEQ also provides forward compatibility, i.e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_1",
  "x": "FAIRSEQ implements dynamic loss scaling in order to avoid underflows for activations and gradients because of the limited precision offered by FP16. This scales the loss right after the forward pass to fit into the FP16 range while the backward pass is left unchanged. After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights. Inference. FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017;<cite> Vaswani et al., 2017</cite>; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used. This can speed up a na\u00efve implementation without caching by up to an order of magnitude, since only new states are computed for each token. For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture. During inference we build batches with a variable number of examples up to a user-specified number of tokens, similar to training. FAIRSEQ also supports inference in FP16 which increases decoding speed by 54% compared to FP32 with no loss in accuracy (Table 1) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_2",
  "x": "This can speed up a na\u00efve implementation without caching by up to an order of magnitude, since only new states are computed for each token. For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture. During inference we build batches with a variable number of examples up to a user-specified number of tokens, similar to training. FAIRSEQ also supports inference in FP16 which increases decoding speed by 54% compared to FP32 with no loss in accuracy (Table 1) . ---------------------------------- **APPLICATIONS** FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; Edunov et al., 2018b,a; Chen et al., 2018; Song et al., 2018; Wu et al., 2019) , language modeling , abstractive document summarization (Fan et al., 2018a; Narayan et al., 2018) , story generation (Fan et al., 2018b , error correction (Chollampatt and Ng, 2018) , multilingual sentence embeddings (Artetxe and Schwenk, 2018) , and dialogue (Miller et al., 2017; Dinan et al., 2019) . ---------------------------------- **MACHINE TRANSLATION** We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_3",
  "x": "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 .",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_4",
  "x": "The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING**",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_5",
  "x": "FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and ---------------------------------- **ABSTRACTIVE DOCUMENT SUMMARIZATION** Next, we experiment with abstractive document summarization where we use a base Transformer to encode the input document and then generate a summary with a decoder network.",
  "y": "differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_6",
  "x": "For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_7",
  "x": "**LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and ---------------------------------- **ABSTRACTIVE DOCUMENT SUMMARIZATION** Next, we experiment with abstractive document summarization where we use a base Transformer to encode the input document and then generate a summary with a decoder network. We use the CNN-Dailymail dataset (Hermann et al., 2015; Nallapati et al., 2016) of news articles paired with multi-sentence summaries. We evaluate on Perplexity 31.9 J\u00f3zefowicz et al. (2016) 30.0 28.0 FAIRSEQ Adaptive inputs 23.0 the full-text version with no entity anonymization (See et al., 2017) ; we truncate articles to 400 tokens (See et al., 2017) . We use BPE with 30K operations to form our vocabulary following Fan et al. (2018a) .",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_0",
  "x": "However, it is still vague how the model transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_1",
  "x": "Multi-task training is an effective method to mitigate the data sparsity problem. It has recently been applied for crosslingual transfer learning for paradigm completion-the task of producing inflected forms of lemmata-with sequenceto-sequence networks. However, it is still vague how the model transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language).",
  "y": "background motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_2",
  "x": "Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg). In this work, we intend to shed light on the way cross-lingual transfer learning for paradigm completion with an encoder-decoder model works, and will especially focus on the role of the character and tag embeddings. In particular we aim at answering the following questions: (i) What does the neural model learn from the tags of a highresource language for the tags of a low-resource language? (ii) Is sharing an alphabet important for the transfer? (iii) How much of the transfer learning can be reduced to a regularization effect achieved by multi-task learning? For our analysis, we present a set of detailed experiments for the target language Spanish [ES] tion task in the low-resource language. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_3",
  "x": "Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_4",
  "x": "Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg). In this work, we intend to shed light on the way cross-lingual transfer learning for paradigm completion with an encoder-decoder model works, and will especially focus on the role of the character and tag embeddings.",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_5",
  "x": "In particular we aim at answering the following questions: (i) What does the neural model learn from the tags of a highresource language for the tags of a low-resource language? (ii) Is sharing an alphabet important for the transfer? (iii) How much of the transfer learning can be reduced to a regularization effect achieved by multi-task learning? For our analysis, we present a set of detailed experiments for the target language Spanish [ES] tion task in the low-resource language. ---------------------------------- **TRANSFER LEARNING FOR PARADIGM COMPLETION** In this section, we describe cross-lingual transfer learning for morphology and the model used for it. Cross-lingual transfer. Transfer learning for paradigm completion is much more languagespecific than most semantic natural language processing tasks, like entity typing or machine translation. An extreme example is the infeasible task of transferring morphological knowledge from Chinese to Portuguese as Chinese does not make use of inflection at all. Even between two morphologically rich languages transfer is difficult if they are unrelated, since inflections often mark dissimilar subcategories and word forms do not share similarities. However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_6",
  "x": "An extreme example is the infeasible task of transferring morphological knowledge from Chinese to Portuguese as Chinese does not make use of inflection at all. Even between two morphologically rich languages transfer is difficult if they are unrelated, since inflections often mark dissimilar subcategories and word forms do not share similarities. However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other. They make this possible by training an encoder-decoder model and appending a special tag (i.e., embedding) for each language to the input of the system, similar to (Johnson et al., 2016) . It is currently unclear, though, what the nature of this transfer is, motivating our work which explores this in more detail. Model description. The model<cite> Kann et al. (2017)</cite> use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) . It is trained on maximizing the following log-likelihood: We denote the source training examples as D s and the target training examples as D s . w s represents Figure 1 : Overview of an encoder-decoder RNN, mapping the Spanish lemma so\u00f1ar to the target form sue\u00f1a.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_7",
  "x": "**EXPLORATION OF TRANSFER LEARNING** In order to answer the questions raised in the introduction, we conduct the following experiments. ---------------------------------- **DATA** We use the Romance and Arabic language data from<cite> Kann et al. (2017)</cite> . In particular, each training file contains 12, 000 high-resource examples mixed with 50 or 200 fixed Spanish instances. We ---------------------------------- **EXPERIMENTS** Letter cipher (l-ciph).",
  "y": "uses"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_8",
  "x": "This explains the unexpected result that l-emb performs best for Arabic (200) and Portuguese (200): both source languages potentially confuse the language model; in Portuguese we contribute this to a big overlap of lemmata in the two languages with Portuguese often inflecting in a different way<cite> (Kann et al., 2017)</cite> . Further, the differences in performance between original and t-emb show that the model indeed learns information from the tags, supposedly which output sequence is more likely to appear with which tag. The l-emb-sep and t-emb-sep results show that a separation symbol clearly improves the model's performance. ---------------------------------- **RELATED WORK** Transfer learning with encoder-decoder networks. Encoder-decoder RNNs were introduced by Cho et al. (2014) and Sutskever et al. (2014) and extended by an attention mechanism by Bahdanau et al. (2015) . Lately, much work was done on multi-task learning and transfer learning with encoder-decoder RNNs. Luong et al. (2015) investigated multi-task setups for sequence-to-sequence learning, combining multiple encoders and decoders. In contrast, in our experiments, we use only one encoder and one decoder.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_9",
  "x": "Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> , in order to encourage the development of systems for the task. One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015) . Recently, neural sequenceto-sequence models are also widely used (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni and Goldberg, 2017; Zhou and Neubig, 2017) . All the above mentioned work were designed for one single language. ---------------------------------- **CONCLUSION** We conducted a set of experiments to explore the mechanisms behind cross-lingual transfer learning for morphological reinflection. Our findings indicate that knowledge about a language's typical character sequences and outputs for certain morphological tags can be transferred. In particular, this means that the effect cannot be reduced to sole regularization.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_0",
  "x": "Recent work (Liu, 2019; Zhang et al., 2019c ) demonstrates that it is highly beneficial for extractive summarization models to incorporate pretrained language models (LMs) such as BERT (Devlin et al., 2019) into their architectures. However, the performance improvement from the pretrained LMs is known to be relatively small in case of abstractive summarization (Zhang et al., 2019a; Hoang et al., 2019) . This discrepancy may be due to the difference between extractive and abstractive approaches in ways of dealing with the taskthe former classifies whether each sentence to be included in a summary, while the latter generates a whole summary from scratch. In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_1",
  "x": "However, the performance improvement from the pretrained LMs is known to be relatively small in case of abstractive summarization (Zhang et al., 2019a; Hoang et al., 2019) . This discrepancy may be due to the difference between extractive and abstractive approaches in ways of dealing with the taskthe former classifies whether each sentence to be included in a summary, while the latter generates a whole summary from scratch. In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences. To resolve the problem, we present a novel neural extractor exploiting the pre-trained LMs (BERT in this work) which are expected to perform better according to the recent studies (Liu, arXiv:1909 .08752v3 [cs.CL] 26 Sep 2019 2019 Zhang et al., 2019c) .",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_2",
  "x": "However, the performance improvement from the pretrained LMs is known to be relatively small in case of abstractive summarization (Zhang et al., 2019a; Hoang et al., 2019) . This discrepancy may be due to the difference between extractive and abstractive approaches in ways of dealing with the taskthe former classifies whether each sentence to be included in a summary, while the latter generates a whole summary from scratch. In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences. To resolve the problem, we present a novel neural extractor exploiting the pre-trained LMs (BERT in this work) which are expected to perform better according to the recent studies (Liu, arXiv:1909 .08752v3 [cs.CL] 26 Sep 2019 2019 Zhang et al., 2019c) .",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_3",
  "x": "**BACKGROUND** ---------------------------------- **SENTENCE REWRITING** In this paper, we focus on single-document multisentence summarization and propose a neural abstractive model based on the Sentence Rewriting framework<cite> (Chen and Bansal, 2018</cite>; Xu and Dur-rett, 2019) which consists of two parts: a neural network for the extractor and another network for the abstractor. The extractor network is designed to extract salient sentences from a source article. The abstractor network rewrites the extracted sentences into a short summary. ---------------------------------- **LEARNING SENTENCE SELECTION** The most common way to train extractor to select informative sentences is building extractive oracles as gold targets, and training with crossentropy (CE) loss. An oracle consists of a set of sentences with the highest possible ROUGE scores.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_4",
  "x": "For example, for a consecutive sequence of sentences s 1 , s 2 , s 3 , s 4 , s 5 , we assign E A , E B , E A , E B , E A in order. All the words in each sentence are assigned to the same segment embedding, i.e. segment embeddings for w 11 , w 12 , \u00b7 \u00b7 \u00b7 , w 1m is E A , E A , \u00b7 \u00b7 \u00b7 , E A . An illustration for this procedure is shown in Figure 1 . ---------------------------------- **SENTENCE SELECTION** We use LSTM Pointer Network (Vinyals et al., 2015) as the decoder to select the extracted sentences based on the above sentence representations. The decoder extracts sentences recurrently, producing a distribution over all of the remaining sentence representations excluding those already selected. Since we use the sequential model which selects one sentence at a time step, our decoder can consider the previously selected sentences. This property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already. As the decoder structure is almost the same with the previous work, we convey the equations of<cite> Chen and Bansal (2018)</cite> to avoid confusion, with minor modifications to agree with our notations.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_5",
  "x": "---------------------------------- **ABSTRACTOR NETWORK** The abstractor network approximates f , which compresses and paraphrases an extracted document sentence to a concise summary sentence. We use the standard attention based sequence-tosequence (seq2seq) model (Bahdanau et al., 2015; Luong et al., 2015) with the copying mechanism (See et al., 2017) for handling out-of-vocabulary (OOV) words. Our abstractor is practically identical to the one proposed in<cite> Chen and Bansal (2018)</cite> . ---------------------------------- **TRAINING** In our model, an extractor selects a series of sentences, and then an abstractor paraphrases them. As they work in different ways, we need different training strategies suitable for each of them. Training the abstractor is relatively obvious; maximizing log-likelihood for the next word given the previous ground truth words.",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_6",
  "x": "In addition, thus this procedure contains sampling or maximum selection, the extractor performs a nondifferentiable extraction. Lastly, although our goal is maximizing ROUGE scores, neural models cannot be trained directly by maximum likelihood estimation from them. To address those issues above, we apply standard policy gradient methods, and we propose a novel training procedure for extractor which guides to the optimal policy in terms of the summary-level ROUGE. As usual in RL for sequence prediction, we pre-train submodules and apply RL to fine-tune the extractor. ---------------------------------- **TRAINING SUBMODULES** Extractor Pre-training Starting from a poor random policy makes it difficult to train the extractor agent to converge towards the optimal policy. Thus, we pre-train the network using cross entropy (CE) loss like previous work (Bahdanau et al., 2017;<cite> Chen and Bansal, 2018)</cite> . However, there is no gold label for extractive summarization in most of the summarization datasets. Hence, we employ a greedy approach (Nallapati et al., 2017) to make the extractive oracles, where we add one sentence at a time incrementally to the summary, such that the ROUGE score of the current set of selected sentences is maximized for the entire ground truth summary.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_7",
  "x": "Additionally, we add 'stop' action to the action space, by concatenating trainable parameters h stop (the same dimension as h i ) to H. The agent treats it as another candidate to extract. When it selects 'stop', an extracting episode ends and the final return is given. This encourages the model to extract additional sentences only when they are expected to increase the final return. Following<cite> Chen and Bansal (2018)</cite> , we use the Advantage Actor Critic (Mnih et al., 2016) method to train. We add a critic network to estimate a value function V t (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ), which then is used to compute advantage of each action (we will omit the current state (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ) to simplify): where Q t (s i ) is the expected future reward for selecting s i at the current step t. We maximize this advantage with the policy gradient with the where \u03b8 \u03c0 is the trainable parameters of the actor network (original extractor). And the critic is trained to minimize the square loss: where \u03b8 \u03c8 is the trainable parameters of the critic network. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_8",
  "x": "We evaluate the performance of our method using different variants of ROUGE metric computed with respect to the gold summaries. On the CNN/Daily Mail and DUC-2002 dataset, we use standard ROUGE-1, ROUGE-2, and ROUGE- L (Lin, 2004) on full length F 1 with stemming as previous work did (Nallapati et al., 2017; See et al., 2017;<cite> Chen and Bansal, 2018)</cite> . On NYT50 dataset, following Durrett et al. (2016) and Paulus et al. (2018) , we used the limited length ROUGE recall metric, truncating the generated summary to the length of the ground truth summary. Table 1 shows the experimental results on CNN/Daily Mail dataset, with extractive models in the top block and abstractive models in the bottom block. For comparison, we list the performance of many recent approaches with ours. ---------------------------------- **RESULTS** ---------------------------------- **CNN/DAILY MAIL** Extractive Summarization As See et al. (2017) showed, the first 3 sentences (lead-3) in an article form a strong summarization baseline in CNN/Daily Mail dataset.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_9",
  "x": "Extractive Summarization As See et al. (2017) showed, the first 3 sentences (lead-3) in an article form a strong summarization baseline in CNN/Daily Mail dataset. Therefore, the very first objective of extractive models is to outperform the simple method which always returns 3 or 4 sentences at the top. However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins. Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_10",
  "x": "Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences. Our best abstractive model (BERT-ext + abs + RL + rerank) achieves the new state-of-theart performance for abstractive summarization in terms of average ROUGE score, with large margins on ROUGE-L. However, we empirically found that the reranking method has no effect or has negative effect on NYT50 or DUC-2002 dataset.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_11",
  "x": "However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins. Adding reinforcement learning (BERT-ext + RL) gives higher performance, which is competitive with other extractive approaches using pretrained Transformers (see Table 1 ). This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_12",
  "x": "The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences. Our best abstractive model (BERT-ext + abs + RL + rerank) achieves the new state-of-theart performance for abstractive summarization in terms of average ROUGE score, with large margins on ROUGE-L. However, we empirically found that the reranking method has no effect or has negative effect on NYT50 or DUC-2002 dataset. Hence, we don't apply it for the remaining datasets.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_13",
  "x": "This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences. Our best abstractive model (BERT-ext + abs + RL + rerank) achieves the new state-of-theart performance for abstractive summarization in terms of average ROUGE score, with large margins on ROUGE-L.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_14",
  "x": "Sentence-matching finds sentences with the highest ROUGE-L score for each sentence in the gold summary. This search method matches with the best reward from<cite> Chen and Bansal (2018)</cite> . Greedy Search is the same method explained for extractor pre-training in section 4.1. ---------------------------------- **COMBINATION SEARCH SELECTS A SET OF SENTENCES** ---------------------------------- **MODELS** Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences. Due to time constraints, we limited the maximum number of sentences to 5. This method corresponds to our final return in RL training.",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_15",
  "x": "Greedy Search is the same method explained for extractor pre-training in section 4.1. ---------------------------------- **COMBINATION SEARCH SELECTS A SET OF SENTENCES** ---------------------------------- **MODELS** Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences. Due to time constraints, we limited the maximum number of sentences to 5. This method corresponds to our final return in RL training. Table 3 shows the summary-level ROUGE scores of previously explained methods. We see considerable gaps between Sentence-matching and Greedy Search, while the scores of Greedy Search are close to those of Combination Search. Note that since we limited the number of sentences for Combination Search, the exact scores for it would be higher.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_16",
  "x": "Table 3 shows the summary-level ROUGE scores of previously explained methods. We see considerable gaps between Sentence-matching and Greedy Search, while the scores of Greedy Search are close to those of Combination Search. Note that since we limited the number of sentences for Combination Search, the exact scores for it would be higher. The scores can be interpreted to be upper bounds for corresponding training methods. This result supports our training strategy; pretraining with Greedy Search and final optimization with the combinatorial return. Additionally, we experiment to verify the contribution of our training method. We train the same model with different training signals; Sentencelevel reward from<cite> Chen and Bansal (2018)</cite> and combinatorial reward from ours. The results are shown in Table 4 . Both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence-level reward. Human Evaluation We also conduct human evaluation to ensure robustness of our training procedure. We measure relevance and readability of the summaries.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_17",
  "x": "To evaluate both these criteria, we design a Amazon Mechanical Turk experiment based on ranking method, inspired by Kiritchenko and Mohammad (2017) . We randomly select 20 samples from the CNN/Daily Mail test set and ask the human testers (3 for each sample) to rank summaries (for relevance and readability) produced by 3 different models: our final model, that of<cite> Chen and Bansal (2018)</cite> and that of Liu (2019) . 2, 1 and 0 points were given according to the ranking. R-1 R-2 R-L Extractive First sentences (Durrett et al., 2016) 28.60 17.30 -First k words (Durrett et al., 2016) 35.70 21.60 -Full (Durrett et al., 2016) 42.20 24.90 -BERTSUM (Liu, 2019) 46.66 26.35 42.62 Abstractive Deep Reinforced (Paulus et al., 2018) 42.94 26.02 -Two-Stage BERT (Zhang et al., 2019a) The models were anonymized and randomly shuffled. Following previous work, the input article and ground truth summaries are also shown to the human participants in addition to the three model summaries. From the results shown in Table 5 , we can see that our model is better in relevance compared to others. In terms of readability, there was no noticeable difference. Table 6 gives the results on NYT50 dataset. We see our BERT-ext + abs + RL outperforms all the extractive and abstractive models, except ROUGE-1 from Liu (2019) . Comparing with two recent models that adapted BERT on their summarization models (Liu, 2019; Zhang et al., 2019a) , we can say that we proposed another method successfully leveraging BERT for summarization.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_18",
  "x": "**DUC-2002** We also evaluated the models trained on the CNN/Daily Mail dataset on the out-of-domain DUC-2002 test set as shown in Table 7 . BERText + abs + RL outperforms baseline models with large margins on all of the ROUGE scores. This result shows that our model generalizes better. There has been a variety of deep neural network models for abstractive document summarization. One of the most dominant structures is the sequence-to-sequence (seq2seq) models with attention mechanism (Rush et al., 2015; Nallapati et al., 2016) . See et al. (2017) introduced Pointer Generator network that implicitly combines the abstraction with the extraction, using copy mechanism (Gu et al., 2016; Zeng et al., 2016) . More recently, there have been several studies that have attempted to improve the performance of the abstractive summarization by explicitly combining them with extractive models. Some notable examples include the use of inconsistency loss (Hsu et al., 2018) , key phrase extraction (Li et al., 2018; Gehrmann et al., 2018) , and sentence extraction with rewriting<cite> (Chen and Bansal, 2018)</cite> . Our model improves Sentence Rewriting with BERT as an extractor and summary-level rewards to optimize the extractor.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_0",
  "x": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_1",
  "x": "Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; , others consider more shallow forms of representation (Carreras and M\u00e0rquez, 2005; Liang et al., 2009) . However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010) , motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009) ). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007) . The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions \"Steelers\" and \"the Pittsburgh team\" to the same semantic class Steelers, and group expressions \"defeated\" and \"secured the victory over\". Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of syntactic and lexical realization relying instead on the induced semantic representation.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_2",
  "x": "While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions \"Steelers\" and \"the Pittsburgh team\" to the same semantic class Steelers, and group expressions \"defeated\" and \"secured the victory over\". Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of syntactic and lexical realization relying instead on the induced semantic representation. For example, the two sentences in Figure 1 have identical semantic representation, and therefore can be hypothesized to be equivalent. From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010) . In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S\u00e9aghdha, 2010) , and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3.",
  "y": "differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_3",
  "x": "Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_4",
  "x": "Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_5",
  "x": "Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In both cases, we follow <cite>(Poon and Domingos, 2009</cite> ) in using the corpus of biomedical abstracts. Our model achieves favorable results significantly outperforming the baselines, including state-of-theart methods for relation extraction, and achieves scores comparable to those of the MLN model.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_6",
  "x": "---------------------------------- **SEMANTIC PARSING** In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and assumptions relevant to our model. Unlike <cite>(Poon and Domingos, 2009</cite> ), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002) . The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in<cite> (Poon and Domingos, 2009)</cite> . In frame semantics, the meaning of a predicate is conveyed by a frame, a structure of related concepts that describes a situation, its participants and properties (Fillmore et al., 2003) . Each frame is characterized by a set of semantic roles (frame elements) corresponding to the arguments of the predicate. It is evoked by a frame evoking element (a predicate). The same frame can be evoked by different but semantically similar predicates: for example, both verbs \"buy\" and \"purchase\" evoke frame Commerce buy in FrameNet (Fillmore et al., 2003) .",
  "y": "background differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_7",
  "x": "Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does not explicitly assume contiguity of lexical items and does not make this directionality assumption but their clustering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in <cite>(Poon and Domingos, 2009</cite> ), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009) ), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_8",
  "x": "It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009) ), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. ---------------------------------- **RELATION TO THE MLN APPROACH** The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_9",
  "x": "The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive. The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sentences, they would require marginalization over semantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible. In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm. The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to fragments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more natural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models.",
  "y": "motivation background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_10",
  "x": "The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sentences, they would require marginalization over semantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible. In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm. The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to fragments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more natural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models. Our approach makes one step in this direction by proposing a non-parametric version of such generative model. ---------------------------------- **HIERARCHICAL PITMAN-YOR PROCESSES**",
  "y": "differences motivation background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_11",
  "x": "So, instead of selecting a word occurrence uniformly, each occurrence of every word w 2 is weighted by its similarity to w 1 , where the similarity is based on the cosine distance. As the moves are dependent only on syntactic representations, all the proposal distributions can be computed once at the initialization stage. 4 ---------------------------------- **EMPIRICAL EVALUATION** We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003) , a dataset of 1999 biomedical abstracts, and a set of questions produced by<cite> (Poon and Domingos, 2009)</cite> . A example question is shown in Figure 3 . All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_12",
  "x": "All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%. Let us examine some of the induced semantic classes (Table 1) realizations have a clear semantic connection. Cluster 6, for example, clusters lymphocytes with the exception of thymocyte, a type of cell which generates T cells. Cluster 8 contains verbs roughly corresponding to Cause change of position on a scale frame in FrameNet. Verbs in class 9 are used in the context of providing support for a finding or an action, and many of them are listed as evoking elements for the Evidence frame in FrameNet. Argument types of the induced classes also show a tendency to correspond to semantic roles. For example, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from. The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, . . . ) . We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in <cite>(Poon and Domingos, 2009</cite> ).",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_13",
  "x": "by overly coarse clustering corresponding to just 3 classes, namely, 30%, 25% and 20% of errors are due to the clusters 6, 8 and 12 (Figure 1) , respectively. Though all these clusters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. ---------------------------------- **RELATED WORK** There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; ), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model <cite>(Poon and Domingos, 2009</cite> ), another unsupervised method has been proposed in (Goldwasser et al., 2011) . In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006) , however, they do not attempt to discover frames and deal only with isolated predicates.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_0",
  "x": "Concordances are probably the most simple scheme to examine contextual semantic effects, but leave semantic inferences entirely to the human observer. A more complex layer is reached with collocations which can be identified automatically via statistical word co-occurrence metrics (Manning and Sch\u00fctze, 1999; Wermter and Hahn, 2006) , two of which are incorporated in JESEME as well: Positive pointwise mutual information (PPMI), developed by Bullinaria and Levy (2007) as an improvement over the probability ratio of normal pointwise mutual information (PMI; Church and Hanks (1990) ) and Pearson's \u03c7 2 , commonly used for testing the association between categorical variables (e.g., POS tags) and considered to be more robust than PMI when facing sparse information (Manning and Sch\u00fctze, 1999) . The currently most sophisticated and most influential approach to distributional semantics employs word embeddings, i.e., low (usually 300-500) dimensional vector word representations of both semantic and syntactic information. Alternative approaches are e.g., graph-based algorithms (Biemann and Riedl, 2013) or ranking functions from information retrieval (Claveau et al., 2014) . The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (Mikolov et al., 2013) . The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words Hahn, 2016a, 2017) . Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990) ) are not affected by this problem. Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_1",
  "x": "Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990) ) are not affected by this problem. Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) . ---------------------------------- **AUTOMATIC DIACHRONIC SEMANTICS** The use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_2",
  "x": "Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) . ---------------------------------- **AUTOMATIC DIACHRONIC SEMANTICS** The use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_3",
  "x": "We calculated PPMI and \u03c7 2 for each slice, with a context window of 4 words, no random sampling, context distribution smoothing of 0.75 for PPMI, and corpus dependent minimum word frequency thresholds of 50 (COHA, DTA and RSC) respectively 100 (GB subcorpora). 7 The PPMI matrices were then used to create SVD PPMI embeddings with 500 dimensions. These calculations were performed with a modified version of HY-PERWORDS 8 (Levy et al., 2015) , using custom extensions for faster pre-processing and \u03c7 2 . The resulting models have a size of 32 GB and are available for download on JESEME's Help page. 9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and \u03c7 2 . These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space. Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus. In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and <cite>Hamilton et al. (2016</cite> words above the minimum frequency threshold used during PPMI and \u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices. Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus. 5 Website and API JESEME provides both an interactive website and an API for querying the underlying database.",
  "y": "similarities uses"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_4",
  "x": "In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and <cite>Hamilton et al. (2016</cite> words above the minimum frequency threshold used during PPMI and \u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices. Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus. 5 Website and API JESEME provides both an interactive website and an API for querying the underlying database. Both are implemented with the SPARK 11 framework running inside a JETTY 12 Web server. On JESEME's initial landing page, users can enter a word into a search field and select a corpus. They are then redirected to the result page, as depicted in Figure 2 . Query words are automatically lowercased or lemmatized, depending on the respective corpus (see Section 4). The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency. Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time. We follow Kim et al. (2014) in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_6",
  "x": "API calls need to specify the corpus to be searched and one (frequency) or two (similarity, context) words as GET parameters. 13 Calling conventions are further detailed on JESEME's Help page. 14 13 For example http://jeseme.org/api/ similarity?word1=Tag&word2=Nacht&corpus= dta 14 http://jeseme.org/help.html#api ---------------------------------- **CONCLUSION** We presented JESEME, the Jena Semantic Explorer, an interactive website and REST API for exploring changes in lexical semantics over long periods of time. In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (Levy et al., 2015;<cite> Hamilton et al., 2016</cite>; Hahn, 2016a, 2017) and provides access to five popular corpora for the English and German language. JESEME is also the first tool of its kind and under continuous development. Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and provide optional stemming routines. Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
  "y": "future_work"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_0",
  "x": "**INTRODUCTION** Acoustic event detection (AED), the task of detecting whether certain events occur in an audio clip, can be applied in many industry applications [1, 2, 3, 23] . The accuracy of AED models have been increased in a large scale in recent years based on deep learning approaches. However, to ensure high performance, those models are of large scale computation and memory intense, which makes it a challenge to deploy for real industrial applications with limited computation resources and memory. Our paper is focused on increasing the computation efficiency for AED models while maintaining their accuracies, so that AED deployment for resource-constraint industrial applications is feasible. Compression of neural networks has been explored in broad context. We focus on two widely used and effective methods for deep models: low-rank matrix factorization and and quantization. Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_1",
  "x": "We focus on two widely used and effective methods for deep models: low-rank matrix factorization and and quantization. Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware. For the quantization approach, It is important to fine-tune models with quantized weights to reduce the performance loss with quantized networks. Here we refer quantization with fine-turning as quantization training. 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada. ---------------------------------- **METHODS** We start by formulating the multi-class audio event detection problem.",
  "y": "background uses"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_2",
  "x": "**METHODS** We start by formulating the multi-class audio event detection problem. Given an audio signal I (e.g. log mel-filter bank energies (LFBEs)), the task is to train a model f to predict a multi-hot vector y \u2208 {0, 1} C , with C being the size of event set E and y c being a binary indicator whether event c is present in I. We denote D L = {(I, y)} as the labeled training dataset. Model f is trained using cross-entropy loss: L = \u2212 (I,y)\u2208DL C c=1 {w c y c log f c (I) + (1 \u2212 y c ) log(1 \u2212 f c (I))}, where w c is the penalty of positive mis-classification of class c. Specifically we focus on the RNN-based model in this paper. Compared to CNNs, it is more memory efficient and easier to deploy on devices with constrained resources. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32). as formulated in 2. Note the scaling factor \u03b1 and minimum value \u03b2 in equation 2 are not quantized.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_3",
  "x": "---------------------------------- **METHODS** We start by formulating the multi-class audio event detection problem. Given an audio signal I (e.g. log mel-filter bank energies (LFBEs)), the task is to train a model f to predict a multi-hot vector y \u2208 {0, 1} C , with C being the size of event set E and y c being a binary indicator whether event c is present in I. We denote D L = {(I, y)} as the labeled training dataset. Model f is trained using cross-entropy loss: L = \u2212 (I,y)\u2208DL C c=1 {w c y c log f c (I) + (1 \u2212 y c ) log(1 \u2212 f c (I))}, where w c is the penalty of positive mis-classification of class c. Specifically we focus on the RNN-based model in this paper. Compared to CNNs, it is more memory efficient and easier to deploy on devices with constrained resources. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32). as formulated in 2.",
  "y": "background uses"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_4",
  "x": "We compute area under curve (AUC) and equal error rate (EER) as the two quantitative measures. As the distribution of weight matrices' eigenvalues can be different across different LSTM layers, we follow the practice of <cite>[11]</cite> to set the same threshold \u03c4 across layers as the fraction of retained singular values, defined as \u03c4 = Table 1 summarizes the results of low-rank matrix factorization compared to our baseline 3-layer LSTM. There is no accuracy degradation when \u03c4 is reduced to 0.6, which we hypothesize to be related to the regularizing effects. Table 2 summarizes the results with quantization compared to our baseline. Post-mortem (PM) refers to the case that quantization is only applied during inference, while quantization training (QT) refers to the case model fine-tuning is further performed on quantized weights. Our quantization training approach outperforms PM significantly for the 4-bit quantization case. We also note the simple PM quantization preserves the accuracy well (3.0% drop in AUC and 2.7% drop in EER) with 8-bit quantization. Finally, we combine both low-rank matrix factorization and quantization training. Its results are summarized in table 3. The attained singular value ratio is fixed to be 0.6, as parameter size and accuracy is well balanced at this point according to table 1.",
  "y": "uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_0",
  "x": "**INTRODUCTION** Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers. In order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by Ganin and Lempitsky [12] : Untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain-agnostic, i.e. which does not depend on whether the input data comes from a source speaker or a target speaker. We evaluate our system on a subset of the GRID corpus [13] , which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method.",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_1",
  "x": "Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial. ---------------------------------- **INTRODUCTION** Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_2",
  "x": "On multi-speaker training setups, the accuracy improvements are smaller but still substantial. ---------------------------------- **INTRODUCTION** Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_3",
  "x": "For a comprehensive review see [33] . Neural networks have early been applied to the Lipreading task [34] , however, they have become widespread only in recent years, with the advent of state-of-the-art learning techniques (and the necessary hardware). The first deep neural network for lipreading was a seven-layer convolutional net as a preprocessing stage for an HMM-based word recognizer [5] . Since then, several end-to-end trainable systems were presented<cite> [7,</cite> 8, 9] . The current state-of-the-art accuracy on the GRID corpus is 3.3% error [9] using a very large set of additional training data; so their result is not directly comparable to ours. In domain adaptation, it is assumed that a learning task exhibits a domain shift between the training (or source) and test (or target) data. This can be mitigated in several ways [35] ; we apply domain-adversarial training [12] , where an intermediate layer in a multi-layer network is driven to learn a representation of the input data which is optimized to be domain-agnostic, i.e. to make it difficult to detect whether an input sample is from the source or the target domain. A great advantage of this approach is the end-to-end trainability of the entire system. For a summary of further approaches to domain adaptation with neural networks, we refer to the excellent overview in [12] . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_4",
  "x": "We follow the data preprocessing protocol from <cite>[7]</cite> . We use the GRID corpus [13] , which consists of video and audio recordings of 34 speakers (which we name s1 to s34) saying 1000 sentences each. All sentences have a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \"Place red at J 2, please\", where the number of alternative words is given in parentheses. There are 51 distinct words; alternatives are randomly distributed so that context cannot be used for classification. Each sentence has a length of 3 seconds at 25 frames per second, so the total data per speaker is 3000 seconds (50 minutes). Using the annotations contained in the corpus, we segmented all videos at word level, yielding 6000 word samples per speaker. We experiment on speakers s1-s19: speakers 1-9 form the development speakers, used to determine optimal parameters; speakers 10-19 are the evaluation speakers, held back until the final evaluation of the systems. The data from each speaker was randomly subdivided into training, validation, and test sets, where the latter two contain five samples of each word, i.e. a total of 51 \u00b7 5 = 255 samples each. The training data is consequently highly unbalanced: For example, each letter from \"a\" to \"z\" appears 30 times, whereas each color appears 240 times. We converted the \"normal\" quality videos (360 \u00d7 288 pixels) to greyscale and extracted 40\u00d740 pixel windows containing the mouth area, as described in <cite>[7]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_5",
  "x": "We follow the data preprocessing protocol from <cite>[7]</cite> . We use the GRID corpus [13] , which consists of video and audio recordings of 34 speakers (which we name s1 to s34) saying 1000 sentences each. All sentences have a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \"Place red at J 2, please\", where the number of alternative words is given in parentheses. There are 51 distinct words; alternatives are randomly distributed so that context cannot be used for classification. Each sentence has a length of 3 seconds at 25 frames per second, so the total data per speaker is 3000 seconds (50 minutes). Using the annotations contained in the corpus, we segmented all videos at word level, yielding 6000 word samples per speaker. We experiment on speakers s1-s19: speakers 1-9 form the development speakers, used to determine optimal parameters; speakers 10-19 are the evaluation speakers, held back until the final evaluation of the systems. The data from each speaker was randomly subdivided into training, validation, and test sets, where the latter two contain five samples of each word, i.e. a total of 51 \u00b7 5 = 255 samples each. The training data is consequently highly unbalanced: For example, each letter from \"a\" to \"z\" appears 30 times, whereas each color appears 240 times. We converted the \"normal\" quality videos (360 \u00d7 288 pixels) to greyscale and extracted 40\u00d740 pixel windows containing the mouth area, as described in <cite>[7]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_6",
  "x": "Speakers are chosen consecutively, for example, the experiments on four training speakers on the development data are (s1 . . . s4) \u2192 s5, (s2 . . . s5) \u2192 s6, \u00b7 \u00b7 \u00b7 , (s9, s1, s2, s3) \u2192 s4, where \u2192 separates source and target speakers. We also compute baseline results on single speakers. The data sets of each speaker are used as follows: Training data is used for supervised training (on the source speakers) and unsupervised adaptation (on the target speaker). Validation data is used for early stopping, the network is evaluated on the test data. ---------------------------------- **METHODS AND SYSTEM SETUP** The system is based on the lipreading setup from <cite>[7]</cite> , reimplemented in Tensorflow [36] . Raw 40 \u00d7 40 lip images are used as input data, without any further preprocessing except normalization. We stack several fully connected feedforward layers, optionally followed by Dropout [37] , and one LSTM recurrent layer to form a network which is capable of recognizing sequential video data.",
  "y": "extends differences"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_7",
  "x": "Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> . ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> . We run the lipreader as a single-speaker system with different topologies, optionally using Dropout (always with 50% dropout ratio) to avoid overfitting the training set. Adversarial training is not used (i.e. the weight in figure 2 is set to zero).",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_8",
  "x": "Figure 2 shows a graphical overview of the system: The joint part is at the top, at the bottom are word classifier (left) and speaker classifier (right). Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> . ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> . We run the lipreader as a single-speaker system with different topologies, optionally using Dropout (always with 50% dropout ratio) to avoid overfitting the training set.",
  "y": "similarities uses"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_0",
  "x": "---------------------------------- **INTRODUCTION** The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL). While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al., 2006) and prepositions (Gamon et al., 2008) , (Felice and Pullman, 2009) , there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater's judgments then become the gold standard for evaluating a system. So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts would obviously increase both considerably. This is especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions. 1 <cite>(Tetreault and Chodorow, 2008b)</cite> challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_1",
  "x": "In these cases, a handful of untrained AMT workers (or Turkers) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive. Given the success of using AMT in other areas of NLP, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections. The presence of a gold standard in the above papers is crucial. In fact, the usability of AMT for text annotation has been demostrated in those studies by showing that non-experts' annotation converges to the gold standard developed by expert annotators. However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained 46 raters? 2. If so, how many raters does it take to match trained raters? In the experiment, a Turker was presented with a sentence from Microsoft's Encarta encyclopedia, with one preposition in that sentence replaced with a blank.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_2",
  "x": "However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained 46 raters? 2. If so, how many raters does it take to match trained raters? In the experiment, a Turker was presented with a sentence from Microsoft's Encarta encyclopedia, with one preposition in that sentence replaced with a blank. There were 194 HITs (sentences) in all, and we requested 10 Turker judgments per HIT. Some Turkers did only one HIT, while others completed more than 100, though none did all 194. The Turkers' performance was analyzed by comparing their responses to those of two trained annotators and to the Encarta writer's preposition, which was considered the gold standard in this task. Comparing each trained annotator to the writer yielded a kappa of 0.822 and 0.778, and the two raters had a kappa of 0.742.",
  "y": "background extends"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_3",
  "x": "To examine the reliability of Turker preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were in-structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment. The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, <cite>(Tetreault and Chodorow, 2008b)</cite> found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. Among the trained annotators, inter-kappa agreement ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Figure 2 , kappa is shown for the comparisons of Turker responses to each annotator for samples of various sizes ranging from N = 1 to N = 18. At sample size N = 13, the average kappa is 0.608, virtually identical to the mean found among the trained annotators. ---------------------------------- **RETHINKING EVALUATION**",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_0",
  "x": "For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> . In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news articles have been widely used in studies on English readability and text simplification (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Feng et al., 2010) , there are no comparable resources for German. This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets. To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by <cite>Hancke et al. (2012)</cite> , but double in size. 1 The longstanding success of these outlets with their target audiences provides some external validity to the nature of the implicit linguistic adaptation of the language used. As showed for German secondary school textbooks, this is not necessarily the case across all linguistic dimensions and adjustments may even be limited to only the surface level of text, sentence, and word length. We conducted a series of analyses on these two data sets to accomplish the following objectives: 1. Investigate how instances of German educational news language differ in terms of language complexity across adult and child target audiences. 2. Build a binary readability model for German educational language targeting adults and children that shows high, robust classification performance across corpora. For the purposes of our studies, we operationalize child target audience of German educational news language as children aged between 8 and 14.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_1",
  "x": "---------------------------------- **INTRODUCTION** Readability assessment refers to the task of (automatically) linking a text to the appropriate target audience based on its complexity. A diverse spectrum of potential application domains has been identified for this task in the literature, ranging from the design and evaluation of education materials, to information retrieval, and text simplification. Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010) . For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> . In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news articles have been widely used in studies on English readability and text simplification (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Feng et al., 2010) , there are no comparable resources for German. This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets. To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by <cite>Hancke et al. (2012)</cite> , but double in size.",
  "y": "extends"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_2",
  "x": "In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> , but also Russian (Reynolds, 2016) or French (Fran\u00e7ois and Fairon, 2012) . The readability checker DeLite of Vor der Br\u00fcck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German. The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings. However, due to the specific nature of the data, the robustness of the approach across genres is unclear. Municipal administration language is so particular that results are unlikely to generalize to educational or literary materials, which are more attractive in first and second language acquisition contexts. Later work by <cite>Hancke et al. (2012)</cite> also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite. They investigate readability of educational magazines on the GEO/GEOlino data set, which they compiled from online articles freely available at the GEO magazine's web page. Their work illustrates the relevance of rich linguistic modeling for readability assessment and in particular the value of morphological complexity features for German. The latest large scale research endeavor for the assessment of German text readability has focused more on identifying linguistic differences between texts targeting different audiences than on building readability models: In the Reading Demands project, complexity differences in German secondary school book texts across grade levels and school types were investigated. and analyze to which extent publishers successfully adapt their reading material to their target audiences.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_3",
  "x": "The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings. However, due to the specific nature of the data, the robustness of the approach across genres is unclear. Municipal administration language is so particular that results are unlikely to generalize to educational or literary materials, which are more attractive in first and second language acquisition contexts. Later work by <cite>Hancke et al. (2012)</cite> also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite. They investigate readability of educational magazines on the GEO/GEOlino data set, which they compiled from online articles freely available at the GEO magazine's web page. Their work illustrates the relevance of rich linguistic modeling for readability assessment and in particular the value of morphological complexity features for German. The latest large scale research endeavor for the assessment of German text readability has focused more on identifying linguistic differences between texts targeting different audiences than on building readability models: In the Reading Demands project, complexity differences in German secondary school book texts across grade levels and school types were investigated. and analyze to which extent publishers successfully adapt their reading material to their target audiences. They find a lack of consistent adaptation for passive constructions, concessive and adversative connectives, and relative clauses, and only some limited adaptation in terms of lexical variation, noun complexity, and dependency length measures. ---------------------------------- **DATA SETS**",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_4",
  "x": "<cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words. We further cleaned our data from crawling artifacts and performed near-duplicate detection with the Simhash algorithm. We then grouped all texts into topic categories based on the subdomains they were published under, following the web page topic structure. 4 Table 1 shows the composition of the corpus in terms of the topic groups. Since the number of documents in the different topic groups differ between GEO and the smaller GEOlino set, we created a more balanced subset (GEO/GEOlino S ). For this, we included only topic categories existing in both GEO and GEOlino, included all GEOlino texts in those categories and sampled from the GEO texts in those categories until we reached the same overall size of 2480 texts each. Table 1 : Distribution of topics in the full and sampled GEO/GEOlino data set. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_5",
  "x": "They find a lack of consistent adaptation for passive constructions, concessive and adversative connectives, and relative clauses, and only some limited adaptation in terms of lexical variation, noun complexity, and dependency length measures. ---------------------------------- **DATA SETS** ---------------------------------- **GEO/GEOLINO** The GEO/GEOlino data set consists of online articles from one of the leading German monthly educational magazines, GEO, and the counterpart for children, GEOlino. 3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words.",
  "y": "uses"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_6",
  "x": "For this, we train two binary Sequential Minimal Optimization (SMO) support vector classifier (Platt, 1998) with linear kernels using the WEKA machine learning toolkit (Hall et al., 2009 ). Each model is tested i) on the same corpus it is trained on, using 10-folds cross-validation, and afterwards ii) on the other data set for cross-corpus testing after training on the full data set. For model performance evaluation, we report classification accuracy and the classification confusion matrices, and random baselines as reference point. Table 6 shows the accuracy of our SMO models on both data sets and compares them with a random baseline. Both models clearly outperform the baseline of 50.0%. On GEO/GEOlino S , the performance is comparable to the performance observed by <cite>Hancke et al. (2012)</cite> on the original GEO/GEOlino data. 11 As Table 7a shows, erroneous classifications are roughly balanced across both classes, showing that the model does not prefer one class over the other. When training a model using only the 20 most informative measures identified in Study 1, we reach an accuracy of 85.1%, i.e., the additional measures only account only for 3.3%. 12 When testing the models on the Tagesschau/Logo corpus, accuracy increases to 98.8% for both models. The confusion matrix for the model using 400 measures in Table 7b Table 7 : Confusion matrices for testing models with 400 features trained on GEO/GEOlino S .",
  "y": "similarities"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_7",
  "x": "We presented a study of the difference between German targeting adults and children, as far as we know the most broadly based linguistic complexity analysis to date. We created and analyzed a novel data set compiled from German news subtitles that consists of news broadcasts for adults and children from the same days, ensuring a relatively parallel selection of topics. We compared this with a newly compiled GEO/GEOlino corpus consisting of online articles of two magazines for adults and children by the same publisher discussing the similar topics. Based on these two data sets, we presented within-corpus (10-fold CV) and cross-corpus experiments and built binary classification models of German educational media text readability that perform with very high accuracy across both data sets. The model is based on a broad range of features that are highly informative for both data sets. This model is a valuable contribution since i) it is based on a considerably broader data basis than previous approaches to German readability, and ii) it successfully generalizes across the data sets, illustrating surprising robustness across rather different text types. The approach presented thus extends the state-of-the-art in <cite>Hancke et al. (2012)</cite> in terms of the breadth of features integrated and the accuracy and generalizability of the model -and provides two new data sources for this line of research. The paper also contributes some new insights into the linguistic characteristics of German media language targeting adults and children. Since all the language is produced by adults, it is not necessarily clear how well it is in fact adjusted to the target audience. As demonstrated by , German textbook publishers indeed do not seem to be adjusting the complexity of the language used according to school type and grade level in any systematic way.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_0",
  "x": "These families of duplicate bug reports form a semi-parallel 1 http://www.bugzilla.org/ Parallel bug reports with a pair of true paraphrases 1: connector extend with a straight line in full screen mode 2: connector show straight line in presentation mode Non-parallel bug reports referring to the same bug 1: Settle language for part of text and spellchecking part of text 2: Feature requested to improve the management of a multi-language document Context-peculiar paraphrases (shown in italics) 1: status bar appear in the middle of the screen 2: maximizing window create phantom status bar in middle of document However, bug reports have characteristics that raise many new challenges. Different from many other parallel corpora, bug reports are noisy. We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports. Third, even if the bug reports are parallel, we find many cases of context-peculiar paraphrases, i.e., a pair of phrases that have the same meaning in a very narrow context. An example is shown in Table 1 (bottom).",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_1",
  "x": "These families of duplicate bug reports form a semi-parallel 1 http://www.bugzilla.org/ Parallel bug reports with a pair of true paraphrases 1: connector extend with a straight line in full screen mode 2: connector show straight line in presentation mode Non-parallel bug reports referring to the same bug 1: Settle language for part of text and spellchecking part of text 2: Feature requested to improve the management of a multi-language document Context-peculiar paraphrases (shown in italics) 1: status bar appear in the middle of the screen 2: maximizing window create phantom status bar in middle of document However, bug reports have characteristics that raise many new challenges. Different from many other parallel corpora, bug reports are noisy. We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports. Third, even if the bug reports are parallel, we find many cases of context-peculiar paraphrases, i.e., a pair of phrases that have the same meaning in a very narrow context. An example is shown in Table 1 (bottom).",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_2",
  "x": "In the area of text mining for software engineering, paraphrases have been used in many tasks, e.g., Tan et al., 2007) . However, most paraphrases used are obtained manually. A recent study using synonyms from WordNet highlights the fact that these are not effective in software engineering tasks due to domain specificity (Sridhara et al., 2008) . Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_3",
  "x": "Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words. (2) Contextual patterns which consist of the POS tags before and after the phrases. For example, the contexts \"in the middle of\" and \"in middle of\" in Table 1 (bottom) are represented as \"",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_4",
  "x": "In the area of text mining for software engineering, paraphrases have been used in many tasks, e.g., Tan et al., 2007) . However, most paraphrases used are obtained manually. A recent study using synonyms from WordNet highlights the fact that these are not effective in software engineering tasks due to domain specificity (Sridhara et al., 2008) . Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_5",
  "x": "A recent study using synonyms from WordNet highlights the fact that these are not effective in software engineering tasks due to domain specificity (Sridhara et al., 2008) . Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_6",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. <cite>The authors</cite> first used identical words and phrases as seeds to find and score contextual patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_7",
  "x": "Therefore, reports belonging to the same group can be naturally regarded as parallel sentences. However, these sentences are only partially parallel because two users may describe the same bug in very different ways. An example is shown in Table 1 (middle). This kind of sentence pairs should not be regarded as parallel. To address this problem, we take a heuristic approach and only select sentence pairs that have strong similarities. Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_8",
  "x": "Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom). For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase. We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_9",
  "x": "An example is shown in Table 1 (middle). This kind of sentence pairs should not be regarded as parallel. To address this problem, we take a heuristic approach and only select sentence pairs that have strong similarities. Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_10",
  "x": "We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences. This is defined by the following formula: where n is the number of parallel bug reports with the two phrases occurring in parallel, and s i is the score for the i'th occurrence. s i is computed as follows: 1. We compute the set of patterns with affixed pattern scores based on (<cite>Barzilay and McKeown, 2001</cite> ). 2. For the i'th parallel occurrence of the pair of phrases we want to score, we try to find a pattern that matches the occurrence and assign the pattern score to the pair of phrases as s i . If no such pattern exists, we set s i to 0. By taking the average of s i as the global score for a pair of phrases, we do not rely much on a single s i and can therefore prevent context-peculiar paraphrases to some degree. Co-occurrence-Based Scoring We also consider another global co-occurrence-based score that is commonly used for finding collocations. A general observation is that noise tends to appear in random but random things do not occur in the same way often.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_11",
  "x": "We normalize the S c score to the range of 0 to 1 by dividing it with the size of the corpus. Holistic Solution We employ the parallel sentence selection as a pre-processing step, and merge co-occurrence-based scoring with global contextbased scoring. For each parallel sentence pairs, a chunker is used to get chunks from each sentence. All possible pairings of chunks are then formed. This set of chunk pairs are later fed to the method in (<cite>Barzilay and McKeown, 2001</cite> ) to produce a set of patterns with affixed scores. With this we compute our global-context based scores. The cooccurrence based scores are computed following the approach described above. Two thresholds are used and candidate paraphrases whose scores are below the respective thresholds are removed. Alternatively, one of the score is used as a filter, while the other is used to rank the candidates. The next section describes our experimental results.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_12",
  "x": "After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_13",
  "x": "We build our corpus in the following steps. We collect a total of 13,898 duplicate bug reports from OpenOffice. Each duplicate bug report is associated to a master report-there is one master report for each unique bug. From this information, we create duplicate bug report groups where each member of a group is a duplicate of all other members in the same group. Finally, we extract duplicate bug report pairs by pairing each two members of each group. We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_14",
  "x": "Each duplicate bug report is associated to a master report-there is one master report for each unique bug. From this information, we create duplicate bug report groups where each member of a group is a duplicate of all other members in the same group. Finally, we extract duplicate bug report pairs by pairing each two members of each group. We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_15",
  "x": "We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_16",
  "x": "After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_17",
  "x": "After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "motivation extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_18",
  "x": "As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. These patterns are later used to select paraphrases. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases. For each threshold in the range of 0.45-0.95 (step size: 0.05), we extract paraphrases and compute the corresponding precision.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_19",
  "x": "In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns. Using these patterns we compute the global context-based scores S g . We also compute the co-occurrence scores S c . We rank and extract top-k paraphrases based on these scores. We consider 4 different methods: We can use either S g or S c to rank the discovered paraphrases. We call them Rk-S g and Rk-S c . We also consider using one of the scores for ranking and the other for filtering bad candidate paraphrases. A threshold of 0.05 is used for filtering. We call these two methods Rk-S c +Ft-S g and Rk-S g +Ft-S c . With ranked lists from these 4 methods, we can compute precision@k for the top-k paraphrases.",
  "y": "extends uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_20",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. From the figure we can see that our holistic approach using global-context score to rank and co-occurrence score to filter (i.e., Rk-S g +Ft-S c ) has higher precision than the <cite>baseline approach</cite> (i.e., <cite>BL</cite>) in all ks.",
  "y": "differences"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_21",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. Interestingly, the graph shows that using only one of the scores alone (i.e., Rk-S g and Rk-S c ) does not result in a significantly higher precision than the <cite>baseline approach</cite>.",
  "y": "differences similarities"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_0",
  "x": "2) The lexical chasm can be crossed by grounding queries and documents in an external multilingual knowledge source (e.g., Wikipedia or BabelNet) [4, 20] . However, the concept coverage is limited for resource-lean languages, and all content not present in a knowledge base is effectively ignored by a CLIR system. Bilingual text embeddings, while displaying a wider applicability and versatility than the two other paradigms, still suffer from one important limitation: a bilingual supervision signal is required to induce shared cross-lingual semantic spaces. This supervision takes form of sentence-aligned parallel data [5] , pre-built word translation pairs [11,<cite> 19]</cite> or document-aligned comparable data [21] . 1 Recently, methods for inducing shared cross-lingual embedding spaces without the need for any bilingual signal (not even word translation pairs) have been proposed [1, 3] . These methods exploit inherent structural similarities of induced monolingual embedding spaces to learn vector space transformations that align the source language space to the target language space, with strong results observed for bilingual lexicon extraction. In this work, we show that these unsupervised cross-lingual word embeddings offer strong support to the construction of fully unsupervised adhoc CLIR models. We propose two different CLIR models: 1) termby-term translation through the shared cross-lingual space, and 2) query and document representations as IDF-weighted sums of constituent word vectors. To the best of our knowledge, our CLIR methodology is the first to allow the construction of CLIR models without any bilingual data and supervision at all, relying solely on monolingual corpora. Experimental evaluation on standard CLEF CLIR data for three different language pairs shows that the proposed fully unsupervised CLIR models outperform competitive baselines and models that exploit word translation pairs or comparable corpora.",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_1",
  "x": "We then explain in detail the query and document representations as well as the ranking functions of our CLIR models. ---------------------------------- **CROSS-LINGUAL WORD VECTOR SPACES** For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs <cite>[19]</cite> ; and 3) no bilingual data at all [3] . ---------------------------------- **CROSS-LINGUAL EMBEDDINGS FROM COMPARABLE DOCUMENTS (CL-CD).** The BWE Skip-Gram (BWESG) model from Vuli\u0107 and Moens [21] exploits large document-aligned comparable corpora (e.g., Wikipedia). BWESG first creates a merged corpus of bilingual pseudo-documents by intertwining pairs of available comparable documents. Then it applies a standard monolingual log-linear Skip-Gram model with negative sampling (SGNS) [10] on the merged corpus in which words have bilingual contexts instead of monolingual ones. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_2",
  "x": "**CROSS-LINGUAL WORD VECTOR SPACES** For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs <cite>[19]</cite> ; and 3) no bilingual data at all [3] . ---------------------------------- **CROSS-LINGUAL EMBEDDINGS FROM COMPARABLE DOCUMENTS (CL-CD).** The BWE Skip-Gram (BWESG) model from Vuli\u0107 and Moens [21] exploits large document-aligned comparable corpora (e.g., Wikipedia). BWESG first creates a merged corpus of bilingual pseudo-documents by intertwining pairs of available comparable documents. Then it applies a standard monolingual log-linear Skip-Gram model with negative sampling (SGNS) [10] on the merged corpus in which words have bilingual contexts instead of monolingual ones. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS FROM WORD TRANSLATION PAIRS (CL-WT).** This class of models [1, 11,<cite> 19]</cite> focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_3",
  "x": "According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network. In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. <cite>[19]</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_4",
  "x": "**CROSS-LINGUAL EMBEDDINGS FROM WORD TRANSLATION PAIRS (CL-WT).** This class of models [1, 11,<cite> 19]</cite> focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces. Let { S w i } V S i =1 , S w i \u2208 R ds be the monolingual word embedding space of the source language L S with V S vectors, and { T w i } V T i =1 , T w i \u2208 R dt the monolingual space for the target language L T containing V T vectors; ds and dt are the respective space dimensionalities. The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques. According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ----------------------------------",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_5",
  "x": ". The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques. According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network.",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_6",
  "x": "According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network. In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. <cite>[19]</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_7",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Language Pairs and Training Data. We experiment with three language pairs of varying degree of similarity: English (EN) -{Dutch (NL), Italian (IT), Finnish (FI)}. 6 We use precomputed monolingual T vectors [2] (available online) 7 as monolingual word embeddings required by CL-WT and CL-UNSUP embedding models. For the CL-CD embeddings, the BWESG model trains on full documentaligned Wikipedias 8 using SGNS with suggested parameters from prior work [22] : 15 negative samples, global decreasing learning rate is .025, subsampling rate is 1e \u2212 4, window size is 16. The CL-WT embeddings of Smith et al. <cite>[19]</cite> use 10K translation pairs obtained from Google Translate to learn the linear mapping functions. The CL-UNSUP training setup closely follows the default setup of Conneau et al. [3] : we refer the reader to the original 4 Note that with both variants of BWE-Agg, we effectively ignore both query and document terms that are not represented in the cross-lingual embedding space. 5 If the representation of a query term t q i is not present in the cross-lingual embedding space, we retain the query term t q i itself. We have also attempted eliminating out-ofvocabulary query terms, but the former consistently leads to better performance. 6 English and Dutch are Germanic languages, Italian is a Romance language, whereas Finnish is an Uralic language (i.e., not Indo-European) 7 Table 1 : Basic statistics of used CLEF test collections: number of documents (#doc), number of tokens (#tok), and average number of relevant documents per query (#rel).",
  "y": "differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_8",
  "x": "However, even though NL is linguistically closer to EN than IT, for the unsupervised CLIR models we generally observe slightly better performance for EN\u2192IT than for EN\u2192NL. We speculate that this is due to the compounding phenomenon in word formation, which is present in NL, but is not a property of EN and IT. The reported performance on bilingual lexicon extraction (BLE) using cross-lingual embedding spaces is also lower for EN-NL compared to EN-IT (see, e.g., <cite>[19]</cite> ). We observe the same pattern (4-5% lower BLE performance for EN-NL than for EN-IT) with the CL-UNSUP embedding spaces. The weighted variant of BWE-Agg (BWE-Agg-IDF) outperforms the simpler non-weighted summation model (BWE-Agg-Add) across the board. These results suggest that the common IR assumption about document-specific terms being more important than the terms occurring collection-wide is also valid for constructing dense document representations by summing word embeddings. ---------------------------------- **CONCLUSION** We have presented a fully unsupervised CLIR framework that leverages unsupervised cross-lingual word embeddings induced solely on the basis of monolingual corpora. We have shown the ability of our models to retrieve relevant content cross-lingually without any bilingual data at all, by reporting competitive performance on standard CLEF CLIR evaluation data for three test language pairs.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_0",
  "x": "Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint. Work in progress. is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise. The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the less loss in accuracy was the system by Chen et al. (2018) which utilizes external knowledge taken from WordNet (Miller, 1995) . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> . There, the authors show that the generalization capabilities of stateof-the-art NLI systems, in cases where some kind of external lexical knowledge is needed, drops dramatically when the SNLI test set is replaced by a test set where the premise and the hypothesis are otherwise identical except for at most one word.",
  "y": "motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_1",
  "x": "These approaches can be usually categorized into: a) sentence encoding models, and b) other neural network models. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.1% (Kim et al., 2018) and 86.7% (Devlin et al., 2018) respectively. However, a big question w.r.t to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint. Work in progress. is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise. The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the less loss in accuracy was the system by Chen et al. (2018) which utilizes external knowledge taken from WordNet (Miller, 1995) . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ----------------------------------",
  "y": "differences similarities extends"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_2",
  "x": "The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the less loss in accuracy was the system by Chen et al. (2018) which utilizes external knowledge taken from WordNet (Miller, 1995) . In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> . There, the authors show that the generalization capabilities of stateof-the-art NLI systems, in cases where some kind of external lexical knowledge is needed, drops dramatically when the SNLI test set is replaced by a test set where the premise and the hypothesis are otherwise identical except for at most one word. The results show a very significant drop in accuracy. recognize the generalization problem that comes with training on datasets like SNLI, which tend to be homogeneous with linguistic variation. In this context, they propose to better train NLI models by making use of adversarial examples.",
  "y": "motivation background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_3",
  "x": "A man poses in front of an ad for beer. Enthusiasm for Disney's Broadway production of The Lion King dwindles. The broadway production of The Lion King was amazing, but audiences are getting bored. sentence attention and utilizes external knowledge. We also selected one model involving a pretrained language model, namely ESIM + ELMo . All of the models perform well on the SNLI dataset, reaching near stateof-the-art accuracy in the sentence encoding and the other category respectively. KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by <cite>Glockner et al. (2018)</cite> . For BiLSTM-max we used the Adam optimizer (Kingma and Ba, 2014) and a learning rate of 5e-4. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve. We used a batch size of 64.",
  "y": "background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_4",
  "x": "This is surprising as both of these datasets have been constructed with a similar data collection method using the same definition of inference (i.e. same definition of entailment, contradiction and neutral). The sentences included in SNLI are also much simpler compared to those in MultiNLI. This might also explain why the drop in accuracy for all of the four models is lowest when the models are trained on MultiNLI and tested on SNLI. It is also very surprising that the model with biggest drop in accuracy was ESIM + ELMo which includes a pretrained ELMo language model. ESIM + ELMo did, however, get the highest accuracy of 69.1% in this experiment. All the models perform almost equally poorly across all the experiments. Both BiLSTM-max and HBMP have an average drop in accuracy of 24.4 points, while the average for KIM is 25.5 and for ESIM + ELMo 25.6. ESIM has the highest average drop of 27.0 points. In contrast to the findings of <cite>Glockner et al. (2018)</cite> , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained language model did not improve the results significantly.",
  "y": "differences"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_5",
  "x": "ESIM has the highest average drop of 27.0 points. In contrast to the findings of <cite>Glockner et al. (2018)</cite> , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained language model did not improve the results significantly. ---------------------------------- **CONCLUSION** In this paper we have shown that neural network models for NLI fail to generalize across different NLI benchmarks. We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_6",
  "x": "In this paper we have shown that neural network models for NLI fail to generalize across different NLI benchmarks. We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset. b) NLI is to some extent also genre/context dependent. Training on SNLI and testing on MultiNLI gives worse results than vice versa. This can be seen as an indication that training on multiple genres helps. However, this is still not enough given that, even in case of training on MultiNLI and testing on SNLI, accuracy drops significantly.",
  "y": "future_work"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_0",
  "x": "Bracketing evaluation may count a single error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSE-VAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003) . At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_1",
  "x": "In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_2",
  "x": "At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_3",
  "x": "At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_4",
  "x": "In addition, and most directly relevant for this paper, PARSE-VAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003) . At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_5",
  "x": "<cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora.",
  "y": "motivation"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_6",
  "x": "3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ---------------------------------- **THE CORPORA USED** As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T\u00fcBa-D/Z, which differ significantly in the syntactic representations used -thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained. ---------------------------------- **NEGRA** The Negra corpus (Brants et al., 1999) consists of newspaper text from the Frankfurter Rundschau, a German newspaper. Version 2 of the corpus contains 20,602 sentences. It uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation.",
  "y": "extends"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_7",
  "x": "3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ---------------------------------- **THE CORPORA USED** As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T\u00fcBa-D/Z, which differ significantly in the syntactic representations used -thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained. ---------------------------------- **NEGRA** The Negra corpus (Brants et al., 1999) consists of newspaper text from the Frankfurter Rundschau, a German newspaper. Version 2 of the corpus contains 20,602 sentences. It uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_8",
  "x": "The goal of the following experiments is a comparison of parsing performance across different types of evaluation metrics for parsers trained on Negra (Ver. 2) and T\u00fcBa-D/Z (Ver. 2). ---------------------------------- **DATA PREPARATION** Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1 , the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node. 4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a part of context-free grammar rules used by a PCFG parser. In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_9",
  "x": "This difference is mainly due to the extra level of topological fields annotation and the use of more contoured structures in many places where Negra uses flatter structures. ---------------------------------- **EXPERIMENTS** The goal of the following experiments is a comparison of parsing performance across different types of evaluation metrics for parsers trained on Negra (Ver. 2) and T\u00fcBa-D/Z (Ver. 2). ---------------------------------- **DATA PREPARATION** Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_10",
  "x": "To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1 , the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node. 4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a part of context-free grammar rules used by a PCFG parser. In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it. Both corpora include edge labels above all phrases and words. However the flatter structures in Negra result in 39 different edge labels on words while T\u00fcBa-D/Z has only 5. Unlike <cite>K\u00fcbler et al. (2006)</cite> , which ignored edge labels on words, we incorporate all edge labels present in both corpora. As a consequence of this, providing a parser with perfect lexical tags would also provide the edge label for that word.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_11",
  "x": "---------------------------------- **DEPENDENCY EVALUATION** Complementing the issue of the ratio of terminals to non-terminals raised in the last section, one can question whether counting all brackets in the sentence equally, as done by the PARSEVAL metric, provides a good measure of how accurately the basic functor-argument structure of the sentence has been captured in a parse. Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of <cite>K\u00fcbler et al. (2006)</cite> . For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb. form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects. 9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. ---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION**",
  "y": "similarities"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_12",
  "x": "Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of <cite>K\u00fcbler et al. (2006)</cite> . For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb. form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects. 9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. ---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION** <cite>K\u00fcbler et al. (2006)</cite> present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) . In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_14",
  "x": "In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing. This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra. As discussed in the data preparation section, Negra was modified for our experiment so as not to provide the parser with the grammatical function labels for single word phrases as part of the perfect tags provided. This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments. These results also form a clear contrast to the PARSEVAL results we saw in Table 2 .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_15",
  "x": "In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra. As discussed in the data preparation section, Negra was modified for our experiment so as not to provide the parser with the grammatical function labels for single word phrases as part of the perfect tags provided. This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments. These results also form a clear contrast to the PARSEVAL results we saw in Table 2 . Contrary to the finding in <cite>K\u00fcbler et al. (2006)</cite> , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation. In keeping with the results from Rehbein and van Genabith (2007a) , we find that PARSEVAL is not an adequate predictor of performance in an evaluation targeting the functorargument structure of the sentence for comparisons between PCFG parsers trained on corpora with different annotation schemes. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_16",
  "x": "This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments. These results also form a clear contrast to the PARSEVAL results we saw in Table 2 . Contrary to the finding in <cite>K\u00fcbler et al. (2006)</cite> , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation. In keeping with the results from Rehbein and van Genabith (2007a) , we find that PARSEVAL is not an adequate predictor of performance in an evaluation targeting the functorargument structure of the sentence for comparisons between PCFG parsers trained on corpora with different annotation schemes. ---------------------------------- **LABELED DEPENDENCY TRIPLE EVALUATION** While determining the grammatical function of an element is an important part of determining the functor-argument structure of a sentence, the other necessary component is determining the head of each function. To evaluate whether both the functor and the argument have been correctly found, an evaluation of labeled dependency triples is needed.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_17",
  "x": "Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments. The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> . When the grammatical function label evaluation is extended into a labeled dependency evaluation by finding the verbal head to complete the labeled dependency triple, the parser trained on Negra outperforms that trained on T\u00fcBa-D/Z. The more significant drop in results for T\u00fcBa-D/Z compared to the grammatical function label evaluation may be due to the fact that a verbal lexical head in T\u00fcBa-D/Z is not in the same local tree as its dependents, whereas it is in Negra. The presence of intervening topological field nodes in T\u00fcBa-D/Z may make it difficult for the parser to consistently identify the elements of the dependency triple across several subtrees. The Negra corpus annotation scheme makes it simple to identify the heads of verb arguments, but the flat NP and PP structures make it difficult to extend a labeled dependency analysis beyond verb arguments. On the other hand, T\u00fcBa-D/Z has marked heads in NPs and PPs, but it is not as easy to pair verb arguments with their heads because the verbs are in separate topological fields from their argu-ments. For a constituent-based corpus annotation scheme to lend itself to a thorough labeled dependency evaluation, heads should be marked clearly for all phrase categories and all non-head elements need to have marked grammatical functions. The presence of topological field nodes in T\u00fcBa-D/Z deserves more discussion in relation to a grammatical dependency evaluation. The corpus contains two very different types of nodes in its syntactic trees: nodes such as NP and PP that correspond to constituents and nodes such as VF (Vorfeld) and MF (Mittelfeld) that correspond to word order domains.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_18",
  "x": "Table 5 : Labeled Dependency Evaluation tion, we confirm that the PARSEVAL scores do not correlate with the scores in the other two evaluations, which given their closeness to the semantic functor argument structure make meaningful targets for evaluating parsers. Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments. The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> . When the grammatical function label evaluation is extended into a labeled dependency evaluation by finding the verbal head to complete the labeled dependency triple, the parser trained on Negra outperforms that trained on T\u00fcBa-D/Z. The more significant drop in results for T\u00fcBa-D/Z compared to the grammatical function label evaluation may be due to the fact that a verbal lexical head in T\u00fcBa-D/Z is not in the same local tree as its dependents, whereas it is in Negra. The presence of intervening topological field nodes in T\u00fcBa-D/Z may make it difficult for the parser to consistently identify the elements of the dependency triple across several subtrees. The Negra corpus annotation scheme makes it simple to identify the heads of verb arguments, but the flat NP and PP structures make it difficult to extend a labeled dependency analysis beyond verb arguments. On the other hand, T\u00fcBa-D/Z has marked heads in NPs and PPs, but it is not as easy to pair verb arguments with their heads because the verbs are in separate topological fields from their argu-ments. For a constituent-based corpus annotation scheme to lend itself to a thorough labeled dependency evaluation, heads should be marked clearly for all phrase categories and all non-head elements need to have marked grammatical functions. The presence of topological field nodes in T\u00fcBa-D/Z deserves more discussion in relation to a grammatical dependency evaluation.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_19",
  "x": "**FUTURE WORK** An evaluation on arguments of verbs is just a first step in working towards a more complete labeled dependency evaluation. Because Negra and T\u00fcBa-D/Z do not have parallel uses of many grammatical function labels beyond arguments of verbs, a more detailed evaluation on more types of dependency relations will require a complex dependency conversion method to provide comparable results. Since previous work on head-lexicalized parsing models for German has focused on PARSEVAL evaluations, it would also be useful to perform a labeled dependency evaluation to determine what effect head lexicalization has on particular constructions for the parsers. Because of the concerns discussed in the previous section and the difference in which types of clauses have marked heads in Negra and T\u00fcBa-D/Z, the effect of head lexicalization on the parsing results may differ for the two corpora. ---------------------------------- **CONCLUSION** Addressing the general question of how to compare parsing results for different annotation schemes, we revisited the comparison of PCFG parsing results for the Negra and T\u00fcBa-D/Z corpora. We show that these different annotation schemes lead to very significant differences in PARSEVAL scores for unlexicalized PCFG parsing models, but grammatical function label and labeled dependency evaluations for arguments of verbs show that this difference does not carry over to measures which are relevant to the semantic functor-argument structure. In contrast to <cite>K\u00fcbler et al. (2006)</cite> a grammatical function evaluation on subjects, accusative objects, and dative objects establishes that Negra and T\u00fcBa-D/Z perform similarly when all types of words and phrases appearing as arguments are taken into consideration.",
  "y": "differences"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_0",
  "x": "This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms ). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by . The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. ---------------------------------- **BACKGROUND**",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_1",
  "x": "The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. ---------------------------------- **BACKGROUND** In this section, we introduce a grammar conversion ) and CFG filtering<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000) . ---------------------------------- **GRAMMAR CONVERSION**",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_2",
  "x": "We call the remaining edges that are used for the phase 2 parsing essential edges. The parsers with CFG filtering used in our experiments follow the above parsing strategy, but are different in the way the CF approximation and the elimination of impossible parse trees in phase 2 are performed. In the following sections, we briefly describe the CF approximation and the elimination of impossible parse trees in each realization. ---------------------------------- **CF APPROXIMATION OF LTAG** In CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) , every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1 . ---------------------------------- **CFG RULES** Figure 2: Extraction of CFG from HPSG Because the obtained CFG can reflect only local constraints given in each local structure of the elementary trees, it generates invalid parse trees that connect local trees in different elementary trees. In order to eliminate such parse trees, a link between branchings is preserved as a node number which records a unique node address (a subscript attached to each node in Figure 1 ).",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_3",
  "x": "As described in Section 2.1, the grammar conversion preserves the whole structure of each elementary tree (precisely, a canonical elementary tree) in a stack, and grammar rules manipulate a head element of the stack. A generated feature structure in the approximation process thus corresponds to the whole unprocessed portion of a canonical elementary tree. This implies that successful context-free derivations obtained by CFG TNT basically involve elementary trees in which all substitution and adjunction have succeeded. However, CFG PB (also a CFG produced by the other work<cite> (Harbusch, 1990)</cite> ) cannot avoid generating invalid parse trees that connect two lo-cal structures where adjunction takes place between them. We measured with G 2-21 the proportion of the number of ok-prop between two node numbers of nodes that take adjunction and its success rate. It occupied 87% of the total number of ok-prop and its success rate was only 22%. These results suggest that the global contexts in a given grammar is essential to obtain an effective CFG filter. It should be noted that the above investigation also tells us another way of CF approximation of LTAG. We first define a unique way of tree traversal such as head-corner traversal (van Noord, 1994) on which we can perform a sequential application of substitution and adjunction. We then recursively apply substitution and adjunction on that traversal to an elementary tree and a generated tree structure.",
  "y": "motivation uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_0",
  "x": "In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster. ---------------------------------- **INTRODUCTION** Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster).",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_1",
  "x": "Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster. ---------------------------------- **INTRODUCTION** Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself-games are puzzleswhich results in inefficient training.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_2",
  "x": "Specifically, we explore transfer learning at multiple levels and across different dimensions. We first look at the effects of playing a text-adventure game given a strong prior in the form of a knowledge graph extracted from generalized textual walk-throughs of interactive fiction as well as those made specifically for a given game. Next, we explore the transfer of control policies in deep Q-learning (DQN) by pre-training portions of a deep Q-network using question-answering and by DQN-to-DQN parameter transfer between games. We evaluate these techniques on two different sets of human authored and computer generated games, demonstrating that our transfer learning methods enable us to learn a higher-quality control policy faster. ---------------------------------- **BACKGROUND AND RELATED WORK** Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> : (1) The agent must act based only on potentially incomplete textual descriptions of the world around it. The world is thus partially observable, as the agent does not have access to the state of the world at any stage. (2) the action space is combinatorially large-a consequence of the agent having to declare commands in natural language. These two problems together have kept commercial text adventure games out of the reach of existing deep reinforcement learning methods, especially given the fact that most of these methods attempt to train on a particular game from scratch.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_3",
  "x": "Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> . Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019) design an agent that uses multiple modules to identify a general set of game play rules for text games across various domains. None of these works study how to transfer policies between different text-adventure games in any depth and so there exists a gap between the two bodies of work. Transferring policies across different textadventure games requires implicitly learning a mapping between the games' state and action spaces. The more different the domain of the two games, the harder this task becomes. Previous work <cite>(Ammanabrolu and Riedl, 2019)</cite> introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld that uses grammars to generate a series of similar (but not exact same) games.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_4",
  "x": "This can be represented as a 7-tuple of S, T, A, \u2126, O, R, \u03b3 : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively . Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> . Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019) design an agent that uses multiple modules to identify a general set of game play rules for text games across various domains. None of these works study how to transfer policies between different text-adventure games in any depth and so there exists a gap between the two bodies of work. Transferring policies across different textadventure games requires implicitly learning a mapping between the games' state and action spaces. The more different the domain of the two games, the harder this task becomes. Previous work <cite>(Ammanabrolu and Riedl, 2019)</cite> introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_5",
  "x": "1 For each step that the agent takes, it automatically extracts a set of RDF triples from the received observation through the use of OpenIE (Angeli et al., 2015) in addition to a few rules to account for the regularities of text-adventure games. The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in in a map. The graph also makes a distinction with respect to items that are in the agent's possession or in their immediate surrounding environment. We make minor modifications to the rules used in<cite> Ammanabrolu and Riedl (2019)</cite> to better achieve such a graph in general interactive fiction environments. The agent also has access to all actions accepted by the game's parser, following Narasimhan et al. (2015) . For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is \"place OBJ in OBJ\". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of A = O(|V | \u00d7 |O| 2 ) where V is the number of action verbs, and O is the number of distinct objects in the world that the agent can interact with.",
  "y": "extends"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_6",
  "x": "These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of A = O(|V | \u00d7 |O| 2 ) where V is the number of action verbs, and O is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <cite>(Ammanabrolu and Riedl, 2019)</cite> The architecture for the deep Q-network consists of two separate neural networks-encoding state and action separately-with the final Q-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1 ). We train with a standard DQN training loop; the policy is determined by the Q-value of a particular state-action pair, which is updated using the Bellman equation (Sutton and Barto, 2018) : ( 1) where \u03b3 refers to the discount factor and r t+1 is the observed reward. The whole system is trained using prioritized experience replay Lin (1993) , a modified version of -greedy learning, and a temporal difference loss that is computed as: where A k+1 represents the action set at step k + 1 and s t , a t refer to the encoded state and action representations respectively. ---------------------------------- **KNOWLEDGE GRAPH SEEDING** In this section we consider the problem of transferring a knowledge graph from a static text resource to a DQN-which we refer to as seeding.",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_7",
  "x": "The portions that are pre-trained are the same parts of the architecture as in<cite> Ammanabrolu and Riedl (2019)</cite> . This game is referred to as the source task. The seeding of the knowledge graph is not strictly necessary but given that state-of-theart DRL agents cannot complete real games, this makes the agent more effective at the source task. We then transfer the knowledge and skills acquired from playing the source task to another game from the same genre-the target task. The parameters of the deep Q-network trained on the source game are used to initialize a new deep Qnetwork for the target task. All the weights indicated in the architecture of KG-DQN as shown in Fig. 1 are transferred. Unlike Rusu et al. (2016), we do not freeze the parameters of the deep Qnetwork trained on the source task nor use the two networks to jointly make decisions but instead just use it to initialize the parameters of the target task deep Q-network. This is done to account for the fact that although graph embeddings can be transferred between games, the actual graph extracted from a game is non-transferable due to differences in structure between the games. ---------------------------------- **EXPERIMENTS**",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_8",
  "x": "We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and sourceto-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ---------------------------------- **SLICE OF LIFE EXPERIMENTS** TextWorld uses a grammar to generate similar games. Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system.",
  "y": "uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_9",
  "x": "A summary of the statistics for the games is given in Table 1 . Vocabulary overlap is calculated by measuring the percentage of overlap between a game's vocabulary and the domain's vocabulary, i.e. the union of the vocabularies for all the games we use within the domain. We observe that in both of these domains, the complexity of the game increases steadily from the game used for the question-answering system to the target and then source task games. We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and sourceto-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ----------------------------------",
  "y": "differences similarities"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_10",
  "x": "We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and sourceto-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ---------------------------------- **SLICE OF LIFE EXPERIMENTS** TextWorld uses a grammar to generate similar games. Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system.",
  "y": "uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_0",
  "x": "Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the stateof-the-art approach. ---------------------------------- **INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_1",
  "x": "The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the stateof-the-art approach. ---------------------------------- **INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_2",
  "x": "The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a text document is then computed by accumulating the differences between each codeword vector and each word vector that is both present in the document and associated to the respective codeword. Since our approach considers cluster centroids as reference for building the representation, it can easily accommodate new words, not seen during k-means training, simply by associating them to the nearest cluster centroids. Thus, VLAWE is robust to vocabulary distribution gaps between training and test, which can appear when the training set is particularly smaller or from a different domain. Certainly, the robustness holds as long as the word embeddings are pretrained on a very large set of documents, e.g. the entire Wikipedia. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: Reuters-21578 (Lewis, 1997) , RT-2k (Pang and Lee, 2004) , MR (Pang and Lee, 2005) , TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004) . We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 , demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of Cheng et al. (2018) by almost 10%. The rest of the paper is organized as follows. We present related works on learning documentlevel representations in Section 2.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_3",
  "x": "There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) . ---------------------------------- **METHOD** The Vector of Locally-Aggregated Descriptors (VLAD) (J\u00e9gou et al., 2010 (J\u00e9gou et al., , 2012 was introduced in computer vision to efficiently represent images for various image classification and retrieval tasks. We propose to adapt the VLAD representation in order to represent text documents instead of images. Our adaptation consists of replacing the Scale-Invariant Feature Transform (SIFT) image descriptors (Lowe, 2004) useful for recognizing object patterns in images with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) useful for recognizing semantic patterns in text documents. We coin the term Vector of LocallyAggregated Word Embeddings (VLAWE) for the resulting document representation.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_4",
  "x": "While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) . ---------------------------------- **METHOD** The Vector of Locally-Aggregated Descriptors (VLAD) (J\u00e9gou et al., 2010 (J\u00e9gou et al., , 2012 was introduced in computer vision to efficiently represent images for various image classification and retrieval tasks. We propose to adapt the VLAD representation in order to represent text documents instead of images. Our adaptation consists of replacing the Scale-Invariant Feature Transform (SIFT) image descriptors (Lowe, 2004) useful for recognizing object patterns in images with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) useful for recognizing semantic patterns in text documents. We coin the term Vector of LocallyAggregated Word Embeddings (VLAWE) for the resulting document representation. The VLAWE representation is derived as follows.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_5",
  "x": "The task is to classify a sentence as being either subjective or objective. ---------------------------------- **EVALUATION AND IMPLEMENTATION DETAILS** In the experiments, we used the pre-trained word embeddings computed with the GloVe toolkit provided by Pennington et al. (2014) . The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases. Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library (Vedaldi and Fulkerson, 2008) . We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k \u00b7 d = 10 \u00b7 300 = 3000 components. Similar to J\u00e9gou et al. (2012) , we set \u03b1 = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets. The top three results on each data set are highlighted in red, green and blue, respectively.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_6",
  "x": "Best viewed in color. set the SVM regularization parameter to C = 1 in all our experiments. In the SVM, we use the linear kernel. For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 .",
  "y": "similarities uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_7",
  "x": "Best viewed in color. set the SVM regularization parameter to C = 1 in all our experiments. In the SVM, we use the linear kernel. For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_8",
  "x": "---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> . In most cases, our improvements over the baselines are higher than 5%. On the Reuters-21578 data set, we surpass the closely-related approach 93.2 VLAWE (full, k = 10) 93.3 Table 2 : Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA. of by around 2%. On the RT-2k data set, we surpass the related works of Fu et al. (2018) and by around 4%.",
  "y": "differences"
 },
 {
  "id": "3ebfa05038431571701a7199163832_0",
  "x": "The TORGO database [11] is a collection of annotated speech recordings and articulatory measurements from speakers with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), as well as control patients. [12, 13, 14] have used this database to provide speech recognition systems with robustness to dysarthria. [15] trains various linear classifiers on TORGO and the NKI CCRT corpus [16] to detect dysarthria. More recently, [17] has trained fully connected neural networks to classify the severity of the disease, using TORGO and the UASPEECH [18] database. All these models are trained on standard low-level features. In this work we show that dysarthria detection benefits significantly from learning directly from the raw waveform. Previous work has explored learnable alternatives to speech features that rely on a similar computation to spectral representations [19, 20, 21, <cite>22,</cite> 5] . These approaches learn convolutions that are then passed through a non-linearity, eventually a pooling operator and then a log compression to replicate the dynamic range compression typically performed on spectrograms or mel-filterbanks. This compression function remains fixed and is chosen beforehand, which could impact the final performance, as various compression functions including logarithm, cubic root, or 10th root have been previously showed to perform better depending on the task (see Table 2 of [23] ). A second fixed component is the meanvariance normalization of speech features.",
  "y": "background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_1",
  "x": "Our experiments show that by training a PCEN block on top of mel-filterbanks or replacing them by learnable time-domain filterbanks from<cite> [22]</cite> , we get a gain in accuracy around 10% in absolute when training an identical neural network for dysarthria detection. Finally, by combining time-domain filterbanks and PCEN we propose the first audio frontend that can learn features, compression and normalization jointly with a neural network using backpropagation. ---------------------------------- **MODEL** ---------------------------------- **TIME-DOMAIN FILTERBANKS** As the first step of our computational pipeline, we use TimeDomain filterbanks from<cite> [22]</cite> . Time-Domain filterbanks are neural network layers that take the raw waveform as input. They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand. The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_2",
  "x": "---------------------------------- **MODEL** ---------------------------------- **TIME-DOMAIN FILTERBANKS** As the first step of our computational pipeline, we use TimeDomain filterbanks from<cite> [22]</cite> . Time-Domain filterbanks are neural network layers that take the raw waveform as input. They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand. The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters. More formally, the n th melfilterbank of a signal in t is: where is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f .",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_3",
  "x": "Time-Domain filterbanks are neural network layers that take the raw waveform as input. They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand. The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters. More formally, the n th melfilterbank of a signal in t is: where is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks). The waveform goes through a complex-valued convolution, a modulus operator and the a convolution with a lowpass-filter (the squared hanning window) that performs the decimation. When not combined with PCEN, a log-compression is added on top of TD-filterbanks after adding 1 to their absolute value to avoid numerical issues. Table 1 shows the detailed layers.",
  "y": "background uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_4",
  "x": "is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks). The waveform goes through a complex-valued convolution, a modulus operator and the a convolution with a lowpass-filter (the squared hanning window) that performs the decimation. When not combined with PCEN, a log-compression is added on top of TD-filterbanks after adding 1 to their absolute value to avoid numerical issues. Table 1 shows the detailed layers. Following<cite> [22]</cite> , the first 1D convolution filters are initialized with Gabor wavelets, to replicate mel-filterbanks, and are then learnt at the same time as the rest of the model. The second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering. ---------------------------------- **PER CHANNEL ENERGY NORMALIZATION**",
  "y": "uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_0",
  "x": "We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts <cite>(Dang et al., 1998)</cite> . Whereas previous research on tying semantics to Levin classes (Dorr, 1997) has not explicitly implemented the close relation between syntax and semantics hypothesized by Levin, our lexical resource combines traditional lexical semantic information, such as thematic roles and semantic predicates, with syntactic frames and selectional restrictions. In order to increase the utility of VerbNet, we also include links to entries in WordNet, which is one of the most widely used online lexical databases in Natural Language Processing applications. ---------------------------------- **LEVIN CLASSES AND WORDNET** Two current approaches to English verb classifications are WordNet and Levin classes. WordNet is an on-line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each representing a lexicalized concept. A synset (synonym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and -in most cases -an example sentence. Words and synsets are interrelated by means of lexical and semantic-conceptual links, respectively.",
  "y": "background uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_1",
  "x": "When an auxiliary tree across is adjoined, the dependency for the adjunction is reversed, so that variables associated with the host tree can be referenced as arguments in the adjoining tree's predicates. With this dependency from across to hit (labeled arg0), it is now possible for the semantic predicates associated with across to predicate over variables in the dependent tree hit , including the variable X arg0:arg1 instantiated as apple, resulting in the predicates motion(during(e2),apple)^via(during(e2),apple,room). Verbs in the intersective class formed by the Push/Pull verbs and the Carry verbs behave in a similar manner. The core meaning of this verb class is exertion of force. Adjunction of a path PP implying motion modifies membership of these verbs to the Carry class. Push/Pull verbs can appear in the conative construction, which emphasizes their forceful semantic component and ability to express an attempted action where any result that might be associated with the verb is not necessarily achieved; Carry verbs (used with a goal or directional phrase) cannot take the conative alternation because this would conflict with the causation of motion which is the intrinsic meaning of the class <cite>(Dang et al., 1998)</cite> . Palmer et al. (1999) and Bleam et al. (1998) also defined compositional semantics for classes of verbs implemented in FB-LTAG, but they represented general semantic components (e.g., motion, manner) as features on the nodes of the trees. Our use of separate logical forms gives a more detailed semantics for the sentence, so that for an event involving motion, it is possible to know not only that the event has a motion semantic component, but also which entity is actually in motion. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_0",
  "x": "Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics. by w p . Here, words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags B r and I r , respectively.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_1",
  "x": "Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics. by w p . Here, words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags B r and I r , respectively.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_2",
  "x": "---------------------------------- **MODEL CONSTRUCTION MODULES** As can be seen in Fig. 1 , the framework divides model construction in four phases: (I) word representation, (II) sentence representation, (III) output modeling, and (IV) inference. Phase I: The word representation of a word w i consist of three optional concatenated components: a word-embedding, a Boolean indicating if w i is the predicate of the semantic frame (w p ), and a character representation. DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (Zhou and Xu, 2015) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4). Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding. ----------------------------------",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_3",
  "x": "As can be seen in Fig. 1 , the framework divides model construction in four phases: (I) word representation, (II) sentence representation, (III) output modeling, and (IV) inference. Phase I: The word representation of a word w i consist of three optional concatenated components: a word-embedding, a Boolean indicating if w i is the predicate of the semantic frame (w p ), and a character representation. DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (Zhou and Xu, 2015) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4). Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding. ---------------------------------- **EXPERIMENTS 4.1 SETTINGS** To evaluate our framework, and show the benefits of choosing certain model components, we construct five models: HLstm, Char, CRFm, Att, and CharAtt, whose configurations are shown in Tab.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_0",
  "x": "****SPMRL'13 SHARED TASK SYSTEM: THE CADIM ARABIC DEPENDENCY PARSER**** **ABSTRACT** We describe the submission from the Columbia Arabic & Dialect Modeling group (CADIM) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL'2013). We participate in the Arabic Dependency parsing task for predicted POS tags and features. Our system is based on <cite>Marton et al. (2013)</cite> . ---------------------------------- **INTRODUCTION** In this paper, we discuss the system that the Columbia Arabic & Dialect Modeling group (CADIM) submitted to the 2013 Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) . We used a system for Arabic dependency parsing which we had previously developed, but retrained it on the training data splits used in this task. We only participated in the Arabic dependency parsing track, and in it, only optimized for predicted (non-gold) POS tags and features.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_1",
  "x": "**INTRODUCTION** In this paper, we discuss the system that the Columbia Arabic & Dialect Modeling group (CADIM) submitted to the 2013 Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) . We used a system for Arabic dependency parsing which we had previously developed, but retrained it on the training data splits used in this task. We only participated in the Arabic dependency parsing track, and in it, only optimized for predicted (non-gold) POS tags and features. We first summarize our previous work (Section 2). We then discuss our submission and the results (Section 3). ---------------------------------- **APPROACH** In this section, we summarize <cite>Marton et al. (2013)</cite> . We first present some background information on Arabic morphology and then discuss our methodology and main results.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_2",
  "x": "For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_3",
  "x": "2 Even if relevant, a feature may not necessarily contribute to optimal performance since it may be redundant with other features that surpass it in relevance. For example, the DET and STATE features alone both help parsing because they help identify the idafa construction (the modificiation of a nominal by a genitive noun phrase), but they are redundant with each other and the DET feature is more helpful since it also helps with adjectival modification of nouns. Finally, the accuracy of automatically predicting the feature values (ratio of correct predictions out of all predictions) of course affects the value of a feature on unseen text. Even if relevant and non-redundant, a feature may be hard to predict with sufficient accuracy by current technology, in which case it will be of little or no help for parsing, even if helpful when its gold values are provided. The CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful. Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic.",
  "y": "motivation"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_5",
  "x": "In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features. We defined a core set of 12 POS features, and then explored combinations of morphological features in addition to this POS tagset. This core set of POS tags is similar to those proposed in cross-lingual work (Rambow et al., 2006; Petrov et al., 2012) . We performed this search independently for Gold input features and predicted input features.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_6",
  "x": "For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_7",
  "x": "As the EasyFirst Parser predicts links separately before labels, we first optimized for unlabeled attachment score, and then optimized the Easy-First Parser labeler for label score. As had been found in previous results, assignment features, specifically CASE and STATE, are very helpful in MSA. However, in MSA this is true only under gold conditions: since CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in machine-predicted (real, non-gold) condition. In contrast with previous results, we showed that agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. Additionally, almost all work to date in MSA morphological analysis and part-of-speech (POS) tagging has concentrated on the morphemic form of the words. However, often the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the \"surface\" (form-based) morphology; a well-known example of this are the \"broken\" (irregular) plurals of nominals, which often have singular-form morphemes but are in fact plurals and show plural agreement if the referent is rational. In <cite>Marton et al. (2013)</cite> , we showed that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (2013) test (old split) 81.0 84.0 92.7 Table 2 : Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for reference also on the data splits used in our previous work <cite>(Marton et al., 2013)</cite> ; \"\u2264 70\" refers to the test sentences with 70 or fewer words. Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score 5K (SPMRL'2013) test \u2264 70 86.4 89.9 All (SPMRL'2013) test \u2264 70 87.8 90.8 Table 3 : Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags; \"\u2264 70\" refers to the test sentences with 70 or fewer words (again, both when using gold and when using predicted POS and morphological features). We also showed that for parsing with predicted POS and morphological features, training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_8",
  "x": "We also showed that for parsing with predicted POS and morphological features, training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios. ---------------------------------- **BEST PERFORMING FEATURE SET** The best performing set of features on non-gold input, obtained in <cite>Marton et al. (2013)</cite> , are shown in Table 1 . The features are clustered into three types. \u2022 First is part-of-speech, represented using a \"core\" 12-tag set. \u2022 Second are the inflectional morphological features: determiner clitic, person and functional gender and number. \u2022 Third are the rationality (humanness) feature, which participates in morphosyntactic agreement in Arabic (Alkuhlani and Habash, 2011) , and a form of the lemma, which abstract over all inflectional morphology. For the training corpus, we use a combination of the gold and predicted features. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_9",
  "x": "\u2022 Second are the inflectional morphological features: determiner clitic, person and functional gender and number. \u2022 Third are the rationality (humanness) feature, which participates in morphosyntactic agreement in Arabic (Alkuhlani and Habash, 2011) , and a form of the lemma, which abstract over all inflectional morphology. For the training corpus, we use a combination of the gold and predicted features. ---------------------------------- **OUR SUBMISSION** ---------------------------------- **DATA PREPARATION** The data split used in the shared task is different from the data split we used in <cite>(Marton et al., 2013)</cite> , so we retrained our models on the new splits (Diab et al., 2013) . The data released for the Shared Task showed inconsistent availability of lemmas across gold and predicted input, so we used the ALMOR analyzer (Habash, 2007) with the SAMA databases (Graff et al., 2009 ) to determine a lemma given the word form and the provided (gold or predicted) POS tags. In addition to the lemmas, the ALMOR analyzer also provides morphological features in the feature-value representation our approach requires.",
  "y": "differences"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_10",
  "x": "3 For simplicity reasons, we used the MLE:W2+CATiB model (Alkuhlani and Habash, 2012) , which was the best performing model on seen words, as opposed to the combination system that used a syntactic component with better results on unseen words. We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data. ---------------------------------- **RESULTS** Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_11",
  "x": "3 For simplicity reasons, we used the MLE:W2+CATiB model (Alkuhlani and Habash, 2012) , which was the best performing model on seen words, as opposed to the combination system that used a syntactic component with better results on unseen words. We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data. ---------------------------------- **RESULTS** Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] .",
  "y": "background"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_1",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [8] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_0",
  "x": "**ABSTRACT** PredPatt is a pattern-based framework for predicate-argument extraction. While it works across languages and provides a well-formed syntax-semantics interface for NLP tasks, a large-scale and reproducible evaluation has been lacking, which prevents comparisons between PredPatt and other related systems, and inhibits the updates of the patterns in PredPatt. In this work, we improve and evaluate PredPatt by introducing a large set of high-quality annotations converted from PropBank, which can also be used as a benchmark for other predicate-argument extraction systems. We compare PredPatt with other prominent systems and shows that PredPatt achieves the best precision and recall. ---------------------------------- **INTRODUCTION** PredPatt 1 <cite>(White et al., 2016</cite> ) is a pattern-based framework for predicate-argument extraction. It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de Marneffe et al., 2014) , and extracts predicates and arguments through these manual patterns. Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_1",
  "x": "Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems. The scripts for creating gold annotations and evaluation are available at: https: //github.com/hltcoe/PredPatt/tree/master/eval ---------------------------------- **CREATING GOLD ANNOTATIONS** Open Information Extraction (Open IE) and Semantic Role Labeling (SRL) (Carreras and M\u00e0rquez, 2005) are quite related: semantically labeled arguments correspond to the arguments in Open IE extractions, and verbs often match up with Open IE relations (Christensen et al., 2011) .",
  "y": "motivation background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_2",
  "x": "Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface for NLP tasks: Zhang et al. (2016) utilizes PredPatt to extract possibilistic propositions in automatic common-sense inference generation. White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics. Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems. The scripts for creating gold annotations and evaluation are available at: https: //github.com/hltcoe/PredPatt/tree/master/eval",
  "y": "motivation extends"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_3",
  "x": "---------------------------------- **IMPROVING PREDPATT** PredPatt is a pattern-based system, comprising an extensible set of clean, interpretable linguistic patterns over UD parses. By analyzing PredPatt extractions in comparison with gold annotations (Sec. 2), we are able to refine and improve PredPatt's pattern set. From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT. We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below. Fixed-MWE-pred: The UD version 2.0 introduces a new dependency relation fixed for identifying fixed function-word \"multiword expressions\" (MWEs).",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_4",
  "x": "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below. Fixed-MWE-pred: The UD version 2.0 introduces a new dependency relation fixed for identifying fixed function-word \"multiword expressions\" (MWEs). To accommodate this new feature, we add patterns to identify the MWE predicate and its argument. As shown in Figure 3 , the predicate root in this case is the dependent of fixed that is tagged as a verb (i.e., \"opposed\"); the root of its argument is the token which indirectly governs the predicate root via the case and fixed relation (i.e., \"one\"). Please use this new file as opposed to the one I sent earlier . Cut-complex-pred: The existing patterns take clausal complements (ccomp and xcomp) as predicatives of complex predicates in the argument resolution stage, where the arguments of the clausal complement will be merged into the argument set of their head predicate. For example, in the sentence \"Chris, the designer, wants to launch a new brand\", PredPatt merges the argument \"a new brand\" of the predicate \"to launch\" into the argument set of the complex predicate \"wants to launch\".",
  "y": "background extends"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_0",
  "x": "****CONVOLUTIONAL SELF-ATTENTION NETWORKS**** **ABSTRACT** Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters. ---------------------------------- **INTRODUCTION** Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_1",
  "x": "Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) . One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention <cite>(Vaswani et al., 2017)</cite> , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns Guo et al., 2019) . Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018) , which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014) . To this end, we propose novel convolutional self-attention networks (CSANs), which model locality for self-attention model and interactions between features learned by different attention heads in an unified framework.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_2",
  "x": "Moreover, we extend the convolution to a 2-dimensional area with the axis of attention head. Thus, the proposed model allows each head to interact local features with its adjacent subspaces at attention time. We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model <cite>(Vaswani et al., 2017)</cite> across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Sperber et al., 2018) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks SANs produce representations by applying attention to each pair of tokens from the input sequence, regardless of their distance. Vaswani et al. (2017) found it is beneficial to capture different contextual features with multiple individual attention functions. Given an input sequence X = {x 1 , . . . , x I } \u2208 R I\u00d7d , the model first transforms it into queries Q, keys K, and values V:",
  "y": "differences"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_3",
  "x": "We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model <cite>(Vaswani et al., 2017)</cite> across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Sperber et al., 2018) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks SANs produce representations by applying attention to each pair of tokens from the input sequence, regardless of their distance. Vaswani et al. (2017) found it is beneficial to capture different contextual features with multiple individual attention functions. Given an input sequence X = {x 1 , . . . , x I } \u2208 R I\u00d7d , the model first transforms it into queries Q, keys K, and values V: where where ATT(\u00b7) is an attention model (Bahdanau et al., 2015;<cite> Vaswani et al., 2017)</cite> that retrieves the keys K h with the query q h i .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_4",
  "x": "Concerning modeling locality for SANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is fed to the subsequent SAN layer. Several researches proposed to revise the attention distribution with a parametric localness bias, and succeed on machine translation and natural language inference (Guo et al., 2019) . While both models introduce additional parameters, our approach is a more lightweight solution without introducing any new parameters. Closely related to this work, Shen et al. (2018a) applied a positional mask to encode temporal order, which only allows SANs to attend to the previous or following tokens in the sequence. In contrast, we employ a positional mask (i.e. the tokens outside the local window is masked as 0) to encode the distance-aware local information. In the context of distance-aware SANs, Shaw et al. (2018) introduced relative position encoding to consider the relative distances between sequence elements. While they modeled locality from position embedding, we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism <cite>(Vaswani et al., 2017)</cite> employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_5",
  "x": "**EXPERIMENTS** We conducted experiments with the Transformer model <cite>(Vaswani et al., 2017)</cite> on English\u21d2German (En\u21d2De), Chinese\u21d2English (Zh\u21d2En) and Japanese\u21d2English (Ja\u21d2En) translation tasks. For the En\u21d2De and Zh\u21d2En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018) , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. ---------------------------------- **EFFECTS OF WINDOW/AREA SIZE**",
  "y": "uses"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_6",
  "x": "Following Shaw et al. (2018) , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. ---------------------------------- **EFFECTS OF WINDOW/AREA SIZE** We first investigated the effects of window size (1D-CSANs) and area size (2D-CSANs) on En\u21d2De validation set, as plotted in Figure 2 . For 1D-CSANs, the local size with 11 is superior to other settings. This is consistent with Luong et al. (2015) who found that 10 is the best window size in their local attention experiments. Then, we fixed the number of neighboring tokens being 11 and varied the number of heads. As seen, by considering the features across heads (i.e. > 1), 2D-CSANs further improve the translation quality.",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_0",
  "x": "---------------------------------- **INTRODUCTION** Recurrent neural networks (RNNs), in particular Long Short-Term Memory networks (LSTMs), have become a dominant tool in natural language processing. While LSTMs appear to be a natural choice for modeling sequential data, recently a class of non-recurrent models (Gehring et al., 2017; Vaswani et al., 2017) have shown competitive performance on sequence modeling. Gehring et al. (2017) propose a fully convolutional sequence-tosequence model that achieves state-of-the-art performance in machine translation. Vaswani et al. (2017) introduce Transformer networks that do not use any convolution or recurrent connections while obtaining the best translation performance. These non-recurrent models are appealing due to their highly parallelizable computations on modern GPUs. But do they have the same ability to exploit hierarchical structures implicitly in comparison to RNNs? In this work, we provide a first answer to this question. Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations <cite>(Bowman et al., 2015b</cite>; Tran et al., 2016; Linzen et al., 2016) . We choose Transformer as a non-recurrent model to study in this paper.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_1",
  "x": "We now proceed to measure both models' ability to learn hierarchical structure with a set of controlled experiments. ---------------------------------- **TASKS** We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by<cite> Bowman et al. (2015b)</cite> to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017) . 4 Subject-Verb Agreement Linzen et al. (2016) propose the task of predicting number agreement between subject and verb in naturally occurring English sentences as a proxy for the ability of LSTMs to capture hierarchical structure in natural language. We use the dataset provided by Linzen et al. (2016) and follow their experimental protocol of training each model using either (a) a general language model, i.e., next word prediction objective, and (b) an explicit supervision objective, i.e., predicting the number of the verb given its sentence history. Table 1 illustrates the training and testing conditions of the task.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_2",
  "x": "Specifically, for each attention head at each layer of the FAN, we compute the percentage of 2 We note that our LSTM results are better than those in Linzen et al. (2016) . Also surprising is that the language model objective yields higher accuracies than the number prediction objective. We believe this may be due to better model optimization and to the embedding-output layer weight sharing, but we leave a thorough investigation to future work. times the subject is the most attended word among all words in the history. Figure 3 shows the results for all cases where the model made the correct prediction. While it is hard to interpret the exact role of attention for different heads and at different layers, we find that some of the attention heads at the higher layers ( 2 h1, 3 h0) frequently point to the subject with an accuracy that decreases linearly with the distance between subject and verb. ---------------------------------- **LOGICAL INFERENCE** In this task, we choose the artificial language introduced by<cite> Bowman et al. (2015b)</cite> . The vocabulary of this language includes six word types {a, b, c, d, e, f } and three logical operators {or, and, not}. The task consists of predicting one of seven mutually exclusive logical relations that describe the relationship between a pair of sentences: entailment ( , ), equivalence (\u2261), exhaustive and non-exhaustive contradiction ( \u2227 , |), and two types of semantic independence (#, ).",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_3",
  "x": "We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors. These two vectors are then concatenated and fed to a 3-layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation. The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence. Here, we use a 2-layer LSTM with skip connections. The FAN maps a sentence x of length n to H = [h 1 , . . . , h n ] \u2208 R d\u00d7n . To obtain a fixedsize representation z, we use a self-attention layer with two trainable queries q 1 , q 2 \u2208 R 1\u00d7d : We find the best hyperparameters for each model by running a grid search as explained in \u00a74. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_4",
  "x": "The best of the three models achieve less than 59% accuracy on the logical inference versus 77% on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015a) . This shows that the SNLI task can be largely solved by exploiting shallow features without understanding the underlying linguistic structures, which has also been pointed out by recent work (Glockner et al., 2018; Gururangan et al., 2018) . Concurrently to our work Evans et al. (2018) proposed an alternative data set for logical inference and also found that a FAN model underperformed various other architectures including LSTMs. ---------------------------------- **MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors. These two vectors are then concatenated and fed to a 3-layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation. The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence. Here, we use a 2-layer LSTM with skip connections.",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_5",
  "x": "The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence. Here, we use a 2-layer LSTM with skip connections. The FAN maps a sentence x of length n to H = [h 1 , . . . , h n ] \u2208 R d\u00d7n . To obtain a fixedsize representation z, we use a self-attention layer with two trainable queries q 1 , q 2 \u2208 R 1\u00d7d : We find the best hyperparameters for each model by running a grid search as explained in \u00a74. ---------------------------------- **RESULTS** Following the experimental protocol of<cite> Bowman et al. (2015b)</cite> , the data is divided into 13 bins based on the number of logical operators. Both FANs and LSTMs are trained on samples with at most n logical operators and tested on all bins.",
  "y": "uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_0",
  "x": "We use two dependency relation extraction methods to extract dependency relations from each self-attention heads of BERT and RoBERTa. The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> . The second-maximum spanning tree (MST)-computes a maximum spanning tree over the attention matrix, and is meant to identify generalist heads that can form complete, syntactically informative dependency trees. We analyze the extracted dependency relations and trees to investigate whether the attention heads of these models track syntactic dependencies significantly better than chance or baselines, and what type of dependency relations they learn best. In contrast to probing models (Adi et al., 2017; Conneau et al., 2018) , our methods require no further training. In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general. We are able to replicate their results on BERT using our MAX method. In addition, we also perform a similar analysis on BERT models fine-tuned on natural language understanding tasks as well as RoBERTa. Our experiments suggest that there are particular attention heads of BERT and RoBERTa that encode certain dependency relation types such as nsubj, obj with substantially higher accuracy than our baselines-a randomly initialized Transformer and relative positional baselines. We find that fine-tuning BERT on the syntax-oriented CoLA does not significantly impact the accuracy of extracted dependency relations.",
  "y": "background uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_1",
  "x": "The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> . The second-maximum spanning tree (MST)-computes a maximum spanning tree over the attention matrix, and is meant to identify generalist heads that can form complete, syntactically informative dependency trees. We analyze the extracted dependency relations and trees to investigate whether the attention heads of these models track syntactic dependencies significantly better than chance or baselines, and what type of dependency relations they learn best. In contrast to probing models (Adi et al., 2017; Conneau et al., 2018) , our methods require no further training. In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general. We are able to replicate their results on BERT using our MAX method. In addition, we also perform a similar analysis on BERT models fine-tuned on natural language understanding tasks as well as RoBERTa. Our experiments suggest that there are particular attention heads of BERT and RoBERTa that encode certain dependency relation types such as nsubj, obj with substantially higher accuracy than our baselines-a randomly initialized Transformer and relative positional baselines. We find that fine-tuning BERT on the syntax-oriented CoLA does not significantly impact the accuracy of extracted dependency relations. However, when fine-tuned on the semantics-oriented MNLI dataset, we see improvements in accuracy for longer-distance clausal relations and a slight loss in accuracy for shorter-distance relations.",
  "y": "motivation background extends"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_2",
  "x": "They find that the best dependency score is not significantly higher than a right-branching tree baseline. Voita et al. (2019) find the most confident attention heads of the Transformer NMT encoder based on a heuristic of the concentration of attention weights on a single token, and find that these heads mostly attend to relative positions, syntactic relations, and rare words. Additionally, researchers have investigated the syntactic knowledge that BERT learns by analyzing the contextualized embeddings (Warstadt et al., 2019a) and attention heads of BERT<cite> (Clark et al., 2019)</cite> . Goldberg (2018) analyzes the contextualized embeddings of BERT by computing language model surprisal on subject-verb agreement and shows that BERT learns significant knowledge of syntax. Tenney et al. (2019b) introduce a probing classifier for evaluating syntactic knowledge in BERT and show that BERT encodes syntax more than semantics. Hewitt and Manning (2019) train a structural probing model that maps the hidden representations of each token to an inner-product space that corresponds to syntax tree distance. They show that the learned spaces of strong models such as BERT and ELMo (Peters et al., 2018) are better for reconstructing dependency trees compared to baselines. Clark et al. (2019) train a probing classifier on the attentionheads of BERT and show that BERT's attention heads capture substantial syntactic information. While there has been prior work on analysis of the attention heads of BERT, we believe we are the first to analyze the dependency relations learned by the attention heads of fine-tuned BERT models and RoBERTa. ----------------------------------",
  "y": "background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_3",
  "x": "Both methods operate on the attention weight matrix W \u2208 (0, 1) T \u00d7T for a given head at a given layer, where T is the number of tokens in the sequence, and the rows and columns correspond to the attending and attended tokens respectively (such that each row sums to 1). Method 1: Maximum Attention Weights (MAX) Given a token A in a sentence, a selfattention mechanism is designed to assign high attention weights on tokens that have some kind of relationship with token A (Vaswani et al., 2017) . Therefore, for a given token A, a token B that has the highest attention weight with respect to the token A should be related to token A. Our aim is to investigate whether this relation maps to a universal dependency relation. We assign a relation (w i , w j ) between word w i and w j if j = argmax W [i] for each row (that corresponds to a word in attention matrix) i in attention matrix W . Based on this simple strategy, we extract relations for all sentences in our evaluation datasets. This method is similar to<cite> Clark et al. (2019)</cite> , and attempts to recover individual arcs between words; the relations extracted using this method need not form a valid tree, or even be fully connected, and the resulting edge directions may or may not match the canonical directions. Hence, we evaluate the resulting arcs individually and ignore their direction. After extracting dependency relations from all heads at all layers, we take the maximum UUAS over all relations types. ---------------------------------- **METHOD 2: MAXIMUM SPANNING TREE (MST)**",
  "y": "similarities extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_0",
  "x": "The recently introduced BERT model (Devlin et al., 2018) , which is based on transformers, achieves state-of-the-art results on eleven natural language processing tasks. In this work, we assess BERT's ability to learn structure-dependent linguistic phenomena of agreement relations. To test whether BERT is sensitive to agreement relations, we use the cloze test (Taylor, 1953 , also called the \"masked language model\" objective), in which we mask out one of two words in an agreement relation and ask BERT to predict the masked word, one of the two tasks on which BERT is initially trained. <cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_1",
  "x": "This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study. Section 3 introduces our newly curated cross-linguistic dataset of agreement relations, while section 4 discusses our experimental setup. We report the results of our experiments in section 5. All data and code are available at https://github.com/ geoffbacon/does-bert-agree. ---------------------------------- **STRUCTURE-DEPENDENT AGREEMENT RELATIONS**",
  "y": "motivation"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_2",
  "x": "<cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study. Section 3 introduces our newly curated cross-linguistic dataset of agreement relations, while section 4 discusses our experimental setup. We report the results of our experiments in section 5. All data and code are available at https://github.com/ geoffbacon/does-bert-agree.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_3",
  "x": "The subject and verb in (2) agree for number, while the noun and determiner in (3), the noun and attributive adjective in (4) and the subject and predicated adjective in (5) agree for both number and gender. (2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. ---------------------------------- **DATA**",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_4",
  "x": "Languages with richer inflectional morphology tend to display more agreement types and involve more features. French, for example, employs all four types of agreement relations. Examples are given in (2)- (5). The subject and verb in (2) agree for number, while the noun and determiner in (3), the noun and attributive adjective in (4) and the subject and predicated adjective in (5) agree for both number and gender. (2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_5",
  "x": "French, for example, employs all four types of agreement relations. Examples are given in (2)- (5). The subject and verb in (2) agree for number, while the noun and determiner in (3), the noun and attributive adjective in (4) and the subject and predicated adjective in (5) agree for both number and gender. (2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_6",
  "x": "' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. ---------------------------------- **DATA** Our study requires two types of data. First, we need sentences containing agreement relations. We mask out one of the words in the agreement relation and ask BERT to predict the masked word.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_7",
  "x": "The UniMorph project also uses a consistent schema across all languages to annotate word types with morphological features. Although this schema is not the same as that used in UD, there is a deterministic mapping between the two (McCarthy et al., 2018) . In data, we say a word can take on a particular bundle if we ever see it with that bundle of feature values in a Universal Dependencies corpus for that language. Both sources individually allow for a word to have multiple feature bundles (e.g. sheep in English can be singular or plural). In these cases, we keep all possible feature bundles. Finally, we filter out words that do not appear in BERT's vocabulary. ---------------------------------- **EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> .",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_8",
  "x": "**EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_9",
  "x": "---------------------------------- **EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_10",
  "x": "This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word. By \"incorrect words\", we mean words of the same part of speech as the masked word but that differ from the masked word with respect to at least one feature value. We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_11",
  "x": "To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word. By \"incorrect words\", we mean words of the same part of speech as the masked word but that differ from the masked word with respect to at least one feature value. We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_12",
  "x": "To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words. By \"correct words\", we mean words with the exact same feature values and the same part of speech as the masked word. By \"incorrect words\", we mean words of the same part of speech as the masked word but that differ from the masked word with respect to at least one feature value. We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_13",
  "x": "Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_14",
  "x": "Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_15",
  "x": [
   "We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target."
  ],
  "y": "differences"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_0",
  "x": "---------------------------------- **INTRODUCTION** Increasingly complex neural networks have achieved highly competitive results for many NLP tasks (Vaswani et al., 2017; Devlin et al., 2018) , but they prevent human experts from understanding how and why a prediction is made. Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) . For example, in order to make diagnoses based on the medical report in Table 1 , the highlighted symptoms may be extracted as evidence. Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation. She was begun on heparin anticoagulated carefully secondary to the petechial bleed . She started weaning from the vent the next day.",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_1",
  "x": "Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) . For example, in order to make diagnoses based on the medical report in Table 1 , the highlighted symptoms may be extracted as evidence. Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation. She was begun on heparin anticoagulated carefully secondary to the petechial bleed . She started weaning from the vent the next day. She was started on Digoxin to control her rate and her Cardizem was held. She was started on antibiotics for possible aspiration pneumonia . Her chest xray showed retrocardiac effusion . She had some bleeding after nasogastric tube insertion .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_2",
  "x": "Her chest xray showed retrocardiac effusion . She had some bleeding after nasogastric tube insertion . Diagnoses: Cerebral artery occlusion; Unspecified essential hypertension; Atrial fibrillation; Diabetes mellitus. nism to show the importance of basic units (words or ngrams) through their attention weights. However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1 , such as \"bleeding after nasogastric tube insertion\", are larger than a single word. Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting.",
  "y": "motivation background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_3",
  "x": "For instance, useful symptoms in Table 1 , such as \"bleeding after nasogastric tube insertion\", are larger than a single word. Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting. Thus the explainability of the model is limited. Lei et al. (2016) introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases. However, their method can not tell how much a selected unit contributes to the model's decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all highlighted symptoms in Table 1 ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_4",
  "x": "However, their method can not tell how much a selected unit contributes to the model's decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all highlighted symptoms in Table 1 ---------------------------------- **GENERIC ARCHITECTURE AND BASELINES** Our work leverages the attention-based selfexplaining method<cite> (Lin et al., 2017)</cite> , as shown in Figure 1 . First, our text encoder ( \u00a73) formulates an input text into a list of basic units, learning a vector representation for each, where the basic units can be words, phrases, or arbitrary ngrams. Then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights {\u03b1 1 , ..., \u03b1 n }. Eventually, the attention weight \u03b1 i will be used to reveal how important a basic unit h i is. The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction.",
  "y": "uses"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_5",
  "x": "Then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights {\u03b1 1 , ..., \u03b1 n }. Eventually, the attention weight \u03b1 i will be used to reveal how important a basic unit h i is. The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction. ---------------------------------- **BASELINES:** We compare two types of baseline text encoders in Figure 1 . (1)<cite> Lin et al. (2017)</cite> (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) . The original model in (Mullenbach et al., 2018) only utilizes 4-grams. Here we extend this model to take all unigrams, bigrams, and up to n-grams as the basic units. For a fair comparison, both our approach and the baselines share the same architecture, and the only difference is the text encoder used. volving seven pathogens , discusses pitfalls of routine blood cultures and examines the role of the laboratory in microbiologic diagnosis .",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_0",
  "x": "Section 3 describes the classifier and the features in our proposed WSD approach. Section 4 describes the experiments and the analysis of our results. Section 5 is the conclusion. ---------------------------------- **RELATED WORK** Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , <cite>[11]</cite> ) and maximum entropy [10] . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_1",
  "x": "A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express \" \", the bi-grams in their system are ( , , , Some bi-grams such as may have higher frequency but may introduce noise when considering it as features in disambiguating the sense \"human| \" and \"symbol| \" like in the example case of \" \". In our system, we do not rely on co-occurrence information. Instead, we utilize true collocation information ( , ) which fall in the window size of (-5, +5) as fea-tures and the sense of \"human| \" can be decided clearly using this features. The collocation information is a pre-prepared collocation list obtained from a collocation extraction system and verified with syntactic and semantic methods ( [21] , [24] ). Yarowsky [9] used the one sense per collocation property as an essential ingredient for an unsupervised Word-Sense Disambiguation algorithm to perform bootstrapping algorithm on a more general high-recall disambiguation.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_2",
  "x": "A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express \" \", the bi-grams in their system are ( , , , Some bi-grams such as may have higher frequency but may introduce noise when considering it as features in disambiguating the sense \"human| \" and \"symbol| \" like in the example case of \" \". In our system, we do not rely on co-occurrence information. Instead, we utilize true collocation information ( , ) which fall in the window size of (-5, +5) as fea-tures and the sense of \"human| \" can be decided clearly using this features. The collocation information is a pre-prepared collocation list obtained from a collocation extraction system and verified with syntactic and semantic methods ( [21] , [24] ). Yarowsky [9] used the one sense per collocation property as an essential ingredient for an unsupervised Word-Sense Disambiguation algorithm to perform bootstrapping algorithm on a more general high-recall disambiguation.",
  "y": "background differences"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_3",
  "x": "As expected, the collocation method achieved a good precision around 79% in English and 82% in German but a very low recall which is 3% in English and 1% in German. The low recall is due to the nature of UMLS where many collocations would almost never occur in natural text. To avoid this problem, we combine the contextual features in the target context with the pre-prepared collocations list to build our classifier. As stated early, an important issue is what features will be used to construct the classifier in WSD. Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results [10] for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features. These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_4",
  "x": "These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature. Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet. ---------------------------------- **LOCAL COLLOCATION FEATURES** We chose collocations as the local features.",
  "y": "background uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_5",
  "x": "**LOCAL COLLOCATION FEATURES** We chose collocations as the local features. A collocation is a recurrent and conventional fixed expression of words which holds syntactic and semantic relations [21] . Collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations. Fixed collocations means the appearance of one word implies the co-occurrence of another one such as \" \" (\"burden of history\"), while strong collocations allows very limited substitution of the components, for example, \" \" (\"local college\"), or \" \" (\"local university\"). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_6",
  "x": "Collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations. Fixed collocations means the appearance of one word implies the co-occurrence of another one such as \" \" (\"burden of history\"), while strong collocations allows very limited substitution of the components, for example, \" \" (\"local college\"), or \" \" (\"local university\"). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = \u00b1 5).",
  "y": "differences background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_0",
  "x": "One can create a small dictionary of common input \u2192 category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017 (Bui et al., , 2018 , which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016) . In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017) . SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017;<cite> Ortega and Vu, 2017)</cite> . The main contributions of the paper are: \u2022 Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.",
  "y": "motivation"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_1",
  "x": "Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in (Lee and Dernoncourt, 2016;<cite> Ortega and Vu, 2017)</cite> . ---------------------------------- **EXPERIMENTAL SETUP** We setup our experimental evaluation, as follows: given a classification task and a dataset, we generate an on-device model. The size of the model can be configured (by adjusting the projection matrix P) to fit in the memory footprint of the device, i.e. a phone has more memory compared to a smart watch. For each classification task, we report Accuracy on the test set. ---------------------------------- **HYPERPARAMETER AND TRAINING** For both datasets we used the following: 2-layer SGNN (P T =80,d=14 \u00d7 FullyConnected 256 \u00d7 FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (Loshchilov and Hutter, 2016) .",
  "y": "uses"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_2",
  "x": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN (Lee and Dernoncourt, 2016) , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. ---------------------------------- **DISCUSSION ON MODEL SIZE AND INFERENCE**",
  "y": "background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_3",
  "x": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN (Lee and Dernoncourt, 2016) , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. ---------------------------------- **DISCUSSION ON MODEL SIZE AND INFERENCE**",
  "y": "differences"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_5",
  "x": "Majority Class (baseline) <cite>(Ortega and Vu, 2017)</cite> 33.7 Naive Bayes (baseline) (Khanpour et al., 2016) 47.3 HMM (Stolcke et al., 2000) 71.0 DRLM-conditional training (Ji and Bilmes, 2006) 77.0 DRLM-joint training (Ji and Bilmes, 2006) 74.0 LSTM (Lee and Dernoncourt, 2016) 69.9 CNN (Lee and Dernoncourt, 2016) 73.1 Gated-Attention&HMM (Tran et al., 2017) 74.2 RNN+Attention <cite>(Ortega and Vu, 2017)</cite> 73.8 RNN (Khanpour et al., 2016) 80.1 SGNN: Self-Governing Neural Network (ours) 83.1 <cite>(Ortega and Vu, 2017)</cite> 59.1 Naive Bayes (baseline) (Khanpour et al., 2016) 74.6 Graphical Model (Ji and Bilmes, 2006) 81.3 CNN (Lee and Dernoncourt, 2016) 84.6 RNN+Attention <cite>(Ortega and Vu, 2017)</cite> 84.3 RNN (Khanpour et al., 2016) 86.8 SGNN: Self-Governing Neural Network (ours) 86.7 Table 3 : MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). ---------------------------------- **CONCLUSION** We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and Dernoncourt, 2016; Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly. In the future, we are interested in extending this approach to more natural language tasks. For instance, we built a multilingual SGNN model for customer feedback classification (Liu et al., 2017) and obtained 73% on Japanese, close to best performing system on the challenge (Plank, 2017) .",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_0",
  "x": "Equally as importantly, this model enables us to realize the concrete computations in lower dimensional spaces, thus reduce the space complexity of the implementation. We experiment in two different tasks with promising results: First, we repeat the disambiguation experiment of <cite>Grefenstette and Sadrzadeh (2011a)</cite> for transitive verbs. Then we proceed to a novel task: We use The Oxford Junior Dictionary (Sansome et al., 2000) , Oxford Concise School Dictionary (Hawkins et al., 2004) , and WordNet in order to derive a set of term/definition pairs, measure the similarity of each term with every definition, and use this measurement to classify the definitions to specific terms. ---------------------------------- **AN OVERVIEW OF THE CATEGORICAL MODEL** Using the abstract framework of category theory, Coecke et al. (2010) equip the distributional models of meaning with compositionality in a way that every grammatical reduction is in one-to-one correspondence with a linear map defining mathematical manipulations between vector spaces. In other words, given a sentence s = w 1 w 2 \u00b7 \u00b7 \u00b7 w n there exists a syntax-driven linear map f from the context vectors of the individual words to a vector for the whole sentence: allowing us to compare the synonymy of two different sentences as if they were words, by constructing their vectors and measuring the distance between them. This result is based on the fact that the base type-logic of the framework, a pregroup grammar (Lambek, 2008) , shares the same abstract structure with vector spaces, that of a compact closed category. If P is the free pregroup generated by such a grammar and FVect the category of finite dimensional vector spaces (with linear maps) over , it is possible then for one to work on the product category FVect \u00d7 P, pairing each grammatical type \u03b1 \u2208 P with a vector space V to an object (V, \u03b1).",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_1",
  "x": "Even more importantly, it does not specify the exact form of the sentence space S, leaving these details as open questions for the implementor. ---------------------------------- **STIPULATING S = N \u2297 N** The work of <cite>Grefenstette and Sadrzadeh (2011a)</cite> was the first large-scale practical implementation of this framework for intransitive and transitive sentences, and thus a first step towards providing some concrete answers to these questions. Following ideas from formal semantics that verbs are actually relations, the authors argue that the distributional meaning of a verb is a weighted relation representing the extent according to which the verb is related to its subjects and objects. In vector spaces, these relations are represented by linear maps, equivalent to matrices for the case of binary relations and to tensors for relations of arity n. Hence transitive verbs can be represented by matrices created by structurally mixing and summing up all the contexts (subject and object pairs) in which the verb appears. More precisely, we have: where \u2212\u2192 sb j i and \u2212 \u2212 \u2192 ob j i are the context vectors of subject and object, respectively, and i iterates over all contexts in which the specific verb occurs. This method (which we refer to as \"relational\") is also extended to other relational words, such as adjectives whose vectors are constructed as the sum of all the nouns that the adjective modifies. One important design decision was that the meaning of a sentence was represented as a rank-n tensor, where n is the number of arguments for the head word of the sentence.",
  "y": "background"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_2",
  "x": "---------------------------------- **STIPULATING S = N** The work presented in this paper stems from the observation that the theory does not impose a special choice of sentence space, in particular it does not impose that tensors for S should have ranks greater than 1. Hence we stipulate that S = N and show how this instantiation works by performing the computations on the example transitive sentence 'dogs chase cats'. Take \u2212 \u2212 \u2192 do g and \u2212\u2192 cat be the context vectors for the subject and the object, both living in N as prescribed by their types. As any vector, these can be expressed as weighted sums of their basis vectors, that is, . By putting everything together, the meaning of the sentence is calculated as follows; this result lives in N , since it is a weighted sum over \u2212 \u2192 n j : An important consequence of our design decision is that it enables us to reduce the space complexity of the implementation from \u0398(d n )<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> to \u0398(d), making the problem much more tractable. What remains to be solved is a theoretical issue, that in practice the meaning of relational words such as 'chase' as calculated by Equation 5 is a matrix living in N 2 -however, the mathematical framework above prescribes that it should be a rank-3 tensor in N 3 . The necessary expansions are achieved by using Frobenius algebraic operations, for which the following sections first provide the mathematical definitions and then a linguistic justification.",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_3",
  "x": "Together, they faithfully encode tensors of a lower dimensional N into a higher dimensional tensor space N \u2297 N . In linear algebraic terms, \u03c3(v) is a diagonal tensor whose diagonal elements consist of weights of v. The uncopying map \u00b5, on the other hand, loses some information when encoding a higher dimensional tensor into a lower dimensional space. For w \u2208 N \u2297 N , we have that \u00b5(w) is a tensor consisting only of the diagonal elements of w, hence losing the information encoded in the non-diagonal part. ---------------------------------- **FROBENIUS PARAMETERS IN DISTRIBUTIONAL LINGUISTIC PRACTICE** It would be instructive to see how our decision for taking S = N and the Frobenius constructions affect the meaning of a sentence in practice. We use a pictorial calculus that allows convenient graphical representations of the derivations. In this notation, each tensor is represented by a triangle, and its rank can be determined by the outgoing wires. The tensor product is depicted as juxtaposition of triangles. We also remind to the reader that the relational method for constructing a tensor for the meaning of a verb<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> provides us with a matrix in N 2 .",
  "y": "similarities"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_4",
  "x": "We should bring to the reader's attention the fact that equipped with the above closed forms we do not need to create or manipulate rank-3 tensors at any point of the computation, something that would cause high computational overhead. Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS** We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> . Specifically, we use the 2000 most frequent words as the basis for our vector space; this single space will serve as a semantic space for both nouns and sentences. The weights of the vectors are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. As our similarity measure we use the cosine distance. ---------------------------------- **DISAMBIGUATION**",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_5",
  "x": "Taking as an example the diagram of the copy-subject method, we see that: (a) the object interacts with the verb; (b) the result of this interaction serves as input for the \u03c3 function; (c) one wire of the output of \u03c3 interacts with the object, while the other branch delivers the result. In terms of linear algebra, this corresponds to the computation \u03c3(ver b \u00d7 \u2212\u2192 o b j) \u00d7 \u2212 \u2192 s b j (where \u00d7 denotes matrix multiplication), which is equivalent to the following: where the symbol denotes component-wise multiplication and \u00d7 is matrix multiplication. Similarly, the meaning of a transitive sentence for the copy-object case is given by: We should bring to the reader's attention the fact that equipped with the above closed forms we do not need to create or manipulate rank-3 tensors at any point of the computation, something that would cause high computational overhead. Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS** We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> . Specifically, we use the 2000 most frequent words as the basis for our vector space; this single space will serve as a semantic space for both nouns and sentences.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_6",
  "x": "We first test our models against the disambiguation task for transitive sentences described in <cite>Grefenstette and Sadrzadeh (2011a)</cite> . The goal is to assess how well a model can discriminate between the different senses of an ambiguous verb, given the context (subject and object) of that verb. The entries of this dataset consist of a target verb, a subject, an object, and a landmark verb used for the comparison. One such entry for example is, \"write, pupil, name, spell\". A good compositional model should be able to understand that the sentence \"pupil write name\" is closer to the sentence \"pupil spell name\" than, for example, to \"pupil publish name\". On the other hand, given the context \"writer, book\" these results should be reversed. The dataset contains 200 such entries with verbs from CELEX, hence 400 sentences. The evaluation of this experiment is performed by calculating Spearman's \u03c1 correlation against the judgements of 25 human evaluators. As our baselines we use an additive (ADDTV) and a multiplicative (MULTP) model, where the meaning of a sentence is computed by adding and point-wise multiplying, respectively, the context vectors of its words. The results are shown in Table 1 .",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_7",
  "x": "On the other hand, given the context \"writer, book\" these results should be reversed. The dataset contains 200 such entries with verbs from CELEX, hence 400 sentences. The evaluation of this experiment is performed by calculating Spearman's \u03c1 correlation against the judgements of 25 human evaluators. As our baselines we use an additive (ADDTV) and a multiplicative (MULTP) model, where the meaning of a sentence is computed by adding and point-wise multiplying, respectively, the context vectors of its words. The results are shown in Table 1 . The most successful S = N model for this task is the copyobject model, which is performing really close to the original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> , with the difference to be statistically insignificant. This is a promising result, since it suggests that the lower-dimensional new model performs similarly with the richer structure of the old model for transitive sentences, while at the same time allows generalisation to even more complex sentences 1 . More importantly, note that the categorical models are the only ones that respect the word order and grammatical structure of sentences; a feature completely dismissed in the simple multiplicative model. ---------------------------------- **UPPER-BOUND**",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_8",
  "x": "The ability of reliably comparing the meaning of single words with larger textual fragments, e.g. phrases or even sentences, can be an invaluable tool for many challenging NLP tasks, such as definition classification, paraphrasing, sentiment analysis, or even the simple everyday search on the Internet. In this task we examine the extent to which our models can correctly match a number of terms (single words) with a number of definitions (phrases). To our knowledge, this is the first time a compositional distributional model is tested for its ability to match words with phrases. Our dataset consists of 112 terms (72 nouns and 40 verbs) and their main definitions, extracted from The Oxford Junior Dictionary (Sansome et al., 2000) . For each term, and in order to get a richer dataset, we added two more definitions that expressed the same or an 1 The original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> with S = N 2 , provided a \u03c1 of 0.21. When computed with our program with the exact same parameters (without embedding them in the S = N model), we obtained a \u03c1 of 0.195. The differences between both of these and our best model are statistically insignificant. In Grefenstette and Sadrzadeh (2011b) , a direct non-relational model was used to compute verb matrices; this provided a \u03c1 of 0.28. However, as explained by the authors themselves, this method is not general and for instance cannot be used for intransitive verbs.",
  "y": "extends differences"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_0",
  "x": "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder (Bowman et al., 2016; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. Bowman et al. (2016) uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context. They also introduced a loss clipping strategy in order to make the model more robust. Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_1",
  "x": "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 . ---------------------------------- **METHODOLOGY** ---------------------------------- **BACKGROUND OF VAE**",
  "y": "uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_2",
  "x": "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 . ---------------------------------- **METHODOLOGY**",
  "y": "differences uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_3",
  "x": "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing. Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder. However, one should note that our architecture can be readily applied to other types of RNN such as GRU. For each time stamp t (see Figure 1b) , we concatenate the hidden state h t and the cell state c t of the encoder. The concatenation (i.e., [h t ; c t ]) is then fed into two linear transformation layers for estimating \u00b5 t and \u03c3 2 t , which are parameters of a normal distribution corresponding to the concatenation of h t and c t . Let Q \u03c6t (z t |x) = N (z t |\u00b5 t , \u03c3 2 t ), we wish Q \u03c6t (z t |x) to be close to a prior P (z t ), which is a standard Gaussian. Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q \u03c6t and P (z t )) will contribute to the overall KL loss of the ELBO.",
  "y": "motivation"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_4",
  "x": "Each utterance in a mini-batch was padded to the maximum length for that batch, and the maximum batch-size allowed is 128. ---------------------------------- **BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance. Table 2 shows the language modelling results of our approach and the baselines. We report negative log likelihood (NLL), KL loss, and perplexity (PPL) on the test set.",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_5",
  "x": "**BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance. Table 2 shows the language modelling results of our approach and the baselines. We report negative log likelihood (NLL), KL loss, and perplexity (PPL) on the test set. As expected, all the models have a higher KL loss in the inputless setting than the standard setting, as z is required to encode more information about the input data for reconstruction. In terms of overall performance, our model outperforms all the baselines in both datasets (i.e., PTB and E2E).",
  "y": "background uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_0",
  "x": "It shows that attention mechanism is crucial for word inflection. However, in contrast to machine translation, a symbol of output word is less prone to depend from multiple input symbols, than a translated word from multiple source words. Consequently, the attention weight is usually concentrated on a single source symbol, being more a pointer than a distributed probability mass. Moreover, this pointer traverses the source word from left to right in order to generate the inflected form. All that motivated the hard attention model of (Aharoni and Goldberg, 2017) , which outperformed the soft attention approaches. The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol. This model was further improved by<cite> (Makarov et al., 2017)</cite> , whose system was the winner of Sigmorphon 2017 evaluation campaign (Cotterell et al., 2017) . The approach of Makarov et al. was especially successful in low and medium resource setting, while in high resource setting it achieves an impressive accuracy of over 95% 1 . Does it mean that no further research is required and hard attention method equipped with copy mechanism is the final solution for automatic inflection problem? Actually, not, since the quality of the winning approach was much lower on medium (about 85%) and low (below 50%) datasets.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_1",
  "x": "In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_2",
  "x": "In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages.",
  "y": "extends"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_3",
  "x": "In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages.",
  "y": "differences"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_4",
  "x": "Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction. ---------------------------------- **MODEL STRUCTURE** ---------------------------------- **BASELINE MODEL** As the state-of-the-art baseline we choose the model of Makarov et al.<cite> (Makarov et al., 2017)</cite> , the winner of previous Sigmorphon Shared Task.",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_5",
  "x": "We also use the copy gate from<cite> (Makarov et al., 2017)</cite> : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above. The weight \u03c3 i is the output of another one-layer perceptron: ---------------------------------- **CHARACTER-BASED MODEL** Our proposal is to explicitly equip the decoder with the information from the character-based language model. We suppose it will help the model to avoid outputting phonetically implausible sequences of letters. We choose the simplest possible architecture of the language model, namely, on each step it takes a concatenation of d previous symbol embeddings u i = [g i\u2212d , . . . , g i\u22121 ] and applies an LSTM cell to obtain a vector v i and update LSTM hidden state h i . v i is propagated through a two-layer perceptron to predict the next output symbol analogously to the output layer of the baseline model: The model is trained to predict next output symbol separately from the basic model. In principle, one can use more complex neural architectures, for example, a multilayer LSTM or apply attention mechanism.",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_6",
  "x": "The dataset also contained a development set containing 1000 instances most of the time, for all languages we used this subset as validation data. Overall, there were 86 languages in the high setting, 102 in medium and 103 in low. ---------------------------------- **RESULTS AND DISCUSSION** We submitted three systems, one replicating the algorithm of<cite> (Makarov et al., 2017)</cite> , the second equipped with language models. The third one used only the language models: we extracted all possible abstract inflection paradigms for a given set of grammatical features and created a set of possible candidate forms applying all paradigms to the lemma. For example, consider the word \u0434\u0435\u043b\u0430\u0442\u044c and paradigms 1+\u0430\u0442\u044c#1+\u0435\u0442, 1+\u0430\u0442\u044c#1+\u0438\u0442, 1+\u044c#1 and 1+\u0447\u044c#1+\u0436\u0435\u0442; the first three produce the forms \u0434\u0435\u043b\u0430\u0435\u0442, \u0434\u0435\u043b\u0438\u0442, \u0434\u0435\u043b\u0430\u0442, while the fourth yields nothing since the given word does not end in -\u0447\u044c. Then all these forms are ranked using sum of logarithmic probabilities from forward and backward language models. Our results are mostly negative, since our language-model based architecture produced only marginal improvement over the model of Makarov et al. which it is based on. Moreover, for the lowresource setting the performance of both system was mediocre, even our third paradigm-based system was able to overperform them despite its obvious weakness.",
  "y": "uses"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_0",
  "x": "Natural language processing (NLP) is a sub-field of artificial intelligence that focuses on making natural languages understandable to computers. Similarly, the translation between different natural languages is a task for machine translation (MT). Neural machine translation (NMT) is a recent approach in MT which learns patterns between source and target language corpora to produce text translations using deep neural networks (Sutskever et al., 2014) . One downside of models trained with human generated corpora is that social biases present in the data are learned. This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation<cite> (Bolukbasi et al., 2016)</cite> . This can affect downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017) . The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems. An example of this gender bias is the word \"friend\" in the English sentence \"She works in a hospital, my friend is a nurse\" would be correctly translated to \"amiga\" (feminine of friend) in Spanish, while \"She works in a hospital, my friend is a doctor\" would be incorrectly translated to \"amigo\" (masculine of friend) in Spanish. We consider that this translation contains gender bias since it ignores the fact that, for both cases, \"friend\" is a female and translates by focusing on the occupational stereotypes, i.e. translating doctor as male and nurse as female. The main contribution of this study is providing progress on the recent detected problem which is gender bias in MT (Prates et al., 2018) .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_1",
  "x": "The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings. It consists of two main parts: First the direction of the embeddings where the bias is present is identified. Second, the gender neutral words in this direction are neutralized to zero and also equalizes the sets by making the neutral word equidistant to the remaining ones in the set. The disadvantage of the first part of the process is that it can remove valuable information in the embed-dings for semantic relations between words with several meanings that are not related to the bias being treated.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_2",
  "x": "For nouns, such as countries and their respective capitals or for the conjugations of verbs. While there are many techniques for extracting word embeddings, in this work we are using Global Vectors, or GloVe (Pennington et al., 2014) . Glove is an unsupervised method for learning word embeddings. This count-based method, uses statistical information of word occurrences from a given corpus to train a vector space for which each vector is related to a word and their values describes their semantic relations. ---------------------------------- **DEBIASING WORD EMBEDDINGS** The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_3",
  "x": "More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings. It consists of two main parts: First the direction of the embeddings where the bias is present is identified. Second, the gender neutral words in this direction are neutralized to zero and also equalizes the sets by making the neutral word equidistant to the remaining ones in the set. The disadvantage of the first part of the process is that it can remove valuable information in the embed-dings for semantic relations between words with several meanings that are not related to the bias being treated. (Zhao et al., 2018b) is an algorithm for learning gender netural word embeddings models.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_4",
  "x": "The parameter values for training the word embedding models with GloVe and GN-GloVe methods are listed in Table 3 . Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a debiasing post-process performed on trained embeddings. Instead of having parameters for learning the representation it uses a set of words to define the gender direction and to neutralize and equalize the bias from the word vectors. Three set of words are used in the Debiaswe algorithm. One set of ten pairs of words such as woman-man, girl-boy, she-he... are used to define the gender direction. Another set of 218 genderspecific words such as aunt, uncle, wife, husband... are used for learning a larger set of genderspecific words. Finally, a set of crowd-sourced male-female equalization pairs such as dad-mom, boy-girl, granpa-grandma... that represent gender direction are equalized in the algorithm. For the Spanish side, the sets are adapted for the task and slightly modified to avoid unclear words from the English language or unnecessary repetitions. The sets from GN-GloVe are similarly adapted to the Spanish language. The architecture to train the models for the translation task is the Transformer (Vaswani et al., 2017) .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_5",
  "x": "The most neutral system is achieved with GloVe and Debiaswe pretrained embeddings, being updated also updated during training. The improvement is over 80% compared to the baseline system and over 4% compared to the non-debiased pre-trained word embeddings. ---------------------------------- **CONCLUSIONS AND FURTHER WORK** Biases learned from human generated corpora in natural language processing applications is a topic that has been gaining relevance over the last few years. Specifically, for machine translation, studies quantifying gender bias present in news corpora and proposing debiasing approaches for word embedding models have shown improvements on this matter. We studied the impact of gender debiasing on neural machine translation. We trained sets of word embeddings with the standard GloVe algorithm. Then, we debiased the embeddings using Debiaswe<cite> (Bolukbasi et al., 2016)</cite> and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) . We used all these different models on the Transformer (Vaswani et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "4bc5fc3bccb704b9978b294ffb07de_0",
  "x": "**ABSTRACT** Given the current growth in research and related emerging technologies in machine learning and deep learning, it is timely to introduce this tutorial to a large number of researchers and practitioners who are attending COLING 2018 and working on statistical models, deep neural networks, sequential learning and natural language understanding. To the best of our knowledge, there is no similar tutorial presented in previous ACL/COLING/EMNLP/NAACL. This three-hour tutorial will concentrate on a wide range of theories and applications and systematically present the recent advances in deep Bayesian and sequential learning which are impacting the communities of computational linguistics, human language technology and machine learning for natural language processing. ---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; <cite>Li et al., 2016a</cite>) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
  "y": "background uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_0",
  "x": "Inspired by the greedy neural network transition-based parser of Chen and Manning (2014) , Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing. 1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words style) feature for each word's morphological attributes, and (2) a weighted set-valued feature for each word's k-best POS tags. These features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_1",
  "x": "Inspired by the greedy neural network transition-based parser of Chen and Manning (2014) , Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing. 1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words style) feature for each word's morphological attributes, and (2) a weighted set-valued feature for each word's k-best POS tags. These features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ---------------------------------- **MODEL**",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_2",
  "x": "Morphology. It is well known that morphological information is very important for parsing morphologically rich languages (see for example Bohnet et al. (2013) ). We incorporate morphological information into our model using a setvalued feature function. We define the feature group morph as the matrix X morph such that, for where N f is the number of morphological features active on the token indexed by f . In other words, we embed a bag of features into a shared embedding space by averaging the individual feature embeddings. k-best Tags. The non-linear network models of Weiss et al. (2015) and Chen and Manning (2014) embed the 1-best tag, according to a first-stage tagger, for a select set of tokens for any configuration. Inspired by the work of<cite> Bohnet and Nivre (2012)</cite> , we embed the set of top tags according to a first-stage tagger. Specifically, we define the feature group ktags as the matrix X ktags such that, for",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_3",
  "x": "The integrated arc-standard transition system of<cite> Bohnet and Nivre (2012)</cite> allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc-standard system. It does this by replacing the shift action in the arc-standard system with an action shift p , which, aside from shifting the top token on the buffer also assigns it one of the k best POS tags from a first-stage tagger. We also experiment with the swap action of Nivre (2009) , which allows reordering of the tokens in the input sequence. This transition system is able to produce non-projective parse trees, which is important for some languages. Results. The effect of using the integrated transition system is quantified in the bottom part of Table 1. The use of both 1) +morph +kbest features and 2) integrated parsing and tagging achieves the best score for 5 out of 7 languages tested. The use of integrated parsing and tagging provides, for example, a 0.8% LAS gain in German. ---------------------------------- **EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_4",
  "x": "We use 1024 units in all hidden layers, a choice made based on the development set. We found network sizes to be of critical importance for the accuracy of our models. For example, LAS improvements can be as high as 0.98% in CoNLL'09 German when increasing the size of the two hidden layers from 200 to 1024. We use B = 16 or B = 32 based on the development set performance per language. For ease of experimentation, we deviate from<cite> Bohnet and Nivre (2012)</cite> and use a single unstructured beam, rather than separate beams for POS tag and parse differences. We train our neural networks on the standard training sets only, except for initializing with word embeddings generated by word2vec and using cluster features in our POS tagger. Unlike Weiss et al. (2015) we train our model only on the treebank training set and do not use tri-training, which can likely further improve the results. ---------------------------------- **CONLL '09** Our multilingual evaluation follows the setup of the CoNLL '09 shared task 2 (Haji\u010d et al., 2009) .",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_5",
  "x": "Results. In Table 3 , we compare our models to the winners of the CoNLL '09 shared task, Gesmundo et al. (2009 ), Bohnet (2009 ), Che et al. (2009 ), Ren et al. (2009 , as well as to more recent results on the same datasets. It is worth pointing out that Gesmundo et al. (2009) is itself a neural net parser. Our models achieve higher labeled accuracy than the winning systems in the shared task in all languages. Additionally, our pipelined neural network parser always outperforms its linear counterpart, an in-house reimplementation of the system of Zhang and Nivre (2011) , as well as the more recent and highly accurate parsers of Zhang and McDonald (2014) and Lei et al. (2014 again outperforms its linear counterpart<cite> (Bohnet and Nivre, 2012)</cite> , however, in some cases the addition of graph-based and cluster features<cite> (Bohnet and Nivre, 2012</cite> )+G+C can lead to even better results. The improvements in POS tagging (Table  2 ) range from 0.3% for English to 1.4% absolute for Chinese and are always higher for the neural network models compared to the linear models. ---------------------------------- **ENGLISH WSJ** We experiment on English using the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) , with standard train/test splits. We convert the constituency trees to Stanford style dependencies (De Marneffe et al., 2006 ) using version 3.3.0 of the converter.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_6",
  "x": "We use predicted POS tags and exclude punctuation from the evaluation, as is standard for English. Results. The results shown in Table 4 , we find that our full model surpasses, to our knowledge, all previously reported supervised parsing models for the Stanford dependency conversions. It surpasses its linear analog, the work of<cite> Bohnet and Nivre (2012)</cite> on Stanford Dependencies UAS by 0.9% UAS and by 1.14% LAS. It also outperforms the pipeline neural net model of Weiss et al. (2015) by a considerable margin and matches the semisupervised variant of Weiss et al. (2015) . ---------------------------------- **ENGLISH TREEBANK UNION** Turning to cross-domain results, and the \"Treebank Union\" datasets, we use an identical setup to the one described in Weiss et al. (2015) . This setup includes the WSJ with Stanford Dependencies, the OntoNotes corpus version 5 (Hovy et al., 2006) , the English Web Treebank (Petrov and McDonald, 2012) , and the updated and corrected Question Treebank (Judge et al., 2006) . We train on the union of each corpora's training set and test on each domain separately.",
  "y": "differences"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_0",
  "x": "<cite>Zhang et al. (2017)</cite> utilize adversarial training to obtain cross-lingual word embeddings without any parallel data. However, their performance is still significantly worse than supervised methods. <cite>Zhang et al. (2017)</cite> apply adversarial training to align monolingual word vector spaces with no supervision.",
  "y": "background"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_1",
  "x": "The pseudo-code for this process is shown in Algorithm 1. It should be noted that there is no variance once completing training. Our experiments could be divided into two parts. In the first part, we conduct experiments on smallscale datasets and our main baseline is <cite>Zhang et al. (2017)</cite> . In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) ---------------------------------- **SMALL-SCALE DATASETS** In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space.",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_2",
  "x": "---------------------------------- **SMALL-SCALE DATASETS** In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space. For each source word embedding x, it would be first transformed into z x . The the its k nearest target embeddings would be retrieved and be compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-1 accuracy. ---------------------------------- **EXPERIMENTS ON CHINESE-ENGLISH DATASET**",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_3",
  "x": "Performance is measured by top-1 accuracy. ---------------------------------- **EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 . We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models.",
  "y": "uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_4",
  "x": "Performance is measured by top-1 accuracy. ---------------------------------- **EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 . We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models.",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_5",
  "x": "For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 . We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models. ---------------------------------- **EXPERIMENTS ON OTHER LANGUAGE PAIRS DATASETS** We also conduct experiments on Spanish-English and Italian-English language pairs.",
  "y": "differences uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_6",
  "x": "We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 . Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset. Our model is able to outperform baseline model in this setting. ---------------------------------- **MODEL** Accuracy ( ---------------------------------- **LARGE-SCALE DATASETS** In this section, we integrate our method with Conneau et al. (2018) , whose method improves <cite>Zhang et al. (2017)</cite> by more sophiscated refinement procedure and validation criterion.",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_7",
  "x": "We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 . Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset. Our model is able to outperform baseline model in this setting. ---------------------------------- **MODEL** Accuracy ( ---------------------------------- **LARGE-SCALE DATASETS** In this section, we integrate our method with Conneau et al. (2018) , whose method improves <cite>Zhang et al. (2017)</cite> by more sophiscated refinement procedure and validation criterion.",
  "y": "differences extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_0",
  "x": "This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model. ---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_1",
  "x": "---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_2",
  "x": "---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_3",
  "x": "We propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model. ---------------------------------- **INTRODUCTION** Natural language processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_4",
  "x": "Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions. This strategy boosts decoding speed by an order of magnitude over stochastic search in our experiments. Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus. ---------------------------------- **RELATED WORK** Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application<cite> (MacCartney et al., 2008)</cite> . Outside of NLI, prior research has also explored the task of monolingual word align-ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002) . ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009 ).",
  "y": "background motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_5",
  "x": "This enables the aligner to properly link paraphrases such as death penalty and capital punishment by exploiting lexical resources. ---------------------------------- **DATASET** MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006) . The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES** A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources).",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_6",
  "x": "Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus. ---------------------------------- **RELATED WORK** Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application<cite> (MacCartney et al., 2008)</cite> . Outside of NLI, prior research has also explored the task of monolingual word align-ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002) . ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009 ). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. ---------------------------------- **THE MANLI ALIGNER** Our alignment system is structured identically to MANLI<cite> (MacCartney et al., 2008)</cite> and uses the same phrase-based alignment representation.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_7",
  "x": "MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006) . The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES** A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. ----------------------------------",
  "y": "uses extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_8",
  "x": "Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. ---------------------------------- **PARAMETER INFERENCE** Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002) , an intuitive structured prediction technique. We deviate from <cite>MacCartney et al. (2008)</cite> and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. ---------------------------------- **DECODING** The decoding problem is that of finding the highestscoring alignment under some parameter values for the model.",
  "y": "extends differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_9",
  "x": "This expresses the score of an alignment as the sum of scores of edits that are present in it, i.e., edits e that have x e = 1. In order to address the phrase segmentation issue discussed in \u00a73.4, we merely need to add linear constraints ensuring that every token participates in exactly one edit. Introducing the notation e \u227a t to indicate that edit e covers token t in one of its phrases, this constraint can be encoded as: highest-scoring alignment under w. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. ---------------------------------- **ALIGNMENT EXPERIMENTS** For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in \u00a73. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. \u00a73.1) using the training parameters specified in <cite>MacCartney et al. (2008)</cite> . Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_10",
  "x": "All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. \u00a73.1) using the training parameters specified in <cite>MacCartney et al. (2008)</cite> . Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1 . We first observe that our reimplemented version of MANLI improves over the results reported in <cite>MacCartney et al. (2008)</cite> , gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations. We attribute at least some part of this gain to our modified parameter inference (cf. \u00a73.3) which avoids normalizing the structured perceptron weights and instead adheres closely to the algorithm of Collins (2002) . Although exact decoding improves alignment performance over the approximate search approach, the gain is marginal and not significant. This seems to indicate that the simulated annealing search strategy is fairly effective at avoiding local maxima and finding the highest-scoring alignments. Table 2 contains the results from timing alignment tasks over various corpora on the same machine using the models trained as per \u00a74.1.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_11",
  "x": "The interaction between the selection of soft features for structured prediction and hard constraints for decoding is an interesting avenue for further research on this task. Initial experiments with a feature that considers the similarity of dependency heads of tokens in an edit (similar to MANLI's contextual features that look at preceding and following words) yielded some improvement over the baseline models; however, this did not perform as well as the simple constraints described above. Specific features that approximate soft variants of these constraints could also be devised but this was not explored here. In addition to the NLI applications considered in this work, we have also employed the MANLI alignment technique to tackle alignment problems that are not inherently asymmetric such as the sentence fusion problems from McKeown et al. (2010) . Although the absence of asymmetric alignment features affects performance marginally over the RTE2 dataset, all the performance gains exhibited by exact decoding with constraints appear to be preserved in symmetric settings. ---------------------------------- **CONCLUSION** We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in <cite>MacCartney et al. (2008)</cite> by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_12",
  "x": "An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by <cite>MacCartney et al. (2008)</cite> with regard to the original MANLI system. In addition, stricter constraints that enforce the alignment of syntactically-related tokens (rather than just their inclusion in the solution) may also yield performance gains. Although MANLI's structured prediction approach to the alignment problem allows us to encode preferences as features and learn their weights via the structured perceptron, the decoding constraints used here can be used to establish dynamic links between alignment edits which cannot be determined a priori. The interaction between the selection of soft features for structured prediction and hard constraints for decoding is an interesting avenue for further research on this task. Initial experiments with a feature that considers the similarity of dependency heads of tokens in an edit (similar to MANLI's contextual features that look at preceding and following words) yielded some improvement over the baseline models; however, this did not perform as well as the simple constraints described above. Specific features that approximate soft variants of these constraints could also be devised but this was not explored here. In addition to the NLI applications considered in this work, we have also employed the MANLI alignment technique to tackle alignment problems that are not inherently asymmetric such as the sentence fusion problems from McKeown et al. (2010) . Although the absence of asymmetric alignment features affects performance marginally over the RTE2 dataset, all the performance gains exhibited by exact decoding with constraints appear to be preserved in symmetric settings. ---------------------------------- **CONCLUSION**",
  "y": "similarities future_work"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_0",
  "x": "Finally, a linear transformation is applied on the concatenation of both weighted contexts. Varis and Bojar (2017) implement and compare two multisource architectures: In the first setup, they use a single encoder with concatenation of src and mt sentences, and in the second setup, they use two character-level encoders for mt and src, separately, along with a character-level decoder. The initial state of this decoder is a weighted combination of the final states of the two encoders. Intuitively, such an integration of sourcelanguage information in APE should be useful in conveying the context information to improve the APE performance. To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe). The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_1",
  "x": "The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> . The entire model is optimized as a single end-to-end transformer network. 2 Transformer-Based Multi-Source APE MT errors originating from the input source sentences suggest that APE systems should leverage information from both the src and mt, instead of considering mt in isolation. This can help the model to disambiguate corrections applied at every time step. Generating the pe output from mt is greatly facilitated by the availability of src. To achieve benefits from both single-source (mt \u2192 pe) and multi-source ({src, mt} \u2192 pe) APEs, our primary submission in the WMT 2018 shared task is an ensemble of these two models.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_2",
  "x": "The initial state of this decoder is a weighted combination of the final states of the two encoders. Intuitively, such an integration of sourcelanguage information in APE should be useful in conveying the context information to improve the APE performance. To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe). The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> . The entire model is optimized as a single end-to-end transformer network. 2 Transformer-Based Multi-Source APE MT errors originating from the input source sentences suggest that APE systems should leverage information from both the src and mt, instead of considering mt in isolation.",
  "y": "similarities uses"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_3",
  "x": "Our single-source model (SS) is based on an encoder-decoder-based transformer architecture <cite>(Vaswani et al., 2017)</cite> . Transformer models can replace sequence-aligned recurrence entirely and follow three types of multi-head attention: encoder-decoder attention (also known as vanilla Figure 1 : Multi-source transformer-based APE attention), encoder self-attention, and masked decoder self-attention. Since for multi-head attention each head uses different linear transformations, it can learn these separate relationships in parallel, thereby improving learning time. ---------------------------------- **MULTI-SOURCE TRANSFORMER FOR APE ({SRC, MT} \u2192 PE)** For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling. One self-attended encoder for src maps s = (s 1 , s 2 , ..., s k ) into a sequence of continuous representations, enc src = (e 1 , e 2 , ..., e k ), and a second encoder for mt, m = (m 1 , m 2 , ..., m l ), returns another sequence of continuous representations, enc mt = (e \u2032 1 , e \u2032 2 , ..., e \u2032 l ).",
  "y": "extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_4",
  "x": "For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling. One self-attended encoder for src maps s = (s 1 , s 2 , ..., s k ) into a sequence of continuous representations, enc src = (e 1 , e 2 , ..., e k ), and a second encoder for mt, m = (m 1 , m 2 , ..., m l ), returns another sequence of continuous representations, enc mt = (e \u2032 1 , e \u2032 2 , ..., e \u2032 l ). The self-attended joint encoder (cf. Figure 1 ) then receives the concatenation of enc src and enc mt , enc concat = [enc src , enc mt ] as an input, and passes it through the stack of 6 layers, with residual connections, a self-attention and a position-wise fully connected feed-forward neural network. As a result, the joint encoder produces a final representation (enc src,mt ) for both src and mt. Self-attention at this point provides the advantage of aggregating information from all of the words, including src and mt, and successively generates a new representation per word informed by the entire src and mt context. The decoder generates the pe output in sequence, dec pe = (p 1 , p 2 , . . . , p n ), one word at a time from left to right by attending previously generated words as well as the final representations (enc src,mt ) generated by the encoder. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_5",
  "x": "For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling. One self-attended encoder for src maps s = (s 1 , s 2 , ..., s k ) into a sequence of continuous representations, enc src = (e 1 , e 2 , ..., e k ), and a second encoder for mt, m = (m 1 , m 2 , ..., m l ), returns another sequence of continuous representations, enc mt = (e \u2032 1 , e \u2032 2 , ..., e \u2032 l ). The self-attended joint encoder (cf. Figure 1 ) then receives the concatenation of enc src and enc mt , enc concat = [enc src , enc mt ] as an input, and passes it through the stack of 6 layers, with residual connections, a self-attention and a position-wise fully connected feed-forward neural network. As a result, the joint encoder produces a final representation (enc src,mt ) for both src and mt. Self-attention at this point provides the advantage of aggregating information from all of the words, including src and mt, and successively generates a new representation per word informed by the entire src and mt context. The decoder generates the pe output in sequence, dec pe = (p 1 , p 2 , . . . , p n ), one word at a time from left to right by attending previously generated words as well as the final representations (enc src,mt ) generated by the encoder. ---------------------------------- **ENSEMBLE** In order to leverage the network architecture for both single-source and multi-source APE as discussed above, we decided to ensemble several expert neural models.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_6",
  "x": "The output dimension produced by all sub-layers and embedding layers is defined as d model = 256. All dropout values in the network are set to 0.2. Each encoder and decoder contains a fully connected feed-forward network having dimensionality d model = 256 for the input and output and dimensionality d f f = 1024 for the inner layer. This is a similar setting to<cite> Vaswani et al. (2017)</cite> 's C \u2212 model 1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional- For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> . At training time, the batch size is set to 32 samples, with a maximum sentence length of 80 subwords, and a vocabulary of the 50K most frequent subwords.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_7",
  "x": "During training, we employ label smoothing of value \u03f5 ls = 0.1. The output dimension produced by all sub-layers and embedding layers is defined as d model = 256. All dropout values in the network are set to 0.2. Each encoder and decoder contains a fully connected feed-forward network having dimensionality d model = 256 for the input and output and dimensionality d f f = 1024 for the inner layer. This is a similar setting to<cite> Vaswani et al. (2017)</cite> 's C \u2212 model 1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional- For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_8",
  "x": "The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> . At training time, the batch size is set to 32 samples, with a maximum sentence length of 80 subwords, and a vocabulary of the 50K most frequent subwords. After each epoch, the training data is shuffled. For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since<cite> Vaswani et al. (2017)</cite> reported nearly identical results to sinusoidal encodings. After finishing training, we save the 5 best checkpoints saved at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints. During decoding, we perform greedy-search-based decoding. We follow a similar hyper-parameter setup for mt \u2192 pe. The total number of parameters for our {src, mt} \u2192 pe and mt \u2192 pe model is 46 \u00d7 10 6 and 28 \u00d7 10 6 , respectively. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_9",
  "x": "**CONCLUSIONS AND FUTURE WORK** In this paper, we investigated a novel transformerbased multi-source APE approach that jointly attends over a combination of src and mt to capture dependencies between the two. This architecture yields statistically significant improvements over single-source transformer-based models. An ensemble of both variants increases the performance further. For the PBSMT task, the baseline MT system was outperformed by 3.2 BLEU points, while the NMT baseline remains 0.51 BLEU points better than our APE approach on the 2018 test set. In the future, we will investigate if the performance of each system can be improved by using a different hyper-parameter setup. Unfortunately, we could not test either the 'big' or the 'base' hyper-parameter configuration in<cite> Vaswani et al. (2017)</cite> due to unavailable computing resources at the time of submission. As additional future work, we would like to explore whether using re-ranking and ensembling of different neural APEs helps to improve the performance further. Moreover, we will incorporate word-level quality estimation features of mt into the encoding layer. Lastly, we will evaluate if our model indeed is able to better handle word order errors and to capture longrange dependencies, as we expect.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_0",
  "x": "However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: WIKIHOP (Welbl et al., 2018) and HotpotQA . These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1 , we show an excerpt from one sample in WIKIHOP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_1",
  "x": "Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning. The study in Song et al. (2018) In this paper, we propose a new method to solve the multi-hop RC problem across multiple documents. Inspired by the success of GNN based methods (Song et al., 2018; De Cao et al., 2018) for multi-hop RC, we introduce a new type of graph, called Heterogeneous Document-Entity (HDE) graph. Our proposed HDE graph has the following advantages: \u2022 Instead of graphs with single type of nodes (Song et al., 2018; De Cao et al., 2018) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information. Specifically, instead of only entity nodes as in (Song et al., 2018; De Cao et al., 2018) , we include nodes corresponding to candidates, documents and entities. In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; \u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_2",
  "x": "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK** The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015) , MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_3",
  "x": "Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates. Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task. Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK** The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_4",
  "x": "\u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates. Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task. Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK** The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_5",
  "x": "For example, the entity \"get ready\" in query and two entities \"Mase\" and \"Sean Combs\" co-occur in the 2nd support document, and both \"Mase\" and \"Sean Combs\" can lead to the correct answer \"bad boy records\". Based on this observation, we propose to extract mentions of both query subject s and candidates C q from documents. We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_6",
  "x": "Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration. Same operations can be applied to other documents, or between the query and extracted entities. Given RNN-encoded sequences of the query H q \u2208 R lq\u00d7h and a document H i s \u2208 R l i s \u00d7h , the affinity matrix between the query and document can be calculated as where denotes matrix transpose. Each entry of the matrix A i qs indicates how related two words are, one from the query and one from the document.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_7",
  "x": "Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration. Same operations can be applied to other documents, or between the query and extracted entities. Given RNN-encoded sequences of the query H q \u2208 R lq\u00d7h and a document H i s \u2208 R l i s \u00d7h , the affinity matrix between the query and document can be calculated as where denotes matrix transpose.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_8",
  "x": "We further encode the co-attended document context using a bidirectional RNN f with GRU: The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by <cite>Zhong et al. (2019)</cite> . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence. Formally, the self-attention module can be formulated as the following operations given S ca as input: where M LP (\u00b7) is a two-layer MLP with tanh as activation function.",
  "y": "differences uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_9",
  "x": "We further encode the co-attended document context using a bidirectional RNN f with GRU: The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by <cite>Zhong et al. (2019)</cite> . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence. Formally, the self-attention module can be formulated as the following operations given S ca as input: where M LP (\u00b7) is a two-layer MLP with tanh as activation function.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_10",
  "x": "Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query. ---------------------------------- **REASONING OVER HDE GRAPH** Graph building: let a HDE graph be denoted as G = {V, E}, where V stands for node representations and E represents edges between nodes. In our proposed HDE graph based model, we treat each document, candidate and entity extracted from documents as nodes in the HDE graph, i.e., each document (candidate/entity) corresponds to one node in the HDE graph. These nodes represent different granularity levels of query-aware information: document nodes encode documentlevel global information regarding to the query; candidate nodes encode query-aware information in candidates; entity nodes encode query-aware information in specific document context or the query subject. The HDE graph is built to enable graph-based reasoning. It exploits useful structural information among query, support documents and candidates.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_11",
  "x": "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query. ---------------------------------- **REASONING OVER HDE GRAPH** Graph building: let a HDE graph be denoted as G = {V, E}, where V stands for node representations and E represents edges between nodes. In our proposed HDE graph based model, we treat each document, candidate and entity extracted from documents as nodes in the HDE graph, i.e., each document (candidate/entity) corresponds to one node in the HDE graph. These nodes represent different granularity levels of query-aware information: document nodes encode documentlevel global information regarding to the query; candidate nodes encode query-aware information in candidates; entity nodes encode query-aware information in specific document context or the query subject. The HDE graph is built to enable graph-based reasoning. It exploits useful structural information among query, support documents and candidates. We expect our HDE graph could perform multi-hop reasoning to locate the answer nodes or entity nodes of answers given a query.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_13",
  "x": "**RESULTS** In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results. We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6%<cite> (Zhong et al., 2019)</cite> to 70.9%. Compared to two previous studies using GNN for multi-hop reading comprehension (Song et al., 2018; De Cao et al., 2018) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (Peters et al., 2018) . ---------------------------------- **ABLATION STUDIES** In order to better understand the contribution of different modules to the performance, we conduct several ablation studies on the development set of WIKIHOP. If we remove the proposed HDE graph and directly use the representations of candidates and entities corresponding to mentions of candidates (equation 7) for score accumulation, the accuracy on WIKIHOP development set drops 2.6% absolutely. This proves the efficacy of the proposed HDE graph on multi-hop reasoning across multiple documents. If we treat all edge types equally without using different GNN parameters for different edge types (equation 9), the accuracy drops 1.4%, which indicates that different information encoded by different types of edges is also important to retain good performance; If only scores of entity nodes (right part of equation 12) are considered in score accumulation, the accuracy on dev set degrades by 1.0%; if only scores of candidates nodes (left part of equation 12) are considered, the accuracy degrades by 1.5%.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_0",
  "x": "In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents. ---------------------------------- **INTRODUCTION** Cultural transmission of language occurs when one group of agents passes their language to a new group of agents, e.g. parents who teach their children to speak as they do. Of the many design features which make human language unique, cultural transmission is important partially because it allows language itself to change over time via cultural evolution (Tomasello, 1999; Christiansen & Kirby, 2003a) . This helps explain how a modern language like English emerged from some proto-language, an \"almost language\" precursor lacking the functionality of modern languages. Figure 1 . We introduce cultural transmission into language emergence between neural agents. The starting point of our study is the goal oriented dialogue task of <cite>Kottur et al. (2017)</cite> , summarized in Fig. 2 .",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_1",
  "x": "This kind of structure helps give human language the ability to express infinitely many concepts using finitely many elements, and to generalize in obviously correct ways despite a dearth of training examples (Lake & Baroni, 2018) . For example, an agent who understands blue square and purple triangle should also understand purple square without directly experiencing it; we use this sort of generalization to measure compositionality. Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> , but it only investigates how language changes within a generation. Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, 2001 ) and with human subjects (Kirby et al., 2008) . In this model, language is directly but incompletely transmitted (taught) to one generation of agents from the previous generation. Because learning is incomplete and biased, the student language may differ from the teacher language. With the right learning and transmission mechanisms, a noncompositional language becomes compositional after many generations. This is cast as a trade-off between expressivity and compressibility, where a language must be expressive enough to differentiate between all possible meanings (e.g., objects) and compressible enough to be learned (Kirby et al., 2015) . The explanation is so prominent that it was somewhat surprising when it was recently found that other factors can cause enough compressibility pressure to get compositional language emergence without generational transmission (Raviv et al., 2018) . In AI, emergence work aims to influence efforts to build intelligent agents that communicate with each other and especially with humans using language.",
  "y": "motivation background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_2",
  "x": "Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017) , possibly because the target languages can't express the same concepts as the source languages. This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Choi et al., 2018) . In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically. The resulting knowledge gap makes it easier for the new agent to learn from the older agents than to create a new language. In this way our approach introduces cultural transmission and thus a compressibility pressure, causing compositionality to emerge. One difference between our approach and evolutionary methods applied elsewhere in deep learning (Stanley & Miikkulainen, 2002; Stanley et al., 2009; Real et al., 2017) is that we emulate cultural evolution instead of biological evolution. In biological evolution agents change from generation to generation while in cultural evolution the language itself evolves, so the same agent can have different languages at different times.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_3",
  "x": "Other approaches use less supervision, placing multiple agents in carefully designed environments and giving them goals which require communication (Mikolov et al., 2016; Gauthier & Mordatch, 2016; Kiela et al., 2016) . If some of the agents in the environment already know a language like English then the other agents can indirectly learn that language. But even this is expensive because it requires some agent that already knows a language. On the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b) . Unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human. Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017) , possibly because the target languages can't express the same concepts as the source languages. This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Choi et al., 2018) . In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_4",
  "x": "Each conversation has T rounds (we use T = 2). Q-bot starts by observing A-bot's previous response m t\u22121 A , its view of the world x Q (e.g., task), and its previous memory h t\u22121 Q , then outputs memory h t Q summarizing its current view of the dialog and utters message m A ). After the conversation, Q-bot tries to solve the given task by predicting u (e.g., corresponding to red square) as a function of its observation and final memory:\u00fb = U (x Q , h T Q ). Both agents are rewarded if both attributes are correct. As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) . Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task. ---------------------------------- **1**",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_5",
  "x": "This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t. both Q-bot and A-bot parameters appropriate agent and vocabulary configurations. ---------------------------------- **LANGUAGE EMERGENCE WITH CULTURAL TRANSMISSION** Here we add cultural transmission to neural dialog models by considering an implicit model of cultural transmission. Implicit cultural transmission does not use word-level supervision, as opposed to explicit cultural transmission in which students are told which words refer to which objects. In implicit cultural transmission shared language emerges from shared goals. We develop an implicit model of cultural transmission 2 that periodically replaces agents.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_6",
  "x": "As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) . Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task. ---------------------------------- **1** This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t. both Q-bot and A-bot parameters appropriate agent and vocabulary configurations. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_7",
  "x": "Consequentially, older agents remember the old language while new agents learn it, favoring more easily compressible compositional language. Replacement. One open choice in this approach is how to select which agents to re-initialize -we explore different options in this section. Every E epochs, replacement policy \u03c0 is called and returns a list of agents to be re-initialized, as seen at the blue lines (10-12) of Algorithm 1 3 . This process creates generations of agents such that each generation learns languages that are slightly different but eventually improve upon those of previous generations. Single Agent. In <cite>Kottur et al. (2017)</cite> there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost. Instead, we consider two strategies that only replace one bot at a time: -Random. Sample Q-bot or A-bot uniformly -Alternate.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_8",
  "x": "This pressure for cultural transmission comes from the imbalance in knowledge between young and old agents created by reinitializing old bots. Section 4 supports this argument by showing that languages evolved via our approach are more similar to each other than otherwise. ---------------------------------- **EXPERIMENTS** In this section we investigate how our language evolution mechanism affects the structure of emergent languages. We show that our replacement approach causes compositionality and that cultural transmission does occur despite the implicit nature of our implementation. ---------------------------------- **NEURAL QUESTION ANSWERING DETAILS** Task Description. As in <cite>Kottur et al. (2017)</cite> , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_9",
  "x": "This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) . Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_10",
  "x": "The explicit annotation of independent properties like shape and color allows compositionality to be tested, a necessarily domain specific evaluation. Certain combinations of attributes (e.g., purple square) are held out of the training set while ensuring that at least one instance of each attribute is present (e.g., at least one purple thing and one square thing). If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) . Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_11",
  "x": "For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000) . We add one setting where A-bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors.",
  "y": "differences similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_12",
  "x": "For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000) . We add one setting where A-bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_13",
  "x": "In the Multi Agent setting we use N A = N Q = 5. To decide when to stop we measure validation set accuracy averaged over all Q-bot-Abot pairs and choose the first population whose validation accuracy did not improve for 200k epochs. 7 This differs from <cite>Kottur et al. (2017)</cite> , which stopped once train accuracy reached 100%. Furthermore, we do not mine negatives for each training batch. ---------------------------------- **BASELINES. TWO BASELINES HELP VERIFY OUR APPROACH:** -No Replacement. Never replace any agent (i.e., Algorithm 1 without blue lines). -Replace All. Always replace every agent (i.e., with B = all agents at line 11 of Algorithm 1).",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_14",
  "x": "However, each time lines 11 and 12 of Algorithm 1 are executed there is one more chance of getting a lucky random initialization. Since the No Replacement baseline never does this it has a smaller chance of running in to one such lucky agent. Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in<cite> (Kottur et al., 2017)</cite> . 7 There are few objects in the environment, so each batch contains all objects and is an entire epoch. are not simply due to luck. In the Multi Agent setting we increased E from 5000 to 25000 because agents were slower to converge. ---------------------------------- **IMPACT OF CULTURAL TRANSMISSION** Agent performance has a lot of variance, so we split the train data into 4 separate folds and perform cross-validation, averaging across folds as well as 4 different random seeds within each fold for a total of 16 runs per experiment. Results with standard deviations are reported in Fig. 3 and p-values for all t-tests are reported in the supplement.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_15",
  "x": "Variations in replacement strategy do not appear to significantly affect outcomes. The Single Agent Random/Alternate replacement strategies are usually not significantly different than each other (p \u2264 0.05). The same is true for the Multi Agent Uniform Random/Epsilon Greedy/Oldest strategies. Significant differences only occur in the Overcomplete setting, where Single Agent Alternate outperforms Multi Agent Uniform Random, and in a few Small Vocab settings. This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_16",
  "x": "The same is true for the Multi Agent Uniform Random/Epsilon Greedy/Oldest strategies. Significant differences only occur in the Overcomplete setting, where Single Agent Alternate outperforms Multi Agent Uniform Random, and in a few Small Vocab settings. This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_17",
  "x": "This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_18",
  "x": "Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts. Removing memory always makes a significant difference (p \u2264 0.05) to Small Vocab models and only sometimes makes a difference for Overcomplete models. When it is significant, it helps Small Vocab models and hurts Overcomplete models. As the Memoryless + Overcomplete setting has not been reported before, these results suggest that the relationship between inter-round memory and compositionality is not clear. Overall, these results show that adding cultural transmission to neural dialog agents improves the compositional generalization of the languages learned by those agents in a way complementary to other priors. It thereby shows how to transfer the cultural transmission principle from evolutionary linguistics to deep learning. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_19",
  "x": "Researchers have spent decades studying how unique properties of human language like compositionality could have emerged. There is general agreement that people acquire language using a combination of innate cognitive capacity and learning from other language speakers (cultural transmission), with the degree of each being widely disputed (Perfors, 2002; Pinker & Bloom, 1990) . Most agree the answer is something in between. Both innate cognitive capacity and specific modern human languages like English coevolved (Briscoe, 2000) via biological (Pinker & Bloom, 1990) and cultural (Tomasello, 1999; Smith, 2006) evolution, respectively. Thus an explanation of these evolutionary processes is essential to explaining human language. In particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017 ). An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1. This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al., 2008; Cornish et al., 2009; Scott-Phillips & Kirby, 2010) experiments. Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge. While existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_20",
  "x": "In particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017 ). An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1. This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al., 2008; Cornish et al., 2009; Scott-Phillips & Kirby, 2010) experiments. Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge. While existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents. We consider such a setting because of its potential to complement our existing understanding. Language Emergence in Deep Learning. Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_21",
  "x": "We consider such a setting because of its potential to complement our existing understanding. Language Emergence in Deep Learning. Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> . Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality. Cultural Evolution and Neural Nets. Some work has considered the evolution of ideas by cultural transmission using neural agents.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_0",
  "x": "**ABSTRACT** Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. <cite>Koppel and Ordan (2011)</cite> performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties. In this paper, we are validating both types of translation properties using original and translated texts from six European languages. ---------------------------------- **INTRODUCTION** Even though it is content words that are semantically rich, function words also play an important role in a text. Function words are more frequent and predictable than content words.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_1",
  "x": "Recently, scholars in this area identified several properties of the translation process with the aid of corpora (Baker, 1993; Baker, 1996; Olohan, 2001; Laviosa, 2002; Hansen, 2003; Pym, 2005) . These properties are subsumed under four keywords: explicitation, simplification, normalization and levelling out. They focus on the general effects of the translation process. Toury (1995) has a different theory from these. That is, a translated text will carry some fingerprints of its source language. Recently, Pastor et al. (2008) and Ilisei et al. (2009; have provided empirical evidence of simplification translation properties using a comparable corpus of Spanish. <cite>Koppel and Ordan (2011)</cite> perform empirical studies to validate both theories, using a subcorpus extracted from the Europarl (Koehn, 2005) and IHT corpora<cite> (Koppel and Ordan, 2011)</cite> . They used a comparable corpus of original English and English translated from five other European languages. In addition, original English and English translated from Greek and Korean was also used in their experiment. They have found that a translated text contains both source language dependent and independent features. Obviously, corpora of this sort, which focus on a single language (e.g., English), are not adequate for claiming the universal validity of translation properties. Different languages (and language families) have different linguistic properties.",
  "y": "motivation background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_2",
  "x": "Van Halteren (2008) uses the Europarl corpus for the first time to identify the source language of text for which the source language marker was missing. Support vector regression was the best performing method. Pastor et al. (2008) and Ilisei et al. (2009; perform classification of Spanish original and translated text. The focus of their works is to investigate the simplification relation that was proposed by (Baker, 1996) . In total, 21 quantitative features (e.g. a number of different POS, average sentence length, the parse-tree depth etc.) were used where, nine (9) of them are able to grasp the simplification translation property. <cite>Koppel and Ordan (2011)</cite> have built a classifier that can identify the correct source of the translated text (given different possible source languages). They have built another classifier which can identify source text and translated text. Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.",
  "y": "motivation background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_3",
  "x": "Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages. A list of 300 function words (Pennebaker et al., 2001 ) was used as feature vector for these classifications. Popescu (2011) uses string kernels (Lodhi et al., 2002) to study translation properties. A classifier was built to classify English original texts and English translated texts from French and German books that were written in the nineteenth century. The p-spectrum normalized kernel was used for the experiment. The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_4",
  "x": "We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language. ---------------------------------- **DATA** The field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars. There are many multilingual corpora available used for different NLP applications. A customized version of the Europarl corpus (Islam and Mehler, 2012 ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> .",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_5",
  "x": "---------------------------------- **DATA** The field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars. There are many multilingual corpora available used for different NLP applications. A customized version of the Europarl corpus (Islam and Mehler, 2012 ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their",
  "y": "uses"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_6",
  "x": "A customized version of the Europarl corpus (Islam and Mehler, 2012 ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their ---------------------------------- **EXPERIMENT** In order to validate two different kinds of translation properties mentioned in Section 1, two different experiments will be performed. For the first experiment, our hypothesis is that texts translated into the same language from different source languages have different properties, a trained classifier will be able to classify texts based on different sources.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_7",
  "x": "The randomly generated training sets contain 80% of the data while the remaining data is used as a test set. To evaluate the classification results, we use standard F-Score and Accuracy measures. ---------------------------------- **SOURCE LANGUAGE IDENTIFICATION** In this experiment, our goal is to validate the translation properties postulated by Toury (1995) . He stated that a translated text inherits some fingerprints from the source language. The experimental result of <cite>Koppel and Ordan (2011)</cite> shows that text translated into English holds this property. If this characteristic also holds for text translated into other languages, then it will corroborate the claim by Toury (1995) . If it does not hold for a single language then it might be claimed that this translation property is not universal. In order to train a classifier, we use texts translated into the same language from different source languages.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_8",
  "x": "Later, each corpus is divided into a number of chunks (see Table 2 ). Each chunk contains at least seven sentences. Our hypothesis is again similar to <cite>Koppel and Ordan (2011)</cite> , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text. If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text. Table 3 and Table 4 show the evaluation results. Table 4 : Source language identification evaluation (Accuracy) ancestor. In the vast majority of cases, members of the same language family share a considerable number of words and grammatical structures. In the experiment, we consider three language families: Romance languages (French and Spanish), Germanic languages (German and Dutch), and Slavic languages (Polish and Czech). With a Romance target language, 5 the identification of other Romance and of Germanic languages as translation sources performs high, with an F-Score of between 0.86 and 0.95.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_9",
  "x": "According to their findings, a translated text will be similar to another translated text but will be different from a source text. In the past, researchers have used comparable corpora to validate these translation properties (Baroni and Bernardini, 2006; Pastor et al., 2008; Ilisei et al., 2009; Ilisei et al., 2010;<cite> Koppel and Ordan, 2011)</cite> . Most of them used comparable corpora for two-class classification, distinguishing translated texts from the original texts. Only<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> used English texts translated from multiple source languages. We perform similar experiments only for six European languages as shown in Table 1 . In this experiment, the translated text in our training and test set will be a combination of all languages other than the target language. For example: when the original class contains original texts (source) in German, then the translation class contains texts that are translated German texts, translated from French, Dutch, Spanish, Polish, and Czech texts. Each class contains 200 chunks of texts, where as the translated class has 40 chunks from each of the source languages. The source language texts are extracted for the corresponding languages in a similar way from the Europarl corpus. <cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_10",
  "x": "<cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above. The training and test data are generated in similar ways as in our previous experiment. That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing. Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different. The classifiers for other languages also display very high accuracy. The result of Table 5 shows that general translation properties exist for all languages used in this experiment. ---------------------------------- **DISCUSSION**",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_11",
  "x": "In this experiment, the translated text in our training and test set will be a combination of all languages other than the target language. For example: when the original class contains original texts (source) in German, then the translation class contains texts that are translated German texts, translated from French, Dutch, Spanish, Polish, and Czech texts. Each class contains 200 chunks of texts, where as the translated class has 40 chunks from each of the source languages. The source language texts are extracted for the corresponding languages in a similar way from the Europarl corpus. <cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above. The training and test data are generated in similar ways as in our previous experiment. That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing. Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_12",
  "x": "Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different. The classifiers for other languages also display very high accuracy. The result of Table 5 shows that general translation properties exist for all languages used in this experiment. ---------------------------------- **DISCUSSION** The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results. We find our results to be compatible with <cite>Koppel and Ordan (2011)</cite> who used 300 function words. A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort. While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.",
  "y": "similarities"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_0",
  "x": "****GDBANK: THE BEGINNINGS OF A CORPUS OF DEPENDENCY STRUCTURES AND TYPE-LOGICAL GRAMMAR IN SCOTTISH GAELIC**** **ABSTRACT** We present gdbank, a small handbuilt corpus of 32 sentences with dependency structures and categorial grammar type assignments. The sentences have been chosen to illustrate as broad a range of the unusual features of Scottish Gaelic as possible, particularly nouns being used to represent psychological states where more thoroughly-studied languages such as English and French would prefer a verb, and prepositions marking aspect, as is also seen in Welsh and, for example, Irish Gaelic. We provide hand-built dependency trees, building on previous work on Irish Gaelic and using the Universal Dependency Scheme. We also provide a tentative categorial grammar account of the words in the sentences, based largely on previous work on English. ---------------------------------- **INTRODUCTION** Scottish Gaelic (usually hereafter Gaelic) is a Celtic language, rather closely related to Irish, with around 59,000 speakers as of the last UK census in 2011. As opposed to the situation for Irish Gaelic (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available from Sabhal M\u00f2r Ostaig.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_1",
  "x": "A very important scheme is the Dublin scheme for Irish (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013) , which is of a similar size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large number of dependencies intended to handle grammatical features found in Irish but not in English. Lastly we mention the Universal Dependency Scheme developed in (McDonald et al., 2013) , which we have adopted, despite its being coarser-grained than the Dublin scheme, on account of its simplicity and utility for cross-lingual comparisons and cross-training (Lynn et al., 2014) . Table 1 gives examples of the dependency relations used along with their mapping to the GR scheme. ---------------------------------- **CATEGORIAL GRAMMAR** Combinatory categorial grammar (CCG) is a type-logical system which was developed to represent natural languages such as English but has subsequently been extended to other systems such as chord se-quences in jazz (Granroth-Wilding and Steedman, 2012) . For a full description the reader is referred to (Steedman and Baldridge, 2003) , but in order to follow the rest of this paper you merely need to know that the type N/N is a function which takes an argument of N to its right, returning N, and that the type N\\N is a function expecting an argument of N to its left and that these are combined by application, composition, where A/B combines with B/C to yield A/C, and type-raising where N is converted to T/(N\\T). Attractive features of CCG for modelling a less-well-studied language include that it is a lexical theory in which it is the lexicon contains the rules for how words are combined to make sense rather than an external grammar, that it allows all manner of unconventional constituents, which is particularly powerful for parsing coordinated structures in English, that it is equivalent to a weakly context-sensitive grammar and hence has the power of a real natural language. In Steedman and Baldridge (2003) there are examples of the application of multimodal CCG to Irish Gaelic. However, to the best of our knowledge this paper is the first application of CCG to Scottish Gaelic.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_2",
  "x": "To say \"I am a teacher\", the Gaelic is 'S e tidsear a th' annam. This, at least on the surface, equates pronoun e, with a noun described by a relative clause including the verb bi. Fig. 1 shows our dependency tree for this. Note that this is different from the scheme in <cite>Lynn et al. (2012b)</cite> because of a difference between the two languages. They treat the analogous sentence Is tusa an m\u00fainteoir \"You are the teacher\" as having a subject, \"the teacher\", and a clausal predicate, tusa, \"you indeed\". The most straightforward way of expressing a preference is the assertive is followed by an adjective or noun, a PP marking the preferrer, and then the object. If you dislike music, you might say Is beag orm ce\u00f2l. There are exactly analogous constructions in Irish with is + adjective + PP[le] + object, for example Is maith liom... \"I like...\", which in (U\u00ed Dhonnchadha, 2009 ) is treated as having the prepositional phrase as the subject and the adjective as predicate. We modify this to use adpmod as in the Universal Dependency Scheme as shown in Fig. 1 . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_0",
  "x": "Can machines help us do this work? Ni and Wang (2017) have proposed a task of generating a definition for a phrase given its local context. However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by Gadetsky et al. (2018) that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases. In this study, we tackle the task of describing (defining) a phrase when given its local and global contexts. We present LOG-CaD, a neural description generator (Figure 1 ) to directly solve this task. Given an unknown phrase without sense definitions, our model obtains a phrase embedding as its global context by composing word embeddings while also encoding the local context. The model therefore combines both pieces of information to generate a natural language description.",
  "y": "background"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_1",
  "x": "Given an unknown phrase without sense definitions, our model obtains a phrase embedding as its global context by composing word embeddings while also encoding the local context. The model therefore combines both pieces of information to generate a natural language description. Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities. Our contributions are as follows: \u2022 We propose a general task of defining unknown phrases given their contexts. This task is a generalization of three related tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) and involves various situations where we need definitions of unknown phrases ( \u00a7 2). \u2022 We propose a method for generating natural language descriptions for unknown phrases with local and global contexts ( \u00a7 3). \u2022 As a benchmark to evaluate the ability of the models to describe entities, we build a largescale dataset from Wikipedia and Wikidata for the proposed task. We release our dataset and the code 1 to promote the reproducibility of the experiments ( \u00a7 4). \u2022 The proposed method achieves the state-ofthe-art performance on our new dataset and the three existing datasets used in the related studies (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) ( \u00a7 5) .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_2",
  "x": "**LOCAL & GLOBAL CONTEXTS** When we find an unfamiliar phrase in text and it is not defined in dictionaries, how can we humans come up with its meaning? As discussed in Section 1, we may first try to figure out the meaning of the phrase from the immediate context, and then read through the entire document or search the web to understand implicit information behind the text. In this paper, we refer to the explicit contextual information included in a given sentence with the target phrase (i.e., the X in Eq. (1)) as \"local context,\" and the implicit contextual information in massive text as \"global context.\" While both local and global contexts are crucial for humans to understand unfamiliar phrases, are they also useful for machines to generate descriptions? To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016) , it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to <cite>Noraset et al. (2017)</cite> to dynamically control how the global and local contexts influence the description. ---------------------------------- **PROPOSED MODEL**",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_3",
  "x": "**USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . ---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_4",
  "x": "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . ---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step. ---------------------------------- **WIKIPEDIA DATASET**",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_5",
  "x": "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . ---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_6",
  "x": "Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet using the dict-definition 9 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described. word. We split this dataset to obtain Train, Validation, and Test sets. If a word has multiple definitions/examples, we treat them as different entries. Note that the words are mutually exclusive across the three sets. The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of <cite>Noraset et al. (2017)</cite> . In addition to WordNet, we use the Oxford Dictionary following Gadetsky et al. (2018) , the Urban Dictionary following Ni and Wang (2017) and our Wikipedia dataset described in the previous section. Table 1 and Table 2 show the properties and statistics of the four datasets, respectively.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_7",
  "x": "word. We split this dataset to obtain Train, Validation, and Test sets. If a word has multiple definitions/examples, we treat them as different entries. Note that the words are mutually exclusive across the three sets. The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of <cite>Noraset et al. (2017)</cite> . In addition to WordNet, we use the Oxford Dictionary following Gadetsky et al. (2018) , the Urban Dictionary following Ni and Wang (2017) and our Wikipedia dataset described in the previous section. Table 1 and Table 2 show the properties and statistics of the four datasets, respectively. To simulate a situation in a real application where we might not have access to global context for the target phrases, we did not train domainspecific word embeddings on each dataset. Instead, for all of the four datasets, we use the same Table 3 : Hyperparameters of the models pre-trained CBOW 10 vectors trained on Google news corpus as global context following previous work (Noraset et al., 2017; Gadetsky et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_8",
  "x": "Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ). Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS WE IMPLEMENTED FOUR METHODS: (1)** Global<cite> (Noraset et al., 2017)</cite> , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in <cite>Noraset et al. (2017)</cite> . It can access the global context of a phrase to be described, but has no ability to read the local context. The Local model is the reimplementation of the best model (dual encoder) in Ni and Wang(2017) . In order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by Ni and Wang(2017); as the character-level encoder in the Local model, we adopt CNNs that are exactly the same as the other two models instead of the original LSTMs. The I-Attention is our reimplementation of the best model (S + I-Attention) in Gadetsky (2018) . Similar to our model, it uses both local and global contexts.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_9",
  "x": "Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. They presented a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context. While their method use local context for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. The most related work to this paper is Ni and Wang (2017) . Focusing on non-standard English phrases, they proposed a model to generate the explanations solely from local context. They followed the strict assumption that the target phrase was newly emerged and there was only a single local context available, which made the task of generating an accurate and coherent definition difficult.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_0",
  "x": "**INTRODUCTION** Multilingual speech recognition has been investigated for many years [1, 2, 3, 4, 5] . Conventional studies concentrate on the area of multilingual acoustic modeling by the contextdependent deep neural network hidden Markov models (CD-DNN-HMM) [6] . The hidden layers of DNN in CD-DNN-HMM can be thought of complicated feature transformation through multiple layers of nonlinearity, which can be used to extract universal feature transformation from multilingual datasets [1] . Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN [1] , in which the hidden layers are shared across multiple languages while the softmax layers are language dependent, is a significant progress in the area of multilingual ASR. These shared hidden layers and language dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM [5] further explores long short-term memory (LSTM) [7] with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN. Although these models achieve encouraging results on multilingual ASR tasks, a hand-designed language-specific pronunciation lexicon must be employed. This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] .",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_1",
  "x": "These shared hidden layers and language dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM [5] further explores long short-term memory (LSTM) [7] with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN. Although these models achieve encouraging results on multilingual ASR tasks, a hand-designed language-specific pronunciation lexicon must be employed. This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets. Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_2",
  "x": "Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training. A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages. Experimental results reveal that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5% average WER reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token. The rest of the paper is organized as follows.",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_3",
  "x": "The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training. A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages. Experimental results reveal that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5% average WER reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token. The rest of the paper is organized as follows. After an overview of the related work in Section 2, Section 3 describes the proposed method in detail. We then show experimental results in Section 4 and conclude this work in Section 5. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_4",
  "x": "**SYSTEM OVERVIEW 3.1. ASR TRANSFORMER MODEL ARCHITECTURE** The ASR Transformer architecture used in this work is the same as our work<cite> [9,</cite> 12] which is shown in Figure 1 . It stacks multihead attention (MHA) [17] and position-wise, fully connected layers for both the encode and decoder. The encoder is composed of a stack of N identical layers. Each layer has two sublayers. The first is a MHA, and the second is a position-wise fully connected feed-forward network. Residual connections are employed around each of the two sub-layers, followed by a layer normalization. The decoder is similar to the encoder except inserting a third sub-layer to perform a MHA over the output of the encoder stack. To prevent leftward information flow and preserve the auto-regressive property in the decoder, the self-attention sub-layers in the decoder mask out all values corresponding to illegal connections. In addition, positional encodings [17] are added to the input at the bottoms of these encoder and decoder stacks, which inject some information about the relative or absolute position of the tokens in the sequence.",
  "y": "similarities uses"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_5",
  "x": "The detailed information is listed below in Table 2 . We train the ASR Transformer with a given number of epochs, so validation sets are not employed in this paper. All experiments are conducted using 80-dimensional log-Mel filterbank features, computed with a 25ms window and shifted every 10ms. The features are normalized via mean subtraction and variance normalization on the speaker basis. Similar to [20, 21] , at the current frame t, these features are stacked with 3 frames to the left and downsampled to a 30ms frame rate. We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ---------------------------------- **MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_6",
  "x": "We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ---------------------------------- **MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters. The Adam algorithm [23] with gradient clipping and warmup is used for optimization. During training, label smoothing of value ls = 0.1 is employed [24] . After trained, the last 20 checkpoints are averaged to make the performance more stable [17] . At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good. We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_7",
  "x": "After trained, the last 20 checkpoints are averaged to make the performance more stable [17] . At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good. We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work. To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work <cite>[9]</cite> . Its softmax layer is replaced by the language-specific softmax layer which is initialized randomly. Through this initialization method, the ASR Transformer can converge very well. All experiments in this paper are conducted by this initialization method. ---------------------------------- **NUMBER OF MERGE OPERATIONS** First, we evaluate how the number of merge operations \u03b1 in BPE affects the performance of the ASR Transformer.",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_0",
  "x": "**INTRODUCTION** Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing , POS tagging (Lample et al., 2016) , relation extraction (dos Santos et al., 2015) , translation (Bahdanau et al., 2015) , and joint tasks (Miwa and Bansal, 2016) . However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017) ), this paper -to the best of our knowledge -is the first attempt investigating regularization effects of AT in a joint setting for two related tasks. We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see <cite>Adel and Sch\u00fctze (2017)</cite> ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; Bekoulis et al. (2018a) ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2). To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5).",
  "y": "differences"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_1",
  "x": "Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness. ---------------------------------- **RELATED WORK** Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) . Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. <cite>Adel and Sch\u00fctze (2017)</cite> solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive.",
  "y": "background"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_2",
  "x": "where\u03b8 is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in Goodfellow et al. (2015) defined as: \u03b7 adv = g/ g , with g = \u2207 w L JOINT (w;\u03b8), where is a small bounded norm treated as a hyperparameter. Similar to Yasunaga et al. (2018) , we set to be \u03b1 \u221a D (where D is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the final loss is computed as: L JOINT (w;\u03b8) + L JOINT (w + \u03b7 adv ;\u03b8). ---------------------------------- **EXPERIMENTAL SETUP** We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> . We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation.",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_3",
  "x": "Compared to Miwa and Bansal (2016) , who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand, Li et al. (2017) use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to Miwa and Sasaki (2014) , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_0",
  "x": "Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008) . In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in \u00a74 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_1",
  "x": "Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008) . In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in \u00a74 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_2",
  "x": "This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages. Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data. This permits effective relexicalization and target language adaptation of the transfer parser. Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ---------------------------------- **MULTI-SOURCE DELEXICALIZED TRANSFER** The methods proposed in this paper fall into the delexicalized transfer approach to multilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; S\u00f8gaard, 2011) . In contrast to annotation projection approaches (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009; Spreyer and Kuhn, 2009) , delexicalized transfer methods do not rely on any bitext.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_3",
  "x": "In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in \u00a74 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages. Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data. This permits effective relexicalization and target language adaptation of the transfer parser. Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ---------------------------------- **MULTI-SOURCE DELEXICALIZED TRANSFER**",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_4",
  "x": "However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout. We then delve into details on how to incorporate selective sharing in this model in \u00a74. In \u00a75, we show how learning with ambiguous labelings in this parser can be used for further target language adaptation, both through self-training and through ensemble-training.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_5",
  "x": "The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout. We then delve into details on how to incorporate selective sharing in this model in \u00a74.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_6",
  "x": "This idea was explored by McDonald et al. (2011) , who showed that target language accuracy can be improved by simply concatenating delexicalized treebanks in multiple languages. In similar work, Cohen et al. (2011) proposed a mixture model in which the parameters of a generative target language parser is expressed as a linear interpolation of source language parameters, whereas S\u00f8gaard (2011) showed that target side language models can be used to selectively subsample training sentences to improve accuracy. Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010) , S\u00f8gaard and Wulff (2012) proposed -among other ideas -a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_7",
  "x": "The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout. We then delve into details on how to incorporate selective sharing in this model in \u00a74.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_8",
  "x": "Let x denote an input sentence and let y \u2208 Y(x) denote a dependency tree, where Y(x) is the set of well-formed dependency trees spanning x. Henceforth, we restrict Y(x) to projective dependency trees, but all our methods are equally applicable in the nonprojective case. Provided a vector of model parameters \u03b8, the probability of a dependency tree y \u2208 Y(x), conditioned on a sentence x, has the following form: Without loss of generality, we restrict ourselves to first-order models, where the feature function \u03a6(x, y) factors over individual arcs (h, m) in y, such that We use the standard gradient-based L-BFGS algorithm (Liu and Nocedal, 1989) to maximize the loglikelihood. Eisner's algorithm (Eisner, 1996) is used for inference of the Viterbi parse and arc-marginals. ---------------------------------- **DATA SETS AND EXPERIMENTAL SETUP** To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as <cite>Naseem et al. (2012)</cite> . Notably, we use the mapping proposed by Naseem et al. (2010) to map from fine-grained treebank specific part-of-speech tags to coarse-grained \"universal\" tags, rather than the more recent mapping proposed by Petrov et al. (2012) . For",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_9",
  "x": "X for X = 81A, 85A, 86A, 87A (see Table 1 ). [\u00b7] denotes an optional template, e.g., p, so that the template also falls back on its undirectional variant. each target language evaluated, the treebanks of the remaining languages are used as labeled training data, while the target language treebank is used for testing only (in \u00a75 a different portion of the target language treebank is additionally used as unlabeled training data). We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_10",
  "x": "Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models. The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_11",
  "x": "p, so that the template also falls back on its undirectional variant. each target language evaluated, the treebanks of the remaining languages are used as labeled training data, while the target language treebank is used for testing only (in \u00a75 a different portion of the target language treebank is additionally used as unlabeled training data). We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models. The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_12",
  "x": "We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS** We compare our models to two multi-source baseline models. The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 .",
  "y": "differences extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_13",
  "x": "Note that there is no selective sharing in this model. The second and third columns of Table 2 show the unlabeled attachment scores of the baseline models for each target language. We see that Delex performs well on target languages that are related to the majority of the source languages. However, for languages 3 Model \"D-,To\" in Table 2 from <cite>Naseem et al. (2012)</cite> . that diverge from the Indo-European majority family, the selective sharing model, <cite>NBG</cite>, achieves substantially higher accuracies. ---------------------------------- **FEATURE-BASED SELECTIVE SHARING** The results for the baseline models are not surprising considering the feature templates used by Delex. There are two fundamental issues with these features when used for direct transfer. First, all but one template include the arc direction.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_14",
  "x": "First, all but one template include the arc direction. Second, some features are sensitive to local word order; e.g., p, which models direction as well as word order in the local contexts of the head and the dependent. Such features do not transfer well across typologically different languages. In order to verify that these issues are the cause of the poor performance of the Delex model, we remove all directional features and all features that model local word order from Delex. The feature templates of the resulting Bare model are shown in the center of Figure 2 . These features only model selectional preferences and dependency length, analogously to the selection component of <cite>NBG</cite>. The performance of Bare is shown in the fourth column of Table 2 . The removal of most of the features results in a performance drop on average. However, for languages outside of the Indo-European family, Bare is often more accurate, especially for Basque, Hungarian and Japanese, which supports our hypothesis.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_15",
  "x": "The performance of Bare is shown in the fourth column of Table 2 . The removal of most of the features results in a performance drop on average. However, for languages outside of the Indo-European family, Bare is often more accurate, especially for Basque, Hungarian and Japanese, which supports our hypothesis. ---------------------------------- **SHARING BASED ON TYPOLOGICAL FEATURES** After removing all directional features, we now carefully reintroduce them. Inspired by <cite>Naseem et al.</cite> Table 2 from <cite>Naseem et al. (2012)</cite> . (2012), we make use of the typological features from WALS (Dryer and Haspelmath, 2011), listed in Table 1, to selectively share directional parameters between languages. As a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of Delex with all WALS properties, similarly to a common domain adaptation technique (Daum\u00e9 III, 2007; Finkel and Manning, 2009 ). However, this approach has two issues.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_16",
  "x": "First, it results in a huge number of features, making the model prone to overfitting. Second, and more critically, it ties together languages via features for which they are not typologically similar. Consider English and French, which are both prepositional and thus have the same value for WALS property 85A. These languages will end up sharing a parameter for the feature 85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives. This problem applies to any method for parameter mixing that treats all the parameters as equal. Like <cite>Naseem et al. (2012)</cite> , we instead share parameters more selectively. Our strategy is to use the relevant part-of-speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge. The resulting features are shown to the right in Figure 2 . For example, there is a shared directional feature that models the order of Subject, Object and Verb by conjoining WALS feature 81A with the arc direction and an indicator feature that fires only if the head is a verb and the dependent is a noun.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_17",
  "x": "These features would not be very useful by themselves, so we combine them with the Bare features. The accuracy of the resulting Share model is shown in column five of Table 2 . Although this model still performs worse than <cite>NBG</cite>, it is an improvement over the Delex baseline and actually outperforms the former on 5 out of the 16 languages. ---------------------------------- **SHARING BASED ON LANGUAGE GROUPS** While Share models selectional preferences and arc directions for a subset of dependency relations, it does not capture the rich local word order information captured by Delex. We now consider two ways of selectively including such information based on language similarity. While more complex sharing could be explored (Berg-Kirkpatrick and Klein, 2010) , we use a flat structure and consider two simple groupings of the source and target languages. First, the Similar model consists of the features used by Share together with the features from Delex in Figure 2 . The latter are conjoined with an indicator feature that fires only when the source and target languages share values for all the WALS features in Table 1 .",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_18",
  "x": "The remaining languages do not share all WALS properties with at least one source language and thus revert to Share, since they cannot exploit these grouped features. Second, instead of grouping languages according to WALS, the Family model is based on a simple subdivision into Indo-European languages (Bulgarian, Catalan, Czech, Greek, English, Spanish, Italian, Dutch, Portuguese, Swedish) and Altaic languages (Japanese, Turkish). This is accomplished with indicator features analogous to those used in Similar. The remaining languages are again treated as isolates and revert to Similar. The results for these models are given in the last two columns of Table 2 . We see that by adding these rich features back into the fold, but having them fire only for languages in the same group, we can significantly increase the performance -from 57.4% to 62.0% on average when considering Family. If we consider our original Delex baseline, we see an absolute improvement of 6.9% on average and a relative error reduction of 15%. Particular gains are seen for non-Indo-European languages; e.g., Japanese increases from 38.9% to 65.9%. Furthermore, Family achieves a 7% relative error reduction over the <cite>NBG</cite> baseline and outperforms it on 12 of the 16 languages. This shows that a discriminative graph-based parser can achieve higher accuracies compared to generative models when the features are carefully constructed. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_19",
  "x": "Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) . Similarly T\u00e4ckstr\u00f6m (2012) used self-training to adapt a multi-source direct transfer named-entity recognizer to different target languages, \"relexicalizing\" the model with word cluster features. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation. ---------------------------------- **AMBIGUITY-AWARE TRAINING** In this section, we propose a related training method: ambiguity-aware training. In this setting a discriminative probabilistic model is induced from automatically inferred ambiguous labelings over unlabeled target language data, in place of gold-standard dependency trees. The ambiguous labelings can combine multiple sources of evidence to guide the estimation or simply encode the underlying uncertainty from the base parser. This uncertainty is marginalized out during training.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_20",
  "x": "---------------------------------- **TARGET LANGUAGE ADAPTATION** While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) . Similarly T\u00e4ckstr\u00f6m (2012) used self-training to adapt a multi-source direct transfer named-entity recognizer to different target languages, \"relexicalizing\" the model with word cluster features. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation. ---------------------------------- **AMBIGUITY-AWARE TRAINING** In this section, we propose a related training method: ambiguity-aware training.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_21",
  "x": "Bottom: The Viterbi parse of the AAST model, which has selected the correct arcs from\u1ef9(x). we propose an ambiguity-aware ensemble-training (AAET) method that treats the union of the ensemble predictions for a sentence x as an ambiguous labeling y(x). An additional advantage of this approach is that the ensemble is compiled into a single model and therefore does not require multiple models to be stored and used at runtime. It is straightforward to construct\u1ef9(x) from multiple parsers. Let A k (x, m) be the set of arcs for the mth token in x according to the kth parser in the ensemble. When arc-marginals are used to construct the ambiguity set, |A k (x, m)| \u2265 1, but when the Viterbiparse is used, A k (x, m) is a singleton. We next form , m) as the ensemble arc ambiguity set from which\u1ef9(x) is assembled. In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( \u00a75.2); and second, the Viterbi arc set from the <cite>NBG</cite> parser of <cite>Naseem et al. (2012)</cite> in Table 2 . 4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_22",
  "x": "When arc-marginals are used to construct the ambiguity set, |A k (x, m)| \u2265 1, but when the Viterbiparse is used, A k (x, m) is a singleton. We next form , m) as the ensemble arc ambiguity set from which\u1ef9(x) is assembled. In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( \u00a75.2); and second, the Viterbi arc set from the <cite>NBG</cite> parser of <cite>Naseem et al. (2012)</cite> in Table 2 . 4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ---------------------------------- **ADAPTATION EXPERIMENTS** We now study the different approaches to target language adaptation empirically. As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments. We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings. The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) .",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_23",
  "x": "We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings. The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) . Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text. Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model. The results of each of these models are shown in Table 3 . There are a number of things that can be observed.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_24",
  "x": "Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text. Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model. The results of each of these models are shown in Table 3 . There are a number of things that can be observed. First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian.",
  "y": "extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_25",
  "x": "There are a number of things that can be observed. First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_26",
  "x": "Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model. The results of each of these models are shown in Table 3 . There are a number of things that can be observed. First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_27",
  "x": "<cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially. Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_28",
  "x": "AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially. Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_29",
  "x": "First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially. Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ---------------------------------- **CONCLUSIONS** We contributed to the understanding of multi-source syntactic transfer in several complementary ways. First, we showed how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based model of dependency parsing. We then showed how ambiguous labelings can be used to integrate heterogenous knowledge sources in parser training.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_30",
  "x": [
   "Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ---------------------------------- **CONCLUSIONS** We contributed to the understanding of multi-source syntactic transfer in several complementary ways. First, we showed how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based model of dependency parsing. We then showed how ambiguous labelings can be used to integrate heterogenous knowledge sources in parser training. Two instantiations of this framework were explored. First, an ambiguity-aware self-training method that can be used to effectively relexicalize and adapt a delexicalized transfer parser using unlabeled target language data. Second, an ambiguityaware ensemble-training method, in which predictions from different parsers can be incorporated and further adapted."
  ],
  "y": "differences"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_0",
  "x": "Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances that challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_1",
  "x": "We find that training on SQuAD produces better zero-shot performance and more robust generalisation compared to the task specific training set. We also show that standard QA architectures (e.g. FastQA or BiDAF) can be applied to the slot filling queries without the need for model modification. ---------------------------------- **INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances that challenge the models' ability to identify relations between subject and object.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_2",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model. In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_3",
  "x": "Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_4",
  "x": "Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model. In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task. We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_5",
  "x": "Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_6",
  "x": "Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_7",
  "x": "In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task. We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_8",
  "x": "**PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model. In this section, we evaluate whether the same model trained on QA data, specifically SQuAD (Rajpurkar et al., 2016) , can be applied to the relation extraction task. We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_9",
  "x": "The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set. In contrast, the latter tests on the easier task of generalising from one set of entities to another for the same set of relations. We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance. To build a dataset using SQuAD (Rajpurkar et al., 2016) , we construct negative examples by removing sentences that contain the answer, based on the spans provided by the annotators.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_10",
  "x": "We use this dataset to investigate how having access to various quantities of data about the test set relations changes performance. To build a dataset using SQuAD (Rajpurkar et al., 2016) , we construct negative examples by removing sentences that contain the answer, based on the spans provided by the annotators. In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_11",
  "x": "We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_12",
  "x": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_13",
  "x": "In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_14",
  "x": "Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_15",
  "x": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_16",
  "x": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_17",
  "x": "In this case, the dedicated relation extraction model is able to achieve an F1 of around 90%, with or without augmentation with SQuAD. This level of performance suggests that such a model would be accurate enough for practical applications. However, test set performance may not be a reliable indicator of the model's ability to generalise to more challenging examples (Jia and Liang, 2017) . ---------------------------------- **GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity. A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_18",
  "x": "---------------------------------- **GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity. A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer. We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_19",
  "x": "**GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity. A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer. We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data.",
  "y": "uses extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_20",
  "x": "Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity. A model that has merely learnt to identify answer spans of the right form, irrespective of their relation to the rest of the sentence, is likely to return the original span rather than recognise that the sentence no longer contains an answer. We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_21",
  "x": "Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly. Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ---------------------------------- **DISCUSSION** The results on our challenge test set suggest that the model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it. In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_22",
  "x": "Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly. Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_23",
  "x": "Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ---------------------------------- **DISCUSSION** The results on our challenge test set suggest that the model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it. In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_24",
  "x": "In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original <cite>UWRE</cite> test set. In contrast, FastQA's performance is substantially worse.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_0",
  "x": "This improved speed enables us to scale up the model by training on an extended training set that is 5x times larger, to further move up the state of the art by an additional 2.3% BLEU and 0.9% exact match. ---------------------------------- **INTRODUCTION** When programmers translate Natural Language (NL) specifications into executable source code, they typically start with a high-level plan of the major structures required, such as nested loops, conditionals, etc. and then proceed to fill in specific details into these components. We refer to these high-level structures (Figure 1 (b) ) as code idioms (Allamanis and Sutton, 2014) . In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016) . State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the grammar of the target programming language (Yin and Neubig, 2017; Rabinovich et al., 2017; <cite>Iyer et al., 2018</cite>) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps before ultimately filling in the slots for the if condition, the then expression and the else expression. However, the if-then-else block can be seen as a higher level structure such as shown in Figure 1 (b) that can be applied in one decoding step and reused in many different programs.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_1",
  "x": "We introduce a simple iterative method to extract idioms from a dataset of programs by repeatedly collapsing the most frequent depth-2 subtrees of syntax parse trees. Analogous to the byte pair encoding (BPE) method (Gage, 1994; Sennrich et al., 2016 ) that creates new subtokens of words by repeatedly combining frequently occurring adjacent pairs of subtokens, our method takes a depth-2 syntax subtree and replaces it with a tree of depth-1 by removing all the internal nodes. This method is in contrast with the approach using probabilistic tree substitution grammars (pTSG) taken by Allamanis and Sutton (2014) , who use the explanation quality of an idiom to prioritize idioms that are more interesting, with an end goal of suggesting useful idioms to programmers using IDEs. Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ---------------------------------- **RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> .",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_2",
  "x": "Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ---------------------------------- **RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) . Iy<cite>er et al. (2018</cite>) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_3",
  "x": "Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ---------------------------------- **RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) . Iy<cite>er et al. (2018</cite>) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation. We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time. Another different but related method to produce source code is with the help of sketches, which are code snippets containing slots in the place of low-level information such as variable names and arguments.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_4",
  "x": "This is shown in Figure 2 (c) which contains the idiom extracted in (b) within it owing to the post-processing of the dataset after idiom (b) is extracted (Step 10 of Algorithm 1) which effectively makes the idiom in (d) a depth-3 idiom. ---------------------------------- **MODEL TRAINING WITH IDIOMS** Once a set of idioms I is obtained, we next train our semantic parsing models to apply these idioms while decoding. We do this by supervising production rule generation in the decoder using a compressed set of rules for each example, using the idiom set I (see Algorithm 2). More concretely, we first obtain the parse tree t i (or produc- tion rule set p i ) for each training example program y i under grammar G (Step 3) and then greedily collapse each depth-2 subtree in t i corresponding to every idiom in I (Step 5). Once t i cannot be further collapsed, we translate t i into production rules r i based on the collapsed tree, with |r i | \u2264 |p i | (Step 7). This process is illustrated in Figure 3 where we perform two applications of the first idiom from Figure 2 (b), followed by one application of the second idiom from Figure 2 (d) , after which, the tree cannot be further compressed using those two idioms. The final tree can be represented using |r i | = 2 rules instead of the original |p i | = 5 rules. The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; <cite>Iyer et al., 2018</cite>) using the compressed set of rules.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_5",
  "x": "We do this by supervising production rule generation in the decoder using a compressed set of rules for each example, using the idiom set I (see Algorithm 2). More concretely, we first obtain the parse tree t i (or produc- tion rule set p i ) for each training example program y i under grammar G (Step 3) and then greedily collapse each depth-2 subtree in t i corresponding to every idiom in I (Step 5). Once t i cannot be further collapsed, we translate t i into production rules r i based on the collapsed tree, with |r i | \u2264 |p i | (Step 7). This process is illustrated in Figure 3 where we perform two applications of the first idiom from Figure 2 (b), followed by one application of the second idiom from Figure 2 (d) , after which, the tree cannot be further compressed using those two idioms. The final tree can be represented using |r i | = 2 rules instead of the original |p i | = 5 rules. The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; <cite>Iyer et al., 2018</cite>) using the compressed set of rules. In later experiments, we find that this results in a rule set compression of more than 50% (see Section 7). ---------------------------------- **EXPERIMENTAL SETUP** We apply our approach to the context dependent encoder-decoder model of <cite>Iyer et al. (2018</cite>) on the CONCODE dataset, and compare performance to a better tuned instance of their best model.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_6",
  "x": "Figure 4 (a) shows an example where the context comprises variables and methods (with types) that would normally exist in a class that implements a vector, such as vecElements and dotProduct(). Conditioned on Source code: AST Derivation: this context, the task involves mapping the NL query Adds a scalar to this vector in place into a sequence of parsing rules to generate the source code in Figure 4 (b). Formally, their task is: Given a NL utterance q, a set of context variables {v i } with types {t i }, and a set of context methods {m i } with return types {r i }, predict a set of parsing rules {a i } of the target program. Their best performing model is an encoder-decoder model with a context aware encoder and a decoder that produces production rules from the grammar of the target programming language. ---------------------------------- **BASELINE MODEL** We follow the approach of <cite>Iyer et al. (2018</cite>) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) . First, in addition to camel-case splitting of identifier tokens, we further use byte-pair encoding (BPE) (Sennrich et al., 2016) on all NL tokens, identifier names and types and embed all these BPE tokens using a single embedding matrix. Next, we replace their RNN that contextualizes the subtokens of identifiers and types with an average of the subtoken embeddings instead.",
  "y": "extends"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_7",
  "x": "Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp). ---------------------------------- **HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_8",
  "x": "Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp). ---------------------------------- **HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_9",
  "x": "**HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_10",
  "x": "---------------------------------- **HYPERPARAMETERS** To create models that use idioms, we augment this decoder by first retrieving the top-K most frequent idioms from the training set (Algorithm 1), followed by post-processing the training set by greedily applying these idioms (Algorithm 2; we denote this model as Iyer-Simp-K). We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_12",
  "x": "We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible. ---------------------------------- **RESULTS AND DISCUSSION** taining comparable EM accuracy. Using the top-200 idioms results in a target AST compression of more than 50%, which results in fewer decoder RNN steps being performed. This reduces training time further by more than 50%, from 27 hours to 13 hours.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_13",
  "x": "**RESULTS AND DISCUSSION** taining comparable EM accuracy. Using the top-200 idioms results in a target AST compression of more than 50%, which results in fewer decoder RNN steps being performed. This reduces training time further by more than 50%, from 27 hours to 13 hours. In Table 2 , we illustrate the changes in EM, BLEU and training time as we vary the number of idioms. We find that 200 idioms performs best overall in terms of balancing accuracy and training time. Adding more idioms continues to reduce training time, but accuracy also suffers. Since we permit idioms to contain identifier names in order to capture frequently used library methods in idioms, having too many idioms hurts generalization, especially since the test set is built using repositories disjoint from the training set. Finally, the amount of compression, and therefore the training time, plateaus after the top-600 idioms are incorporated. Compared to the model of <cite>Iyer et al. (2018)</cite> , our significantly reduced training time enables us to train on their extended training set.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_0",
  "x": "Clearly, the polarization of society and its underlying discourses are not limited to social media, but rather reflected also in political dynamics (e.g., like those found in the US Congress [1] ): even in this domain, however, social media can provide a useful signal to estimate partisanship [4] . Closely related to the concept of controversy and the \"filter bubble effect\" is the concept of bias [2] , which refers to the presentation of information according to the standpoints or interests of the journalists and the news agencies. Detecting bias is very important to help users to acquire balanced information. Moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience, which may have large social implications (especially in very controversial topics such as terror attacks and religion issues). In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links. <cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations. We build upon <cite>previous work</cite> and use the dataset from <cite>[5]</cite> : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, three each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing.",
  "y": "background"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_2",
  "x": "Moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience, which may have large social implications (especially in very controversial topics such as terror attacks and religion issues). In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links. <cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations. We build upon <cite>previous work</cite> and use the dataset from <cite>[5]</cite> : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, three each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. To detect hyperpartisanship, we apply a masking technique that transforms the original texts in a form where the textual structure is maintained, while letting the learning algorithm focus on the writing style or the topic-related information. This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_3",
  "x": "The articles originated from 9 well-known political publishers, three each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. To detect hyperpartisanship, we apply a masking technique that transforms the original texts in a form where the textual structure is maintained, while letting the learning algorithm focus on the writing style or the topic-related information. This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias. The rest of the paper is structured as follows. In Section 2 we describe our method to hyperpartisan news detection based on masking. Section 3 presents details on the dataset, experimental results and a discussion of our results. Finally, Section 4 concludes with some directions for future work. ---------------------------------- **INVESTIGATING MASKING FOR HYPERPARTISANSHIP DETECTION**",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_4",
  "x": "We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) . As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS**",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_5",
  "x": "With the former we obtain distorted texts that allow for learning a topic-based model; on the other hand, with the latter, it is possible to learn a style-based model. One of the options to choose the terms to be masked or maintained without masking is to take the most frequent words of the target language [7] . In the original text from the table, we highlight some of the more frequent words in English. ---------------------------------- **EXPERIMENTS** We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) .",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_6",
  "x": "**EXPERIMENTS** We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) . As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_7",
  "x": "Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) . As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS** In this section, we reported the results of the masking technique from two different perspectives. In one setting, we masked topic-related information in order to maintain the predominant writing style used in each orientation.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_8",
  "x": "In another setting, we masked style-related information to allow the system to focus only on the topic-related differences between the orientations. We call this a topic-based model. For this, we masked the k most frequent words and maintained intact the rest. After the text transformation by the masking process in both the training and test sets, we represented the documents with character n-grams and compared the results obtained with the style-based and the topic-related models. Machine (SVM) and Random Forest (RF); for the three classifiers we used the versions implemented in sklearn with the parameters set by default. Evaluation: We performed 3-fold cross-validation with the same configuration used in <cite>[5]</cite> . Therefore, each fold comprised one publisher from each orientation (the classifiers did not learn a publisher's style). We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_9",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_10",
  "x": "Therefore, each fold comprised one publisher from each orientation (the classifiers did not learn a publisher's style). We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_11",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_12",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_13",
  "x": "We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time. ---------------------------------- **RESULTS AND DISCUSSION**",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_14",
  "x": "We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_15",
  "x": "**RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM. Figures 1 shows the results of the variation of k \u2208 {100, 200, ..., 5000}. When k > 5000, we clearly can see that the topic-related model, in which the k most frequent terms are masked, is decreasing the performance. This could be explained by the fact that relevant topic-related terms start to be masked too. However, a different behavior is seen in the style-related model, in which we tried to maintain only the style-related words without masking them. In this model, the higher is k the better is the performance.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_16",
  "x": "**RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM. Figures 1 shows the results of the variation of k \u2208 {100, 200, ..., 5000}. When k > 5000, we clearly can see that the topic-related model, in which the k most frequent terms are masked, is decreasing the performance. This could be explained by the fact that relevant topic-related terms start to be masked too. However, a different behavior is seen in the style-related model, in which we tried to maintain only the style-related words without masking them. In this model, the higher is k the better is the performance.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_17",
  "x": "From this experiment, we conclude that: (i) the topic-related model is less sensitive than the style-related model when k < 500, i.e. the k most frequent terms are stylerelated ones; and (ii) when we vary the value of k, both models achieve worse results than our baseline. On the other hand, the results of extracting character 5-grams are higher than extracting smaller n-grams, as can be seen in Figures 2. These results confirm that perhaps the performance of our approach overcomes the models proposed in <cite>[5]</cite> because of the length of the n-grams 7 . Relevant features. Table 4 shows the features with the highest weights from the SVM (we use scikit-learn's method to collect feature weights). It is possible to note that the mention of cnn was learned as a discriminative feature when the news from that publisher were used in the training (in the topic-based model). However, this feature is infrequent in the test set where no news from CNN publisher was included. Baseline model left main right imag cnn e are that said lary e tru said your e don y con n pla here ry co e thi s of cnn s to e tru n ame your for h said illar donal said hilla racis ore t llary here said and s kill story hill that said let trum tory comm trump ed be lary Style-based model left main right but * n thi y ** out w s * s out a t ** how as to you h at he o you t and m * t ell * is a * * u and n h * a e # * hat w of # and * * # # or hi ** * it t for h t the e of ** * and * o you in o * tw n it hat * * two and n f ** s ** all * so * onl f * w Topic-based model left main right ant * cnn hilla imag cs * * da lies ics * als * * ex sday * le etty ed be dail donal cnn * te n * c day * * ame onald cs * * am ying ics * illar thing * * e llary * * ed be * le e * y con * ri n * tory hill t * story bomb imag d bel * * r Features like donal and onal are related to Donald Trump, while illar and llary refer to Hillary Clinton. Each of these names is more frequent in one of the hyperpartisan Table 5 : Fragments of original texts and their transformation by masking the k most frequent terms. Some of the features from Table 4 using the topic-related model are highlighted.",
  "y": "differences"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_0",
  "x": "I var_1 a new car var_2 bought . Reference Japanese: Japanese-ordered English: English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of<cite> Isozaki et al. (2010b)</cite> , and its reference Japanese translation. Semi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; Currey et al., 2017) . However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; .",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_1",
  "x": "Japanese-ordered English: English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of<cite> Isozaki et al. (2010b)</cite> , and its reference Japanese translation. Semi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; Currey et al., 2017) . However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering. This paper proposes a method to create pseudoparallel sentences for NMT for language pairs with divergent syntactic structures. Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; . Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ;<cite> Isozaki et al. (2010b)</cite> ; Fig. 1 ). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) .",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_2",
  "x": "Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) . Hence we focus on semi-supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017) , largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016) . However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask \"how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?\" We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1 , (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and add them as the source side of a pseudo-parallel corpus. Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method. ---------------------------------- **THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_3",
  "x": "Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of t s into pseudo-source sentence\u015d using a bilingual dictionary (Xie et al., 2018) . 3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available. In addition, we can induce word translations for more words in target language using methods for bilingual lexicon induction over pre-trained word embeddings (e.g. Grave et al. (2018) ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_4",
  "x": "Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method. ---------------------------------- **THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers.",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_5",
  "x": "Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method. ---------------------------------- **THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_6",
  "x": "As noted above, we use HF<cite> (Isozaki et al., 2010b)</cite> as our re-ordering rule. HF was designed for transforming English into Japanese order, but we use it as-is for the Uyghur-English pair as well to demonstrate that simple, linguistically motivated rules can generalize across pairs with similar syntax with little or no modification. Further details regarding the experimental settings are in the supplementary material. ---------------------------------- **SIMULATED JAPANESE TO ENGLISH EXPERIMENTS** We first evaluate on a simulated low-resource ja-en translation task using the ASPEC dataset (Nakazawa et al., 2016) . We randomly select 400k ja-en parallel sentence pairs to use as our full training data. We then randomly sub-sample low-resource datasets of 3k, 6k, 10k, and 20k parallel sentences, and use the remainder of the 400k English sentences as monolingual data. We duplicate the number of parallel sentences by 5 times in the training data augmented with the reordered pairs. For settings with supervised parallel sentences of 3k, 6k, 10k and 20k, we set the maximum vocabulary size of both Japanese and English to be 10k, 10k, 15k and 20k respectively.",
  "y": "uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_0",
  "x": "Machine comprehension of text is the overarching goal of a great deal of research in natural language processing. The Machine Comprehension Test (Richardson et al., 2013) was recently proposed to assess methods on an open-domain, extensible, and easy-to-evaluate task consisting of two datasets. In this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution. We show that the proposed method outperforms the baseline of <cite>Richardson et al. (2013)</cite> , and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created. ---------------------------------- **INTRODUCTION** Machine comprehension of text is the central goal in NLP. The academic community has proposed a variety of tasks, such as information extraction (Sarawagi, 2008) , semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010) .",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_1",
  "x": "---------------------------------- **INTRODUCTION** Machine comprehension of text is the central goal in NLP. The academic community has proposed a variety of tasks, such as information extraction (Sarawagi, 2008) , semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010) . However, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text. To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) .",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_2",
  "x": "To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) . In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution. These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> . Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by Narasimhan and Barzilay (2015) , Wang et al. (2015) and Sachan et al. (2015) . Furthermore, we examine the types of questions and answers in the two datasets.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_3",
  "x": "However, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text. To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) . In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution. These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> . Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by Narasimhan and Barzilay (2015) , Wang et al. (2015) and Sachan et al. (2015) .",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_4",
  "x": "He was a sailor who had visited the zoo on vacation and fallen asleep on a bench right before closing time. The zoo worker saw how hairy he was and thought he was a monkey that had escaped from his cage, so they put him in a cage. two datasets contain 160 and 500 stories respectively, with 4 questions per story, and 4 candidate answers per question (Figure 1 ). All stories and questions were crowd-sourced using Amazon Mechanical Turk. 2 MC160 was manually curated by Richardson et al., while MC500 was curated by crowdworkers. Both datasets are divided into training, development, and test sets. All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_5",
  "x": "2 MC160 was manually curated by Richardson et al., while MC500 was curated by crowdworkers. Both datasets are divided into training, development, and test sets. All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest overlap score between a story text window and the question-answer pair is taken as the score for the answer. Therefore, their algorithm makes a single pass over the story text per answer. In comparison, our system scores each answer by making multiple passes and summing the obtained scores. Concretely, on the first pass, we set the sliding window size to 2 tokens, and increment this size on each subsequent pass, up to a length of 30 tokens.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_6",
  "x": "We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest overlap score between a story text window and the question-answer pair is taken as the score for the answer. Therefore, their algorithm makes a single pass over the story text per answer. In comparison, our system scores each answer by making multiple passes and summing the obtained scores. Concretely, on the first pass, we set the sliding window size to 2 tokens, and increment this size on each subsequent pass, up to a length of 30 tokens. We then combine this score with the overall number of matches of the question-answer pair across the story as a whole. This enables our algorithm to catch long-distance relations in the story. Similar to <cite>Richardson et al. (2013)</cite> , we use a linear combination of this score with their distancebased scoring function, and we weigh tokens with their inverse document frequencies in each individual story. By itself, this simple enhancement gives substantial improvements over the MSR baseline as shown in Table 1 (Enhanced SW+D), as it measures the overlap of the question-answer pair with multiple portions of the story text.",
  "y": "similarities uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_7",
  "x": "Nevertheless, these rules were helpful in analyzing problem areas in the datasets, as discussed in Section 6. ---------------------------------- **RESULTS** We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2 . Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_8",
  "x": "Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model. Sachan et al. (2015) treated MCTest as a structured prediction problem, searching for a latent structure connecting the question, answer and the text, dubbed the answer-entailing structure. Their model performs better on MC500 (was Wang et al. (2015) is the most similar to ours, in the sense that they combine a baseline feature set with more advanced linguistic analyses, namely syntax, frame semantics, coreference, and word embeddings. Instead of a rule-based approach, they combine them through a latent-variable classifier achieving the current state-of-the-art performance on MCTest. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_9",
  "x": "Questions such as \"What was the first character mentioned in the story?\", which relate to the overall narrative flow of the passage, or questions concerning the state of the story environment, such as \"Where is the story set?\", are difficult to solve without a system which understands the concept of a story. Typical question-answering methods would also struggle here. Another type of challenging question are those which require an implicit temporal understanding of the text, i.e. questions concerning time without using a temporal modifier. For example, given a story which states that \"John is at the beach\", then later \"John went home\", a question such as \"What did John do at home?\" would prove itself difficult for traditional methods to answer. These questions are difficult to identify automatically by the form of the question alone, thus we cannot provide accuracies for them. Our results confirm that it is easier to achieve better performance on MC160 with simple lexical techniques, while the MC500 has proved more resilient to the same improvements. We also observed that the MC500 registers smaller improvements in accuracy when adding components such as co-reference. This is a consequence of the design and curation process of the MC500 dataset, which stipulated that answers must not be contained directly within the story text, or if they are, that two or more misleading choices included. <cite>Richardson et al. (2013)</cite> demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both. However, in many cases MC500 appears to be designed in such a way to confuse lexical algorithms and encourage the use of more sophisticated techniques necessary to deal with phenomena such as elimination questions, negation, and common knowledge not explicitly written in the story.",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_0",
  "x": "Word alignment is a fundamental step in machine translation. Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix<cite> (Liu et al., 2009)</cite> . Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model. ---------------------------------- **INTRODUCTION** Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006) , but also syntax-based models (Chiang, 2005; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008) , usually extract rules from word aligned corpora.",
  "y": "motivation background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_1",
  "x": "Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1 (a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X 1 jingji, China X 1 economy).To alleviate this problem, a natural solution is to extract rules from nbest alignments (Venugopal et al., 2008) . However, using n-best alignments still face two major challenges. First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1 . Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a). Since<cite> Liu et al. (2009)</cite> show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (Chiang, 2005) and the tree-to-string model Huang et al., 2006) . While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices. Our work faces two major challenges.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_2",
  "x": "Another challenge is how to achieve a balance between performance and rule table size. Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices<cite> (Liu et al., 2009</cite> ). If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases. Experiments (Section 4) show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.8 points for tree-to-string model. ---------------------------------- **WEIGHTED ALIGNMENT MATRIX** A weighted alignment matrix<cite> (Liu et al., 2009)</cite> m is a J \u00d7 I matrix to encode the probabilities of n-best alignments of the same sentence pair. Each element in the matrix stores a link probability p m (j, i), which is estimated from an n-best list by calculating relative frequencies:",
  "y": "differences motivation background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_3",
  "x": "If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases. Experiments (Section 4) show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.8 points for tree-to-string model. ---------------------------------- **WEIGHTED ALIGNMENT MATRIX** A weighted alignment matrix<cite> (Liu et al., 2009)</cite> m is a J \u00d7 I matrix to encode the probabilities of n-best alignments of the same sentence pair. Each element in the matrix stores a link probability p m (j, i), which is estimated from an n-best list by calculating relative frequencies: where Here N is an n-best list, p(a) is the probability of an alignment a in the n-best list.",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_4",
  "x": [
   "Since p m (j, i) is the probability that f j and e i are aligned, the probability that the two words are not aligned is Figure 2 shows an example. The probability for the two words zhongguo and China being aligned is 1.0 and the probability that they are not aligned is 0.0. In another way, the two words are definitely aligned. Given a phrase pair (f The key point to calculate the relative frequency of the phrase pair is to obtain its fractional count. Liu et al. (2009) use the product of inside and outside probabilities as the fractional count of a phrase pair. Liu et al. (2009) define that inside probability indicates the probability that at least one word in source phrase is aligned to a word in target phrase, and outside probability indicates the chance that no words in one phrase are aligned to a word outside the other phrase. The fractional count is calculated: where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column."
  ],
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_5",
  "x": "Given a phrase pair (f The key point to calculate the relative frequency of the phrase pair is to obtain its fractional count. Liu et al. (2009) use the product of inside and outside probabilities as the fractional count of a phrase pair. Liu et al. (2009) define that inside probability indicates the probability that at least one word in source phrase is aligned to a word in target phrase, and outside probability indicates the chance that no words in one phrase are aligned to a word outside the other phrase. The fractional count is calculated: where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column. Figure 3 shows an example. The light shading area is the outside area of phrase pair and the area inside the pane with bold lines is the inside area. To calculate the lexical weights,<cite> Liu et al. (2009)</cite> adapt p m (j, i) as the fractional count count(f j , e i ).",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_6",
  "x": "We apply weighted alignment matrix to the hierarchical phrase-based model (Chiang, 2007) and the tree-to-string model Huang et al., 2006) . ---------------------------------- **RULE EXTRACTION** In hierarchical rules, both source and target sides are strings with NTs. In tree-to-string rules, the source side is a tree with NTs, while the target side is a string with NTs. Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below: where X is a nonterminal, \u03b3 and \u03b1 are source and target strings (consist of terminals and NTs), and \u223c represents word alignments between NTs in \u03b3 and \u03b1. The bulk of syntax grammars consists of two parts: phrase pairs and variable rules. The difference between them is containing NTs or not. Since we can calculate relative frequencies and lexical weights of phrase pairs as in<cite> Liu et al. (2009)</cite> , we only focus on the calculation of variable rules.",
  "y": "background uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_7",
  "x": "In practice, we set \u03c9 = 0.5. 1 Suppose p w (China 's economy | zhongguo de jingji) is 0.7 and p w (of China 's economy | zhongguo de jingji) is 0.4, then we should choose the target phrase China 's economy although of China 's economy has a larger fractional count. Note that we select the best target phrase for each source phrase for just one sentence. It means there could still be many target phrases for each source phrase during decoding. Figure 5 shows an example of the matrix of a hierarchical rule, which is generated from the phrase pair in Figure 3 . Due to the existence of subphrase pairs, the inside and outside areas changes (see the difference between Figure 3 and Figure 5 ). Therefore, we can not simply calculate the outside probability of the hierarchical rule using the product of outside probabilities of phrase pair and subphrase pairs. ---------------------------------- **CALCULATING RELATIVE FREQUENCIES** We follow<cite> Liu et al. (2009)</cite> to calculate relative frequencies using the product of inside and outside probabilities.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_8",
  "x": "We also use Equation 5 to calculate the fractional counts of hierarchical rules. We follow<cite> Liu et al. (2009)</cite> to prune rule table using a threshold of frequency. Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 . If the threshold is 0.2, we retain all the rules in Table 2 . ---------------------------------- **CALCULATING LEXICAL WEIGHTS** We denote S R as all words in source side of the inside area of variable rule R, and T R as the words in target side. For the rule (X 1 de jingji, X 1 's economy) in Figure 5 , S R is {de, jingji} and T R is {'s, economy}. Then, we can calculate the lexical weight as: Note that we only consider each word pair (f j , e i ) in the inside area of the variable rule. For example, the lexical weight of (X 1 de jingji,",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_9",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **DATA PREPARATION** Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system . We train a 4-gram language model on the Xinhua portion of GIGA-WORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995 To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (Koehn et al., 2003) to all 20 \u00d7 20 bidirectional alignment pairs. We follow<cite> Liu et al. (2009)</cite> to use p s2t \u00d7 p t2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair.",
  "y": "uses"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_0",
  "x": "---------------------------------- **INTRODUCTION** Named entity recognition (NER) is a subtask of information extraction that seeks to locate and classify predefined entities, such as names of persons, locations, organizations, etc. in unstructured texts. It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption<cite> (Krishnan and Manning, 2006)</cite> . But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information.",
  "y": "motivation background"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_1",
  "x": "But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features. However, in the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005) . Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference,<cite> Krishnan and Manning (2006)</cite> propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes.",
  "y": "motivation background"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_2",
  "x": "Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class (Kambhatla, 2006) . In this paper, we employ non-local information to recall the missed entities. Similar to<cite> Krishnan and Manning (2006)</cite> , we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification. Our features are also inspired by Wong and Ng (2007) . They extract entity majority type features from unlabelled data with an initial maximum entropy classifier. Our approach is validated on the third International Chinese language processing bakeoff (SIGHAN 2006) MSRA and CityU NER closed track, the experimental results show that non-local features can significantly improve the recall of the state-of-the-art NER system using local context alone. The remainder of the paper is structured as follows.",
  "y": "differences similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_3",
  "x": "---------------------------------- **OUR BASELINE NER SYSTEM** To validate the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to<cite> (Krishnan and Manning, 2006)</cite> , we employ two-stage architecture under conditional random fields (CRFs) framework. In the first stage, we build the baseline with local features only, and then we build the second NER system with non-local features. We will introduce them step by step. ---------------------------------- **CONDITIONAL RANDOM FIELDS** We regard the NER task as a sequence labeling problem and apply Conditional Random Fields (Lafferty et al., 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at NER task. It is undirected graph established on G = (V, E), where V is the set of random variables Y = {Y i |1\u2264i\u2264 n} for each the n tokens in an input sequence and E = {(Y i\u22121 , Y i ) |1\u2264i\u2264n} is the set of (n \u2212 1) edges forming a linear chain.",
  "y": "similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_4",
  "x": "Entity-majority features (F3): These refer to the majority label assigned to the token sequence which is matched with the entity list exactly. These features enable us to capture the dependencies between the identical entities and their classes, so that the same candidate entities of different occurrences can be recalled favorably, and their label consistencies can be considered too. Token-position & entity-majority features (F4): These features capture non-local information from F2 and F3 simultaneously. They take into account the entity boundary and semantic class information at the same time. These non-local features are applied in English NER in one-step approach<cite> (Krishnan and Manning, 2006</cite>; Wong and Ng, 2007) , they employ these features to improve entity consistence among their different occurrences. These features are assigned to token sequences that are matched exactly with the (entity, majority-type) list in forward maximum matching (FMM) way. During training or testing, when the CRFs tagger encounters a token sequence C 1 ...C n such that (C k ...C s ) (k\u22651, s \u2264n) is the longest token sequence existing in the entity list; the correspondent features will be turned on to each token in C k ....C s . For example, considering the following ---------------------------------- **SENTENCE: \u6211(WO)\u7231 (AI)\u5317(BEI)\u4eac(JING)\u5929(TIAN)\u5b89(AN)\u95e8 (MEN)(I LOVE BEIJING TIANANMEN). IF (\u5317 \u4eac, MAJ-LOC), (\u4eac, MAJ-LOC), AND (\u5929\u5b89\u95e8, MAJ-LOC) ARE PRESENTED IN THE (ENTITY, MAJORITY** type) list, the features below will be turned on as table 1 shows.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_0",
  "x": "In this study we compare performance of a dozen of pretrained word embedding models on lyrics sentiment analysis and movie review polarity tasks. According to our results, Twitter Tweets is the best on lyrics sentiment analysis, whereas Google News and Common Crawl are the top performers on movie polarity analysis. Glove trained models slightly outrun those trained with Skipgram. Also, factors like topic relevance and size of corpus significantly impact the quality of the models. When medium or large-sized text sets are available, obtaining word embeddings from same training dataset is usually the best choice. ---------------------------------- **INTRODUCTION** Semantic vector space models of language were developed in the 90s to predict joint probabilities of words that appear together in a sequence. A particular upturn was proposed by Bengio et al. in [1] , replacing sparse n-gram models with word embeddings which are more compact representations obtained using feed-forward or more advanced neural networks. Recently, high quality and easy to train Skip-gram shallow architectures were presented in <cite>[10]</cite> and considerably improved in [11] with the introduction of negative sampling and subsampling of frequent words.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_1",
  "x": "Google News is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases <cite>[10]</cite> . It was trained using Skipgram word2vec with negative sampling, windows size 5 and 300 dimensions. Even bigger is Common Crawl 840, a huge corpus of 840 billion tokens and 2.2 million word vectors also used at [15] . It contains data of Common Crawl (http://commoncrawl.org), a nonprofit organization that creates and maintains public datasets by crawling the web. Common Crawl 42 is a reduced version made up of 42 billion tokens and a vocabulary of 1.9 million words. Common Crawl 840 and Common Crawl 42 were trained with Glove method producing vectors of 300 dimensions for each word. The last Glove corpus is the collection of Twitter Tweets. It consists of 2 billion tweets, 27 billion tokens and 1.2 million words. To observe the role of corpus size in quality of generated embeddings, we train and use Text8Corpus, a smaller corpus consisting of 17 million tokens and 25,000 words. The last model we use is MoodyCorpus, a collection of lyrics that followed our work in [3] where we build and evaluate MoodyLyrics, a sentiment annotated dataset of songs.",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_0",
  "x": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] .",
  "y": "motivation extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_1",
  "x": "**ABSTRACT** Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities.",
  "y": "background motivation"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_2",
  "x": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] .",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_3",
  "x": "e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] . In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] .",
  "y": "extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_4",
  "x": "To support short documents, a special-purpose padding token can be used to ll up word sequences that are longer than a particular document. A er word sequence extraction, a weight can be assigned to each word sequence/entity pair that can be used to re-weight the training objective. For example, in the case of expert nding [9] , this weight is the reciprocal of the document length of the document where the sequence was extracted from. is avoids a bias in the objective towards long documents. An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [<cite>8</cite>] . Code snippet 1: Illustrative example of the SERT model interface. e full interface supports more functionality omitted here for brevity. Users can de ne a symbolic graph of computation using the eano library [6] in combination with Lasagne [3] . ---------------------------------- **REPRESENTATION LEARNING** A er the collection has been processed and packaged in a machinefriendly format, representations of words and entities can be learned.",
  "y": "similarities extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_5",
  "x": "e full interface supports more functionality omitted here for brevity. Users can de ne a symbolic graph of computation using the eano library [6] in combination with Lasagne [3] . ---------------------------------- **REPRESENTATION LEARNING** A er the collection has been processed and packaged in a machinefriendly format, representations of words and entities can be learned. e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [<cite>8</cite>] . Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework. Code snippet 1 shows an example of a model implemented in the SERT toolkit where users can de ne a symbolic cost function that will be optimized using eano [6] . Due to the component-wise organization of the toolkit (Fig. 1) , modeling and text processing are separated from each other.",
  "y": "background extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_7",
  "x": "**ENTITY RANKING & OTHER USES OF THE REPRESENTATIONS** Once a model has been trained, SERT can be used to rank entities w.r.t. a textual query. e concrete implementation used to rank entities depends on the model that was trained. In the most generic case, a matching score is computed for every entity and entities are ranked in decreasing order of his score. However, in the special case when the model is interpreted as a metric vector space [2, <cite>8</cite>] , SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5] . A er ranking, SERT outputs the entity rankings as a TREC-compatible le that can be used as input to the trec eval 1 evaluation utility. In this paper we described the Semantic Entity Retrieval Toolkit, a toolkit that learns latent representations of words and entities. e toolkit contains implementations of state-of-the-art entity representations algorithms [<cite>8</cite>, 9] and consists of three components: text processing, representation learning and inference. Users of the toolkit can easily make changes to existing model implementations or contribute their own models by extending an interface provided by the SERT framework.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_0",
  "x": "Abstract. Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as <cite>MPII-MD</cite> <cite>[28]</cite> allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging <cite>MPII-MD dataset</cite>. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_2",
  "x": "Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging <cite>MPII-MD dataset</cite>. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task. ---------------------------------- **INTRODUCTION** Automatic description of visual content has lately received a lot of interest in our community. Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35] . Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_3",
  "x": "In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ---------------------------------- **RELATED WORK** Image captioning.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_4",
  "x": "Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ---------------------------------- **RELATED WORK** Image captioning. Automatic image description has been studied in the past [9, 19, 20, 24] , however it regained attention just recently. Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37] .",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_5",
  "x": "This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ---------------------------------- **RELATED WORK** Image captioning. Automatic image description has been studied in the past [9, 19, 20, 24] , however it regained attention just recently. Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37] . Many of them rely on Recurrent Neural Networks (RNNs) and in particular on Long-Short Term Memory networks (LSTMs).",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_6",
  "x": "Most works (with a few exceptions, e.g. [27] ) study the task of describing a short clip with a single sentence. [6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27] . [34] extended this work to extract CNN features from frames which are max-pooled over time. They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_7",
  "x": "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27] . [34] extended this work to extract CNN features from frames which are max-pooled over time. They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_8",
  "x": "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27] . [34] extended this work to extract CNN features from frames which are max-pooled over time. They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_9",
  "x": "[34] extended this work to extract CNN features from frames which are max-pooled over time. They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_10",
  "x": "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15] . However, we analyze different aspects and variants of this architecture for movie description. To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach. We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36] ), LSDA (large scale object detector [14] ) and PLACES (Places-CNN [41] ). Next, we concatenate the scores from a subset of selected robust classifiers and use them as input to our LSTM. ---------------------------------- **APPROACH** In this section we present our two-step approach to video description.",
  "y": "differences uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_11",
  "x": "However, we analyze different aspects and variants of this architecture for movie description. To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach. We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36] ), LSDA (large scale object detector [14] ) and PLACES (Places-CNN [41] ). Next, we concatenate the scores from a subset of selected robust classifiers and use them as input to our LSTM. ---------------------------------- **APPROACH** In this section we present our two-step approach to video description. The first step performs visual recognition, while the second step generates textual descriptions.",
  "y": "differences uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_12",
  "x": "For the visual recognition we propose to use the visual classifiers trained according to the labels' semantics and \"visuality\". For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33] . We discuss various design choices for building and training the LSTM. An overview of our approach is given in Figure 1 . ---------------------------------- **VISUAL LABELS FOR ROBUST VISUAL CLASSIFIERS** For training we rely on a parallel corpus of videos and weak sentence annotations. As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_13",
  "x": "An overview of our approach is given in Figure 1 . ---------------------------------- **VISUAL LABELS FOR ROBUST VISUAL CLASSIFIERS** For training we rely on a parallel corpus of videos and weak sentence annotations. As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure. Not all sentences can be parsed successfully, as e.g. some sentences are incomplete or grammatically incorrect. To avoid loosing the potential labels in these sentences, we match our set of initial labels to the sentences which the parser failed to process. Semantic groups.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_14",
  "x": "The intuition behind this is to discard \"wrong negatives\" (e.g. using object \"bed\" as negative for place \"bedroom\"). Visual labels. Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of <cite>[28]</cite> . Next, we look up the list of \"places\" used in [41] and search for corresponding words among our labels. We look up the object classes used in [14] and search for these \"objects\", as well as their base forms (e.g. \"domestic cat\" and \"cat\"). We discard all the labels that do not belong to any of our three groups of interest as we assume that they are likely not visual and thus are difficult to recognize. Finally, we discard labels which the classifiers could not learn, as these are likely to be noisy or not visual. For this we require the classifiers to have have minimum area under the ROC-curve (Receiver Operating Characteristic). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_15",
  "x": "We find the best learning rate and step size on the validation set. Additionally we compare this to a polynomial learning strategy, where the learning rate is continuously decreased. The polynomial learning strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio Guadarrama in Caffe [15] . ---------------------------------- **EVALUATION** In this section we first analyze our approach on the <cite>MPII-MD</cite> <cite>[28]</cite> dataset and explore different design choices. Then, we compare our best system to <cite>prior work</cite>. ---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_17",
  "x": "Then, we compare our best system to <cite>prior work</cite>. ---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips).",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_18",
  "x": "We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips). We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32] , supersedes other popular measures, such as BLEU [26] , ROUGE [22] , in terms of agreement with human judgments. The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of <cite>MPII-MD</cite> we have typically only a single reference. ---------------------------------- **ROBUST VISUAL CLASSIFIERS.**",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_19",
  "x": "In this section we first analyze our approach on the <cite>MPII-MD</cite> <cite>[28]</cite> dataset and explore different design choices. Then, we compare our best system to <cite>prior work</cite>. ---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_21",
  "x": "In our experiments we combine three in an ensemble, averaging the resulting word predictions. In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4 ). To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_22",
  "x": "Additionally we train multiple LSTMs with different random orderings of the training data. In our experiments we combine three in an ensemble, averaging the resulting word predictions. In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4 ). To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_23",
  "x": "In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_24",
  "x": "Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_25",
  "x": "We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes. We also propose a retrieval upperbound (last line in Table 5 ). For every test sentence we retrieve the closest training sentence according to the METEOR.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_26",
  "x": "The rather low METEOR score of 19.43 reflects the difficulty of the dataset. A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 . Here, e.g. the verb pour, object drink and place courtyard only appear in our output. We attribute this, on one hand, to our diverse and robust visual classifiers. On the other hand, the architecture and parameter choices of our LSTM allow us to learn better correspondance between words and visual classifiers' scores. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_27",
  "x": "Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes. We also propose a retrieval upperbound (last line in Table 5 ). For every test sentence we retrieve the closest training sentence according to the METEOR. The rather low METEOR score of 19.43 reflects the difficulty of the dataset. A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_28",
  "x": "Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 . Here, e.g. the verb pour, object drink and place courtyard only appear in our output. We attribute this, on one hand, to our diverse and robust visual classifiers. On the other hand, the architecture and parameter choices of our LSTM allow us to learn better correspondance between words and visual classifiers' scores. ---------------------------------- **ANALYSIS** Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_30",
  "x": "Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 . Here, e.g. the verb pour, object drink and place courtyard only appear in our output. We attribute this, on one hand, to our diverse and robust visual classifiers. On the other hand, the architecture and parameter choices of our LSTM allow us to learn better correspondance between words and visual classifiers' scores. ---------------------------------- **ANALYSIS** Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_35",
  "x": "(d) When comparing all four plots (Figures 4a  and 4b, Figures 5a and 5b) , we find that the strongest correlation between the methods' performance and the difficulty is observed for the Textual difficulty, while the least correlation we observe for the Sentence length. (960) contact (562) Table 6 : Entropy and top 5 frequent verbs of each WordNet topic in the <cite>MPII-MD</cite>. ---------------------------------- **SEMANTIC ANALYSIS** WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. \"motion\", \"perception\", \"competition\", \"emotion\"), defined for most synsets in WordNet [10] . We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_36",
  "x": "We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> . The best performing topic, \"cognition\", is highly biased to \"look at\" verb. The most frequent topics, \"motion\" and \"contact\", which are also visual (e.g. \"turn\", \"walk\", \"open\", \"sit\"), are nevertheless quite challenging, which we attribute to their high diversity (see their entropy w.r.t. different verbs and their frequencies in Table 6 ). At the same time \"perception\" is far less diverse and mainly focuses on verbs like \"look\" or \"stare\", which are quite frequent in the dataset, resulting in better performance. Topics with more abstract verbs (e.g. \"be\", \"have\", \"start\") tend to get lower scores. Top 100 best and worst sentences. We look at 100 Test sentences, where our method obtains highest and lowest METEOR scores.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_37",
  "x": "WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. \"motion\", \"perception\", \"competition\", \"emotion\"), defined for most synsets in WordNet [10] . We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> . The best performing topic, \"cognition\", is highly biased to \"look at\" verb. The most frequent topics, \"motion\" and \"contact\", which are also visual (e.g. \"turn\", \"walk\", \"open\", \"sit\"), are nevertheless quite challenging, which we attribute to their high diversity (see their entropy w.r.t.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_38",
  "x": "c) The sentences without verbs (e.g. describing a scene), without subjects or with non-human subjects get lower scores, which can be explained by a dataset bias towards \"Someone\" as subject. ---------------------------------- **CONCLUSION** We propose an approach to automatic movie description which trains visual classifiers and uses the classifier scores as input to LSTM. To handle the weak sentence annotations we rely on three main ingredients. First, we distinguish three semantic groups of labels (verbs, objects and places), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers. For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations. As the result we obtain the highest performance on the <cite>MPII-MD</cite> dataset as shown by all automatic evaluation measures and extensive human evaluation. We analyze the challenges in the movie description task using our and <cite>two prior works</cite>. We find that the factors which contribute to higher performance include: presence of frequent words, sentence length and simplicity as well as presence of \"visual\" verbs (e.g. \"nod\", \"walk\", \"sit\", \"smile\").",
  "y": "uses"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_0",
  "x": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information. ---------------------------------- **CONDITIONAL ACTION MODEL(CAM) USING SURFACE PHRASAL TYPE (SPT)** CAM is devised based on the hypothesis that this model can actively use rich information provided by the partially constructed parse built on the graph-structured stack, and thus estimate the probability of the shift/reduce actions more precisely [Kwak et al., 2001] . Surface Phrasal Type (SPT) is represented by a sequence of the primitive mnemonics which describes the specific types of phrases based on their terminal nodes. In this work, we use functional words for mnemonics in SPT.",
  "y": "background"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_1",
  "x": "**INTRODUCTION** Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information. ---------------------------------- **CONDITIONAL ACTION MODEL(CAM) USING SURFACE PHRASAL TYPE (SPT)** CAM is devised based on the hypothesis that this model can actively use rich information provided by the partially constructed parse built on the graph-structured stack, and thus estimate the probability of the shift/reduce actions more precisely [Kwak et al., 2001] . Surface Phrasal Type (SPT) is represented by a sequence of the primitive mnemonics which describes the specific types of phrases based on their terminal nodes.",
  "y": "motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_0",
  "x": "Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect. ---------------------------------- **INTRODUCTION** Recent work has shown evidence of substantial bias in machine learning systems, which is typically a result of bias in the training data. This includes both supervised (Blodgett and O'Connor, 2017; Tatman, 2017; Kiritchenko and Mohammad, 2018; De-Arteaga et al., 2019) and unsupervised natural language processing systems (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) . Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube. The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016;<cite> Waseem and Hovy, 2016</cite>; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter. We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (Blodgett et al., 2016) .",
  "y": "background motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_1",
  "x": "**RESEARCH DESIGN** ---------------------------------- **HATE SPEECH AND ABUSIVE LANGUAGE DATASETS** We focus on Twitter, the most widely used data source in abusive language research. We use all available datasets where tweets are labeled as various types of abuse and are written in English. We now briefly describe each of these datasets in chronological order. <cite>Waseem and Hovy (2016)</cite> collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful. They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory. These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias. This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_2",
  "x": "The substantial differences in the distributions for these two terms alone are consistent with our intuition that some of the results in Experiment 1 may be driven by differences in the frequencies of words associated with negative classes in the training datasets. Since we are using a subsample of the available data, we use smaller bootstrap samples, drawing k = 100 tweets each time. ---------------------------------- **RESULTS** The results of Experiment 1 are shown in Table 2. We observe substantial racial disparities in the performance of all classifiers. In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the <cite>Waseem and Hovy (2016)</cite> classifier. Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups. On the other hand, this classifier is 1.7 times more likely to classify tweets in the black-aligned corpus as sexist.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_3",
  "x": "For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low. Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus. Moving onto , we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set. Similarly, 17% of black-aligned tweets are predicted to contain offensive language compared to 6.5% of whitealigned tweets. The classifier trained on the Golbeck et al. (2017) dataset predicts black-aligned tweets to be harassment 1.4 times as frequently as white-aligned tweets. The Founta et al. (2018) classifier labels around 11% of tweets in the blackaligned corpus as hate speech and almost 18% as abusive, compared to 6% and 8% of white-aligned tweets respectively. It also classifies black-aligned tweets as spam 1.8 times as frequently. The results of Experiment 2 are consistent with the previous results, although there are some notable differences. In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",
  "y": "similarities uses"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_4",
  "x": "Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see a very similar result for Golbeck et al. (2017) compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus. Finally, for the Founta et al. (2018) classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_5",
  "x": "For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see a very similar result for Golbeck et al. (2017) compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus. Finally, for the Founta et al. (2018) classifier we see a substantial racial disparity, with black-aligned tweets classified as hate speech at 2.7 times the rate of white aligned ones, a higher rate than in Experiment 1. ---------------------------------- **DISCUSSION**",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_6",
  "x": "In almost every case, black-aligned tweets are classified as sexism, hate speech, harassment, and abuse at higher rates than whitealigned tweets. To some extent, the results in the first experiment may be driven by underlying differences in the rates at which speakers of different dialects use particular words and phrases associated with these negative classes in the training data. For example, the word \"n*gga\" appears fifteen times as frequently in the black-aligned corpus compared to the white-aligned corpus. 7 However, the second experiment shows that these disparities tend to persist even when comparing tweets containing keywords likely to be associated with negative classes. While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes. We now discuss the results as they pertain to each of the datasets used. Classifiers trained on data from <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_7",
  "x": "If they are used in this way we expect that they will systematically penalize African-Americans more than whites, resulting in racial discrimination. We have not evaluated these datasets for bias related to other ethnic and racial groups, nor other protected categories like gender and sexuality, but expect that such bias is also likely to exist. We recommend that efforts to measure and mitigate bias should start by focusing on how bias enters into datasets as they are collected and labeled. In particular, future work should focus on the following three areas. First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors<cite> (Waseem and Hovy, 2016</cite>; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) . Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives Founta et al., 2018) . In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities. In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories. For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_8",
  "x": "In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories. For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled. Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017;<cite> Waseem and Hovy, 2016)</cite> and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) . Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets. While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers. Further work is therefore necessary to better understand how to integrate expertise into the process and how training can be used to help to mitigate bias. We also need to consider how sociocultural context influences annotators' decisions. For example, 48% of the workers employed by Founta et al. (2018) were located in Venezuela but the authors did not consider whether this affected their results (or if the annotators understood English sufficiently for the task).",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_0",
  "x": "It is a commonly occurring language phenomenon in Indo-European languages, which is a verb mood typically used in subordinate clauses to express action that has not yet occurred, in the form of a wish, possibility, necessity etc. (Guan, 2012) . Oxford dictionary defines it as, Relating to or denoting a mood of verbs expressing what is imagined or wished or possible. Sentiment terms present in such sentences may not necessarily contribute to the actual sentiment of the sentence, for example 'I wish it tasted as amazing as it looked' is not positive. While this is considered as a challenge for sentiment analysis, we adopt a different perspective, and discover benefits of the presence of subjunctive mood in opinionated text. Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements<cite> (Ramanand et al., 2010)</cite> (Table 1) . Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing. Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) .",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_1",
  "x": "While this is considered as a challenge for sentiment analysis, we adopt a different perspective, and discover benefits of the presence of subjunctive mood in opinionated text. Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements<cite> (Ramanand et al., 2010)</cite> (Table 1) . Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing. Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) . To the best of our knowledge, subjunctive mood has never been analysed in the context of wish and suggestion detection. We collect a sample dataset comprising of example sentences of subjunctive mood, and identify features of subjunctive mood. We then employ a state of the art statistical classifier, and use subjunctive features in order to perform two kind of tasks on a given set of sentences: 1. Detect wish expressing sentences, and 2. Detect suggestion expressing sentences.",
  "y": "motivation background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_2",
  "x": "Other concepts like 'event modality', 'irrealis' (Palmer, 1986) , have definitions similar to that of subjunctive mood. Benamara et al. (2012) studied modality and negation for French language, with an objective to examine its effect on sentiment polarity. Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: <cite>Ramanand et al. (2010)</cite> pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion detection, where they focussed only on suggestion bearing wishes, and used manually formulated syntactic patterns for their detection. Brun (2013) also extracted suggestions from product reviews and used syntactico-semantic patterns for suggestion detection. None of these works on suggestion detection used a statistical classifier.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_3",
  "x": "<cite>Ramanand et al. (2010)</cite> worked on product review dataset of the wish corpus, with an objective to extract suggestions for improvements. They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes. They also annotated additional product reviews, but their data is not available for open research. \u2022 Suggestion Detection Product reviews (new): We re-annotated the product review dataset from Goldberg et al. (2009) , for suggestions. This also includes wishes for improvements and new features. Out of 1235 sentences, 6% are annotated as suggestions. Table 1 presents some examples from this dataset. Annotation Details: We had 2 annotators annotate each sentence with a suggestion or non-suggestion tag. We support the observation of <cite>Ramanand et al. (2010)</cite> that wishes for improvements and new features are implicit expression of suggestions. Therefore, annotators were also asked to annotate suggestions which were expressed as wishes.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_4",
  "x": "Table 1 presents some examples from this dataset. Annotation Details: We had 2 annotators annotate each sentence with a suggestion or non-suggestion tag. We support the observation of <cite>Ramanand et al. (2010)</cite> that wishes for improvements and new features are implicit expression of suggestions. Therefore, annotators were also asked to annotate suggestions which were expressed as wishes. For inter-annotator agreement, a kappa value of 0.874 was obtained. In the final dataset, we only retained the sentences where both the annotators agree. ---------------------------------- **SUBJUNCTIVE FEATURE EXTRACTION** Subjunctive Mood Dataset (new): Since we did not come across any corpus of subjunctive mood, we collected example sentences of subjunctive mood from various grammar websites and forums 2 , which resulted in a sample dataset of 229 sentences. Table 1 shows examples from this dataset.",
  "y": "motivation similarities"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_5",
  "x": "We collect all members of the VerbNet verb classes advice, wish, want, urge, require; 28 different verbs were obtained. <cite>Ramanand et al. (2010)</cite> also used a similar but much smaller subset {love, like, prefer and suggest} in their rules. ---------------------------------- **SYNTACTIC FEATURES:** \u2022 Frequent POS sequences: This is a set of 3,4 length sequences of Part Of Speech (POS) tags, which are automatically extracted from the subjunctive mood dataset. Words in the sentences are replaced by their corresponding POS tag, and top 200 sequences are extracted based on their weight. The weight of each sequence is a product of Term Frequency (TF) and Inverse Document Frequency (IDF). In order to apply the concept of TF and IDF to POS tag sequences, every 3 and 4 length tag sequence occurring in the corpus is treated as a term. We separate tags within a sequence with an underscore. An example of a sequence of length 3 would be PRP VB PRP ie.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_0",
  "x": "Both models assume that word frequency, predictability, and length affect eye movements in reading by affecting word recognition, yet neither one models the process of identifying words from visual information. Rather, each of these models directly specifies the effects of these variables on exogenous word processing functions, and the eye movements the models produce are sensitive to these functions' output. Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_1",
  "x": "These models capture a large range of the known properties of eye movements in reading, including effects of the best-documented linguistic variables on eye movements: the frequency, predictability, and length of words. Both models assume that word frequency, predictability, and length affect eye movements in reading by affecting word recognition, yet neither one models the process of identifying words from visual information. Rather, each of these models directly specifies the effects of these variables on exogenous word processing functions, and the eye movements the models produce are sensitive to these functions' output. Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_2",
  "x": "Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans. We argue from these results that uncertainty about word length is a necessary component of a full understanding of word length effects in reading. ---------------------------------- **REASONS FOR WORD LENGTH EFFECTS** The empirical effects of word length displayed by human readers are simple to describe: longer words receive more and longer fixations. The major reason proposed in the literature on eye movements in reading for this effect is that when fixating longer words, the average visual acuity of all the letters in the word will be lower than for shorter words, and this poorer average acuity is taken to lead to longer and more fixations.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_3",
  "x": "The model presented by <cite>Bicknell and Levy (2010)</cite> fits this description, and includes visual acuity limitations (in fact, identical to the visual acuity function in SWIFT). As already mentioned, how-22 ever, Bicknell (2011) showed that the model did not yield a humanlike length effect. Instead, while longer words were skipped less often and refixated more (as for humans), fixation durations generally fell with word length -the opposite of the pattern shown by humans. This result suggests that visual acuity limitations alone cannot explain the positive effect of word length on fixation durations in the presence of an opposing force such as the fact that longer words have smaller visual neighborhoods. We hypothesize that the reason for this pattern of results relates to a simplifying assumption made by Bicknell and Levy's model. Specifically, while visual input in the model yields noisy information about the identities of letters, it gives veridical information about the number of letters in each word, for reasons of computational convenience. There are theoretical and empirical reasons to believe that this simplifying assumption is incorrect, that early in the word identification process human readers do have substantial uncertainty about the number of letters in a word, and further, that this may be especially so for long words. For example, results with masked priming have shown that recognition of a target word is facilitated by a prime that is a proper subset of the target's letters (e.g., blcn-balcon; Peressotti & Grainger, 1999; Grainger, Granier, Farioli, Van Assche, & van Heuven, 2006) , providing evidence that words of different length have substantial similarity in early processing. For these reasons, some recent models of isolated word recognition (Gomez, Ratcliff, & Perea, 2008; Norris, Kinoshita, & van Casteren, 2010) have suggested that readers have some uncertainty about the number of letters in a word early in processing. If readers have uncertainty about the length of words, we may expect that the amount of uncertainty would grow proportionally to length, as uncertainty is proportional to set size in other tasks of number estimation (Dehaene, 1997) .",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_4",
  "x": "There are theoretical and empirical reasons to believe that this simplifying assumption is incorrect, that early in the word identification process human readers do have substantial uncertainty about the number of letters in a word, and further, that this may be especially so for long words. For example, results with masked priming have shown that recognition of a target word is facilitated by a prime that is a proper subset of the target's letters (e.g., blcn-balcon; Peressotti & Grainger, 1999; Grainger, Granier, Farioli, Van Assche, & van Heuven, 2006) , providing evidence that words of different length have substantial similarity in early processing. For these reasons, some recent models of isolated word recognition (Gomez, Ratcliff, & Perea, 2008; Norris, Kinoshita, & van Casteren, 2010) have suggested that readers have some uncertainty about the number of letters in a word early in processing. If readers have uncertainty about the length of words, we may expect that the amount of uncertainty would grow proportionally to length, as uncertainty is proportional to set size in other tasks of number estimation (Dehaene, 1997) . This would agree with the intuition that an 8-character word should be more easily confused with a 9-character word than a 3-character word with a 4-character word. Including uncertainty about word length that is larger for longer words would have the effect of increasing the number of visual neighbors for longer words more than for shorter words, providing another reason (in addition to visual acuity limitations) that longer words may require more and longer fixations. In the remainder of this paper, we describe an extension of<cite> Bicknell and Levy's (2010)</cite> model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length. We then present two sets of simulations with this extended model demonstrating that it produces more humanlike effects of word length, suggesting that uncertainty about word length may be an important component of a full understanding of the effects of word length in reading. ---------------------------------- **A RATIONAL MODEL OF READING**",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_5",
  "x": "This would agree with the intuition that an 8-character word should be more easily confused with a 9-character word than a 3-character word with a 4-character word. Including uncertainty about word length that is larger for longer words would have the effect of increasing the number of visual neighbors for longer words more than for shorter words, providing another reason (in addition to visual acuity limitations) that longer words may require more and longer fixations. In the remainder of this paper, we describe an extension of<cite> Bicknell and Levy's (2010)</cite> model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length. We then present two sets of simulations with this extended model demonstrating that it produces more humanlike effects of word length, suggesting that uncertainty about word length may be an important component of a full understanding of the effects of word length in reading. ---------------------------------- **A RATIONAL MODEL OF READING** In this section, we describe our extension of<cite> Bicknell and Levy's (2010)</cite> rational model of eye movement control in reading. Except for the visual input system, and a small change to the behavior policy to allow for uncertainty about word length, the model is identical to that described by Bicknell and Levy. The reader is referred to that paper for further computational details beyond what is described here. In this model, the goal of reading is taken to be efficient text identification.",
  "y": "extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_6",
  "x": "If the model chooses option (a), time simply advances, and if it chooses option (c), then reading immediately ends. If a saccade is initiated (b), there is a lag of two timesteps, representing the time required to plan and execute a saccade, during which the model again obtains visual input around the current position, and then the eyes move toward the intended target. Because of motor error, the actual landing position of the eyes is normally distributed around the intended target with the standard deviation in characters given by a linear function of the intended distance d (.87 + .084d; Engbert et al., 2005) . 1 ---------------------------------- **LANGUAGE KNOWLEDGE** Following <cite>Bicknell and Levy (2010)</cite>, we use very simple probabilistic models of language knowledge: word n-gram models (Jurafsky & Martin, 2009) , which encode the probability of each word conditional on the n \u2212 1 previous words. ---------------------------------- **FORMAL MODEL OF VISUAL INPUT** Visual input in the model consists of noisy information about the positions and identities of the characters in the text.",
  "y": "similarities uses"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_7",
  "x": "Intuitively, then, the model reads by making a rightward sweep to bring its confidence in each character up to \u03b1, but pauses to move left to reread any character whose confidence falls below \u03b2 . ---------------------------------- **SIMULATION 1: FULL MODEL** We now assess the effects of word length produced by the extended version of the model. Following Bicknell (2011), we use the model to simulate reading of a modified version of the Schilling, Rayner, and Chumbley (1998) corpus of typical sentences used in reading experiments. We compare three levels of length uncertainty: \u03b4 \u2208 {0, .05, .1}. The first of these (\u03b4 = 0) corresponds to<cite> Bicknell and Levy's (2010)</cite> model, which has no uncertainty about word length. We predict that increasing the amount of length uncertainty will make effects of word length more like those of humans, and we compare the model's length effects to those of human readers of the Schilling corpus. ---------------------------------- **METHODS** ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_8",
  "x": "---------------------------------- **OPTIMIZATION OF POLICY PARAMETERS** We set the parameters of the behavior policy (\u03b1, \u03b2 ) to values that maximize reading efficiency. We define reading efficiency E to be an interpolation of speed and accuracy, E = (1\u2212\u03b3)L\u2212\u03b3T , where L is the log probability of the true identity of the text under the model's beliefs at the end of reading, T is the number of timesteps before the model stopped reading, and \u03b3 gives the relative value of speed. For the present simulations, we use \u03b3 = .1, which produces reasonably accurate reading. To find optimal values of the policy parameters \u03b1 and \u03b2 for each model, we use the PEGASUS method (Ng & Jordan, 2000) to transform this stochastic optimization problem into a deterministic one amenable to standard optimization algorithms, and then use coordinate ascent. ---------------------------------- **TEST CORPUS** We test the model on a corpus of 33 sentences from the Schilling corpus slightly modified by <cite>Bicknell and Levy (2010)</cite> so that every bigram occurred in the BNC, ensuring that the results do not depend on smoothing. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_9",
  "x": "---------------------------------- **RESULTS** ---------------------------------- **CONCLUSION** In this paper, we argued that the success of major models of eye movements in reading to reproduce the (positive) human effect of word length via acuity limitations may be a result of not including opposing factors such as the negative correlation between visual neighborhood size and word length. We described the failure of the rational model presented in <cite>Bicknell and Levy (2010)</cite> to obtain humanlike effects of word length, despite including all of these factors, suggesting that our understanding of word length effects in reading is incomplete. We proposed a new reason for word length effects -uncertainty about word length that is larger for longer wordsand noted that this reason was not implemented in Bicknell and Levy's model because of a simplifying assumption. We presented an extension of the model relaxing this assumption, in which readers obtain noisy information about word length, and showed through two sets of simulations that the new model produces effects of word length that look more like those of human readers. Interestingly, while adding length uncertainty made both models more humanlike, it was only in Simulation 2 -in which words had more realistic visual neighborhoods -that all measures of the effect of word length on eye movements showed the human pattern, underscoring the importance of the structure of the language for this account of word length effects. We take these results as evidence that word length effects cannot be completely explained through limitations on visual acuity.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_0",
  "x": "Moreover, the resulting models show robust performance on a new test set we create from the task's original datasets. ---------------------------------- **INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_1",
  "x": "By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK**",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_2",
  "x": "Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_3",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_4",
  "x": "Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_5",
  "x": "Figure 1 gives an overview of using QA on the slot-filling task. Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_6",
  "x": "Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) .",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_7",
  "x": "However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The UWRE data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_8",
  "x": "In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the UWRE entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction, using models trained on the original UWRE and SQuAD datasets.",
  "y": "similarities uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_9",
  "x": "Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction, using models trained on the original UWRE and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the UWRE test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data is available about the relations in the test set. We compare training purely on UWRE instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available.",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_10",
  "x": "---------------------------------- **DISCUSSION** The results on our challenge test set suggest that the model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it. In the case of SQuAD, the fact that the answer has to be found within a multi-sentence paragraph provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach Model Acc BiDAF 0.82 FastQA 0.99 merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_11",
  "x": "In the case of SQuAD, the fact that the answer has to be found within a multi-sentence paragraph provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach Model Acc BiDAF 0.82 FastQA 0.99 merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original UWRE test set. In contrast, FastQA's performance is substantially worse.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_0",
  "x": "Recent work by <cite>Nerbonne and Wiersma (2006)</cite> has provided a foundation for measuring syntactic differences between corpora. It uses part-of-speech trigrams as an approximation to syntactic structure, comparing the trigrams of two corpora for statistically significant differences. This paper extends the method and its application. It extends the method by using leafpath ancestors of Sampson (2000) instead of trigrams, which capture internal syntactic structure-every leaf in a parse tree records the path back to the root. The corpus used for testing is the International Corpus of English, Great Britain (Nelson et al., 2002) , which contains syntactically annotated speech of Great Britain. The speakers are grouped into geographical regions based on place of birth. This is different in both nature and number than previous experiments, which found differences between two groups of Norwegian L2 learners of English. We show that dialectal variation in eleven British regions from the ICE-GB is detectable by our algorithm, using both leaf-ancestor paths and trigrams. ---------------------------------- ****",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_1",
  "x": "For example, a method for determining phonetic distance is given by Heeringa (2004) . Heeringa and others have also done related work on phonological distance in Nerbonne and Heeringa (1997) and Gooskens and Heeringa (2004) . A measure of syntactic distance is the obvious next step: <cite>Nerbonne and Wiersma (2006)</cite> provide one such method. This method approximates internal syntactic structure using vectors of part-of-speech trigrams. The trigram types can then be compared for statistically significant differences using a permutation test. This study can be extended in a few ways. First, the trigram approximation works well, but it does not necessarily capture all the information of syntactic structure such as long-distance movement. Second, the experiments did not test data for geographical dialect variation, but compared two generations of Norwegian L2 learners of English, with differences between ages of initial acquisition. We address these areas by using the syntactically annotated speech section of the International Corpus of English, Great Britain (ICE-GB) (Nelson et al., 2002) , which provides a corpus with full syntactic annotations, one that can be divided into groups for comparison. The sentences of the corpus, being represented as parse trees rather than a vector of POS tags, are converted into a vector of leafancestor paths, which were developed by Sampson (2000) to aid in parser evaluation by providing a way to compare gold-standard trees with parser output trees.",
  "y": "motivation background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_2",
  "x": "First, the trigram approximation works well, but it does not necessarily capture all the information of syntactic structure such as long-distance movement. Second, the experiments did not test data for geographical dialect variation, but compared two generations of Norwegian L2 learners of English, with differences between ages of initial acquisition. We address these areas by using the syntactically annotated speech section of the International Corpus of English, Great Britain (ICE-GB) (Nelson et al., 2002) , which provides a corpus with full syntactic annotations, one that can be divided into groups for comparison. The sentences of the corpus, being represented as parse trees rather than a vector of POS tags, are converted into a vector of leafancestor paths, which were developed by Sampson (2000) to aid in parser evaluation by providing a way to compare gold-standard trees with parser output trees. In this way, each sentence produces its own vec-tor of leaf-ancestor paths. Fortunately, the permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is already designed to normalize the effects of differing sentence length when combining POS trigrams into a single vector per region. The only change needed is the substitution of leaf-ancestor paths for trigrams. The speakers in the ICE-GB are divided by place of birth into geographical regions of England based on the nine Government Office Regions, plus Scotland and Wales. The average region contains a little over 4,000 sentences and 40,000 words. This is less than the size of the Norwegian corpora, and leaf-ancestor paths are more complex than trigrams, meaning that the amount of data required for obtaining significance should increase.",
  "y": "motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_3",
  "x": "The limit seems to be around 250,000 words for leaf-ancestor paths, and 100,000 words for POS trigrams, but more careful tests are needed to verify this. Comparisons to judgments of dialectologists have not yet been made. The comparison is difficult because of the difference in methodology and amount of detail in reporting. Dialectology tends to collect data from a few informants at each location and to provide a more complex account of relationship than the like/unlike judgments provided by permutation tests. ---------------------------------- **METHODS** The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_4",
  "x": "The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus. Normalization is needed to account for the frequency differences caused by sentence length variation; it is presented below.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_5",
  "x": "---------------------------------- **METHODS** The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_6",
  "x": "The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora. The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_7",
  "x": "However, to find out if the value of R is significant, we must use a permutation test with a Monte Carlo technique described by Good (1995) , following closely the same usage by <cite>Nerbonne and Wiersma (2006)</cite> . The intuition behind the technique is to compare the R of the two corpora with the R of two random subsets of the combined corpora. For comparison to the experiment conducted by <cite>Nerbonne and Wiersma (2006)</cite> , the experiment was also run with POS trigrams.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_8",
  "x": "Some speakers had to be thrown out at this point because they lacked brithplace information or were born outside the UK. Each region varied in size; however, the average number of sentences per corpus was 4682, with an average of 44,726 words per corpus (see table 1). Thus, the average sentence length was 9.55 words. The average corpus was smaller than the Norwegian L2 English corpora of <cite>Nerbonne and Wiersma (2006)</cite> , which had two groups, one with 221,000 words and the other with 84,000. Significant differences (at p < 0.05) were found when comparing the largest regions, but no significant differences were found when comparing small regions to other small regions. The significant differences found are given in table 2 and 3. It seems that summed corpus size must reach a certain threshold before differences can be observed reliably: about 250,000 words for leaf-ancestor paths and 100,000 for trigrams. There are exceptions in both directions; the total size of London compared to Wales is larger than the size of London compared to the East Midlands, but the former is not statistically different. On the other hand, the total size of Southeast England compared to Scotland is only half of the other significantly different comparisons; this difference may be a result of more extreme syntactic differences than the other areas. Finally, it is interesting to note that the summed Norwegian corpus size is around 305,000 words, which is about three times the size needed for significance as estimated from the ICE-GB data.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_9",
  "x": "Our work extends that of <cite>Nerbonne and Wiersma (2006)</cite> in a number of ways. We have shown that an alternate method of representing syntax still allows the permutation test to find significant differences between corpora. In addition, we have shown differences between corpora divided by geographical area rather than language proficiency, with many more corpora than before. Finally, we have shown that the size of the corpus can be reduced somewhat and still obtain significant results. Furthermore, we also have shown that both leafancestor paths and POS trigrams give similar results, although the more complex paths require more data. However, there are a number of directions that this experiment should be extended. A comparison that divides the speakers into traditional British dialect areas is needed to see if the same differences can be detected. This is very likely, because corpus divisions that better reflect reality have a better chance of achieving a significant difference. In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in <cite>Nerbonne and Wiersma (2006)</cite> by up to a factor of 10. This raises the question of a lower limit on corpus size.",
  "y": "extends"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_10",
  "x": "Finally, we have shown that the size of the corpus can be reduced somewhat and still obtain significant results. Furthermore, we also have shown that both leafancestor paths and POS trigrams give similar results, although the more complex paths require more data. However, there are a number of directions that this experiment should be extended. A comparison that divides the speakers into traditional British dialect areas is needed to see if the same differences can be detected. This is very likely, because corpus divisions that better reflect reality have a better chance of achieving a significant difference. In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in <cite>Nerbonne and Wiersma (2006)</cite> by up to a factor of 10. This raises the question of a lower limit on corpus size. Our experiment suggests that the two corpora must have at least 250,000 words, although we suspect that better divisions will allow smaller corpus sizes. While we are reducing corpus size, we might as well compare the increasing numbers of smaller and smaller corpora in an advantageous order. It should be possible to cluster corpora by the point at which they fail to achieve a significant difference when split from a larger corpus.",
  "y": "differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_0",
  "x": "****NRC: INFUSED PHRASE VECTORS FOR NAMED ENTITY RECOGNITION IN TWITTER**** **ABSTRACT** Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by <cite>Cherry and Guo (2015)</cite> , who use a discriminative, semi-Markov tagger, augmented with multiple word representations. We enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase. Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1. ---------------------------------- **INTRODUCTION** Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo-location (Nadeau and Sekine, 2007) . NER is the first step in many information extraction tasks, but in social media, this task is extremely challenging.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_1",
  "x": "As such, we are quite interested in the W-NUT Named Entity Recognition in Twitter task (Baldwin et al., 2015) as a platform to benchmark and drive forward work on NER in social media. Our submission to this competition closely follows <cite>Cherry and Guo (2015)</cite> , who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings. We augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership. Our novel infusion technique allows us to adapt existing vectors to NER regardless of their source, by training a typelevel auto-encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector. Our submitted system achieved a typed F1 of 44.7, placing third in the competition, while training only on the provided training data. The competition organizers provided two development sets, one (dev) that is close to the training data, with both train and dev being drawn from the year 2010, and another (dev 2015) that is close to the test data, with both dev 2015 and test being drawn from the winter of 2014-2015. We present a postcompetition system that achieves an F1 of 54.2 using the same features and hyper-parameters as our submitted system, except that our tagger is also trained on all provided development data. We close with an analysis of dev 2015's relation to the test set, and argue that these results may overestimate the impact that a small, in-domain training set can have on NER performance. ---------------------------------- **DATA RESOURCES**",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_2",
  "x": "We make use of two external data resources: gazetteers and unlabeled tweets. For gazetteers, we begin with the word lists provided with the W-NUT baseline system, which appear to be mostly derived from Freebase. We treat each file in the lexicon directory as a distinct word list. We update and augment these lists with our own Freebase queries in Section 3.2. We use unannotated tweets to build various word representations (see Section 3.1). Our unannotated corpus collects 98M tweets (1,995M tokens) from between May 2011 and April 2012. The same corpus is used by <cite>Cherry and Guo (2015)</cite> . These tweets have been tokenized and post-processed to remove many special Unicode characters; they closely resemble those that appear in the provided training and development sets. Furthermore, the corpus consists only of tweets in which the NER system of Ritter et al. (2011) detects at least one entity. The automatic NER tags are used only to select tweets for inclusion in the corpus, after which the annotations are discarded.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_3",
  "x": "---------------------------------- **METHODS** ---------------------------------- **BASE TAGGER** We first summarize the approach of <cite>Cherry and Guo (2015)</cite> , which we build upon for our system. Tagger: We tag each tweet independently using a semi-Markov tagger (Sarawagi and Cohen, 2004) , which tags phrasal entities using a single operation, as opposed to traditional word-based entity tagging schemes. An example tag sequence, drawn from the 2010 development data, is shown in Figure 1 . Semi-Markov tagging gives us the freedom to design features at either the phrase or the word level, while also simplifying our tag set. Furthermore, with our semi-Markov tags, we find we have no need for Markov features that track previous tag assignments, as our entity labels cohere naturally. This speeds up tagging dramatically.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_4",
  "x": "For gazetteers, we first segment the tweet into longest matching gazetteer phrases, resolving overlapping phrases with a greedy left-toright walk through the tweet. Each word then generates a set of features indicating which gazetteers (if any) contain its phrase. For cluster representations, we train Brown clusters on our unannotated corpus, using the implementation by Liang (2005) to build 1,000 clusters over types that occur with a minimum frequency of 10. Following Miller et al. (2004) , each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. For word embeddings, we use an in-house Java re-implementation of word2vec (Mikolov et al., 2013a) to build 300-dimensional vector representations for all types that occur at least 10 times in our unannotated corpus. Each word then reports a real-valued feature (as opposed to an indicator) for each of the 300 dimensions in its vector representation. A single random vector is created to represent all out-ofvocabulary words. Our vectors and clusters cover 2.5 million types. Note that we do not include part-of-speech tags as features, as they were not found to be useful by <cite>Cherry and Guo (2015)</cite> . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_5",
  "x": "Phrase embeddings enable more gazetteer matches for gazetteer-infused vectors, which we discuss next. ---------------------------------- **GAZETTEER-INFUSED PHRASE VECTORS** We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010;<cite> Cherry and Guo, 2015)</cite> . Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014) . A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014) . Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder. The auto-encoder eliminates the need to have access to the original training data and the vector training model, requiring only the trained distributed vectors. In this sense, it can be considered computationally lighter than the above mentioned information fusion methods.",
  "y": "background"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_6",
  "x": "C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations. We found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge (E=30, C=0.001, P=8). Although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments. The next three systems test the three extensions described in Section 3. Neither [P]hrase vectors nor [U]pdated gazetteers were able to improve both dev and dev 2015 when applied alone, while the [A]dapted vectors did boost performance on both sets, increasing average F-measure by 0.6. In particular, the adapted vectors improved the rare entity types such as movie and sports team. Unfortunately, these improvements do not seem to carry over to the test set.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_7",
  "x": "The Rnk column lists the retro-active rank of each system in the competition. ---------------------------------- **ABLATION** The systems above [A]+[U] are intended to demonstrate our development process. Our baseline is our attempt to re-implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers. C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations. We found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge (E=30, C=0.001, P=8).",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_8",
  "x": "While this bot appears only 4 times in dev 2015, it appears 23 times in test. If we remove these 23 tweets from the test set, our submitted system increases is performance to a Precision / Recall / F-measure of 53.5 / 40.0 / 45.8, while our best post-competition system decreases to 61.0 / 46.2 / 52.6, narrowing the gap in F-measure by 2.6 points. The ability to extract entities from formulaic bots such as this one could be useful, but the core purpose of NER technology is to enable the extraction of information from human-written text. ---------------------------------- **CONCLUSION** We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_9",
  "x": "While this bot appears only 4 times in dev 2015, it appears 23 times in test. If we remove these 23 tweets from the test set, our submitted system increases is performance to a Precision / Recall / F-measure of 53.5 / 40.0 / 45.8, while our best post-competition system decreases to 61.0 / 46.2 / 52.6, narrowing the gap in F-measure by 2.6 points. The ability to extract entities from formulaic bots such as this one could be useful, but the core purpose of NER technology is to enable the extraction of information from human-written text. ---------------------------------- **CONCLUSION** We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "extends differences"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_0",
  "x": "It is well known that sentences with difficult vocabulary, passive voice or complex structures, such as relative and subordinated clauses, can be challenging to understand. Text simplification has been found to be beneficial for language learners (Shirzadi, 2014) , children (Kajiwara et al., 2013) , and adults with low literacy skills (Arnaldo Candido Jr. and Erick Maziero and Caroline Gasperin and Thiago A. S. Pardo and Lucia Specia and Sandra M. Aluisio, 2009) or language disabilities (John Carroll and Guido Minnen and Darren Pearce and Yvonne Canning and Siobhan Devlin and John Tait, 1999; Luz Rello and Ricardo Baeza-Yates, 2014) . To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014) . The performance of the state-of-the-art systems has improved significantly<cite> (Horn et al., 2014</cite>; Siddharthan and Angrosh, 2014) . Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output. To support human post-editing, a number of researchers have developed specialized editors for text simplification. While the editor described in Max (2006) shares similar goals as ours, it requires human intervention in much of the simplification process. The Automatic Text Adaptation tool suggests synonyms (Burstein et al., 2007) , but does not perform syntactic simplification.",
  "y": "background"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_1",
  "x": "By default, the editor uses a list of approximately 4,000 words that all students in Hong Kong are expected to know upon graduation from primary school (EDB, 2012). However, the user can also upload his or her own vocabulary list. Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list. Following <cite>Horn et al. (2014)</cite> , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (Bird et al., 2009) , nor words in our stoplist, which are already simple. In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step. We trained the model with all sentences from Wikipedia. For each target word, the model returns a list of the most similar words; we extract the top 20 in this list that are included in the user-supplied vocabulary list. In the next step, substitution selection, we re-rank these 20 words with a language model. We trained a trigram model with the kenlm (Heafield, 2011) , again using all sentences from Wikipedia. We then place the 10 words with the highest probabilities in a drop-down list in our editor 2 ; for example, Figure 1 shows the ten candidates offered for the word \"municipal\".",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_2",
  "x": "We trained a trigram model with the kenlm (Heafield, 2011) , again using all sentences from Wikipedia. We then place the 10 words with the highest probabilities in a drop-down list in our editor 2 ; for example, Figure 1 shows the ten candidates offered for the word \"municipal\". If none of the candidates are appropriate, the user can easily revert to the original word, which is also included in the drop-down list; alternatively, the user can click on the text to directly edit it. ---------------------------------- **EVALUATION** We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> . This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators. To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) . To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list. Precision is at 31% for the top candidate; it is at 57% for the top ten candidates.",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_3",
  "x": "In Figure 1 , the input sentence matches the subtree pattern for coordination; it is therefore split into two shorter sentences, S 1 =\"City of Faizabad ... India.\" and S 2 =\"and situated ... river Ghaghra\". Since S 1 then matches the pattern for appositive phrase, the phrase \"the headquarters of Faizabad District\" is taken out to form its own sentence. If the user finds a sentence split to be inappropriate, he or she can click on the \"Merge\" button to undo the split. Finally, in the regeneration step, the editor restores the subject (e.g., \"City of Faizabad\") to newly formed sentences. Often, this step also requires generation of referring expressions, determiners, conjunctions and sentence re-ordering. Since most of these tasks require real-world knowledge, the editor currently leaves it to the user for post-editing. ---------------------------------- **EVALUATION** We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> . For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification (",
  "y": "uses"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_0",
  "x": "The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. ---------------------------------- **INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_1",
  "x": "**INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. The overall effect obtained is twofold: on the one hand, in a complex situation of reference (in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness) the algorithm simply selects frequent attributes.",
  "y": "background extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_2",
  "x": "---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. The overall effect obtained is twofold: on the one hand, in a complex situation of reference (in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness) the algorithm simply selects frequent attributes. This may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the 'right' attribute straight away. On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_3",
  "x": "**BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. The overall effect obtained is twofold: on the one hand, in a complex situation of reference (in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness) the algorithm simply selects frequent attributes. This may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the 'right' attribute straight away. On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> .",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_4",
  "x": "\u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. The overall effect obtained is twofold: on the one hand, in a complex situation of reference (in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness) the algorithm simply selects frequent attributes. This may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the 'right' attribute straight away. On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results. This clear gap in our previous work represents an opportunity for improvement discussed in the next section.",
  "y": "motivation"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_5",
  "x": "The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 <cite>(Lucena & Paraboni, 2008)</cite> , now taking also the trial condition (+/-LOC) into account. In the TUNA data, +LOC indicates the instances of the experiment in which participants were told that they were allowed to refer to the X,Y coordinates of the screen (i.e., selecting the X-and/or Y-DIMENSION attributes), whereas -LOC indicates the trials in which they were discouraged (but not prevented) to do so. In practice, references in +LOC trials are more likely to convey the X-and Y-DIMENSION attributes than those in which the -LOC condition was applied. Our modified algorithm simply consists of computing separated frequency lists for +LOC and -LOC trial conditions, and then using the original frequency-based greedy approach with each list accordingly. In practice, descriptions are now generated in two different ways, depending on the trial condition, which may promote the Xand Y-DIMENSION attributes to higher positions in the list P when +LOC applies. Using the TUNA Challenge 2009 development data set, the attribute selection task was performed as above. For the surface realisation task, we have reused the English language surface realisation module provided by Irene Langkilde-Geary for the TUNA Challenge 2008. ---------------------------------- **RESULTS** The following Figure 1 shows mean sting-edit distance and BLEU-3 scores computed using the evaluation tool provided by the TUNA Challenge team.",
  "y": "extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_6",
  "x": "Our modified algorithm simply consists of computing separated frequency lists for +LOC and -LOC trial conditions, and then using the original frequency-based greedy approach with each list accordingly. In practice, descriptions are now generated in two different ways, depending on the trial condition, which may promote the Xand Y-DIMENSION attributes to higher positions in the list P when +LOC applies. Using the TUNA Challenge 2009 development data set, the attribute selection task was performed as above. For the surface realisation task, we have reused the English language surface realisation module provided by Irene Langkilde-Geary for the TUNA Challenge 2008. ---------------------------------- **RESULTS** The following Figure 1 shows mean sting-edit distance and BLEU-3 scores computed using the evaluation tool provided by the TUNA Challenge team. For ease of comparison with our previous work, we also present Dice and MASI scores computed as in the previous TUNA Challenge, although these scores were not required for the current competition. The most relevant comparison with our previous work is observed in the overall string-edit distance values in Figure 1 : considering that in<cite> Lucena & Paraboni (2008)</cite> we reported 6.12 editdistance for Furniture and 7.38 for People, the overall improvement (driven by the descriptions in the Furniture domain) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the TUNA trials (the trial condition +/-LOC. ) On the other hand, the divide between +LOC and -LOC strategies does not have a significant effect on the results based on the semantics of the description (i.e., Dice and MASI scores), which remain the same as those obtained previously.",
  "y": "differences"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_0",
  "x": "**INTRODUCTION** The majority of existing supervised relation extraction models can only extract a fixed set of relations which has been specified at training time. They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt\u00e4schel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017) . It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_1",
  "x": "They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt\u00e4schel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017) . It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_2",
  "x": "They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt\u00e4schel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017) . It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_3",
  "x": "While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations. Empirically, we evaluate on lifelong versions of the datasets by Bordes et al. (2015) and Han et al. (2018) and demonstrate con-siderable performance improvements over prior state-of-the-art approaches. ---------------------------------- **BACKGROUND** Lifelong Learning In the lifelong learning setting, also referred to as continual learning (Ring, 1994; Thrun, 1996; Zhao and Schmidhuber, 1996) , a model f \u03b8 is presented with a sequence of tasks {T t } t=1,2,3..,T , one task per round, and the goal is to learn model parameters {\u03b8 t } t=1,2,3,..,T with the best performance on the observed tasks.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_4",
  "x": "In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations. Empirically, we evaluate on lifelong versions of the datasets by Bordes et al. (2015) and Han et al. (2018) and demonstrate con-siderable performance improvements over prior state-of-the-art approaches. ---------------------------------- **BACKGROUND** Lifelong Learning In the lifelong learning setting, also referred to as continual learning (Ring, 1994; Thrun, 1996; Zhao and Schmidhuber, 1996) , a model f \u03b8 is presented with a sequence of tasks {T t } t=1,2,3..,T , one task per round, and the goal is to learn model parameters {\u03b8 t } t=1,2,3,..,T with the best performance on the observed tasks. Each task T can be a conventional supervised task with its own distinct train (T train ), development (T dev ) and test (T test ) splits.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_5",
  "x": "In principle the learner model f \u03b8 could be any gradient-optimized relation extraction model. In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task. Note that the meta-learning algorithm uses SGD as the update rule (U), and does not add any additional trainable parameters to the learner model. ---------------------------------- **EXPERIMENTS** ---------------------------------- **SETUP**",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_6",
  "x": "**RELATION CLASSIFICATION MODEL** In principle the learner model f \u03b8 could be any gradient-optimized relation extraction model. In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task. Note that the meta-learning algorithm uses SGD as the update rule (U), and does not add any additional trainable parameters to the learner model. ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_7",
  "x": "**EXPERIMENTS** ---------------------------------- **SETUP** We conduct experiments in two settings. In the full supervision setting, we provide all models with all supervision available in the training set of each task. In the second, we limit the amount of supervision for each task to measure how the models are able to cope with limited supervision. Each experiment is run five (5) times and we report the average result. ---------------------------------- **DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_8",
  "x": "**DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation. The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_10",
  "x": "<cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_11",
  "x": "The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION**",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_12",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation. The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_13",
  "x": "---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_14",
  "x": "Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations. ---------------------------------- **LIMITED SUPERVISION RESULTS** The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by <cite>Wang et al. (2019)</cite> to using our approach when only limited supervision is available for all tasks. We compare three approaches, Full EA-EMR (which uses their alignment module), its variant without the alignment module (EA-EMR NoAlign) and our approach (MLLRE). Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>. Figures 2(a) and 2(b) show the corresponding plots using 200 supervision instances for each task. From the figures, we observe that the use of a separate alignment model results in only minor gains when supervision for the tasks is limited, whereas the use of our approach leads to wide gains on both datasets.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_15",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_0",
  "x": "****MR. BENNET, HIS COACHMAN, AND THE ARCHBISHOP WALK INTO A BAR BUT ONLY ONE OF THEM GETS RECOGNIZED: ON THE DIFFICULTY OF DETECTING CHARACTERS IN LITERARY TEXTS**** **ABSTRACT** Characters are fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. ---------------------------------- **INTRODUCTION** How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature <cite>(Elson et al., 2010</cite>; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015) .",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_1",
  "x": "However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010) . This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by<cite> Elson et al. (2010)</cite> . ---------------------------------- **RELATED WORK** Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014) . To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_2",
  "x": "---------------------------------- **RELATED WORK** Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014) . To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net-works without relying on dialogue, unlike the methods of<cite> Elson et al. (2010)</cite> and He et al. (2013) .",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_3",
  "x": "**COMPARISON SYSTEMS** The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in<cite> Elson et al. (2010)</cite> , BookNLP (Bamman et al., 2014) , and the method described in Ardanuy and Sporleder (2014) . For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character. As a baseline, we use the output of Stanford NER, where every name is considered a separate character; this baseline represents the upper-bound in recall from any system using only NER to identify character names. Table 1 shows the results for the manually-annotated and SparkNotes corpora. The Sherlock Holmes corpus presents a notable challenge due to the presence of many minor characters, which are not detected by NER. An error analysis for our approach revealed that while many characters were extracted, the coreference resolution did not link a characters' different referents together and hence, each name was reported as a separate character, which caused a drop in performance. Nevertheless, our system provided the highest performance for character recognition. ferent set of challenges due to multiple characters sharing the same last name or the same first name.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_4",
  "x": "An error analysis of our system revealed that majority of mistakes were due to the multiple names for a character not being merged into a single identity. Nevertheless, our system performs best of those tested. For the SparkNotes data, the NER baseline achieves the highest recall, indicating that many of the major character names listed in SparkNotes' data can be directly found by NER. Nevertheless, in reality, the baseline's performance is offset by its significantly lower precision, as shown in its performance on the other novels; indeed the baseline grossly overestimates the number of characters for the SparkNotes novels, reporting 339 characters per novel on average. Table 2 shows our system's performance without stage 7, which involved the extraction of minor characters. Stage 7 overall improves recall with a slight hindrance to precision. For the Sherlock Holmes corpus, stage 7 is slightly detrimental to overall performance, which as we stipulated earlier is caused by missing co-referent links. Finally, returning to the initially-posed question of how many characters are present, we find that despite the detection error in our method, the overall predicted number of characters is quite close to the actual: for Sherlock Holmes stories, the number of characters was estimated within 2.4 on average, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories<cite> Elson et al. (2010)</cite> analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) .",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_5",
  "x": "Nevertheless, in reality, the baseline's performance is offset by its significantly lower precision, as shown in its performance on the other novels; indeed the baseline grossly overestimates the number of characters for the SparkNotes novels, reporting 339 characters per novel on average. Table 2 shows our system's performance without stage 7, which involved the extraction of minor characters. Stage 7 overall improves recall with a slight hindrance to precision. For the Sherlock Holmes corpus, stage 7 is slightly detrimental to overall performance, which as we stipulated earlier is caused by missing co-referent links. Finally, returning to the initially-posed question of how many characters are present, we find that despite the detection error in our method, the overall predicted number of characters is quite close to the actual: for Sherlock Holmes stories, the number of characters was estimated within 2.4 on average, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories<cite> Elson et al. (2010)</cite> analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) . Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from<cite> Elson et al. (2010)</cite> and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size.",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_0",
  "x": "---------------------------------- **INTRODUCTION** Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation. This is because the task of abstractive summarization involves two subtasks: salient (important) event detection as well as logical compression, i.e., the summary should not contain any information that is contradictory or unrelated to the original document. Current methods have to learn both these skills from the same dataset and a single model. Therefore, there is benefit in learning the latter ability of logical compression via external knowledge from a separate entailment generation task, that will specifically teach the model how to rewrite and compress a sentence such that it logically follows from the original input. To achieve this, we employ the recent paradigm of sequence-to-sequence multi-task learning (Luong et al., 2016) .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_1",
  "x": "---------------------------------- **INTRODUCTION** Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation. This is because the task of abstractive summarization involves two subtasks: salient (important) event detection as well as logical compression, i.e., the summary should not contain any information that is contradictory or unrelated to the original document. Current methods have to learn both these skills from the same dataset and a single model. Therefore, there is benefit in learning the latter ability of logical compression via external knowledge from a separate entailment generation task, that will specifically teach the model how to rewrite and compress a sentence such that it logically follows from the original input. To achieve this, we employ the recent paradigm of sequence-to-sequence multi-task learning (Luong et al., 2016) .",
  "y": "motivation"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_2",
  "x": "Impor-tantly, these improvements are achieved despite the fact that the domain of the entailment dataset (image captions) is substantially different from the domain of the summarization datasets (general news), which suggests that the model is learning certain domain-independent inference skills. Our next steps to this workshop paper include incorporating stronger pointer-based models and employing the new multi-domain entailment corpus (Williams et al., 2017) . ---------------------------------- **RELATED WORK** Earlier summarization work focused more towards extractive (and compression) based summarization, i.e., selecting which sentences to keep vs discard, and also compressing based on choosing grammatically correct sub-sentences having the most important pieces of information (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015) . Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006) . The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization.",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_3",
  "x": "For the task of entailment generation, we use the Standford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) , where we only use the entailment-labeled pairs and regroup the splits to have a zero overlap traintest split and have a multi-reference test set, as suggested by Pasunuru and Bansal (2017) . Out of 190, 113 entailments pairs, we use 145, 822 unique premise pairs for training, and the rest of them are equally divided into dev and test sets. ---------------------------------- **EVALUATION** Following previous work<cite> (Nallapati et al., 2016</cite>; Chopra et al., 2016; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) . ---------------------------------- **TRAINING DETAILS** We use the following simple settings for all the models, unless otherwise specified. We unroll the encoder RNN's to a maximum of 50 time steps and decoder RNN's to a maximum of 30 time steps.",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_5",
  "x": "This suggests that the entailment generation model is teaching the summarization model some skills about how to choose a logical subset of the events in the full input document. This is especially promising given that the domain of the entailment dataset (image captions) is very different from the domain of the summarization datasets (news), suggesting that the model might be learning some domain-agnostic inference skills. ---------------------------------- **SUMMARIZATION RESULTS: DUC** Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information. ---------------------------------- **ANALYSIS EXAMPLES**",
  "y": "differences"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_6",
  "x": "Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information. ---------------------------------- **ANALYSIS EXAMPLES** ---------------------------------- **CONCLUSION AND NEXT STEPS** We presented a multi-task learning approach to incorporate entailment generation knowledge into summarization models. We demonstrated promising initial improvements based on multiple datasets and metrics, even when the entailment knowledge was extracted from a domain different from the summarization domain.",
  "y": "future_work"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_0",
  "x": "Most of the recent work on exploiting coreference relations in Machine Translation focused on improving the translation of anaphoric pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012; Nov\u00e1k et al., 2015; Guillou and Webber, 2015) , disregarding other types of coreference relations, one of the reasons being the lack of annotated parallel corpora as well as the variability in the annotated data. However, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages. More importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme. In our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains. In particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method. Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language. However, our goal is slightly different: We are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages. Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_1",
  "x": "In our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains. In particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method. Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language. However, our goal is slightly different: We are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages. Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup. In Section 4, we give a detailed error analysis and discuss the results of our experiment. The conclusions and the avenues for future research are presented in Section 5. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_2",
  "x": "The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup. In Section 4, we give a detailed error analysis and discuss the results of our experiment. The conclusions and the avenues for future research are presented in Section 5. ---------------------------------- **RELATED WORK** Annotation projection is a method that allows for automatically transferring annotations from a well-studied (source) language to a low-resource (target) language in a parallel corpus in order to automatically obtain annotated data. It was first introduced in the work of (Yarowsky et al., 2001) Rahman and Ng, 2012; Martins, 2015; Grishina and Stede, 2015) . Thereafter, <cite>(Grishina and Stede, 2017)</cite> proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively). Although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large-scale due to the complexity of the annotation task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_3",
  "x": "**RELATED WORK** Annotation projection is a method that allows for automatically transferring annotations from a well-studied (source) language to a low-resource (target) language in a parallel corpus in order to automatically obtain annotated data. It was first introduced in the work of (Yarowsky et al., 2001) Rahman and Ng, 2012; Martins, 2015; Grishina and Stede, 2015) . Thereafter, <cite>(Grishina and Stede, 2017)</cite> proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively). Although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large-scale due to the complexity of the annotation task. ---------------------------------- **ANNOTATION PROJECTION EXPERIMENT** In our experiment, we propose a fully automatic projection setup: First, we perform coreference resolution on the source language data and then we implement the single-and multi-source approaches to transfer the automatically produced annotations. We use the English-German-Russian unannotated corpus of <cite>(Grishina and Stede, 2017)</cite> as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language). Furthermore, we use manual annotations present in the corpus as the gold standard for our evaluation.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_4",
  "x": "Since the corpus was already sentence-and word-aligned 2 , we use the available alignments to transfer the annotations. Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>: 1. Setting 1 ('add'): disjoint chains from one source language are added to all the chains projected from the other source language; 2. Setting 2 ('unify-intersect'): the intersection of mentions for overlapping chains is selected. 3. Setting 3 ('unify-concatenate'): chains that overlap are treated as one chain starting from a certain percentage of overlap. For both single-and multi-source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages. 2 Sentence alignment was performed using HunAlign (Varga et al., 2007) ; word alignments were computed with GIZA++ (Och and Ney, 2003) on a parallel newswire corpus (Grishina and Stede, 2015) . 3 Computed as Dice coefficient. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_5",
  "x": "Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>: 1. Setting 1 ('add'): disjoint chains from one source language are added to all the chains projected from the other source language; 2. Setting 2 ('unify-intersect'): the intersection of mentions for overlapping chains is selected. 3. Setting 3 ('unify-concatenate'): chains that overlap are treated as one chain starting from a certain percentage of overlap. For both single-and multi-source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages. 2 Sentence alignment was performed using HunAlign (Varga et al., 2007) ; word alignments were computed with GIZA++ (Och and Ney, 2003) on a parallel newswire corpus (Grishina and Stede, 2015) . 3 Computed as Dice coefficient. ---------------------------------- **RESULTS**",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_6",
  "x": "3. Setting 3 ('unify-concatenate'): chains that overlap are treated as one chain starting from a certain percentage of overlap. For both single-and multi-source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages. 2 Sentence alignment was performed using HunAlign (Varga et al., 2007) ; word alignments were computed with GIZA++ (Och and Ney, 2003) on a parallel newswire corpus (Grishina and Stede, 2015) . 3 Computed as Dice coefficient. ---------------------------------- **RESULTS** To evaluate the projection results, we computed the standard coreference metrics -MUC (Vilain et al., 1995) , B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) -and their average for each of the approaches (Table 3) . As one can see from the table, the quality of projections from English to Russian outperforms the quality of projections from German to Russian by 6.5 points F1. Moreover, while Precision number are quite similar, projections from English exhibit higher Recall numbers. As for the multi-source settings, we were able to achieve the highest F1 of 36.2 by combining disjoint chains (Setting 1), which is 1.9 point higher than the best single-source projection scores and constitutes almost 62% of the quality of the projection of gold standard annotations reported in <cite>(Grishina and Stede, 2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_7",
  "x": "However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names. Overall, for all the markables, the projection accuracy for English-Russian is around 10% better than projection accuracy for GermanRussian. en-ru de-ru Nc 64.5 60.7 Np 70.5 66.6 P 83.6 76.5 All 65.1 55.6 Table 5 : Projection accuracy for common nouns, proper nouns and pronouns (%) Moreover, we compare the projected annotations across the two genres. Interestingly, the results for the two languages vary: While the average coreference scores for English-Russian are quite comparable (news: 34.2 F1, stories: 33.3 F1), the scores for German-Russian differ considerably (news: 30.8 F1, stories: 20.8 F1). We attribute this difference to the quality of the source annotations and the performance of the source coreference resolvers on different genres of texts. ---------------------------------- **SUMMARY AND OUTLOOK** In this study, we assessed the applicability of annotation projection in a scenario where we have access to two coreference resolvers in two source languages, the output of which is projected to a third language in a low-resource setting.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_8",
  "x": "Not surprisingly, we see a higher percentage of unaligned words for German-Russian than for English-Russian: 17.03% vs. 14.96% respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs. Furthermore, we computed the distribution of unaligned words: The highest percentage of unaligned tokens disregarding punctuation marks are prepositions; pronouns constitute only 3% and 5% of all unaligned words for the alignments between English-Russian and German-Russian respectively. However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names. Overall, for all the markables, the projection accuracy for English-Russian is around 10% better than projection accuracy for GermanRussian. en-ru de-ru Nc 64.5 60.7 Np 70.5 66.6 P 83.6 76.5 All 65.1 55.6 Table 5 : Projection accuracy for common nouns, proper nouns and pronouns (%) Moreover, we compare the projected annotations across the two genres. Interestingly, the results for the two languages vary: While the average coreference scores for English-Russian are quite comparable (news: 34.2 F1, stories: 33.3 F1), the scores for German-Russian differ considerably (news: 30.8 F1, stories: 20.8 F1). We attribute this difference to the quality of the source annotations and the performance of the source coreference resolvers on different genres of texts. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_0",
  "x": "Raw speech waveforms are densely sampled in time, and thus require downsampling to make many analysis techniques computationally tractable. For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information. Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction. Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1,<cite> 2]</cite> . There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] . While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4] . Features based on the Fourier transform are computationally efficient, but exhibit an intrinsic tradeoff of temporal and frequency resolution. Convolutional filters decouple time and frequency resolution as the number of filters and stride are chosen independently. Despite this, a filter bank is constrained by its window size to a single scale.",
  "y": "background"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_1",
  "x": "Low-frequency features, on the contrary, employ a long window that can be applied at a larger stride. Finally, based on the needs of the speech recognition system we can then independently tune the number of filters for the different frequency bands. While much research has already been conducted on learn- ing directly from waveforms for speech recognition, the unique contributions of this paper are threefold: \u2022 We perform with an in-depth analysis of scaling to low strides and large numbers of filters and discover that a convolutional front end can significantly outperform Fourier features by independently tuning temporal and frequency resolution, at the cost of additional computation and memory. \u2022 We propose a new multiscale convolutional front end, composed of concatenated filters with different window sizes, that requires less computation and outperforms features learned on a single scale (20.7% relative to spectrogram baseline). \u2022 We find that multiscale features naturally learn the frequencies they can most efficiently represent, with large and small windows learning low and high frequencies respectively. This contrasts with the single scale features which try to cover the entire frequency spectrum regardless of window size. ---------------------------------- **EXPERIMENTAL SETUP** The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_2",
  "x": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 . While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech <cite>[2]</cite> . At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_3",
  "x": "We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech <cite>[2]</cite> . At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides. The trend is visualized in Figure 2 . (superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12] . All input data (either spectrogram or waveform) is sampled at 16kHz, and normalized so that each input feature has zero mean and unit variance. For models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1/16ms frame. For models that learn from spectrograms, linear FFT features are extracted with a hop size of 10ms, and window size of 20ms. The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_4",
  "x": "All input data (either spectrogram or waveform) is sampled at 16kHz, and normalized so that each input feature has zero mean and unit variance. For models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1/16ms frame. For models that learn from spectrograms, linear FFT features are extracted with a hop size of 10ms, and window size of 20ms. The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame. We train using stochastic gradient descent with Nesterov momentum and a batch size of 128. Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_5",
  "x": "We train using stochastic gradient descent with Nesterov momentum and a batch size of 128. Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
  "y": "similarities uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_6",
  "x": "We train using stochastic gradient descent with Nesterov momentum and a batch size of 128. Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_7",
  "x": "Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers. This temporal pooling only takes into account local structure and has no explicit knowledge of what information to preserve based on longrange dependencies. Recently proposed architectures that operate simultaneously at different timescales, such as the Clockwork RNN [18] , could provide a more elegant way of combining multiscale signals. Beyond incorporating recurrence, low frequency features that require fewer temporal samples could then also require less recurrent computation and facilitate modeling long-range structure. Finally, from observing representative filters learned at each scale in Figure 4 , we can see that there is some redundancy in the representation. Some filter shapes appear similar at multiple scales.",
  "y": "similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_8",
  "x": "Many modern state of the art systems train on clusters of GPUs, where memory is precious, as requiring memory transfer between GPU and CPU can be prohibitively slow for training. This is especially problematic for training on long utterances, where the amount of memory required to save all the activations increases both with the number of filters and the reduction of stride. It remains to be seen whether the power of learned input features can be combined with the efficiency of analytic signal transformations such as the Fourier transform. One such approach could be to learn basis functions in the real and imaginary domain by performing backpropagation through the Hilbert transformation, enabling the use of larger strides. Alternatively, learned features can be fixed and used to augment other fixed features at train time. Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers.",
  "y": "differences"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_0",
  "x": "Sub Task B aims to categorize the offensive type as targeted text (TIN) or untargeted text (UNT). Sub Task C focuses on identification of target as individual (IND), group (GRP) or others (OTH). Our team SSN NLP participated in all the three subtasks. ---------------------------------- **RELATED WORK** Several research work have been reported since 2010 in this research field of hate speech detection (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; <cite>Davidson et al., 2017</cite>; Malmasi and Zampieri, 2018; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; ElSherief et al., 2018; Gamb\u00e4ck and Sikdar, 2017; Zhang et al., 2018; Mathur et al., 2018) . Schmidt and Wiegand (2017) & Fortuna and Nunes (2018) reviewed the approaches used for hate speech detection. Kwok and Wang (2013) used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\". Burnap and Williams (2015) developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier. Djuric et al. (2015) learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection.",
  "y": "background"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_1",
  "x": [
   "**GRP IND OTH** ---------------------------------- **CONCLUSION** We have implemented both traditional machine learning and deep learning approaches for identifying offensive languages from social media. The approaches are evaluated on OffensEval@SemEval2019 dataset. The given instances are preprocessed and vectorized using word embeddings in deep learning models. We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks. The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two. The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively. Our models outperform the base line for all the three tasks."
  ],
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_0",
  "x": "We would like to thank Sharon Goldwater and Ray Mooney for helpful feedback and suggestions. This work was supported by the Morris Memorial Grant from the New York Community Trust. ---------------------------------- **COPYRIGHT 2009 BY SRIVATSAN RAMANUJAM AND JASON BALDRIDGE** FHMMs to supertagging for the categories defined in CCGbank for English. Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007) . Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging<cite> (Baldridge, 2008)</cite> separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains. Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_1",
  "x": "In this experiment, we use the training and test sets used by <cite>Baldridge (2008)</cite> from CCGbank. We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences. We also vary the transition prior \u03b1 choosing \u03b1 = 1.0 and \u03b1 = 0.05 on the CCG tags. The emission prior \u03b2 was held constant at 1.0. The results of these experiments for \u03b1 = 0.05 are tabulated in Table 3 (a). For comparison, we also show the results of the C&C supertagger of Clark and Curran (2007) in Table 3 (b). The parameter \u03b1, which determines the sparsity of the transition matrix, has been reported to have a greater influence on the performance of the tagger in Goldwater and Griffiths (2007) in weakly supervised POS tagging. We also observed this in supervised supertagging, in the models HMM and FHMMB. The HMM model and FHMMB showed a slight dip in their performance for \u03b1 = 1.0 while FHMMA did slightly better. What stands out in these results is the performance of the FHMM models with minimal amount of training data (for 100 sentences, FHMMB is quite close to the discriminatively trained C&C supertagger).",
  "y": "similarities uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_2",
  "x": "The accuracy of our HMM is lower than the performance of <cite>Baldridge (2008)</cite> for supertags. We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags. We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal. ---------------------------------- **SINGLE ROUND CO-TRAINING EXPERIMENTS** In this section, we use the FHMMB model and the C&C model in a single round of co-training. The idea behind co-training (Blum and Mitchell, 1998) is that when two models learn different views of the task, and are not correlated in the errors they make, they may compliment each other to boost each other's performance. Thus, provided with a large amount of unannotated data, we could use co-training iteratively to enhance the prediction performance of the two models. For example, Clark et al. (2003) co-train the C&C tagger and the TNT tagger (Brants, 2000) and obtain significant performance improvements for POS tagging. Here, we do not perform co-training, strictly speaking.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_3",
  "x": "Finally, note that FHMMB's lone performance of 58.25% with 25 seed sentences is considerably better than C&C's lone performance of 53.86% with the same seed set. ---------------------------------- **WEAKLY SUPERVISED SUPERTAGGING** Since annotation is costly, we are interested in automatic annotation of unlabeled sentences with minimal supervision. In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words. We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization<cite> (Baldridge, 2008)</cite> . We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity: where complexity(c i ) is defined as the number of sub-categories contained in category c i . The POS tag corresponding to an observed word w i is drawn uniformly at random from the set of all tags corresponding to w i in the dictionary. For the FHHMs, we first draw a POS tag t i corresponding to a word w i uniformly at random from the tag dictionary of w i and then from the set of all CCG tags that have occurred with t i and w i in the dictionary, we randomly sample a CCG tag c i based on its complexity, as defined above.",
  "y": "uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_4",
  "x": "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
  "y": "similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_5",
  "x": "---------------------------------- **EFFECT OF FREQUENCY CUT-OFF ON SUPERTAGS** Any category c, that occurs less than k% of the times with a word type w, is removed from the tag dictionary of that word, when the lexicon is constructed. This is in fact a form of supervision, which we use here as an oracle to explore the effect of reducing lexical ambiguity. Results of this experiment for \u03b1 = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a). The results for \u03b1 = 0.05 is shown in Table 6 (a). We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> .",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_6",
  "x": "The results for \u03b1 = 0.05 is shown in Table 6 (a). We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off).",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_7",
  "x": "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_8",
  "x": "Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs.",
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_9",
  "x": "The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs. ---------------------------------- **RELATED WORK** This paper follows the work of Duh (2005) , <cite>Baldridge (2008)</cite> and Goldwater and Griffiths (2007) . Duh (2005) uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (Sang et al., 2000) .",
  "y": "extends"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_10",
  "x": "---------------------------------- **CONCLUSION** We demonstrated that joint inference in supertagging, boosts the prediction accuracy of both POS and CCG tags by a considerable margin. The improvement is more significant when training data is scarce. The results from the single round co-training experiments were encouraging. The generative FHMM model is able to rival a discriminative model like the C&C supertagger, when more labeled sentences are made available by a bootstrapped supertagger. To the best of our knowledge, this is the first work on joint inference in the Bayesian framework for supertagging. There is plenty of scope for further improvements. Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings. Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in <cite>Baldridge (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_0",
  "x": "The model does not yet perform well on datasets describing the consequences of events, such as the destructions after an earthquake. ---------------------------------- **INTRODUCTION** Sentence ordering is a problem in many natural language processing tasks. While it has, historically, mainly been considered a challenging problem in (concept-to-text) language generation tasks, more recently, the issue has also generated interest within summarization research (Barzilay, 2003; Ji and Pulman, 2006) . In the spirit of the latter, this paper investigates the following questions: (1) Does the topic of the text influence the factors that are important to sentence ordering? (2) Which factors are most important for determining coherent sentence orderings? (3) How much performance is gained when using deeper knowledge resources? Past research has investigated a wide range of aspects pertaining to the ordering of sentences in text. The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (<cite>Barzilay and Lapata, 2008</cite>) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) . ---------------------------------- **THE MODEL**",
  "y": "background"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_1",
  "x": "Past research has investigated a wide range of aspects pertaining to the ordering of sentences in text. The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (<cite>Barzilay and Lapata, 2008</cite>) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) . ---------------------------------- **THE MODEL** We view coherence assessment, which we recast as a sentence ordering problem, as a machine learning problem using the feature representation discussed in Section 2.1. It can be viewed as a ranking task because a text can only be more or less coherent than some other text. The sentence ordering task used in this paper can easily be transformed into a ranking problem. Hence, paralleling <cite>Barzilay and Lapata (2008)</cite> , our model has the following structure. The data consists of alternative orderings (x ij , x ik ) of the sentences of the same document d i . In the training data, the preference ranking of the alternative orderings is known.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_2",
  "x": "First, some of the measures make use of WordNet relations (Fellbaum, 1998) , and second, some use the temporal ordering provided by the \"happens-before\" relation in VerbOcean (Chklovski and Pantel, 2004) . While the use of WordNet is self-explanatory, its effect on sentence ordering algorithms does not seem to have been explored in any depth. The use of VerbOcean is meant to reveal the degree to which common sense orderings of events affect the ordering of sentences, or whether the order is reversed. With this background, the sentence ordering features used in this paper can be grouped into three categories: ---------------------------------- **GROUP SIMILARITY** The features in this category are inspired by discourse entity-based accounts of local coherence. Yet, in contrast to <cite>Barzilay and Lapata (2008)</cite> , <cite>who</cite> employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences (similar to Karamanis (2004) 's NOCB metric). We also investigate whether using only the information from the head of the noun group (cf. <cite>Barzilay and Lapata (2008)</cite> ) suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_3",
  "x": "---------------------------------- **LONGER RANGE RELATIONS** The group similarity features only capture the relation between a sentence and its immediate successor. However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., <cite>Barzilay and Lapata (2008)</cite> ). The features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them. ---------------------------------- **EXPERIMENTS** This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings. ---------------------------------- **EVALUATION DATASETS**",
  "y": "motivation"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_4",
  "x": "The group similarity features only capture the relation between a sentence and its immediate successor. However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., <cite>Barzilay and Lapata (2008)</cite> ). The features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them. ---------------------------------- **EXPERIMENTS** This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings. ---------------------------------- **EVALUATION DATASETS** The three datasets used for the automatic evaluation in this paper are based on human-generated texts (Table 1 ). The first two are the earthquake and accident datasets used by <cite>Barzilay and Lapata (2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_5",
  "x": "In contrast to the other two sources, however, this dataset is based on the human summaries from DUC 2005 (Dang, 2005) . It comprises 300 human summaries on 50 document sets, resulting in a total of 6,000 pairwise rankings split into training and test sets. The source furthermore differs from <cite>Barzilay and Lapata (2008)</cite> 's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it). Since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic-independent properties of our model. ---------------------------------- **EXPERIMENT 1** In the first part of this experiment, we consider the problem of the granularity of the syntactic units to be used. That is, does it make a difference whether we use the words in the sentence, the words in the noun groups, the words in the verb groups, or the words in the respective heads of the groups to determine coherence? (The units are obtained by processing the documents using the LT-TTT2 tools (Grover and Tobin, 2006) ; the lemmatizer used by LT-TTT2 is morpha (Minnen and Pearce, 2000) .) We also consider whether lemmatization is beneficial in each of the granularities. The results -presented in Table 2 -indicate that considering only the heads of the verb and noun groups separately provides the best performance. In particular, the heads outperform the whole groups, and the heads separately also outperform noun and verb group heads together.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_6",
  "x": "Table 5 clearly confirms that longer range relations are relevant to the assessment of the coherence of text. An interesting difference between the two approaches is that sentence similarity only provides good results for neighboring sentences or sentences only one sentence apart, while the occurrence-counting method also works well over longer ranges. Having evaluated the potential contributions of the individual features and their modeling, we now use SVMs to combine the features into one comprehensive measure. Given the indications from the foregoing experiments, the results in Table 6 are disappointing. In particular, the performance on the earthquake dataset is below standard. However, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well. (<cite>Barzilay and Lapata (2008)</cite> only perform well when using <cite>their</cite> coreference module, which determines antecedents based on the identified coreferences in the original sentence ordering, thereby biasing <cite>their</cite> orderings towards the correct ordering.) Longer range and WordNet relations together (Chunk+Temp-WN+LongRange+) achieve the best performance. The corresponding configuration is also the only one that achieves reasonable performance when compared with other systems. ---------------------------------- **EXPERIMENT 2**",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_7",
  "x": "The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from <cite>Barzilay and Lapata (2008)</cite>. unknown at the time of training. As a result, model performance on out-of-domain texts is important for summarization. Experiment 2 seeks to evaluate how well our model performs in such cases. To this end, we carry out two sets of tests. First, we crosstrain the models between the accident and earthquake datasets to determine system performance in unseen domains. Second, we use the dataset based on the DUC 2005 model summaries to investigate whether our model's performance on unseen topics reaches a plateau after training on a particular number of different topics. Surprisingly, the results are rather good, when compared to the poor results in part of the previous experiment (Table 7) . In fact, model performance is nearly independent of the training topic. Nevertheless, the results on the earthquake test set indicate that our model is missing essential components for the correct prediction of sentence orderings on this set.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_8",
  "x": "Surprisingly, the results are rather good, when compared to the poor results in part of the previous experiment (Table 7) . In fact, model performance is nearly independent of the training topic. Nevertheless, the results on the earthquake test set indicate that our model is missing essential components for the correct prediction of sentence orderings on this set. When compared to the results obtained by <cite>Barzilay and Lapata (2008)</cite> and Barzilay and Lee (2004) , it would appear that direct sentenceto-sentence similarity (as suggested by the <cite>Barzilay and Lapata</cite> baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. The final experimental setup applies the best Table 8 : Accuracy on 20 test topics (2,700 pairs) with respect to the number of topics used for training using the model Chunk+Temporal-WordNet+LongRange+ model (Chunk+Temporal-WordNet+LongRange+) to the summarization dataset and evaluates how well the model generalises as the number of topics in the training dataset increases. The results -provided in Table 8 -indicate that very little training data (both regarding the number of pairs and the number of different topics) is needed. Unfortunately, they also suggest that the DUC summaries are more similar to the earthquake than to the accident dataset. ---------------------------------- **CONCLUSIONS** This paper investigated the effect of different features on sentence ordering.",
  "y": "similarities"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_0",
  "x": "Multi-hop QA requires finding multiple supporting evidence, and reasoning over them in order to answer a question (Welbl et al., 2018; Talmor and Berant, 2018; <cite>Yang et al., 2018)</cite> . For example, to answer the question shown in figure 1 , the QA system has to retrieve two different paragraphs and reason over them. Moreover, the paragraph containing the answer to the question has very little lexical overlap with the question, making it difficult for search engines to retrieve them from a large corpus. For instance, the accuracy of a BM25 retriever for finding all supporting evidence for a question decreases from 53.7% to 25.9% on the 'easy' and 'hard' subsets of the HOTPOTQA training dataset. 1 We hypothesize that an effective retriever for multi-hop QA should have the \"hopiness\" built into it, by design. That is, after retrieving an initial set of documents, the retriever should be able to \"hop\" onto other documents, if required. We note that, many supporting evidence often share common (bridge) entities between them (e.g. \"Rochester Hills\" in figure 1). In this work, we introduce a model that uses information about entities present in an initially retrieved paragraph to jointly find a passage of text describing the entity (entity-linking) and also determining whether that passage would be relevant to answer the multi-hop query. A major component of our retriever is a re-ranker model that uses contextualized entity representation obtained from a pre-trained BERT (Devlin et al., 2018) language model. Specifically, the entity representation is obtained by feeding the query and a Wikipedia paragraph describing the entity to a BERT model.",
  "y": "background"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_1",
  "x": "Secondly, they also do not have a entity linking component to identify the relevant paragraphs. Our model out-performs them for multi-hop QA. To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in<cite> Yang et al. (2018)</cite> , it improved the QA performance on the hidden test set by 10.59 F1 points. 2 ---------------------------------- **METHODOLOGY** Our approach is summarized in Figure 2 . The first component of our model is a standard IR system that takes in a natural language query 'Q' and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model.",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_2",
  "x": "For all our experiment, unless specified otherwise, we use the open domain corpus 4 released by<cite> Yang et al. (2018)</cite> which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018) . 5 For the BERT encoder, we use the BERT-BASE-UNCASED model. 6 We use the implementation of widely-used BM25 retrieval available in Lucene. Table 1 : Retrieval performance of models on the HOT-POTQA benchmark. A successful retrieval is when all the relevant passages for a question are retrieved from more than 5 million paragraphs in the corpus. systems is via pseudo-relevance feedback (PRF). The PRF methods assume that the top retrieved documents in response to a given query are relevant. Based on this assumption, they expand the query in a weighted manner. PRF has been shown to be effective in various retrieval settings (Xu and Croft, 1996) .",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_4",
  "x": "The intuition is that if there are multiple evidence containing the answer spans then it might be a little easier for a downstream QA model to identify the answer span. Figure 3 shows that our model performs equally well on both type of queries and hence can be applied in a practical setting. Baseline Reader<cite> (Yang et al., 2018)</cite> Table 2 shows the performance on the QA task. We were able to achieve better scores than reported in the baseline reader model of<cite> Yang et al. (2018)</cite> by using Adam (Kingma and Ba, 2014) instead of standard SGD (our re-implementation). Next, we use the top-10 paragraphs retrieved by our system from the entire corpus and feed it to the reader model. We achieve a 10.59 absolute increase in F1 score than the baseline. It should be noted that we use the simple baseline reader model and we are confident that we can achieve better scores by using more sophisticated reader architectures, e.g. using BERT based architectures. Our results show that retrieval is an important component of an opendomain system and equal importance should be given to both the retriever and reader component. ---------------------------------- **ZERO-SHOT EXPERIMENT ON WIKIHOP**",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_0",
  "x": "Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **INTRODUCTION** State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_1",
  "x": "---------------------------------- **INTRODUCTION** State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.",
  "y": "background motivation"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_2",
  "x": "This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **DISCO TRANSFORMER**",
  "y": "differences background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_3",
  "x": "Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **DISCO TRANSFORMER** In this section, we introduce our DisCo transformer for nonautoregressive translation (Fig. 1 ). We propose a DisCo objective as an efficient alternative to masked language modeling and design an architecture that can compute the objective in a single pass.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_4",
  "x": "We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **DISCO TRANSFORMER** In this section, we introduce our DisCo transformer for nonautoregressive translation (Fig. 1 ). We propose a DisCo objective as an efficient alternative to masked language modeling and design an architecture that can compute the objective in a single pass. ---------------------------------- **DISCO OBJECTIVE**",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_5",
  "x": "State-of-the-art neural machine translation systems use autoregressive decoding where words are predicted one-byone conditioned on all previous words (Bahdanau et al., 2015; Vaswani et al., 2017) . Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "background differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_6",
  "x": "Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average. ---------------------------------- **DISCO TRANSFORMER** In this section, we introduce our DisCo transformer for nonautoregressive translation (Fig. 1 ). We propose a DisCo objective as an efficient alternative to masked language modeling and design an architecture that can compute the objective in a single pass. ---------------------------------- **DISCO OBJECTIVE** Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, <cite>Ghazvininejad et al. (2019)</cite> ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs . Namely, for every sentence pair in bitext X and Y ,",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_7",
  "x": "where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right. To address this limitation, we propose a Disentangled Context (DisCo) objective. The objective involves prediction of every token given an arbitrary (thus disentangled) subset of the other tokens. For every 1 \u2264 n \u2264 N where |Y | = N , we predict: ---------------------------------- **DISCO TRANSFORMER ARCHITECTURE** Simply computing conditional probabilities P (Y n |X, Y n obs ) with a vanilla transformer decoder will necessitate N separate transformer passes for each Y n obs . We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs<cite> (Ghazvininejad et al., 2019)</cite> sample the number of masked tokens uniformly from [1, N ].",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_8",
  "x": "2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right. To address this limitation, we propose a Disentangled Context (DisCo) objective. The objective involves prediction of every token given an arbitrary (thus disentangled) subset of the other tokens. For every 1 \u2264 n \u2264 N where |Y | = N , we predict: ---------------------------------- **DISCO TRANSFORMER ARCHITECTURE** Simply computing conditional probabilities P (Y n |X, Y n obs ) with a vanilla transformer decoder will necessitate N separate transformer passes for each Y n obs . We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs<cite> (Ghazvininejad et al., 2019)</cite> sample the number of masked tokens uniformly from [1, N ]. DisCo transformer to compute these N contexts in one shot:",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_9",
  "x": "For each Y n in Y where |Y | = N , we uniformly sample the number of visible tokens from [0, N \u2212 1], and then we randomly choose that number of tokens from Y \\ Y n as Y n obs , similarly to CMLMs<cite> (Ghazvininejad et al., 2019)</cite> . We optimize the negative log likelihood loss from P (Y n |X, Y n obs ) (1 \u2264 n \u2264 N ). Again following CMLMs, we append a special token to the encoder and project the vector to predict the target length for parallel decoding. We add the negative log likelihood loss from this length prediction to the loss from word predictions. ---------------------------------- **DISCO OBJECTIVE AS GENERALIZATION** We designed the DisCo transformer to compute conditional probabilities at every position efficiently, but here we note that the DisCo transformer can be readily used with other training schemes in the literature. We can train an autoregressive DisCo transformer by always setting Y n obs = Y <n . XLNet (Yang et al., 2019b) is also a related variant of a transformer that was introduced to produce general-purpose contextual word representations. The DisCo transformer differs from XLNet in two critical ways.",
  "y": "similarities"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_10",
  "x": "---------------------------------- **INFERENCE ALGORITHMS** In this section, we discuss inference algorithms for our DisCo transformer. We first review mask-predict from prior work as a baseline and introduce a new parallelizable inference algorithm, parallel easy-first (Alg. 1). ---------------------------------- **MASK-PREDICT** Mask-predict is an iterative inference algorithm introduced in <cite>Ghazvininejad et al. (2019)</cite> to decode a conditional masked language model (CMLM). The target length N is first predicted, and then the algorithm iterates over two steps: mask where i t tokens with lowest probability are masked and predict where those masked tokens are updated given the other N \u2212 i t tokens. The number of masked tokens i t decays from N with a constant rate over a fixed number of iterations T . Specifically, at iteration t,",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_11",
  "x": "In a later section, we will explore several variants of choosing Y n,t obs and show that this easyfirst strategy performs best despite its simplicity. ---------------------------------- **LENGTH BEAM** Following <cite>Ghazvininejad et al. (2019)</cite> , we apply length beam. In particular, we predict top K lengths from the distribution in length prediction and run parallel easy-first simultaneously. In order to speed up decoding, we terminate if the one with the highest average log score N n=1 log(p t n )/N converges. It should be noted that for parallel easy-first, obs for all positions n while mask-predict may keep updating tokens even after because Y t obs changes over iterations. See Alg. 1 for full pseudo-code.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_12",
  "x": "In order to speed up decoding, we terminate if the one with the highest average log score N n=1 log(p t n )/N converges. It should be noted that for parallel easy-first, obs for all positions n while mask-predict may keep updating tokens even after because Y t obs changes over iterations. See Alg. 1 for full pseudo-code. Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5<cite> (Ghazvininejad et al., 2019)</cite> unless otherwise noted. In Sec. 5.2, we Algorithm 1 Parallel Easy-First with Length Beam Source sentence: X Predicted lengths: N1, \u00b7 \u00b7 \u00b7 , NK Max number of iterations: T for k \u2208 {1, 2, ..., K} do for n \u2208 {1, 2, ..., N k } do Y 1,k n , p k n = (arg)max w P (yn = w|X) end for Get the easy-first order z k by sorting p k and let z k (i) be the rank of the ith position. end for will illustrate that length beam facilitates decoding both the CMLM and DisCo transformer. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_13",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Benchmark datasets We evaluate on 7 directions from four standard datasets with various training data sizes: WMT'14 EN-DE (4.5M pairs), WMT'16 EN-RO (610K pairs), WMT'17 EN-ZH (20M pairs), and WMT'14 EN-FR (36M pairs, en\u2192fr only). These datasets are all encoded into subword units by BPE (Sennrich et al., 2016) . 4 We use the same preprocessed data and train/dev/test splits as prior work for fair comparisons (EN-DE: Vaswani et al. 2017); 4 We run joint BPE on all language pairs except EN-ZH. Ott et al. (2018) ). We evaluate performance with BLEU scores (Papineni et al., 2002) for all directions except that we use SacreBLEU (Post, 2018) 5 in en\u2192zh again for fair comparison with prior work<cite> (Ghazvininejad et al., 2019)</cite> . For all autoregressive models, we use beam search with b = 5 (Vaswani et al., 2017; Ott et al., 2018) and tune length penalty of \u03b1 \u2208 [0.0, 0.2, \u00b7 \u00b7 \u00b7 , 2.0] in validation. For parallel easy-first, we set the max number of iterations T = 10 and use T = 4, 10 for constant-time mask-predict. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_14",
  "x": "For models that have an autoregressive component (e.g. Kaiser et al. (2018) ; Ran et al. (2019)), we can speed up sequential computation by caching states. Further, many of prior NAT approaches generate varying numbers of translation candidates and rescore them using an autoregressive model. The rescoring process typically costs overhead of one parallel pass of a transformer encoder followed by a decoder. Given this complexity in latency comparison, we highlight two state-of-the-art iteration-based NAT models whose latency is comparable to our DisCo transformer due to the similar model structure. See Sec. 6 for descriptions of more work on NAT. CMLM As discussed earlier, we can generate a translation with mask-predict from a CMLM<cite> (Ghazvininejad et al., 2019)</cite> . We can directly compare our DisCo transformer with this method by the number of iterations required. 6 We provide results obtained by running their code. 7 Levenshtein Transformer Levenshtein transformer (LevT) is a transformer-based iterative model for parallel sequence generation (Gu et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_15",
  "x": "Unfortunately, we lack consensus in evaluation (Post, 2018) . Hyperparameters We generally follow the hyperparameters for a transformer base (Vaswani et al., 2017;<cite> Ghazvininejad et al., 2019)</cite> : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions. We sample weights from N (0, 0.02), initialize biases to zero, and set layer normalization parameters to \u03b2 = 0, \u03b3 = 1 (Devlin et al., 2019) . For regularization, we tune the dropout rate from [0.1, 0.2, 0.3] based on dev performance in each direction, and use 0.01 L 2 weight decay and label smoothing with \u03b5 = 0.1. We train batches of 128K tokens using Adam (Kingma & Ba, 2015) with \u03b2 = (0.9, 0.999) and \u03b5 = 10 \u22126 . The learning rate warms up to 5 \u00b7 10 \u22124 in the first 10K steps, and then decays with the inverse square-root schedule. We train all models for 300K steps apart from en\u2192fr where we make 500K steps to account for the data size. We measure the dev BLEU score at the end of each epoch to avoid stochasticity, and average the 5 best checkpoints to obtain the final model. We use 16 Telsa V100 GPUs and accelerate training by utilizing mixed precision floating point (Micikevicius et al., 2018) , and implement all models with fairseq (Gehring et al., 2017) . We will release our code for easy replication.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_16",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** Seen in Table 1 are the results in the four directions from the WMT'14 EN-DE and WMT'16 EN-RO datasets. First, our re-implementations of CMLM + Mask-Predict outperform <cite>Ghazvininejad et al. (2019)</cite> (e.g. 31.24 vs. 30.53 in de\u2192en with 10 steps). This is probably due to our tuning on the dropout rate and weight averaging of the 5 best epochs based on the validation BLEU performance (Sec. 4.1). Our DisCo transformer with the parallel easy-first inference achieves at least comparable performance to the CMLM with 10 steps despite the significantly fewer steps on average (e.g. 4.82 steps in en\u2192de). The one exception is ro\u2192en (33.25 vs. 33.67), but DisCo + Easy-First requires only 3.10 steps, and CMLM + Mask-Predict with 4 steps achieves similar performance of 33.27. The limited advantage of our DisCo transformer on the EN-RO dataset suggests that we benefit less from the training efficiency of the DisCo transformer on the small dataset (610K sentence pairs). DisCo + Mask-Predict generally underperforms DisCo + Easy-First, implying that the mask-predict inference, which fixes Y n obs across all positions n, fails to utilize the flexibility of the DisCo transformer. DisCo + Easy-First also accomplishes significant reduction in the average number of steps as compared to the adaptive decoding in LevT (Gu et al., 2019) while performing competitively.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_17",
  "x": "This can be because parallel easy-first relies on the easyfirst order as well as the length, and length beam provides opportunity to try multiple orderings. ---------------------------------- **EXAMPLE TRANSLATION SEEN IN** ---------------------------------- **RELATED AND FUTURE WORK** Recent work on non-autoregressive translation developed ways to mitigate the trade-off between decoding parallelism and performance. As in this work, several prior work proposed methods to iteratively refine output predictions (Lee et al., 2018;<cite> Ghazvininejad et al., 2019</cite>; Gu et al., 2019; Mansimov et al., 2019) . Other approaches include adding a lite autoregressive module to parallel decoding (Kaiser et al., 2018; Sun et al., 2019; Ran et al., 2019) , partially decoding autoregressively (Stern et al., 2018; , rescoring output candidates autoregressively (e.g. Gu et al. (2018) ), mimicking hidden states of an autoregressive teacher , training with different objectives than vanilla negative log likelihood (Libovick\u00fd & Helcl, 2018; Wang et al., 2019; Shao et al., 2020) , reordering input sentences (Ran et al., 2019) , and modeling with latent variables (Ma et al., 2019; Shu et al., 2020) . While this work took iterative decoding methods, our DisCo transformer can be combined with other approaches for efficient training. For example, Li et al. (2019) trained two separate non-autoregressive and autoregressive models, but it is possible to train a single DisCo transformer with both autoregressive and random masking and use hidden states from autoregressive masking as a teacher.",
  "y": "similarities"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_0",
  "x": "They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) . For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012 Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence. The domain prediction was based only on the source as the extra token was added at target-side and there was no need for apriori domain information. This approach allowed the model to improve the quality for each domain.",
  "y": "uses"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_1",
  "x": "Therefore, 2 translators were contracted as part of the project to create 30,000 segments of in-domain data, translating public administrations websites. They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) . For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012) . Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence.",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_0",
  "x": "A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_2",
  "x": "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> .",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_3",
  "x": "The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_4",
  "x": "McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) .",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_5",
  "x": "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> .",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_6",
  "x": "from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus.",
  "y": "differences uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_7",
  "x": "Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 . The f-score of 83.7% is lower than the f-score of 85.2% reported by<cite> McClosky et al. (2006b)</cite> for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees. We test the combinations on the BNC gold standard development set and on WSJ Section 00. Table 1 shows that parser accuracy increases with the size of the in-domain selftraining material. 3 The figures confirm the claim of McClosky et al. (2006a) that self-training with a reranking parsing model is effective for improving parser accuracy in general, and the claim of Gildea (2001) that training on in-domain data is effective for parser adaption. They confirm that self-training on in-domain data is effective for parser adaptation. The WSJ Section 00 results suggest that, in order to maintain performance on the seed training domain, it is necessary to combine BNC parse trees Of the self-training combinations with abovebaseline improvements for both development sets, the combination of 1,000K BNC parse trees and Section 2-21 of the WSJ (multiplied by ten) yields the highest improvement for the BNC data, and we present final results with this combination for the BNC gold standard test set and WSJ Section 23. There is an absolute improvement on the original reranking parser of 1.7% on the BNC gold standard test set and 0.4% on WSJ Section 23.",
  "y": "differences"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_0",
  "x": "Recent years have seen an increasing trend in support services provided over the Internet. Many companies have web sites with Frequently Asked Questions (FAQs), and also offer e-mail support. More recently, real-time support via online chat sessions is being offered where customers and support representatives type short messages to each other. Chat sessions are conducted over a network, such as the Internet, where textual messages can be sent and received between interlocutors in real-time. These chat sessions are commonly referred to as instant messaging. Support services that are conducted via instant messaging vary from being person-person dialogue, similar to traditional call centres, through to being entirely automated where customers engage in dialogue with a computer program. Commercial software is available to partially automated online support by suggesting responses to a human agent, which may then be accepted or overwritten. The research presented in this paper aims to provide a degree of natural language understanding to assist in automating task-oriented dialogue, such as support services, by suggesting utterances during the dialogue. We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> .",
  "y": "background uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_1",
  "x": "Support services that are conducted via instant messaging vary from being person-person dialogue, similar to traditional call centres, through to being entirely automated where customers engage in dialogue with a computer program. Commercial software is available to partially automated online support by suggesting responses to a human agent, which may then be accepted or overwritten. The research presented in this paper aims to provide a degree of natural language understanding to assist in automating task-oriented dialogue, such as support services, by suggesting utterances during the dialogue. We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> . The service is designed to assist potential customers with finding various items for sale on the MSN Shopping web site. A sample from one of the dialogues in this corpus is shown in Table 1 . The research presented here advances previous work which examined various models and tech-niques to predict dialogue acts in task-oriented instant messaging. In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts. Probabilistic models were then trained to predict dialogue acts given a sequence of utterances.",
  "y": "background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_2",
  "x": "The present paper concludes this work by applying the models to a dialogue simulation program which suggests utterance responses during a dialogue. The performance of the suggested utterances is then evaluated. ---------------------------------- **BACKGROUND** Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) . The MSN Shopping corpus we use comprises approximately 550 utterances and 6,500 words. <cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data. Kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement (Ivanovic, 2005a ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_3",
  "x": "Probabilistic models were then trained to predict dialogue acts given a sequence of utterances. Ivanovic (2005a) examined probabilistic and linguistic methods to automatically segment messages from the same corpus into utterances. The present paper concludes this work by applying the models to a dialogue simulation program which suggests utterance responses during a dialogue. The performance of the suggested utterances is then evaluated. ---------------------------------- **BACKGROUND** Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) . The MSN Shopping corpus we use comprises approximately 550 utterances and 6,500 words. <cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data.",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_0",
  "x": "About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] . The latest involvement using Wikipedia is the portable cross lingual NER for low resource languages using translation of an annotated NER corpus from English [12, 19] . Another approach to cross lingual and language independent corpora is to learn a model on language independent features of a source language and test the model on other languages using same features [13] . Nothman, et al. (2008) [5] constructed a massive English NER corpus by the classification of Wikipedia articles to its category types by mapping them to CoNLL-2003 NER tagset. A similar approach to massive multilingual NER corpus is found in [14] . A hybrid approach to generate parallel sentences with NE notations reveal strong results on Wikipedia dataset [19] . Kazama and Torisawa (2007) [15] use the external knowledge along with the analysis of first sentence in Wikipedia articles to infer the entity category. Cross script NER corpus from social media in Bengali written in Roman script is also evaluated on various models [10] .",
  "y": "background uses"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_1",
  "x": "About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] . The latest involvement using Wikipedia is the portable cross lingual NER for low resource languages using translation of an annotated NER corpus from English [12, 19] . Another approach to cross lingual and language independent corpora is to learn a model on language independent features of a source language and test the model on other languages using same features [13] . Nothman, et al. (2008) [5] constructed a massive English NER corpus by the classification of Wikipedia articles to its category types by mapping them to CoNLL-2003 NER tagset. A similar approach to massive multilingual NER corpus is found in [14] . A hybrid approach to generate parallel sentences with NE notations reveal strong results on Wikipedia dataset [19] . Kazama and Torisawa (2007) [15] use the external knowledge along with the analysis of first sentence in Wikipedia articles to infer the entity category. Cross script NER corpus from social media in Bengali written in Roman script is also evaluated on various models [10] .",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_2",
  "x": "We strongly take into account the assumption that the entities present on these pages are prominently Hindi NEs. This assumption is based on human assessment that the information on such pages is based on Indian background especially from Hindi linguistic majority sections of India. ---------------------------------- **CORPUS ACQUISITION** Wikipedia being a huge source of information, its articles comprise of: topic and its comprehensive summary in paragraphs and images; reference to reliable resources; and hyperlinks, also called wikilinks to other articles. Our method takes the advantage of wikilinks between the articles from which linktext is extracted. Since wikilinks are links to articles, it may be considered as a named entity. This approach is similar to Nothman et al (2008) <cite>[4]</cite> to generate the NER data from wikilinks. A total number of 7285 tokens and multi-tokens expressions were extracted from the links by parsing the 13 identified Wikipedia webpages. We consider these expressions as probable NEs after the removal of duplicates from the set of tokens obtained.",
  "y": "similarities"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_3",
  "x": "The annotated training corpora common towards NER task is Newswire text, CoNLL-2003 shared task data, the MUC7 dataset, BBN, etc. We evaluated the performance of word level Hi-En-WP corpus on general classification algorithms using the collection of all character n-grams for n=1-5 as feature set. Amongst Logistic Regression, SVM, Na\u00efve Bayes, Random Forest and Stochastic Gradient Classifier, Logistic Regression seemed most effective, in all the cases, as shown in Table 3 . The analysis of class wise precision, recall and F-score on Logistic Regression classifier is shown in Table 4 . When trained with MISC as a separate class, high values for PER suggests that they comparatively easy to identify. It may be due to sufficiently large amount of training data as compared to rest of the tags. Whereas, low precision value for LOC tag suggests that other entities are wrongly classified as location. The MISC F-score is expectedly low, in agreement with the results of Nothman et al (2008) <cite>[4]</cite> . The variation reflected in F-score among all may be the effect of diversity in linguistic attributes. An increase in accuracy from 89% to 92% is observed when the model is trained without MISC tag which reflects that the confusion is created in data by the inclusion of training examples that belong to MISC tag.",
  "y": "similarities"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_0",
  "x": "**ABSTRACT** We describe an efficient neural network method to automatically learn sentiment lexicons without relying on any manual resources. The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveraging emoticons in large tweets, using the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. ---------------------------------- **INTRODUCTION** Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a) . They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; or supervised<cite> (Mohammad et al., 2013</cite>; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_1",
  "x": "Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011) . One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_2",
  "x": "As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011) . One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_3",
  "x": "On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) .",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_4",
  "x": "On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) .",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_5",
  "x": "The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages. Our code and lexicons are publicly available at https://github.com/duytinvo/acl2016. ---------------------------------- **RELATED WORK** Existing methods for automatically learning sentiment lexicons can be classified into three main categories. The first category augments existing lexicons with sentiment information. For example, Esuli and Sebastiani (2006) and Baccianella et al. (2010) use a tuple (pos, neg, neu) to represent each word, where pos, neg and neu stand for possibility, negativity and neutrality, respectively, training these attributes by extracting features from WordNet. These methods rely on the taxonomic structure of existing lexicons, which are limited to specific languages. The second approach expands existing lexicons, which are typically manually labeled.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_6",
  "x": "Turney (2002) proposes to estimate the sentiment polarity of words by calculating PMI between seed words and search hits. Mohammad et al. (2013) improve the method by computing sentiment scores using distance-supervised data from emoticon-baring tweets instead of seed words. This approach can be used to automatically extract multilingual sentiment lexicons Mohammad et al., 2015) without using manual resources, which makes it more flexible compared to the first two methods. We consider it as our baseline. We use the same data source as<cite> Mohammad et al. (2013)</cite> to train lexicons. However, rather than relying on PMI, we take a machine-learning method in optimizing the prediction accuracy of emoticons using the lexicons. To leverage large data, we use a very simple neural network to train the lexicons. ---------------------------------- **BASELINE** Mohammad et al. (2013) employ emoticons and relevant hashtags contained in a tweet as the sentiment label of the tweet.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_7",
  "x": "In order to evaluate both unsupervised and supervised methods, we follow Tang et al. (2014b) and , removing neutral tweets. The statistics is shown in Table 2 . We compare our lexicon with the lexicons of NRC 4<cite> (Mohammad et al., 2013)</cite> , HIT 5 (Tang et al., 2014a) and WEKA 6 (Bravo-Marquez et al., 2015) . As shown in Table  3 , using the unsupervised sentiment classification method (unsup) in Section 5, our lexicon gives significantly better result in comparison with countbased lexicons of NRC. Under both settings, our lexicon yields the best results compared to other methods. ---------------------------------- **ARABIC LEXICONS** We employ the standard Arabic Twitter dataset ASTD (Nabil et al., 2015) , which consists of about 10,000 tweets with 4 labels: objective (obj), negative (neg), positive (pos) and mixed subjective (mix). The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_8",
  "x": "---------------------------------- **ARABIC LEXICONS** We employ the standard Arabic Twitter dataset ASTD (Nabil et al., 2015) , which consists of about 10,000 tweets with 4 labels: objective (obj), negative (neg), positive (pos) and mixed subjective (mix). The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_9",
  "x": "---------------------------------- **ARABIC LEXICONS** We employ the standard Arabic Twitter dataset ASTD (Nabil et al., 2015) , which consists of about 10,000 tweets with 4 labels: objective (obj), negative (neg), positive (pos) and mixed subjective (mix). The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_10",
  "x": "We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0). ---------------------------------- **ANALYSIS**",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_11",
  "x": "The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0).",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_0",
  "x": "---------------------------------- **INTRODUCTION** To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, <cite>18,</cite> 9] have considered a structural re-ranking strategy. The idea is to re-rank the top N documents that some initial search engine produces, where the re-ordering utilizes information about inter-document relationships within that set. Promising results have been previously obtained by using document centrality within the initially retrieved list to perform structural re-ranking, on the premise that if the quality of this list is reasonable to begin with, then the documents that are most related to most of the docuPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_1",
  "x": "To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] . The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents?",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_2",
  "x": "To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] . The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents?",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_3",
  "x": "The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents? Not necessarily, because there may exist graph-construction methods that are more suitable for HITS. Note that the only entities considered in our previous study were documents. If we could introduce entities distinct from documents but enjoying a mutually reinforcing relationship with them, then we might better satisfy the spirit of the hubs-versus-authorities distinction, and thus derive stronger results utilizing HITS. A crucial insight of the present paper is that document clusters appear extremely well-suited to play this complementary role. The intuition is that: (a) given those clusters that are \"most representative\" of the user's information need, the documents within those clusters are likely to be relevant; and (b) the \"most representative\" clusters should be those that contain many relevant documents.",
  "y": "motivation background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_4",
  "x": "Also, clusters have long been considered a promising source of information. The well-known cluster hypothesis [35] encapsulates the intuition that clusters can reveal groups of relevant documents; in practice, the potential utility of clustering for this purpose has been demonstrated for both the case wherein clusters were created in a query-independent fashion [14, 4] and the re-ranking setting [13, 22, 34] . In this paper, we show through an array of experiments that consideration of the mutual reinforcement of clusters and documents in determining centrality can lead to highly effective algorithms for re-ranking an initially retrieved list. Specifically, our experimental results show that the centralityinduction methods that we previously studied solely in the context of document-only graphs<cite> [18]</cite> result in much better re-ranking performance if implemented over bipartite graphs of documents (on one side) and clusters (on the other side). For example, ranking documents by their \"authoritativeness\" as computed by HITS upon these cluster-document graphs yields better performance than that of a previously proposed PageRank implementation applied to documentonly graphs. Interestingly, we also find that cluster authority scores can be used to identify clusters containing a large percentage of relevant documents. ---------------------------------- **ALGORITHMS FOR RE-RANKING** Since we are focused on the structural re-ranking paradigm, our algorithms are applied not to the entire corpus, but to a subset D N,q init (henceforth Dinit), defined as the top N documents retrieved in response to the query q by a given initial retrieval engine. Some of our algorithms also take into account a set C l(Dinit) of clusters of the documents in Dinit.",
  "y": "extends"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_5",
  "x": "---------------------------------- **ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed. Thus, writing \"PR\" for both auth and hub, we conceptually have the (single) equation However, in practice, we incorporate Brin and Page's smoothing scheme [3] together with a correction for nodes with no positive-weight edges emanating from them [27, 21] : where out(u) , and \u03bb \u2208 (0, 1) is the damping factor. ----------------------------------",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_6",
  "x": "Since central clusters are, supposedly, those that accrue the most evidence for relevance, documents that are strongly identified with such clusters should themselves be judged highly relevant. 3 4 But identifying such clusters is facilitated by knowledge of which documents are most likely to be relevant -exactly the mutual reinforcement property that HITS was designed to leverage. ---------------------------------- **ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed. Thus, writing \"PR\" for both auth and hub, we conceptually have the (single) equation However, in practice, we incorporate Brin and Page's smoothing scheme [3] together with a correction for nodes with no positive-weight edges emanating from them [27, 21] : where out(u)",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_7",
  "x": "Moreover, a (non-trivial) closed-form -and quite easily computed -solution exists for one-way bipartite graphs: is an affine transformation (with respect to positive constants) of, and therefore equivalent for ranking purposes to, the unique positive sum-normalized solution to Equation 4. (Proof omitted due to space constraints.) Interestingly, this result shows that while one might have thought that clusters and documents would \"compete\" for PageRank score when placed within the same graph, in our document-as-authority and document-as-hub graphs this is not the case. Earlier work<cite> [18]</cite> also considered scoring a node v by its influx, P u\u2208V w t(u \u2192 v). This can be viewed as either a non-recursive version of Equation 3, or as an un-normalized analog of Equation 5. ---------------------------------- **ALGORITHMS BASED ON CENTRALITY SCORES** Clearly, we can rank documents by their scores as computed by any of the functions introduced above. But when we operate on document-as-authority or document-as-hub graphs, centrality scores for the clusters are also produced. These can be used to derive alternative means for ranking documents. We follow Liu and Croft's approach [25] : first, rank the documents within (or most strongly associated to) each cluster according to the initial retrieval engine's scores; then, derive the final list by concatenating the within-cluster lists in order of decreasing cluster score, discarding repeats.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_8",
  "x": "However, automatically detecting clusters that contain many relevant documents remains a very hard task [36] . Section 5.2 presents results for detecting such clusters using centrality-based cluster ranking. Recently, there has been a growing body of work on graphbased modeling for different language-processing tasks wherein links are induced by inter-entity textual similarities. Examples include document (re-)ranking [7, 24, 9, <cite>18,</cite> 39 ], text summarization [11, 26] , sentence retrieval [28] , and document representation [10] . In contrast to our methods, links connect entities of the same type, and clusters of entities are not modeled within the graphs. While ideas similar to ours by virtue of leveraging the mutual reinforcement of entities of different types, or using bipartite graphs of such entities for clustering (rather than using clusters), are abundant (e.g., [15, 8, 2] ), we focus here on exploiting mutual reinforcement in ad hoc retrieval. Random walks (with early stopping) over bipartite graphs of terms and documents were used for query expansion [20] , but in contrast to our work, no stationary solution was sought. A similar \"short chain\" approach utilizing bipartite graphs of clusters and documents for ranking an entire corpus was recently proposed [19] , thereby constituting the work most resembling ours. However, again, a stationary distribution was not sought. Also, query drift prevention mechanisms were required to obtain good performance; in our re-ranking setting, we need not employ such mechanisms.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_9",
  "x": "Also, query drift prevention mechanisms were required to obtain good performance; in our re-ranking setting, we need not employ such mechanisms. ---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] .",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_10",
  "x": "---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] .",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_11",
  "x": "While ideas similar to ours by virtue of leveraging the mutual reinforcement of entities of different types, or using bipartite graphs of such entities for clustering (rather than using clusters), are abundant (e.g., [15, 8, 2] ), we focus here on exploiting mutual reinforcement in ad hoc retrieval. Random walks (with early stopping) over bipartite graphs of terms and documents were used for query expansion [20] , but in contrast to our work, no stationary solution was sought. A similar \"short chain\" approach utilizing bipartite graphs of clusters and documents for ranking an entire corpus was recently proposed [19] , thereby constituting the work most resembling ours. However, again, a stationary distribution was not sought. Also, query drift prevention mechanisms were required to obtain good performance; in our re-ranking setting, we need not employ such mechanisms. ---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters.",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_12",
  "x": "Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> .",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_13",
  "x": "The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_14",
  "x": "The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.**",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_15",
  "x": "7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.** For a given set Dinit of initially retrieved documents and positive integer \u03b4 (an \"outdegree\" parameter), we consider the following three graphs. Each connects nodes u to the \u03b4 other nodes, drawn from some specified set, that u has the highest relevance flow to. The document-to-document graph d\u2194d has vertex set Dinit and weight function",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_16",
  "x": "is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.** For a given set Dinit of initially retrieved documents and positive integer \u03b4 (an \"outdegree\" parameter), we consider the following three graphs. Each connects nodes u to the \u03b4 other nodes, drawn from some specified set, that u has the highest relevance flow to. The document-to-document graph d\u2194d has vertex set Dinit and weight function The document-as-authority graph c\u2192d has vertex set Dinit\u222a C l(Dinit) and a weight function such that positive-weight edges go only from clusters to documents:",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_17",
  "x": "The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.**",
  "y": "similarities background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_18",
  "x": "We conducted our experiments on three TREC datasets: In many retrieval situations of interest, ensuring that the top few documents retrieved (a.k.a., \"the first page of results\") tend to be relevant is much more important than ensuring that we assign relatively high ranks to the entire set of relevant documents in aggregate [31] . Hence, rather than use mean average precision (MAP) as an evaluation metric, we apply metrics more appropriate to the structural re-ranking task: precision at the top 5 and 10 documents (henceforth prec@5 and prec@10, respectively) and the mean reciprocal rank (MRR) of the first relevant document [31] . All performance numbers are averaged over the set of queries for a given corpus. The natural baseline for the work described here is the standard language-model-based retrieval approach [29, 5] , since it is an effective paradigm that makes no explicit use of inter-document relationships. Specifically, for a given evaluation metric e, the corresponding optimized baseline is the ranking on documents produced by p (q), where \u00b5(e) is the value of the Dirichlet smoothing parameter that results in the best retrieval performance as measured by e. A ranking method might assign different items the same score; we break such ties by item ID. Alternatively, the scores used to determine Dinit can be utilized, if available. Parameter selection for graph-based methods. There are two motivations underlying our approach to choosing values for our algorithms' parameters<cite> [18]</cite> .",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_19",
  "x": "We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work<cite> [18]</cite> ? The answer, as shown in Table 1 , is clearly \"yes\": we see that moving to cluster-to-document graphs results in substantial improvement for HITS, and indeed boosts its results over those for PageRank on document-to-document graphs. Full suite of comparisons. We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.) To focus our discussion, it is useful to first point out that in almost all of our nine evaluation settings (3 corpora \u00d7 3 evaluation measures), all three of the re-ranking algorithms perform better when applied to c\u2192d graphs than to d\u2194d graphs, as the number of dark bars in Figure 2 indicates. Since it is thus clearly useful to incorporate cluster-based information, we will now mainly concentrate on c\u2192d-based algorithms. The results for prec@5, the metric for which the re-ranking algorithms' parameters were optimized, show that all c\u2192d-based algorithms outperform the prec@5-optimized baseline -significantly so for the AP corpus -even though applied to a sub-optimally-ranked initial set. (We hasten to point out that while the initial ranking is always inferior to the corresponding optimized baseline, the differences are never significant.) In contrast, the use of d\u2194d graphs never leads to significantly superior prec@5 results. We also observe in Figure 2 that the doc-Auth[c\u2192d] algorithm is always either the best of the c\u2192d-based algorithms or clearly competitive with the best.",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_20",
  "x": "**EXPERIMENTAL RESULTS** In what follows, when we say that results or the difference between results are \"significant\", we mean according to the two-sided Wilcoxon test at a confidence level of 95%. ---------------------------------- **RE-RANKING BY DOCUMENT CENTRALITY** Main result. We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work<cite> [18]</cite> ? The answer, as shown in Table 1 , is clearly \"yes\": we see that moving to cluster-to-document graphs results in substantial improvement for HITS, and indeed boosts its results over those for PageRank on document-to-document graphs. Full suite of comparisons. We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.)",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_21",
  "x": "As the notation suggests, this corresponds to running HITS and PageRank on the same graph, d\u2194d. But an alternative interpretation<cite> [18]</cite> is that non-smoothed (or no-random-jump) PageRank, as expressed by Equation (3), is applied to a different version of d\u2194d wherein the original edge weights w t(u \u2192 v) have been smoothed as follows: (we ignore nodes with no positive-weight out-edges to simplify discussion, and omit the d\u2194d superscripts for clarity). How does HITS perform on document-to-document graphs that are \"truly equivalent\", in the sense of employing the above edge-weighting regime, to those that PageRank is applied to? One reason this is an interesting question is that HITS assigns scores of zero to nodes that are not in the graph's largest connected component (with respect to positive-weight edges, considered to be bi-directional). Notice that the original graph may have several connected components, whereas utilizing w t [\u03bb] ensures that each node has a positive-weight directed edge to every other node. Additionally, the re-weighted version of HITS has provable stability properties [27] . We found that in nearly all of our evaluation settings for document-to-document graphs (three corpora \u00d7 three evaluation metrics), doc-Auth[d\u2194d] achieved better results using w t [\u03bb] edge weights. However, we cannot discount the possibility that the performance differences might be due simply to the inclusion of the extra interpolation-parameter \u03bb. Moreover, in all but one case, the improved results were still below those for doc-PageRank[d\u2194d] (and always lagged behind those of doc-Auth[c\u2192d]). Interestingly, the situation is qualitatively different if we consider c\u2192d graphs instead.",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_22",
  "x": "However, we cannot discount the possibility that the performance differences might be due simply to the inclusion of the extra interpolation-parameter \u03bb. Moreover, in all but one case, the improved results were still below those for doc-PageRank[d\u2194d] (and always lagged behind those of doc-Auth[c\u2192d]). Interestingly, the situation is qualitatively different if we consider c\u2192d graphs instead. In brief, we applied a smoothing scheme analogous to that described above, but only to edges leading from a left-hand node (cluster) to a right-hand node (document) 9 ; we thus preserved the one-way bipartite structure. Only in two of the nine evaluation settings did this change cause an increase in performance of docAuth[c\u2192d] over the results attained under the original edgeweighting scheme, despite the fact that the re-weighting involves an extra free parameter. Thus, while we have already demonstrated in previous sections of this paper that information about document-cluster similarity relationships is very valuable, the results just mentioned suggest that such information is more useful in \"raw\" form. Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_23",
  "x": "Thus, while we have already demonstrated in previous sections of this paper that information about document-cluster similarity relationships is very valuable, the results just mentioned suggest that such information is more useful in \"raw\" form. Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ----------------------------------",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_24",
  "x": "Thus, while we have already demonstrated in previous sections of this paper that information about document-cluster similarity relationships is very valuable, the results just mentioned suggest that such information is more useful in \"raw\" form. Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_25",
  "x": "Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "74471d4e333ce76fd62b968045eba5_0",
  "x": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; <cite>Zhang et al., 2015</cite>) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; Seo et al., 2016) , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models (Rezende et al., 2014) . The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies are presented to tackle different issues in deep Bayesian learning and understanding. At last, we point out a number of directions and outlooks for future studies.",
  "y": "background uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_0",
  "x": "Finkel and Manning (2009) proposed building a constituency parser with constituents for each named entity in a sentence. Their approach is expensive, i.e., time complexity is cubic in the number of words in the sentence. Lu and Roth (2015) later proposed a mention hypergraph model for nested entity detection with linear time complexity. And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models -for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-based sequence labeling model that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head 2 information, a subtask found to be useful for many information extraction applications. Our model significantly outperforms the previously mentioned hypergraph model of<cite> Lu and Roth (2015)</cite> and Muis and Lu (2017) on entity mention recognition for the ACE2004 and ACE2005 corpora.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_1",
  "x": "Their approach could not identify nested entities of the same type. Finkel and Manning (2009) proposed a CRF-based constituency parser for nested named entities such that each named entity is a constituent in the parse tree. Their model achieved state-of-the-art results on the GENIA dataset. However, the time complexity of their model is O(n 3 ), where n is the number of tokens in the sentence, making inference slow. As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> . Directed hypergraphs were also introduced for parsing by Klein and Manning (2001) . While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergraph representation with differences in the types of nodes and edges in the hypergraph.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_2",
  "x": "They obtained their best results from a cascaded approach, where they applied CRFs in a specific order on the entity types, such that each CRF utilizes the output derived from previous CRFs. Their approach could not identify nested entities of the same type. Finkel and Manning (2009) proposed a CRF-based constituency parser for nested named entities such that each named entity is a constituent in the parse tree. Their model achieved state-of-the-art results on the GENIA dataset. However, the time complexity of their model is O(n 3 ), where n is the number of tokens in the sentence, making inference slow. As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> . Directed hypergraphs were also introduced for parsing by Klein and Manning (2001) . While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure.",
  "y": "background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_3",
  "x": "Also, our model learns the hypergraph greedily and significantly outperforms their approach. Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and<cite> Lu and Roth (2015)</cite> adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection. How-ever, we will show that our model also outperforms their approach on all tasks. Recently, recurrent neural networks (RNNs) have been widely applied to several sequence labeling tasks achieving state-of-the-art results. Lample et al. (2016) proposed neural models based on long short term memory networks (LSTMs) and CRFs for named entity recognition and another transition-based approach inspired by shift-reduce parsers. Both models achieve performance comparable to a state-of-the-art model (Luo et al., 2015) , but neither handles nested named entities. 3 Encoding Scheme Figure 1 shows the desired sequence tagging output for each of three overlapping PER entities (\"his\", \"his fellow pilot\" and \"his fellow pilot David Williams\") according to the standard BILOU tag scheme. Our approach relies on the fact that we can (1) represent these three tag sequences in the single hypergraph structure of Figure 2 and then (2) design an LSTM-based neural network that produces the correct nested entity hypergraph for a given input sentence.",
  "y": "uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_4",
  "x": "If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> . Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ---------------------------------- **EDGE PROBABILITY** In this section, we discuss our assignment of probabilities to all the possible edges from a tail node which helps in the greedy construction of the hypergraph. Thus at any timestep t, let g t\u22121 be the tail node and x be the current word of the sentence; then we model probability distribution over all the possible types of head nodes (different output tag types) conditioned on the tail node and the current word token. In our work we use hidden representations learned from an LSTM model as features to learn these probability distributions using a crossentropy objective.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_5",
  "x": "Thus at any time step, the representation size is bounded by the number of possible output states instead of the potentially exponential number of output sequences. We then also adjust the directed edges such that they have the same type of head node and the same type of tail node as before in Figure 1 . If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> . Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ---------------------------------- **EDGE PROBABILITY** In this section, we discuss our assignment of probabilities to all the possible edges from a tail node which helps in the greedy construction of the hypergraph.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_6",
  "x": "**DATA** We perform experiments on the English section of the ACE2004 and ACE2005 corpora. There are 7 main entity types -Person (PER), Organization (ORG), Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Vehicle (VEH). For each entity type, there are annotations for the entity mention and mention heads. ---------------------------------- **EVALUATION METRICS** We use a strict evaluation metric similar to<cite> Lu and Roth (2015)</cite> : an entity mention is considered correct if both the mention span and the mention type are exactly correct. Similarly, for the task of joint extraction of entity mentions and mention heads, the mention span, head span and the entity type should all exactly match the gold label. ---------------------------------- **BASELINES AND PREVIOUS MODELS**",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_9",
  "x": "**HYPERPARAMETERS AND TRAINING DETAILS** We use Adadelta (Zeiler, 2012) for training our models. We initialize our word vectors with 300-dimensional word2vec (Mikolov et al., 2013) word embeddings. These word embeddings are tuned during training. We regularize our network using dropout (Srivastava et al., 2014) , with the dropout rate tuned on the development set. There are 3 hidden layers in our network and the dimensionality of hidden units is 100 in all our experiments. And we set the threshold T as 0.3. ---------------------------------- **RESULTS** We show the performance of our approaches in Table 1 compared to the previous state-of-the-art system<cite> (Lu and Roth, 2015</cite>; Muis and Lu, 2017) on both the ACE2004 and ACE2005 datasets. We find that our LSTM-flat baseline that ignores embedded entity mentions during training performs worse than<cite> Lu and Roth (2015)</cite> ; however, our other neural network-based approaches all outperform the previous feature-based approach.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_10",
  "x": "**JOINT MODELING OF HEADS** We report the performance of our best performing models on the joint modeling of entity mentions and its head in Table 2. We show that our sparsemax model is still the best performing model. We also find that as the total number of possible labels at any timestep increases because of the way we implemented the entity heads, the gains that we get after incorporating sparsemax are significantly higher compared to the results shown in Table 1 . ---------------------------------- **GENIA EXPERIMENTS** ---------------------------------- **DATA** We also evaluate our model on the GENIA dataset (Ohta et al., 2002) for nested named entity recognition. We follow the same dataset split as Finkel and Manning (2009);<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) .",
  "y": "similarities uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_12",
  "x": "**BASELINES AND PREVIOUS MODELS** We compare our model with Finkel and Manning (2009) based on a constituency CRF-based parser and the mention hypergraph model by<cite> Lu and Roth (2015)</cite> and a recent multigraph model by Muis and Lu (2017) . Table 3 : Performance on the GENIA dataset on nested named entity recognition. ---------------------------------- **RESULTS** Roth (2015) . We suspect that it is because we use pretrained word embeddings 5 trained on PubMed data (Pyysalo et al., 2013) whereas<cite> Lu and Roth (2015)</cite> did not have access to them. We again find that our neural network model outperforms the previous state-of-the-art (Finkel and Manning, 2009; Muis and Lu, 2017) system. However, we see that both softmax and sparsemax models perform comparably on this dataset. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_13",
  "x": "---------------------------------- **PRONOMINAL ENTITY MENTION (IT** ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we present a novel recurrent network-based model for nested named entity recognition and nested entity mention detection. We propose a hypergraph representation for this problem and learn the structure using an LSTM network in a greedy manner. We show that our model significantly outperforms a feature based mention hypergraph model<cite> (Lu and Roth, 2015)</cite> and a recent multigraph model (Muis and Lu, 2017) on the ACE dataset. Our model also outperforms the constituency parser-based approach of Finkel and Manning (2009) on the GENIA dataset. In future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally. We can also experiment with different representations such as the one in Finkel and Manning (2009) and use the recent advances in neural network approaches (Vinyals et al., 2015) to learn the constituency parse tree efficiently.",
  "y": "differences"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_0",
  "x": "Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics. LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (Brody and Lapata 2009) . However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 . ---------------------------------- **LDA: !(#|%)** AutoSense: !('|#, (% ) , %)) Figure 2: Example induced senses when the target word is cold from LDA and AutoSense. Applying our observations to LDA introduces both garbage and fine-grained senses. To this end, we propose a latent variable model called AutoSense that solves all the challenges of WSI, including overcoming the sense granularity problem. Consider Figure  2 on finding the senses of the target word cold. An LDA model naively considers the topics as senses and thus differentiates the usage of cold in the medical and science domains, even though the same sense is commonly used in the two domains.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_1",
  "x": "More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and Chen 2014) and that topics and senses should be inferred jointly (STM) (<cite>Wang et al. 2015</cite>) . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs. HC was also extended to a nonparametric model (BNP-HC) (Teh et al. 2004 ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van Durme 2011; Lau et al. 2012; . In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective. Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (<cite>Wang et al. 2015</cite>) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; Pelevina et al. 2016; Chang et al. 2018 ). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (Komninos and Manandhar 2016) . We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well. In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used (Shu, Long, and Meng 2009) to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features (Tang et al. 2012) .",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_2",
  "x": "STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (<cite>Wang et al. 2015</cite>) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; Pelevina et al. 2016; Chang et al. 2018 ). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (Komninos and Manandhar 2016) . We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well. In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used (Shu, Long, and Meng 2009) to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features (Tang et al. 2012) . In this paper, we study on how to improve the performance of text features for UAND using latent variable models, which can later be combined with non-text features in the future work. Figure 3: Graphical representation of AutoSense. Nodes are random variables, edges are dependencies, and plates are replications. Nodes shaded in black are observed. The node shaded in red is the observed target word.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_3",
  "x": "---------------------------------- **PROPOSED MODEL** There are two reasons why Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage. ----------------------------------",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_4",
  "x": "---------------------------------- **PROPOSED MODEL** There are two reasons why Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_5",
  "x": "However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage. ---------------------------------- **AUTOSENSE MODEL** To solve the problems above, we propose to extend LDA in two parts. First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses. Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and Chen 2014) , or to generate only the neighboring words within a local context, decided by a strict user-specified window (<cite>Wang et al. 2015</cite>) . We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_6",
  "x": "**AB AB** represents the number of a \u2208 A and b \u2208 B assignments. We also calculate the word distribution of each sense using the second equation below to inspect the meaning of sense. ---------------------------------- **EXPERIMENTAL SETUP** Datasets and preprocessing We use two publicly available datasets: SemEval 2010 Task 14 (Manandhar et al. 2010) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (Manning et al. 2014) . We divide the word lists into two contexts: the local and global context. Following (<cite>Wang et al. 2015</cite>), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_7",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Datasets and preprocessing We use two publicly available datasets: SemEval 2010 Task 14 (Manandhar et al. 2010) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (Manning et al. 2014) . We divide the word lists into two contexts: the local and global context. Following (<cite>Wang et al. 2015</cite>), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_8",
  "x": "Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; <cite>Wang et al. 2015</cite>) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ---------------------------------- **MODEL F-S V-M AVG \u0394(#S)** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_9",
  "x": "However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ---------------------------------- **MODEL F-S V-M AVG \u0394(#S)** ---------------------------------- **EXPERIMENTS WORD SENSE INDUCTION** SemEval 2010 For the SemEval 2010 dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (Manandhar et al. 2010) . In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following (<cite>Wang et al. 2015</cite>) . Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as \u03b4(#S).",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_10",
  "x": "The overestimated AutoSense s=100 performs better than previously proposed models, proving the robustness of our model on the different word sense granularities. On the \u03b4(#S) metric, the untuned AutoSense and AutoSense s=5 perform the best. The V-M metric needs to be interpreted carefully, because it can easily be maximized by separating all instances into different sense clusters and thus overestimating the actual number of senses #S and decreasing the F-S metric. The model BNP-HC is an example of such: Though its V-M metric is the highest, it scores the lowest on the F-S metric and greatly overestimates #S, thus having a very high \u03b4(#S). The goal is thus a good balance of V-M and F-S (i.e. highest AVG), and a close estimation of #S (i.e. lowest \u03b4(#S), which is successfully achieved by our models. SemEval 2013 Two metrics are used for the SemEval 2013 dataset: fuzzy B-cubed (F-BC) and fuzzy normalized mutual information (F-NMI). F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_11",
  "x": "F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model. We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed. This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value. For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (<cite>Wang et al. 2015</cite>) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_12",
  "x": "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model. We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed. This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value. For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (<cite>Wang et al. 2015</cite>) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context. With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_13",
  "x": "**SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective. On the other hand, Table 2 also shows that AutoSense is effective even when S is overestimated. We explain why through an example result shown in Table 3 , where the target word is the verb book, the actual number of senses is three, and S is set to 15. First, we see that there are senses which are not assigned to any instance document, signified by * , which we call garbage senses. We notice that effectively representing a new latent variable for sense as a distribution over topics forces the model to throw garbage senses.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_14",
  "x": "The first three senses are senses which are assigned at least once to an instance document. The last three are garbage senses. F-BC metric by at least 2%. Also, considering that both AutoSense and STM are LDA-based models, the same data enhancements can straightforwardly be applied when the needs arise. We similarly apply the actual additional contexts to AutoSense and find that we achieve state-of-the-art performance on AVG. ---------------------------------- **SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_15",
  "x": "Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective. On the other hand, Table 2 also shows that AutoSense is effective even when S is overestimated. We explain why through an example result shown in Table 3 , where the target word is the verb book, the actual number of senses is three, and S is set to 15. First, we see that there are senses which are not assigned to any instance document, signified by * , which we call garbage senses. We notice that effectively representing a new latent variable for sense as a distribution over topics forces the model to throw garbage senses. Second, while it is easy to distinguish the third sense (i.e., book: register in a booker) to the two other senses, the first and second senses both refer to planning or arranging for an event in advance. Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance. We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA (Blei, Ng, and Jordan 2003) , STM (<cite>Wang et al. 2015</cite>) , HC (Chang, Pei, and Chen 2014) , and a nonparametric model HDP (Teh et al. 2004 ), with AutoSense.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_16",
  "x": "Experimental setup We use two publicly available datasets for the UAND task: Arnet 4 and PubMed 5 . The Arnet dataset contains 100 ambiguous author names and a total of 7528 papers as data instance. Each instance includes the title, author list, and publication venue of a research paper authored by the given author name. In addition, we also manually extract the abstracts of the research papers for additional context. The PubMed dataset contains 37 author names with a total of 2875 research papers as instances. It includes the PubMed ID of the papers authored by the given author name. We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website. We use LDA (Blei, Ng, and Jordan 2003) , HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) as baselines. We do not compare with non-text feature-based models (Tang et al. 2012; Cen et al. 2013 ) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_0",
  "x": "Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only demonstrate better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but also achieve 4\u00d7 speedup in model training and inference. ---------------------------------- **INTRODUCTION** Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) argues, most Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate g q and g p ; an attention encoder is applied on post embeddings to generate variable-length queryaware features h i . These features are further aggregated to yield v, which feeds into the final prediction. systems are built in a top-down process: authors proposing a complex architecture and validating design decisions with ablation experiments.",
  "y": "background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_1",
  "x": "Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) argues, most Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate g q and g p ; an attention encoder is applied on post embeddings to generate variable-length queryaware features h i . These features are further aggregated to yield v, which feeds into the final prediction. systems are built in a top-down process: authors proposing a complex architecture and validating design decisions with ablation experiments. However, such experiments often lack comparisons to strong baselines, which raises the question as to whether model complexity is empirically justified. As an alternative, they advocate a bottom-up approach where architectural complexity is gradually increased. We adopt exactly such an approach, focused exclusively on word-level modeling.",
  "y": "background similarities"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_2",
  "x": "---------------------------------- **TRAINING** To obtain the final score, the feature vectors g q , g p and v are concatenated and fed into an MLP with ReLU activate function for dimension reduction and obtain o, followed by batch normalization and fully-connected layer and softmax to output the final prediction. The model is trained endto-end with Stochastic Gradient Decent optimizer, and negative log-likelihood loss function is used. 3 Experiment Experimental Setup. Our models are evaluated on four tweets test collections from the TREC 2011-2014 Microblog (MB) Tracks (Ounis et al., 2011; Soboroff et al., 2012; Lin and Efron, 2013; Lin et al., 2014) . Each dataset contains around 50 queries and the more detailed statistics are shown in Table 1 . Following <cite>Rao et al. (2018)</cite> , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) . We run four-fold cross-validation test split by year (i.e., train on three year's data, test on one year's data), and we randomly sample 10 queries from each year in the training sets (in total 30 queries) as our validation set. The mean average precision (MAP) and precision at top 30 (P30) are adopted as our evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_3",
  "x": "Following <cite>Rao et al. (2018)</cite> , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) . We run four-fold cross-validation test split by year (i.e., train on three year's data, test on one year's data), and we randomly sample 10 queries from each year in the training sets (in total 30 queries) as our validation set. The mean average precision (MAP) and precision at top 30 (P30) are adopted as our evaluation metrics. We also conducted Fisher's two-sided, paired randomization test (Smucker et al., 2007) to test for statistical significance at p < 0.05. We randomly tried ten different seeds with the same hyperparameters and obtain an average score over each query-post pair for final ranking, to eliminate the effects of random parameter initialization (Crane, 2018) . Our code is released for further reproduction. The best model hyper-parameters are shown in Table 4 in Appendix section. Baselines: QL is a competitive language modeling baseline. RM3 (Lavrenko and Croft, 2001 ) is an interpolation model combining the QL score with a relevance model using pseudo-relevance feedback. MP-HCNN<cite> (Rao et al., 2018)</cite> is the first neural model that captures the characteristics of social media domain.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_4",
  "x": "Baselines: QL is a competitive language modeling baseline. RM3 (Lavrenko and Croft, 2001 ) is an interpolation model combining the QL score with a relevance model using pseudo-relevance feedback. MP-HCNN<cite> (Rao et al., 2018)</cite> is the first neural model that captures the characteristics of social media domain. Their method improves current neural IR methods, e.g., K-NRM (Xiong et al., 2017) , DUET (Mitra et al., 2017) , by a signifi- 4 BiCNN+PAtt+QL 0.4728 1-3 5-7 0.4293 1-3 5-7 0.4147 1,2 5,6 0.2621 1-3 5-7 0.5367 1-3 5,6 0.2990 1-3 5 0.6806 1,2 5-8 0.4563 1-3 ---------------------------------- **5,7** Existing Models 5 QL 0.4000 1 0.3576 1 0.3311 1 0.2091 1 0.4450 1 0.2532 1 0.6182 1 0.3924 1 6 RM3 0.4211 1 0.3824 1 0.3452 1 0.2342 1 0.4733 1 0.2766 1 0.6339 1 0.4480 1 7 MP-HCNN(+URL) 0.4306 1 0.3940 1,2 0.3757 1,5 0.2313 1,5 0.5211 1,5 0.2856 1,5 0.6279 1 0.4178 1 8 MP-HCNN(+URL)+QL 0.4435 1-2 5,6 0.4040 1,2 5,6 0.3915 1,5 6 0.2482 1,2 5 0.5250 1,5 6 0.2937 1,2 5 0.6455 1 0.4403 1,5 Table 2 : Results of non-neural and neural models on the TREC Microblog Tracks datasets. Results from 5 -8 are adopted from <cite>Rao et al. (2018)</cite> . Models denoted with (+URL) represents utilizing the URL information. Models denoted with +QL are interpolated with QL baseline.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_5",
  "x": "Models denoted with +QL are interpolated with QL baseline. Bi-CNN denotes general sentence encoder architecture. Both superscripts and subscripts indicate the row indexes for which a metric difference is statistically significant at p < 0.05. cant margin. To the best of our knowledge, <cite>Rao et al. (2018)</cite> is the best neural model to date, and there are no neural models from TREC evaluations for further comparison. We also compared to MP-HCNN+QL, which is a linear interpolation to combine the raw MP-HCNN and QL scores. Table 2 shows our experiment results of all settings and the results of existing models. Model 1 is the effectiveness of BiCNN model with kernel window size 2. 1 Comparing Models 1 and 2, we observe that the models with query-aware kernels significantly improve the BiCNN baselines, achieving competitive effectiveness as QL baseline, demonstrating the effectiveness of the queryaware encoder. Further capturing position information with the position-aware encoder, as shown with Models 3, we obtain competitive effectiveness against MP-HCNN, even interpolated with QL.",
  "y": "background"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_0",
  "x": "---------------------------------- **INTRODUCTION** Humans are powerful and data-efficient learners partially due to the ability to learn from language [6, 30] : for instance, we can learn about robins not by seeing thousands of examples, but by being told that a robin is a bird with a red belly and brown feathers. This language not only helps us learn about robins, but shapes the way we view the world, constraining the hypotheses we form for other concepts [12] : given a new bird like seagulls, even without language we know to attend to salient features including belly and feather color. In this paper, we propose to use language as a guide for representation learning, building few-shot classification models that learn visual representations while jointly predicting task-specific language during training. Crucially, our models can operate without language at test time: a more practical setting, since it is often unrealistic to assume that linguistic supervision is available for unseen classes encountered in the wild. Compared to meta-learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model <cite>[1]</cite> , we find this simple auxiliary training objective results in learned representations that generalize better to new concepts. ---------------------------------- **RELATED WORK** Language has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [16] and with language available at test time in the form of class labels or descriptions for zero- [10,<cite> 1</cite>1, 27] or few-shot [24, 33] learning.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_3",
  "x": "In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored. [13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations. [28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later. ---------------------------------- **LANGUAGE-SHAPED LEARNING** We are interested in N -way, K-shot learning, where a task t consists of N support classes {S The goal is to predict each y The approach we propose is applicable to any meta-learning framework that learns an embedding of its input.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_5",
  "x": "Language has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [16] and with language available at test time in the form of class labels or descriptions for zero- [10,<cite> 1</cite>1, 27] or few-shot [24, 33] learning. Unlike this work, we study a setting where we have no language at test time and test tasks are unseen, so language from training can no longer be used as additional class information [cf.<cite> 1</cite>6] or weak supervision for labeling additional in-domain data [cf.<cite> 1</cite>5] . Our work can thus be seen as an instance of the learning using privileged information (LUPI) [31] framework, where richer supervision augments a model during training only. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored. [13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations. [28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_9",
  "x": "1 In Figure<cite> 1</cite> , we have one description w<cite> 1</cite> = (A, red, . . . , square). Our approach is simple: we constrain f \u03b8 to learn prototypes that can also decode the class language descriptions. Letc n be the prototype formed by averaging the support and query examples of class n. Then define a language model g \u03c6 (e.g. a recurrent neural network), which conditioned onc n provides a probability distribution over descriptions g \u03c6 (\u0175 j |c n ) with a corresponding natural language loss: i.e. the total negative log-likelihood of the class descriptions across all classes in the task. Now we jointly minimize both losses: where \u03bb NL is a tunable parameter controlling the weight of the natural language loss. At test, we simply discard g \u03c6 and use f \u03b8 to classify. With this component, we call our approach language-shaped learning (LSL) (Figure<cite> 1</cite> ). Relation to L3. LSL is similar to another recent model for this setting: Learning with Latent Language (L3) <cite>[1]</cite> , which proposes to use language not only as a supervision source, but as a bottleneck for classification ( Figure<cite> 1</cite> ).",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_10",
  "x": "For full training details and code, see Appendix A. ShapeWorld. First, we use the ShapeWorld [20] dataset devised by <cite>[1]</cite> , which consists of 9000 training,<cite> 1</cite>000 validation, and 4000 test tasks ( Figure 2 ). 3 Each task contains a single support set of K = 4 images representing a visual concept with an associated (artificial) English language description, generated with a minimal recursion semantics representation of the concept [7] . Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept. Model details are identical to <cite>[1]</cite> for easy comparison. f \u03b8 is the final convolutional layer of a fixed ImageNet-pretrained VGG-16 [25] fed through two fully-connected layers: Since this is a binary classification task with only<cite> 1</cite> (positive) support class S and prototype c, we define the similarity function s(a, b) = \u03c3(a \u00b7 b) and the prediction P (\u0177 =<cite> 1</cite> | x) = s (f \u03b8 (x), c). g \u03c6 is a gated recurrent unit (GRU) RNN [5] with hidden size h = 512, trained with teacher forcing.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_11",
  "x": "3 Each task contains a single support set of K = 4 images representing a visual concept with an associated (artificial) English language description, generated with a minimal recursion semantics representation of the concept [7] . Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept. Model details are identical to <cite>[1]</cite> for easy comparison. f \u03b8 is the final convolutional layer of a fixed ImageNet-pretrained VGG-16 [25] fed through two fully-connected layers: Since this is a binary classification task with only<cite> 1</cite> (positive) support class S and prototype c, we define the similarity function s(a, b) = \u03c3(a \u00b7 b) and the prediction P (\u0177 =<cite> 1</cite> | x) = s (f \u03b8 (x), c). g \u03c6 is a gated recurrent unit (GRU) RNN [5] with hidden size h = 512, trained with teacher forcing. Using a grid search on the validation set, we set \u03bb NL = 20. Birds. To see if LSL can scale to more realistic scenarios, we use the Caltech-UCSD Birds dataset [32] , which contains 200 bird species, each with 40-60 images, split into<cite> 1</cite>00 train, 50 validation, The bird has a white underbelly, black feathers in the wings, a large wingspan, and a white beak.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_17",
  "x": "This last point suggests we can shape representations with language from external resources (e.g. the Web), a promising future direction of work. ---------------------------------- **A TASK/MODEL TRAINING DETAILS** Our code is publicly available at https://github.com/jayelm/lsl. A.1 ShapeWorld f \u03b8 . Like <cite>[1]</cite> , f \u03b8 starts with features extracted from the last convolutional layer of a fixed ImageNetpretrained VGG-19 network [25] . These 4608-d embeddings are then fed into two fully connected layers \u2208 R 4608\u00d7512 , R 512\u00d7512 with one ReLU nonlinearity in between. LSL. For LSL, the 512-d embedding from f \u03b8 directly initializes the 512-d hidden state of the GRU g \u03c6 . We use 300-d word embeddings initialized randomly (initializing with GloVe made no significant difference). L3.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_19",
  "x": "For LSL, the 512-d embedding from f \u03b8 directly initializes the 512-d hidden state of the GRU g \u03c6 . We use 300-d word embeddings initialized randomly (initializing with GloVe made no significant difference). L3. f \u03b8 and g \u03c6 are the same as in LSL and Meta. h \u03b7 is a unidirectional<cite> 1</cite>-layer GRU with hidden size 512 sharing the same word embeddings as g \u03c6 . The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training. We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_21",
  "x": "This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping. A.2 Birds f \u03b8 . The 4-layer convolutional backbone f \u03b8 is the same as the one used in much of the few-shot literature [4, 26] . The model has 4 convolutional blocks, each consisting of a 64-filter 3x3 convolution, batch normalization, ReLU nonlinearity, and 2x2 max-pooling layer. With an input image size of 84 \u00d7 84 this results in<cite> 1</cite>600-d image embeddings. Finally, the similarity metric matrix W has dimension<cite> 1</cite>600 \u00d7<cite> 1</cite>600.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_22",
  "x": "Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping. A.2 Birds f \u03b8 . The 4-layer convolutional backbone f \u03b8 is the same as the one used in much of the few-shot literature [4, 26] . The model has 4 convolutional blocks, each consisting of a 64-filter 3x3 convolution, batch normalization, ReLU nonlinearity, and 2x2 max-pooling layer. With an input image size of 84 \u00d7 84 this results in<cite> 1</cite>600-d image embeddings. Finally, the similarity metric matrix W has dimension<cite> 1</cite>600 \u00d7<cite> 1</cite>600. LSL.",
  "y": "similarities differences uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_23",
  "x": "This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping. A.2 Birds f \u03b8 . The 4-layer convolutional backbone f \u03b8 is the same as the one used in much of the few-shot literature [4, 26] . The model has 4 convolutional blocks, each consisting of a 64-filter 3x3 convolution, batch normalization, ReLU nonlinearity, and 2x2 max-pooling layer. With an input image size of 84 \u00d7 84 this results in<cite> 1</cite>600-d image embeddings. Finally, the similarity metric matrix W has dimension<cite> 1</cite>600 \u00d7<cite> 1</cite>600.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_0",
  "x": "The corpus data are drawn from the British National Corpus (BNC) and are analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. ---------------------------------- **INTRODUCTION** The semantics and pragmatics of discourse structure has been a central theme in linguistic research for quite some time. Recent research on large-scale annotation of discourse relations for the purposes of natural language processing applications has resulted in new insights in the properties of such relations and in concrete proposals on how to annotate them. A particularly ambitious and interesting effort of this kind is the Penn Discourse Treebank (PDTB), a corpus of 1 million words which is being annotated for discourse connectives and their arguments, more specifically for connectives such as but, for example, because, after, and when that are either realized lexically (explicit connectives) or that have no overt linguistic realization, but that can be inferred as a logical relation between pieces of discourse (implicit connectives). On the basis of the detailed PDTB annotations, which by now comprise a substantial corpus of linguistic data, it has become possible to revisit an open research question that had been raised repeatedly in the literature, albeit without yielding concrete results. This open research question concerns the similarities and differences between syntactic and semantic relations at the sentence level and at the discourse level.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_1",
  "x": "2. While syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree-like structures and not introducing structural ambiguities of scope or attachment. 3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast. ---------------------------------- **THE DATA** The British National Corpus (BNC; Burnage and Baguley (1996) ) served as the data source for the present investigation. The BNC is a 100 million word collection of samples from a wide range of sources, designed to represent a wide cross-section of current British English, both spoken and written. The reasons for choosing the BNC rather than the Wall Street Journal (WSJ) corpus, which provides the data source for the PDTB, are two-fold: (i) The BNC is a hundred times larger than the 1-million word WSJ corpus and thus yields a much larger data source, and (ii) the BNC is much more balanced in the genres represented than the WSJ.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_2",
  "x": "On the basis of the detailed PDTB annotations, which by now comprise a substantial corpus of linguistic data, it has become possible to revisit an open research question that had been raised repeatedly in the literature, albeit without yielding concrete results. This open research question concerns the similarities and differences between syntactic and semantic relations at the sentence level and at the discourse level. Webber (2006) and Lee et al. (2006) have addressed this very issue in the context of the PDTB annotations and have arrived at the following empirical generalizations: 1. While the arity of predicates at the sentential level can vary, e.g. one argument in the case of intransitive verbs, two in the case of transitives, three for ditransitives, etc., the arity of discourse connectives is fixed and consists of exactly two arguments. 2. While syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree-like structures and not introducing structural ambiguities of scope or attachment. 3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_3",
  "x": "---------------------------------- **DISCOURSE ANAPHORA** This section will focus on the discourse function of the adverbial phrase in contrast. Following <cite>Webber et al. (2003)</cite>, we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora. Discourse anaphora involves a relation between an anaphor, such as a pronoun or a temporal adverbial, and an antecedent that is present in the previous discourse or that can be inferred from it. In the case of pronouns, antecedents are typically NPs, while the antecedents of temporal adverbials can be time-denoting expressions, such as dates, events or states of affairs. For pronouns, discourse anaphora can either involve coreference or more indirect referential relations which do not involve identity of reference with a previous discourse entity, but where the anaphor is merely associated with a previously mentioned discourse entity. Such cases of indirect referential relations include cases of bridging, as in (7), where the anaphor, in this case the receiver stands in a part-whole-relation to its antecedent -in this case a phone. (7) Myra darted to a phone and picked up the receiver. (Webber et al. (2003) , p. 555)",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_4",
  "x": "For pronouns, discourse anaphora can either involve coreference or more indirect referential relations which do not involve identity of reference with a previous discourse entity, but where the anaphor is merely associated with a previously mentioned discourse entity. Such cases of indirect referential relations include cases of bridging, as in (7), where the anaphor, in this case the receiver stands in a part-whole-relation to its antecedent -in this case a phone. (7) Myra darted to a phone and picked up the receiver. (Webber et al. (2003) , p. 555) Other-anaphora (Bierner and Webber (2000) Bierner (2001), Modjeska (2002)), as in (8), provides another instance of such an indirect referential relation. (8) Sue grabbed one phone, as Tom darted to the other phone.<cite> (Webber et al. (2003)</cite>, p. 555) Here the referent of the other phone can be inferred from the antecedent one phone. The referential relation between the anaphor and the antecedent is not one of identity of reference. Rather, the referents of the antecedent and the anaphor together constitute the set of phones owned by Sue and Tom. It is indicative of the anaphoric character of in contrast that it licenses other-anaphora in the same way, as shown in (9). (9) He retired to Hampshire and died in 1832 at the age of 76.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_5",
  "x": "(10) It's a shame, then, that its gearchange is coarse and sloppy. In contrast, the Calibra's is light and quick, although the clutch action could be more progressive. A6W (0763) Here the elliptical the Calibra's is missing its nominal head, which is provided by the antecedent gearchange. Yet another anaphoric effect licensed by in contrast arises with respect to the notion of domain restriction, previously studied by, among many others, Lewis (1979) , Hinrichs (1988) and Hinrichs (1998), and von Fintel (1994) . (11) Few countries have satisfactory legislation on pesticides or the trained manpower to enforce it. In contrast, extensive use of pesticides in Europe, North America and Japan is backed by government legislation or voluntary schemes that provide farmers with detailed advice. B7G (0726) Note that the domain of the set of countries in the quantified NP few countries is subsequently narrowed so as to not include countries in Europe, North America and Japan. It is precisely the explicitly mentioned contrast that leads to this effect. <cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial. (12) a. Stack five blocks on top of one another.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_6",
  "x": "In contrast, extensive use of pesticides in Europe, North America and Japan is backed by government legislation or voluntary schemes that provide farmers with detailed advice. B7G (0726) Note that the domain of the set of countries in the quantified NP few countries is subsequently narrowed so as to not include countries in Europe, North America and Japan. It is precisely the explicitly mentioned contrast that leads to this effect. <cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial. (12) a. Stack five blocks on top of one another. Now close your eyes and try knocking the tower, this tower\u00a1 over with your nose.<cite> (Webber et al. (2003)</cite> , p. 552) b. Do you want an apple? Otherwise you can have a pear. (Webber et al. (2003), p. 552) Notice that the same kind of inference is required for contrast in example (13), providing further evidence for the anaphoric nature of this discourse connective. (13) Jack's heart lurched as he saw the ambulances and the busy, functional building and he immediately forgot everything they had been saying. \"I'll ask where he is,\" said Jamie Shepherd as they walked towards the reception desk. In contrast to the outside, the area was softly carpeted, softly lit, as if illness and death had to be cushioned away, made to look as if they didn't exist. BPD(0200)",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_7",
  "x": "In contrast to the flowing design of fast roads, design elements are angular and of pedestrianscale, typified by low-level lamp posts which avoid the \"sea of light\" provided by high poles in traffic streets (Figure 4 .4). C8F (0297) In this text, which is on the topic of child safety, roads are never explicitly mentioned. Rather the concept of slow neighborhood roads can only be inferred from the description. The first explicit mention of the term road then refers to the opposite term fast roads. Comparison of in contrast with personal pronouns yields yet another similarity with other anaphoric expressions. Like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intrasententially, as in example (15). (15) In contrast to his predecessors who worked at all hours of the day Macmillan tended to keep office hours. B0H (0476) Another property that distinguishes anaphoric discourse adverbials from structural connectives in the sense of <cite>Webber et al. (2003)</cite> , i.e. coordinating and subordinating conjunctions, concerns the type of dependencies that the arguments of the types of connectives can enter into. While structural connectives only allow non-crossing adjacent material as their arguments, discourse adverbials may involve crossing dependencies among non-adjacent material -just like other anaphoric expressions. e.g. pronouns and definite descriptions.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_8",
  "x": "The discourse properties of in contrast are not just of interest from a purely theoretical perspective. Teufel and Moens (2002) and Siddharthan and Teufel (2007) In the previous section we established at some length that in contrast shares with other discourse adverbials its anaphoric behavior. This naturally raises the question whether the semantics that has been proposed for this class of expressions can be naturally generalized to the semantics of in contrast. Following earlier proposals by Hinrichs (1986) and Kamp and Reyle (1993) , <cite>Webber et al. (2003)</cite> assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events. For example, the two clauses in (16) refer to individual events, which are put in the sequence-relation by the adverbial then. There are at least two difficulties associated with modelling the semantics of in contrast as a two-place relation between events: The scope of the two arguments of the contrast relation often extends beyond descriptions of individual events, as illustrated by examples such as (13), where the contrast involves sets of events and states of affairs. Thus, at the very least, one would have to generalize the semantics of in contrast to relations between sets of events and states of affairs, with relations between single events or states of affairs as a special case. However, it is difficult to see how such a modified representation could be suitably generalized to adequately model examples as in (18). (18) The Holsteins also tend to have much more white in the coat so that the white areas predominate and they could almost be described as white-and-blacks in contrast to the black-and-white Friesian type. B0K (0438) (18) explicitly contrasts two sets of individuals, cows in this case, rather than events or states of affairs.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_9",
  "x": "The latter simply denote relations between events and/or states of affairs, namely those denoted by the two conjunct clauses. The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ---------------------------------- **CONCLUSION AND FUTURE WORK** This paper has presented a corpus-based study of the discourse connective in contrast. The corpus data were drawn from the British National Corpus (BNC) and were analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. In future work we plan to consider a wider range of contrast relations in discourse such as by comparison, contrary to and on the other hand in order to ascertain whether the properties of the discourse connective in contrast will generalize to these cases as well. A second line of research will investigate ways of automatically detecting comparison patterns and contrast pairs, which figure prominently in the compositional semantics of in contrast, by means of machine learning techniques.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_10",
  "x": "The latter simply denote relations between events and/or states of affairs, namely those denoted by the two conjunct clauses. The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ---------------------------------- **CONCLUSION AND FUTURE WORK** This paper has presented a corpus-based study of the discourse connective in contrast. The corpus data were drawn from the British National Corpus (BNC) and were analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. In future work we plan to consider a wider range of contrast relations in discourse such as by comparison, contrary to and on the other hand in order to ascertain whether the properties of the discourse connective in contrast will generalize to these cases as well. A second line of research will investigate ways of automatically detecting comparison patterns and contrast pairs, which figure prominently in the compositional semantics of in contrast, by means of machine learning techniques.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_0",
  "x": "Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages. ---------------------------------- **INTRODUCTION** Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016) ; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli\u0107 and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_1",
  "x": "More recent work has focused on reducing that constraint. Vuli\u0107 and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction.",
  "y": "motivation"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_2",
  "x": "In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation,",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_3",
  "x": "These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_4",
  "x": "In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation,",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_5",
  "x": "We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings. By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that where Tr(\u00b7) is the trace operator, the sum of all diagonal entries. The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_6",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings. By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that where Tr(\u00b7) is the trace operator, the sum of all diagonal entries.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_7",
  "x": "where Tr(\u00b7) is the trace operator, the sum of all diagonal entries. The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections. ---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> . To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_8",
  "x": "We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings. By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that where Tr(\u00b7) is the trace operator, the sum of all diagonal entries. The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_9",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_10",
  "x": "**ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> . To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the spelling of the word. This letter count vector is then scaled by a constant before being appended to the base word embedding. After appending, the resulting augmented vector is normalized to have magnitude 1. Mathematically, let A be an ordered set of characters (an alphabet), containing all characters appearing in both language's alphabets: Let O source and O target be the orthographic extension matrices for each language, containing counts of the characters appearing in each word w i , scaled by a constant factor c e : Then, we concatenate the embedding matrices and extension matrices: Finally, in the normalized embedding matrices X and Z , each row has magnitude 1:",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_11",
  "x": "Mathematically, let A be an ordered set of characters (an alphabet), containing all characters appearing in both language's alphabets: Let O source and O target be the orthographic extension matrices for each language, containing counts of the characters appearing in each word w i , scaled by a constant factor c e : Then, we concatenate the embedding matrices and extension matrices: Finally, in the normalized embedding matrices X and Z , each row has magnitude 1: These new matrices are used in place of X and Z in the self-learning process. ---------------------------------- **ORTHOGRAPHIC SIMILARITY ADJUSTMENT** This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_12",
  "x": "This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normalized edit distance function is denoted as NL(\u00b7, \u00b7). We define the orthographic similarity of two words w 1 and w 2 as log(2.0\u2212NL(w 1 , w 2 )). These similarity scores are used to form an orthographic similarity matrix S, where each entry corresponds to a source-target word pair. Each entry is first scaled by a constant factor c s . This matrix is added to the standard similarity matrix, XW Z T . The vocabulary for each language is 200,000 words, so computing a similarity score for each pair would involve 40 billion edit distance calculations.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_13",
  "x": "This is linear with respect to the vocabulary size, as opposed to the quadratic complexity required for computing the entire matrix. However, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in <cite>Artetxe et al. (2017)</cite> . 1 However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel- Table 1 : Comparison of methods on test data.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_14",
  "x": "For example, the Italian-English pair modernomodern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2. The algorithm works by computing all strings formed by k or fewer deletions from each target word, stores them in a hash table, then does the same for each source word and generates sourcetarget pairs that share an entry in the hash table. The complexity of this algorithm can be expressed as O(|V |l k ), where V = V t \u222a V s is the combined vocabulary and l is the length of the longest word in V . This is linear with respect to the vocabulary size, as opposed to the quadratic complexity required for computing the entire matrix. However, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_15",
  "x": "We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in <cite>Artetxe et al. (2017)</cite> . 1 However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel- Table 1 : Comparison of methods on test data. Scaling constants c e and c s were selected based on performance on development data over all three language pairs. The last two rows report the results of using both methods together.",
  "y": "similarities uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_16",
  "x": "These numbers are perhaps unintuitively high. However, the corpora include many other characters, including diacritical markings and various symbols (%, [, !, etc.) that are an indication that tokenization of the data could be improved. We did not filter these characters in this work. For our experiments with orthographic similarity adjustment, the heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. and c s = 1 as our hyperparameters. The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them. Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_17",
  "x": "Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs. In addition to identifying orthographically similar words, the extension method is capable of learning a mapping between source and target letters, which could partially explain its improved performance over our edit distance method. Table 2 shows some correct translations from our system that were missed by the baseline. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this work, we presented two techniques (which can be combined) for improving embedding-based bilingual lexicon induction for related languages using orthographic information and no parallel data, allowing their use with low-resource language pairs.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_0",
  "x": "Even though the hard attention mechanism is thought to be non-differentiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful signal for our mechanism's attentional selection criterion. Because hard attention selects important features of the input information, it can also be more efficient than analogous soft attention mechanisms. This is especially important for recent approaches that use non-local pairwise operations, whereby computational and memory costs are quadratic in the size of the set of features. ---------------------------------- **INTRODUCTION** Visual attention is instrumental to many aspects of complex visual reasoning in humans [1, 2] . For example, when asked to identify a dog's owner among a group of people, the human visual system adaptively allocates greater computational resources to processing visual information associated with the dog and potential owners, versus other aspects of the scene. The perceptual effects can be so dramatic that prominent entities may not even rise to the level of awareness when the viewer is attending to other things in the scene [3, 4, 5] . Yet attention has not been a transformative force in computer vision, possibly because many standard computer vision tasks like detection, segmentation, and classification do not involve the sort of complex reasoning which attention is thought to facilitate. Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_1",
  "x": "Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> . Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated. This can improve accuracy by isolating important information and avoiding interference from unimportant information. Learning becomes more data efficient as the complexity of the interactions among different pieces of information reduces; this, loosely speaking, allows for more unambiguous credit assignment. By contrast, hard attention, in which only a subset of information is selected for further processing, is much less widely used.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_2",
  "x": "The perceptual effects can be so dramatic that prominent entities may not even rise to the level of awareness when the viewer is attending to other things in the scene [3, 4, 5] . Yet attention has not been a transformative force in computer vision, possibly because many standard computer vision tasks like detection, segmentation, and classification do not involve the sort of complex reasoning which attention is thought to facilitate. Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> . Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated. This can improve accuracy by isolating important information and avoiding interference from unimportant information.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_3",
  "x": "We rely on a canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] augmented with a hard attention mechanism that uses the L2-norms of the feature vectors to select subsets of the information for further processing. The first version, called the Hard Attention Network (HAN), selects a fixed number of feature vectors by choosing those with the top norms. The second version, called the Adaptive Hard Attention Network (AdaHAN), selects a variable number of feature vectors that depends on the input. Our results show that our algorithm can actually outperform comparable soft attention architectures on a challenging Visual QA task. This approach also produces interpretable hard attention masks, where the image regions which correspond to the selected features often contain semantically meaningful information, such as coherent objects. We also show strong performance when combined with a form of non-local pairwise model [26, 25, 27, 28] . This algorithm computes features over pairs of input features and thus scale quadratically with number of vectors in the feature map, highlighting the importance of feature selection. ---------------------------------- **RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_4",
  "x": "Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] . Thus, we focus on the recently-introduced VQA-CP <cite>[7]</cite> and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning. One of the core challenges of Visual QA is the problem of grounding language: that is, associating the meaning of a language term with a specific perceptual input [36] . Many works have tackled this problem [37, 38, 39, 40] , enforcing that language terms be grounded in the image. In contrast, our algorithm does not directly use correspondence between modalities to enforce such grounding but instead relies on learning to find a discrete representation that captures the required information from the raw visual input, and question-answer pairs. The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] . In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells. The visual representation is then the weighted-sum of the input tensor across space.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_5",
  "x": "---------------------------------- **RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] . Thus, we focus on the recently-introduced VQA-CP <cite>[7]</cite> and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning. One of the core challenges of Visual QA is the problem of grounding language: that is, associating the meaning of a language term with a specific perceptual input [36] . Many works have tackled this problem [37, 38, 39, 40] , enforcing that language terms be grounded in the image. In contrast, our algorithm does not directly use correspondence between modalities to enforce such grounding but instead relies on learning to find a discrete representation that captures the required information from the raw visual input, and question-answer pairs. The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_6",
  "x": "The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] . In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells. The visual representation is then the weighted-sum of the input tensor across space. The alternative is to select CNN grid cells in a discrete way, but due to many challenges in training non-differentiable architectures, such hard attention alternatives are severely under-explored. Notable exceptions include [6, 13, 14, 53, 54, 55] , but these run state-of-the-art object detectors or proposals to compute the hard attention maps. We argue that relying on such external tools is fundamentally limited: it requires costly annotations, and cannot easily adapt to new visual concepts that aren't previously labeled. Outside Visual QA and captioning, some prior work in vision has explored limited forms of hard attention. One line of work on discriminative patches builds a representation by selecting some patches and ignoring others, which has proved useful for object detection and classification [56, 57, 58] , and especially visualization [59] . However, such methods have recently been largely supplanted by end-to-end feature learning for practical vision problems.",
  "y": "motivation background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_7",
  "x": "Answering questions about images is often formulated in terms of predictive models [24] . These architectures maximize a conditional distribution over answers a, given questions q and images x: where A is a countable set of all possible answers. As is common in question answering<cite> [7,</cite> 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice). Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] . Questions and images are encoded into their vector representations. Next, the spatial encoding of the visual features is unraveled, and the question embedding is broadcasted and concatenated (or added) accordingly to form a multimodal representation of the inputs. Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_8",
  "x": "---------------------------------- **METHOD** Answering questions about images is often formulated in terms of predictive models [24] . These architectures maximize a conditional distribution over answers a, given questions q and images x: where A is a countable set of all possible answers. As is common in question answering<cite> [7,</cite> 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice). Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] . Questions and images are encoded into their vector representations.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_10",
  "x": "work<cite> [7,</cite> 9, 22, 23, 24, 25] . After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms<cite> [7,</cite> 8, 9, 10] at this point in the architecture. Finally, we aggregate features, using either sum-pooling, or relational [25, 27, 65] modules. We train the whole network end-to-end with a standard logistic regression loss over answer categories. ---------------------------------- **ATTENTION MECHANISMS** Here, we describe prior work on soft attention, and our approach to hard attention. Soft Attention. In most prior work, soft attention is implemented as a weighted mask over the spatial cells of the CNN representation. Let x := CN N (x), q := LST M (q) for image x and question q. We compute a weight w ij for every x ij (where i and j index spatial locations), using a neural network that takes x ij and q as input.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_11",
  "x": "VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture. CLEVR. This synthetic dataset [34] consists of 100K images of 3D rendered objects like spheres and cylinders, and roughly 1m questions that were automatically generated with a procedural engine. While the visual task is relatively simple, solving this dataset requires reasoning over complex relationships between many objects. ---------------------------------- **EFFECT OF HARD ATTENTION**",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_12",
  "x": "We then examine AdaHAN, which adaptively chooses the number of attended cells, and briefly investigate the effect of network depth and pretraining. Finally, we present qualitative results, and also provide results on CLEVR to show the method's generality. ---------------------------------- **DATASETS** VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture. CLEVR.",
  "y": "motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_13",
  "x": "We therefore repeat the above experiment with the non-local pairwise aggregation mechanism described in section 3, which computes activations for every pair of attended cells, and therefore scales quadratically with the number of at-tended cells. These results are shown in the middle of Table 1 , where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention. Finally, we compare standard soft attention baselines in the bottom of Table 1. In particular, we include previous results using a basic soft attention network<cite> [7,</cite> 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in<cite> [7,</cite> 9] with the same features used in other experiments. Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. The nonlocal pairwise aggregation performs better than SAN on its own, although the best result includes hard attention. Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor. Table 2 : Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation. Since AdaHAN adaptively selects relevant features, based on the fixed threshold 1 w * h , we report here the average number of attended parts.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_14",
  "x": "We consider a simple summation, and non-local pairwise computations as the aggregation tool. The fact that hard attention can work is interesting itself, but it should be especially useful for models that devote significant processing to each attended cell. We therefore repeat the above experiment with the non-local pairwise aggregation mechanism described in section 3, which computes activations for every pair of attended cells, and therefore scales quadratically with the number of at-tended cells. These results are shown in the middle of Table 1 , where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention. Finally, we compare standard soft attention baselines in the bottom of Table 1. In particular, we include previous results using a basic soft attention network<cite> [7,</cite> 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in<cite> [7,</cite> 9] with the same features used in other experiments. Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. The nonlocal pairwise aggregation performs better than SAN on its own, although the best result includes hard attention. Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor. Table 2 : Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation.",
  "y": "differences"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_15",
  "x": "We do not perform a hyper-parameter search as there is no separated validation set available. Instead, we rather choose default hyper-parameters based on our prior experience on Visual QA datasets. We trained our models until we notice a saturation on the training set. Then we evaluate these models on the test set. Our tables show the performance of all the methods wrt. the second digits precision obtained by rounding. Table 1 shows SAN's [9] results reported by <cite>[7]</cite> together with our in-house implementation (denoted as \"ours\"). Our implementation has 2 attention hops, 1024 dimensional multimodal embedding size, a fixed learning rate 0.0001, and ResNet-101. In these experiments we pool the attended representations by weighted average with the attention weights. Our in-house implementation of the nonlocal pairwise mechanism strongly resembles implementations of [26] , and [27] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_16",
  "x": "We compare our method against our implementation of the SAN method trained using the same simple convolutional neural network. We call the models: simple-SAN, and simple-HAN. Analysis. In our experiments, simple-SAN achieves about 21% performance on the test set. Surprisingly, simple-HAN+sum achieves about 24% performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture [67] ; the results are reported by <cite>[7]</cite> . This result shows that the hard attention mechanism can indeed be tightly coupled within the training process, and that the whole procedure does not rely heavily on the properties of the ImageNet pre-trained networks. In a sense, we see that a discrete notion of entities also \"emerges\" through the learning process, leading to efficient training. Implementation Details. In our experiments we use a simple CNN built of: 1 layer with 64 filters and 7-by-7 filter size followed up by 2 layers with 256 filters and 2 layers with 512 filters, all with 3-by-3 filter size. We use strides 2 for all the layers.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_0",
  "x": "Although authors may explicitly define coherent segments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007) . Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000) , more recent methods (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013 ) and a measure of semantic relatedness of short texts (\u0160ari\u0107 et al., 2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences. We then derive segments using the maximal cliques of such similarity graphs. The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_1",
  "x": "Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013 ) and a measure of semantic relatedness of short texts (\u0160ari\u0107 et al., 2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences. We then derive segments using the maximal cliques of such similarity graphs. The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos. ---------------------------------- **RELATED WORK** Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing. Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 ). Hearst (1994 introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_2",
  "x": "Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert\u00f6z et al., 2004) . Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_3",
  "x": "Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing. Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 ). Hearst (1994 introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_4",
  "x": "More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert\u00f6z et al., 2004) . Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks. Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments. ---------------------------------- **TEXT SEGMENTATION ALGORITHM**",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_5",
  "x": "Segmenting such artificial texts is easier than segmenting real-world documents. This is why besides on the artificial Choi dataset we also evaluate GRAPHSEG on a real-world dataset of political texts from the Manifesto Project, 2,3 manually labeled by domain experts with segments of seven different topics (e.g., economy and welfare, quality of life, foreign affairs). The selected manifestos contain between 1000 and 2500 sentences, with segments ranging in length from 1 to 78 sentences, which is in sharp contrast to the Choi dataset where all segments are of similar size. ---------------------------------- **EXPERIMENTAL SETTING** To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_6",
  "x": "The selected manifestos contain between 1000 and 2500 sentences, with segments ranging in length from 1 to 78 sentences, which is in sharp contrast to the Choi dataset where all segments are of similar size. ---------------------------------- **EXPERIMENTAL SETTING** To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_7",
  "x": "The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments. WindowDiff is a stricter version of P k as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard. Lower scores indicate better performance for both these metrics. The GRAPHSEG algorithm has two parameters: (1) the sentence similarity treshold \u03c4 which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small. In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_8",
  "x": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_9",
  "x": "Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments. WindowDiff is a stricter version of P k as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard. Lower scores indicate better performance for both these metrics. The GRAPHSEG algorithm has two parameters: (1) the sentence similarity treshold \u03c4 which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small. In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters. In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_10",
  "x": "The GRAPHSEG algorithm has two parameters: (1) the sentence similarity treshold \u03c4 which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small. In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters. In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself.",
  "y": "differences"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_11",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself. Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents -some of which belong to the the training set and others to the test set, as admitted by <cite>Riedl and Biemann (2012)</cite> and this is why their reported performance on this dataset is overestimated. In Table 3 we report the results on the Manifesto dataset. Results of both TopicTiling and GRAPHSEG indicate that the realistic Manifesto dataset is much more difficult to segment than the artificial Choi dataset. The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Student's t-test).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_12",
  "x": "The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Student's t-test). In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not significantly improve the performance for neither of the two models. This result contrasts previous findings (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods' with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset. ---------------------------------- **CONCLUSION** In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation. GRAPHSEG employs word embeddings and extends a measure of semantic relatedness to construct a relatedness graph with edges established between semantically related sentences. The segmentation is then determined by the maximal cliques of the relatedness graph and improved by semantic comparison of adjacent segments. GRAPHSEG displays competitive performance compared to best-performing LDA-based methods on a synthetic dataset. However, we identify and discuss evaluation issues pertaining to LDA-based TS on this dataset.",
  "y": "differences"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_0",
  "x": "---------------------------------- **INTRODUCTION** Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 .",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_1",
  "x": "Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 . Nevertheless, we argue that this domain-mismatch problem can be alleviated by using English text in the target-domain as a pivot language (Guo et al., 2019) . To this end, we explore this idea on the task of neural data-to-text generation which has been the subject of much recent research. Neural data-to-Pidgin generation is essential in the African continent especially given the fact that many English There is a pub Blue Spice located in the centre of the city that provides Chinese food. Pidgin",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_2",
  "x": "We first train a data-to-English text generation system, before employing techniques in unsupervised neural machine translation and self-training to establish the Pidgin-to-English cross-lingual alignment. The human evaluation performed on the generated Pidgin texts shows that, though still far from being practically usable, the pivoting + self-training technique improves both Pidgin text fluency and relevance. ---------------------------------- **INTRODUCTION** Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system.",
  "y": "background motivation"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_3",
  "x": "By this means, English-based NLG systems can be locally adapted by translating the output English text into pidgin English. We employ the publicly available parallel data-to-text corpus E2E (Novikova et al., 2017) consisting of tabulated data and English descriptions in the restaurant domain. The training of the in-domain MT system is done with a two-step process: (1) We use the target-side English texts as the pivot, and train an unsupervised NMT (model unsup ) directly between in-domain English text and the available monolingual Pidgin corpus. (2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input.",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_4",
  "x": "This work aims at bridging the gap between many of these English-based systems and Pidgin by training an in-domain English-to-pidgin MT system in an unsupervised way. By this means, English-based NLG systems can be locally adapted by translating the output English text into pidgin English. We employ the publicly available parallel data-to-text corpus E2E (Novikova et al., 2017) consisting of tabulated data and English descriptions in the restaurant domain. The training of the in-domain MT system is done with a two-step process: (1) We use the target-side English texts as the pivot, and train an unsupervised NMT (model unsup ) directly between in-domain English text and the available monolingual Pidgin corpus. (2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup .",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_5",
  "x": "(2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input. We augment this dataset to the existing monolingual corpus. The self-training step involves further updating model unsup on the pseudo parallel corpus and non-parallel monolingual corpus to yield model self . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_0",
  "x": "Although we often come across texts from different domains such as scientific papers, news articles and blogs, which are labeled with keyphrases by the authors, a large portion of the Web content remains untagged. While keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial. Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999) , Turney (2000; , Hulth (2003) , Medelyan et al. (2009)) . A disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains. Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_1",
  "x": "Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue. Worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated. ---------------------------------- **CONSEQUENTLY, WE HAVE LITTLE UNDERSTANDING OF HOW EFFECTIVE THE STATE-OF THE-ART SYSTEMS WOULD BE ON A COMPLETELY NEW DATASET FROM A DIFFERENT DOMAIN.** A few questions arise naturally. How would these systems perform on a different dataset with their original configuration? What could be the underlying reasons in case they perform poorly? Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions.",
  "y": "motivation"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_2",
  "x": "Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ---------------------------------- **CORPORA** Our four evaluation corpora belong to different domains with varying document properties. Table 1 provides an overview of each corpus.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_3",
  "x": "More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ---------------------------------- **CORPORA** Our four evaluation corpora belong to different domains with varying document properties. Table 1 provides an overview of each corpus. The DUC-2001 dataset (Over, 2001) , which is a collection of 308 news articles, is annotated by Wan and Xiao (2008) . We report results on all 308 articles in our evaluation.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_4",
  "x": "**CONSEQUENTLY, WE HAVE LITTLE UNDERSTANDING OF HOW EFFECTIVE THE STATE-OF THE-ART SYSTEMS WOULD BE ON A COMPLETELY NEW DATASET FROM A DIFFERENT DOMAIN.** A few questions arise naturally. How would these systems perform on a different dataset with their original configuration? What could be the underlying reasons in case they perform poorly? Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_5",
  "x": "This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by Hulth (2003) and later by<cite> Mihalcea and Tarau (2004)</cite> and Liu et al. (2009b) . In our evaluation, we use the set of 500 abstracts designated by these previous approaches as the test set and its set of uncontrolled keyphrases. Note that the average document length for this dataset is the smallest among all our datasets. The NUS Keyphrase Corpus (Nguyen and Kan, 2007) includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. We use all the 211 papers in our evaluation. Since the number of annotators can be different for different documents and the annotators are not specified along with the annotations, we decide to take the union 1 See http://www.hlt.utdallas.edu/ saidul/code.html for details. of all the gold standard keyphrases from all the sets to construct one single set of annotation for each paper. As Table 1 shows, each NUS paper, both in terms of the average number of tokens (8291) and candidate phrases (2027) per paper, is more than five times larger than any document from any other corpus. Hence, the number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_6",
  "x": "A generic unsupervised keyphrase extraction system typically operates in three steps (Section 3.1), which will help understand the unsupervised systems explained in Section 3.2. ---------------------------------- **GENERIC KEYPHRASE EXTRACTOR** Step 1: Candidate lexical unit selection The first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. Commonly used heuristics include (1) using a stop word list to remove non-keywords (e.g., Liu et al. (2009b) ) and (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be considered candidate keywords<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009a) , Wan and Xiao (2008) ). In all of our experiments, we follow Wan and Xiao (2008) and select as candidates words with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova and Manning, 2000) . Table 1 : Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase. The U/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other higher order n-grams. Step 2: Lexical unit ranking Once the candidate list is generated, the next task is to rank these lexical units.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_7",
  "x": "Table 1 : Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase. The U/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other higher order n-grams. Step 2: Lexical unit ranking Once the candidate list is generated, the next task is to rank these lexical units. To accomplish this, it is necessary to build a representation of the input text for the ranking algorithm. Depending on the underlying approach, each candidate word is represented by its syntactic and/or semantic relationship with other candidate words. The relationship can be defined using co-occurrence statistics, external resources (e.g., neighborhood documents, Wikipedia), or other syntactic clues. Step 3: Keyphrase formation In the final step, the ranked list of candidate words is used to form keyphrases. A candidate phrase, typically a sequence of nouns and adjectives, is selected as a keyphrase if (1) it includes one or more of the top-ranked candidate words<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009b) ), or (2) the sum of the ranking scores of its constituent words makes it a top scoring phrase (Wan and Xiao, 2008) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_8",
  "x": "In the TextRank algorithm <cite>(Mihalcea and Tarau, 2004)</cite> , a text is represented by a graph. Each vertex corresponds to a word type. A weight, w ij , is assigned to the edge connecting the two vertices, v i and v j , and its value is the number of times the corresponding word types co-occur within a window of W words in the associated text. The goal is to (1) compute the score of each vertex, which reflects its importance, and then (2) use the word types that correspond to the highestscored vertices to form keyphrases for the text. The score for v i , S(v i ), is initialized with a default value and is computed in an iterative manner until convergence using this recursive formula: where Adj(v i ) denotes v i 's neighbors and d is the damping factor set to 0.85 (Brin and Page, 1998) . Intuitively, a vertex will receive a high score if it has many high-scored neighbors. As noted before, after convergence, the T % top-scored vertices are selected as keywords. Adjacent keywords are then collapsed and output as a keyphrase. According to<cite> Mihalcea and Tarau (2004)</cite> , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_9",
  "x": "The score for v i , S(v i ), is initialized with a default value and is computed in an iterative manner until convergence using this recursive formula: where Adj(v i ) denotes v i 's neighbors and d is the damping factor set to 0.85 (Brin and Page, 1998) . Intuitively, a vertex will receive a high score if it has many high-scored neighbors. As noted before, after convergence, the T % top-scored vertices are selected as keywords. Adjacent keywords are then collapsed and output as a keyphrase. According to<cite> Mihalcea and Tarau (2004)</cite> , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words. Hence, our implementation of TextRank follows this configuration. ---------------------------------- **SINGLERANK** SingleRank (Wan and Xiao, 2008 ) is essentially a TextRank approach with three major differences.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_10",
  "x": "Three clustering algorithms are used of which spectral clustering yields the best score. Once the clusters are formed, one representative word, called an exemplar term, is picked from each cluster. Finally, KeyCluster extracts from the document all the longest n-grams starting with zero or more adjectives and ending with one or more nouns, and if such an n-gram includes one or more exemplar words, it is selected as a keyphrase. As a post-processing step, a frequent word list generated from Wikipedia is used to filter out the frequent unigrams that are selected as keyphrases. ---------------------------------- **EVALUATION** ---------------------------------- **EXPERIMENTAL SETUP** TextRank and SingleRank setup Following<cite> Mihalcea and Tarau (2004)</cite> and Wan and Xiao (2008) , we set the co-occurrence window size for TextRank and SingleRank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets. ExpandRank setup Following Wan and Xiao (2008), we find the 5 nearest neighbors for each document from the remaining documents in the same corpus.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_11",
  "x": "We generate the curves for each system as follows. For Tf-Idf, SingleRank, and ExpandRank, we vary the number of keyphrases, N , predicted by each system. On average, TextRank performs much worse compared to Tf-Idf. This certainly gives more insight into TextRank since it was evaluated on Inspec only for T=33% by<cite> Mihalcea and Tarau (2004)</cite> .",
  "y": "differences"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_12",
  "x": "Two points deserve mention. First, in comparison to SingleRank and ExpandRank, Tf-Idf outputs fewer keyphrases to achieve its best F-score on most datasets. Second, the systems output more keyphrases on NUS than on other datasets to achieve their best F-scores (e.g., 60 for Tf-Idf, 190 for SingleRank, and 177 for ExpandRank). This can be attributed in part to the fact that the F-scores on NUS are low for all the systems and exhibit only slight changes as we output more phrases. Our re-implementations Do our duplicated systems yield scores that match the original scores? Table 3 sheds light on this question. First, consider KeyCluster, where our score lags behind the original score by approximately 5%. An examination of Liu et al.'s (2009b) results reveals a subtle caveat in keyphrase extraction evaluations. In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_13",
  "x": "In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec. We score 2 and 5 points less than Wan and Xiao's (2008) implementations of SingleRank and ExpandRank, respectively. We speculate that document pre-processing (e.g., stemming) has contributed to the discrepancy, but additional experiments are needed to determine the reason. SingleRank vs. TextRank Figure 1 shows that SingleRank behaves very differently from TextRank. As mentioned in Section 3.2.3, the two algorithms differ in three major aspects. To determine which aspect is chiefly responsible for the large difference in their performance, we conduct three \"ablation\" experiments. Each experiment modifies exactly one of these aspects in SingleRank so that it behaves like TextRank, effectively ensuring that the two algorithms differ only in the remaining two aspects. More specifically, in the three experiments, we (1) change SingleRank's window size to 2, (2) build an unweighted graph for SingleRank, and (3) incorporate TextRank's way of forming keyphrases into SingleRank, respectively.",
  "y": "similarities"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_0",
  "x": "This factorization encodes dependencies between textual patterns and structured relations using lowdimensional vectors defined for each entity pair; although these factors are effective at combining evidence for an entity pair, they are inaccurate on rare pairs, or for relations that depend crucially on the entity types. On the other hand, tensor factorization is able to overcome these shortcomings when applied to link prediction by maintaining entity-wise factors. However these models have been unsuitable for universal schema. In this paper we first present an illustration on synthetic data that explains the unsuitability of tensor factorization to relation extraction with universal schemas. Since the benefits of tensor and matrix factorization are complementary, we then investigate two hybrid methods that combine the benefits of the two paradigms. We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. ---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; .",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_1",
  "x": "On the other hand, tensor factorization is able to overcome these shortcomings when applied to link prediction by maintaining entity-wise factors. However these models have been unsuitable for universal schema. In this paper we first present an illustration on synthetic data that explains the unsuitability of tensor factorization to relation extraction with universal schemas. Since the benefits of tensor and matrix factorization are complementary, we then investigate two hybrid methods that combine the benefits of the two paradigms. We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. ---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_2",
  "x": "On the other hand, tensor factorization is able to overcome these shortcomings when applied to link prediction by maintaining entity-wise factors. However these models have been unsuitable for universal schema. In this paper we first present an illustration on synthetic data that explains the unsuitability of tensor factorization to relation extraction with universal schemas. Since the benefits of tensor and matrix factorization are complementary, we then investigate two hybrid methods that combine the benefits of the two paradigms. We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. ---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_3",
  "x": "**UNIVERSAL SCHEMA** A universal schema is defined as the union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base<cite> (Riedel et al., 2013)</cite> . The task here is to complete this schema by jointly reasoning over surface form patterns and relations. A successful approach to this joint reasoning is to embed both kinds of relations into the same low-dimensional embedding space, which can be achieved by matrix or tensor factorization methods. We will study such representations for universal schema in this paper. ---------------------------------- **MATRIX FACTORIZATION WITH FACTORS OVER ENTITY-PAIRS** In matrix factorization for universal schema,<cite> Riedel et al. (2013)</cite> construct a sparse binary matrix of size |P| \u00d7 |R| whose rows are indexed by entity-pairs (a, b) \u2208 P and columns by surface form and Freebase relations s \u2208 R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r \u2208 R |R|\u00d7k and entity-pair factors p \u2208 R |P|\u00d7k , the probability of a relation s and two entities a and b is: where \u03c3 is the sigmoid function. Using this factorization, similar entity-pairs and relations are embedded close to each other in a k-dimensional vector space.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_4",
  "x": "We will study such representations for universal schema in this paper. ---------------------------------- **MATRIX FACTORIZATION WITH FACTORS OVER ENTITY-PAIRS** In matrix factorization for universal schema,<cite> Riedel et al. (2013)</cite> construct a sparse binary matrix of size |P| \u00d7 |R| whose rows are indexed by entity-pairs (a, b) \u2208 P and columns by surface form and Freebase relations s \u2208 R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r \u2208 R |R|\u00d7k and entity-pair factors p \u2208 R |P|\u00d7k , the probability of a relation s and two entities a and b is: where \u03c3 is the sigmoid function. Using this factorization, similar entity-pairs and relations are embedded close to each other in a k-dimensional vector space. Since this model uses embeddings for pairs of entities, as opposed to per-entity embeddings, we refer to such models as pairwise models. Pairwise embeddings are especially suitable when working with universal schema data, since they can represent correlations between surface pattern relations and structured relations compactly. Furthermore, they combine multiple evidences specific to an entity-pair to predict a relation between them. Since the observed data matrix contains only true entries, the parameters are learned using Bayesian personalized Ranking (Rendle et al., 2009 ) that supports implicit feedback.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_5",
  "x": "**TRANSE** Another formulation that is based on entity representations is the translating embeddings model by Bordes et al. (2013) . The idea is that if a relation s between two entities a and b holds, that relation's vector representation r s should translate the representation e a to the second argument e b , i.e., In this work we use a variant of TransE in which different embeddings are learned for an entity for each argument position. ---------------------------------- **MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_6",
  "x": "---------------------------------- **MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ---------------------------------- **COMBINED TENSOR AND MATRIX FACTORIZATION FOR UNIVERSAL SCHEMA** In the previous section, we provided background on matrix factorization with pairwise factors, followed by a tensor factorization based formulation of universal schema. Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations.",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_7",
  "x": "Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ---------------------------------- **COMBINED TENSOR AND MATRIX FACTORIZATION FOR UNIVERSAL SCHEMA** In the previous section, we provided background on matrix factorization with pairwise factors, followed by a tensor factorization based formulation of universal schema. Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations. It is worth noting that tensor factorization for universal schema has been proposed by , who also observed that tensor factorization by itself performs poorly (even with additional type constraints), and the predictions need to be combined with matrix factorization to be accurate. In this section we will present the fundamental differences between matrix and tensor factorization, and examine a few hybrid models that can address these concerns.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_8",
  "x": "---------------------------------- **COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_9",
  "x": "**COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ---------------------------------- **PARAMETER ESTIMATION**",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_10",
  "x": "---------------------------------- **COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ----------------------------------",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_11",
  "x": "**PARAMETER ESTIMATION** As by<cite> Riedel et al. (2013)</cite> , we use a Bayesian personalized ranking objective (Rendle et al., 2009 ) to estimate parameters, i.e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using AdaGrad. For all models we use k = 100 as dimension of latent representations, an initial learning rate of 0.1, and 2 -regularization of all parameters with a weight of 0.01. For CANDECOMP/PARAFAC and RESCAL we use the open-source scikit-tensor 2 package with default hyper-parameters. ---------------------------------- **EXPERIMENTS** In order to evaluate whether the hybrid models are able to effectively combine the benefits of matrix and tensor factorization, we first present experiments on synthetic data in Section 4.1. For a more realworld evaluation, we also experiment with universal schema for distantly-supervised relation extraction in Section 4.2. ---------------------------------- **SYNTHETIC RGB RELATIONS**",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_12",
  "x": "---------------------------------- **R13-F TR-R13** Although the same rank corresponds to different numbers of parameters for each method, the trend clearly indicates these results do not depend significantly on the number of parameters. ---------------------------------- **UNIVERSAL SCHEMA RELATION EXTRACTION** With the promising results shown on synthetic data, we now turn to evaluation on real-world information extraction. In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview). In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model .",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_13",
  "x": "Finally, on the Blue relation on which matrix and tensor factorization fare poorly, the combined approaches are able to obtain high accuracy, in particular achieve close to 90% average precision with only a rank of 5. ---------------------------------- **R13-F TR-R13** Although the same rank corresponds to different numbers of parameters for each method, the trend clearly indicates these results do not depend significantly on the number of parameters. ---------------------------------- **UNIVERSAL SCHEMA RELATION EXTRACTION** With the promising results shown on synthetic data, we now turn to evaluation on real-world information extraction. In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview).",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_14",
  "x": [
   "In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in . Models that use the matrix factorization (F, FE, R13-F and RFE) are significantly better, but more importantly, the hybrid appraoch FE achieves the highest accuracy. It is unclear why RFE fails to provide similar gains, in particular, performing slightly worse than matrix factorization. Note that we are not introducing a new state-of-art here, the neighborhood model (NF) that achieves a higher accuracy is omitted for clarity. Table 2 : Nearest-Neighbors for a few randomlyselected entities based on their embeddings, demonstrating that similar entities are close to each other. ---------------------------------- **ENTITY EMBEDDINGS AND TYPES** Although the focus of this work is relation extraction, and the models are trained primarily for finding relations, in this section we explore the learned entity embeddings."
  ],
  "y": "differences"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_0",
  "x": "**ABSTRACT** We present the first open-source graphical annotation tool for combinatory categorial grammar (CCG), and the first set of detailed guidelines for syntactic annotation with CCG, for four languages: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. ---------------------------------- **INTRODUCTION** Combinatory Categorial Grammar (CCG; Steedman, 2000 ) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch.",
  "y": "background"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_1",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation. Their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents. Even though CCG is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon. Moreover, these annotation tools are not open-source and cannot easily be deployed to support other annotation efforts. In this paper, we present an open-source, lightweight, easy-to-use graphical annotation tool that employs a statistical parser to create initial CCG derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints. Together, these constraints make it possible to effect (almost) all annotation decisions consistent with the principles of CCG.",
  "y": "motivation"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_2",
  "x": "**A QUADRILINGUAL PILOT CCG TREEBANK** To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019) , each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank <cite>(Hockenmaier and Steedman, 2007)</cite> , which is in turn based on the Penn Treebank (Marcus et al., 1993) . Since CCGrebank only covers English and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed. We initially annotated ten sentences in four languages and discussed disagreements. The results were recorded in an initial annotation manual, and the initial annotations were discarded. Each of the remaining 4x100 sentences was then annotated independently by at least two of the authors. Table 1 (upper part) shows the number of nonoverlapping category and span constraints that each annotator created on average per sentence before marking the sentence as correct. Annotated sentences were manually classified by the first author into four classes: (0) sentences without any disagreements, (1) sentences with only trivial violations of the annotation guidelines (e.g., concerning attachment of punctuation or underspecifying modifier features), (2) sentences with only apparent oversights, such as giving a determiner a pronoun category, (3) sentences with more intricate disagreements which required additional guidelines to resolve. Table 1 (upper part) shows the distribution of disagreement classes, and Table 2 shows examples of class (3).",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_0",
  "x": "This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) .",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_1",
  "x": "Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes.",
  "y": "similarities"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_2",
  "x": "We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_3",
  "x": "Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 ---------------------------------- **INTRODUCTION** Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) .",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_4",
  "x": "**INTRODUCTION** Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_5",
  "x": "---------------------------------- **INTRODUCTION** Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_6",
  "x": "---------------------------------- **DATASET** We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community. We will evaluate the parser via Labeled Attachment Score (LAS). We will use Bikel's randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees.",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_7",
  "x": "We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community. We will evaluate the parser via Labeled Attachment Score (LAS). We will use Bikel's randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees. Although in general the results from parsing Pennconverter's output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels. For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007) as our baseline. ---------------------------------- **SEMANTIC REPRESENTATION AND DISAMBIGUATION METHODS** We will experiment with the range of semantic representations used in <cite>Agirre et al. (2008)</cite> , all of which are based on WordNet 2.1.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_8",
  "x": "We will use Bikel's randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees. Although in general the results from parsing Pennconverter's output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels. For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007) as our baseline. ---------------------------------- **SEMANTIC REPRESENTATION AND DISAMBIGUATION METHODS** We will experiment with the range of semantic representations used in <cite>Agirre et al. (2008)</cite> , all of which are based on WordNet 2.1. Words in WordNet (Fellbaum, 1998) are organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_9",
  "x": "It is interesting to mention that, although not shown on the tables, using lemmatization to assign semantic classes to wordforms gave a slight increase for all the tests (0.1 absolute point approximately), as it helped to avoid data sparseness. We applied Schmid's (1994) TreeTagger. This can be seen as an argument in favour of performing morphological analysis, an aspect that is many times neglected when processing morphologically poor languages as English. We also did some preliminary experiments using Koo et al.'s (2008) word clusters, both independently and also combined with the WordNetbased features, without noticeable improvements. ---------------------------------- **CONCLUSIONS** We tested the inclusion of several types of semantic information, in the form of WordNet semantic classes in a dependency parser, showing that: \u2022 Semantic information gives an improvement on a transition-based deterministic dependency parsing. \u2022 Feature combinations give an improvement over using a single feature. <cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_10",
  "x": "**CONCLUSIONS** We tested the inclusion of several types of semantic information, in the form of WordNet semantic classes in a dependency parser, showing that: \u2022 Semantic information gives an improvement on a transition-based deterministic dependency parsing. \u2022 Feature combinations give an improvement over using a single feature. <cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 . Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. \u2022 The present work presents a statistically significant improvement for the full treebank using WordNet-based semantic information for the first time. Our results extend those of <cite>Agirre et al. (2008)</cite> , which showed improvements on a subset of the PTB.",
  "y": "extends differences"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_0",
  "x": "The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models (Vaswani et al., 2017) . This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 (Radford et al., 2018 (Radford et al., , 2019 and ELMo (Peters et al., 2018) . BERT <cite>(Devlin et al., 2019)</cite> improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later reimplemented, critically evaluated and improved in the RoBERTa model . These large-scale transformer models provide the advantage of being able to solve NLP tasks by having a common, expensive pre-training phase, followed by a smaller fine-tuning phase. The pretraining happens in an unsupervised way by providing large corpora of text in the desired language. The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks. While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_1",
  "x": "While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) . However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019) . Training a RoBERTa model on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks. In this paper, we introduce RobBERT 1 , a Dutch RoBERTabased pre-trained language model, and critically test its performance using natural language tasks against other Dutch languages models. ---------------------------------- **RELATED WORK** Transformer models have been successfully used for a wide range of language tasks. Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner (Vaswani et al., 2017) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_2",
  "x": "For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) . However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019) . Training a RoBERTa model on a Dutch dataset thus has a lot of potential for increasing performance for many downstream Dutch NLP tasks. In this paper, we introduce RobBERT 1 , a Dutch RoBERTabased pre-trained language model, and critically test its performance using natural language tasks against other Dutch languages models. ---------------------------------- **RELATED WORK** Transformer models have been successfully used for a wide range of language tasks. Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner (Vaswani et al., 2017) . This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <cite>(Devlin et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_3",
  "x": "Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner (Vaswani et al., 2017) . This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <cite>(Devlin et al., 2019)</cite> . BERT is pre-trained with large corpora of text using two unsupervised tasks. The first task is word masking (also called the Cloze task (Taylor, 1953) or masked language model (MLM)), where the model has to guess which word is masked in certain position in the text. The second task is next sentence prediction. This is done by predicting if two sentences are subsequent in the corpus, or if they are randomly sampled from the corpus. These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks. This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures <cite>(Devlin et al., 2019)</cite> . Transformer models are also capable of generating contextualized word embeddings.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_4",
  "x": "As mentioned before, BERT uses next sentence prediction (NSP) as one of its two training tasks. In NSP, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora. The authors of RoBERTa ) showed that while this task made the model achieve a better performance, it was not due to its intended reason, as it might merely predict relatedness rather than subsequent sentences. That<cite> Devlin et al. (2019)</cite> trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every input. Other research improved the NSP task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order (Lan et al., 2019) . Devlin et al. (2019) also presented a multilingual model (mBERT) with the same architecture as BERT, but trained on Wikipedia corpora in 104 languages. Unfortunately, the quality of these multilingual embeddings is often considered worse than their monolingual counterparts. R\u00f6nnqvist et al. (2019) illustrated this difference in quality for German and English models in a generative setting. The monolingual French CamemBERT model (Martin et al., 2019 ) also compared their model to mBERT, which performed poorer on all tasks.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_5",
  "x": "**TRAINING** RobBERT shares its architecture with RoBERTa's base model, which itself is a replication and improvement over BERT . The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads <cite>(Devlin et al., 2019)</cite> . One difference with the original BERT is due to the different pre-training task specified by RoBERTa, using only the MLM task and not the NSP task. The training thus only uses word masking, where the model has to predict which words were masked in certain positions of a given line of text. The training process uses the Adam optimizer (Kingma and Ba, 2017) with polynomial decay of the learning rate l r = 10 \u22126 and a ramp-up period of 1000 iterations, with parameters \u03b2 1 = 0.9 (a common default) and RoBERTa's default \u03b2 2 = 0.98. Additionally, we also used a weight decay of 0.1 as well as a small dropout of 0.1 to help prevent the model from overfitting (Srivastava et al., 2014) . We used a computing cluster in order to efficiently pre-train our model. More specifically, the pre-training was executed on a computing cluster with 20 nodes with 4 Nvidia Tesla P100 GPUs (16 GB VRAM each) and 2 nodes with 8 Nvidia V100 GPUs (having 32 GB VRAM each). This pretraining happened in fixed batches of 8192 sentences by rescaling each GPUs batch size depending on the number of GPUs available, in order to maximally utilize the cluster without blocking it entirely for other users.",
  "y": "similarities"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_6",
  "x": "This allows us to compare the zero-shot BERT models, i.e. without any fine-tuning after pre-training, for which the results can be seen in Table 2 . The second approach uses the same data, but creates two sentences by filling in the mask with both \"die\" and \"dat\", ap-pending both with the [SEP] token and making the model predict which of the two sentences is correct. The fine-tuning was performed using 4 Nvidia GTX 1080 Ti GPUs and evaluated against the same test set of 399k utterances. As before, we fine-tuned the model twice: once with the full training set and once with a subset of 10k utterances from the training set for illustrating the benefits of pre-training on low-resource tasks. ZeroR (majority class) 66.70 mBERT <cite>(Devlin et al., 2019)</cite> 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03 RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table 1 and Table 2 ). It is also able to reach similar performance using less data. The fact that zero-shot RobBERT outperforms other zero-shot BERT models is also an indication that the base model has internalised more knowledge about Dutch than the other two have. The reason RobBERT and other BERT models outperform the previous RNN-based approach is likely the transformers ability to deal better with coreference resolution , and by extension better in deciding which word the \"die\" or \"dat\" belongs to. ----------------------------------",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_0",
  "x": "**319** text material. To deal with phonological variability alternate pronunciations are included in the lexicon, and optional phonological rules are applied during training and recognition. The recognizer uses a time-synchronous graph-search strategy [16] for a first pass with a bigram back-off language model (LM) [10] . A trigram LM is used in a second acoustic decoding pass which makes use of the word graph generated using the bigram LM [6] . Experimental results are reported on the ARPA Wall Street Journal (WSJ) <cite>[19]</cite> and BREF [14] corpora, using for both corpora over 37k utterances for acoustic training and more than 37 million words of newspaper text for language model training. While the number of speakers is larger for WSJ, the total amount of acoustic training material is about the same (see Table 1 ). It is shown that for both corpora increasing the amount of training utterances by an order of magnitude reduces the word error by about 30%. The use of a trigram LM in a second pass also gives an error reduction of 20% to 30%. The combined error reduction is on the order of 50%.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_1",
  "x": "**LANGUAGE MODELING** Language modeling entails incorporating constraints on the allowable sequences of words which form a sentence. Statistical n-gram models attempt to capture the syntactic and semantic constraints by estimating the frequencies of sequences of n words. In this work bigram and trigram language models are estimated on the training text material for each corpus. This data consists of 37M words of the WSJ 1 and 38M words of Le Monde. A backoff mechanism [10] is used to smooth the estimates of the probabilities of rare n-grams by relying on a lower order n-gram when there is insufficient training data, and to provide a means of modeling unobserved n-grams. Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_2",
  "x": "This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:. In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts <cite>[19]</cite> . The main differences are in the treatment of compound words, abbreviations, and case. In BREF the distinction between the cases is kept if it designates a distinctive graphemic feature, but not when the upper case is simply due to the fact that the word occurs at the beginning of the sentence. Thus, the first word of each sentence was semi-automatically verified to determine if a transformation to lower case was needed. Special treatment is also needed for the symbols hyphen (-), quote ('), and period (.) which can lead to ambiguous separations. For example, the hyphen in compound words like beaux-arts and au-dessus is considered word-internal. Alternatively the hyphen may be associated with the first word as in ex-, or anti-, or with the second word as in -Id or -nL Finally, it may appear in the text even though it is not associated with any word.",
  "y": "differences uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_3",
  "x": "It should be noted that this decoding strategy based on two forward passes can in fact be implemented in a single forward pass using one or two processors. We are using a two pass solution because it is conceptually simpler, and also due to memory constraints. ---------------------------------- **EXPERIMENTAL RESULTS** ---------------------------------- **WSJ:** The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies. Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information. The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> .",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_4",
  "x": "**EXPERIMENTAL RESULTS** ---------------------------------- **WSJ:** The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies. Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information. The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> . Two sets of standard training material have been used for these experiments: The standard WSJ0 SI84 training data which include 7240 sentences from 84 speakers, and the standard set of 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers. Only the primary microphone data were used for training. The WSJ corpus provides a wealth of material that can be used for system development.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_5",
  "x": "Except when explicitly stated otherwise, all of the results reported for WSJ use the standard language models <cite>[19]</cite> . Using a set of 1084 CD models trained with the WSJ0 si84 training data, the word error is 6.6% on the Nov92 5k test data and 9.4% on the Nov93 test data. Using the combined WSJ0]WSJ1 si284 training data reduces the error by about 27% for both tests. When a trigram LM is used in the second pass, the word error is reduced by an addition 35% on the Nov92 test and by 22% on the Nov93 test. Results are given in the Table 3 for the Nov92 nvp 64K test data using both closed and open 20k vocabularies. With si84 training (si84c, a slightly smaller model set than si84) the word error rate is doubled when the vocabulary increases from 5k to 20k words and the test perplexity goes from 111 to 244. The higher error rate with the 20k open lexicon can be largely attributed to the out-of-vocabulary (OOV) words, which account for almost 2% of the words in the test sentences. Processing the same test data with a system trained on the si284 training data, reduces the word error by 30%. The word error on the Nov93 20k test is 15.2% with the si284 system. Using the trigrarn LM reduces the error rate by 18% on the Nov92 test and 22% on the Nov93 test.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_6",
  "x": "5 Containing 1115 distinct diphones and over 17,500 triphones, BREF can be used to train vocabulary-independent acoustic models. The text material was read without verbalized punctuation using the verbatim prompts. 6 5This is in contrast to the WSJ texts which were selected so as to contain only words in the most frequent 64,000 words in the original text material. 6Another difference between BREF and WSJ0 is that the prompts for ---------------------------------- **DISCUSSION AND SUMMARY** The recognizer has been evaluated on 5k and 20k test data for the English and French languages using similar style corpora. It should be pointed out however, that although the Nov92 5k WSJ test data and the BREF 5k test data were closed-vocabulary, the conditions are not quite the same. For WSJ, paragraphs were selected ensuring not more than one word was out of the 5.6k most frequent words<cite> [ 19]</cite> , and these additional words were then included as part of the vocabulary. For BREF, a lexicon was first constructed containing the 5k/20k most frequent words, and sentences covered by this vocabulary were selected from the development test material.",
  "y": "uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_0",
  "x": "Since its introduction [8, 14, 9] , the VQA problem has attracted an increasing interest [22] . Its multimodal nature and more precise evaluation protocol than alternative multimodal scenarios, such as image captioning, help to explain this interest. Furthermore, the proliferation of suitable datasets and potential applications, are also key elements behind this increasing activity. Most state-of-the-art methods follow a joint embedding approach, where deep models are used to project the textual question and visual input to a joint feature space that is then used to build the answer. Furthermore, most modern approaches pose VQA as a classification problem, where classes correspond to a set of pre-defined candidate answers. As an example, most entries to the VQA challenge [9] select as output classes the most common 3000 answers in this dataset, which account for 92% of the instances in the validation set. The strategy to combine the textual and visual embeddings and the underlying structure of the deep model are key design aspects that differentiate previous works. Antol et al. [9] propose an element-wise multiplication between image and question embeddings to generate spatial attention map. Fukui et al.<cite> [6]</cite> propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. [26] extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilinear operator.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_1",
  "x": "The common approach is to use a one-way attention scheme, where the embedding of the question is used to generate a set of attention coefficients over a set of predefined image regions. These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21,<cite> 6,</cite> 25, 26] . More elaborated forms of attention has also been proposed. Xu and Saenko [23] suggest use wordlevel embedding to generate attention. Yang et al. [24] iterates the application of a soft-attention mechanism over the visual input as a way to progressively refine the location of relevant cues to answer the question. Lu et al. [13] proposes a bidirectional co-attention mechanism that besides the question guided visual attention, also incorporates a visual guided attention over the input question. In all the previous cases, the attention mechanism is applied using an unsupervised scheme, where attention coefficients are considered as latent variables. Recently, there have been also interest on including a supervised attention scheme to the VQA problem [5, 7, 18] . Das et al. [5] compare the image areas selected by humans and state-of-theart VQA techniques to answer the same visual question. To achieve this, they collect the VQA human attention dataset (VQA-HAT), a large dataset of human attention maps built by asking humans to select images areas relevant to answer questions from the VQA dataset [9] .",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_2",
  "x": "This network generates a set of attention proposals for each image in the VQA dataset, which are used as labels to supervise attention in the VQA model. This strategy results in a small boost in performance compared with a non-attentional strategy. In contrast to our approach, these previous works are based on a supervised attention scheme that does not consider an automatic mechanism to obtain the attention labels. Instead, they rely on human annotated groundings as attention supervision. Furthermore, they differ from our work in the method to integrate attention labels to a VQA model. Specifically, we take the mined grounding labels as weakly-supervised signals and denote two types of attention supervision, namely region-level and object-level labels. Figure 2 shows the main pipeline of our VQA model. We mostly build upon the MCB model in<cite> [6]</cite> , which exemplifies current state-of-the-art techniques for this problem. Our main innovation to this model is the addition of an Attention Supervision Module that incorporates visual grounding as an auxiliary task. Next we describe the main modules behind this model.",
  "y": "extends differences"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_3",
  "x": "The weighted question features Q w \u2208 R G q D are then computed using a soft attention mechanism [3] , which is essentially a weighted sum of the T word features followed by a concatenation according to G q . ---------------------------------- **VQA MODEL STRUCTURE** Image Attention Module: Images are passed through an embedding layer consisting of a pre-trained ConvNet model, such as Resnet pretrained with the ImageNet dataset [10] . This generates image features I f \u2208 R C\u00d7H\u00d7W , where C, H and W are depth, height and width of the extracted feature maps. Fusion Module I is then used to generate a set of image attention coefficients. First, question features Q w are tiled as the same spatial shape of I f . Afterwards, the fusion module models the joint relationship J attn \u2208 R O\u00d7H\u00d7W between questions and images, mapping them to a common space of dimension O. In the simplest case, one can implement the fusion module using either concatenation or Hadamard product [1] , but more effective pooling schemes can be applied <cite>[6,</cite> 11, 25, 26] . The design choice of the fusion module remains an on-going research topic. In general, it should both effectively capture the latent relationship between multi-modal features meanwhile be easy to optimize.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_4",
  "x": "Following previous work<cite> [6]</cite> , we use as candidate outputs the top 3000 most frequent answers in the VQA dataset. At the end of this process, we obtain the highest scoring answer\u00c2. ---------------------------------- **ATTENTION SUPERVISION MODULE:** As a main novelty of the VQA model, we add an Image Attention Supervision Module as an auxiliary classification task, where ground-truth visual grounding labels C gt \u2208 R H\u00d7W \u00d7G v are used to guide the model to focus on meaningful parts of the image to answer each question. To do that, we simply treat the generated attention coefficients C v as a probability distribution, and then compare it with the ground-truth using KL-divergence. Interestingly, we introduce two attention maps, corresponding to relevant region-level and objectlevel groundings, as shown in Figure 3 . Sections 4 and 5 provide details about our proposed method to obtain the attention labels and to train the resulting model, respectively. J a n s Figure 2 . Schematic diagram of the main parts of the VQA model.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_5",
  "x": "Sections 4 and 5 provide details about our proposed method to obtain the attention labels and to train the resulting model, respectively. J a n s Figure 2 . Schematic diagram of the main parts of the VQA model. It is mostly based on the model presented in<cite> [6]</cite> . Main innovation is the Attention Supervision Module that incorporates visual grounding as an auxiliary task. This module is trained through the use of a set of image attention labels that are automatically mined from the Visual Genome dataset. ---------------------------------- **MINING ATTENTION SUPERVISION FROM VISUAL GENOME** Visual Genome (VG) [12] includes the largest VQA dataset currently available, which consists of 1.7M QA pairs. Furthermore, for each of its more than 100K images, VG also provides region and object annotations by means of bounding boxes.",
  "y": "extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_6",
  "x": "We will make these labels publicly available. Here \"men\" in the region description is firstly lemmatized to be \"man\", whose aliases contain \"people\"; the word \"talking\" in the answer also contributes to the matching. So the selected regions have two matchings which is the most among all candidates. (b) Example object-level grounding from VG. Left: image with object instance labels; Right: our mined results. Note that in this case region-level grounding will give us the same result as in (a), but object-level grounding is clearly more localized. ---------------------------------- **IMPLEMENTATION DETAILS** We build the attention supervision on top of the opensourced implementation of MCB<cite> [6]</cite> and MFB [25] . Similar to them, We extract the image feature from res5c layer of Resnet-152, resulting in 14 \u00d7 14 spatial grid (H = 14, W = 14, C = 2048).",
  "y": "differences extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_7",
  "x": "---------------------------------- **DATASETS** VQA-2.0: The VQA-2.0 dataset [9] consists of 204721 images, with a total of 1.1M questions and 10 crowdsourced answers per question. There are more than 20 question types, covering a variety of topics and free-form answers. The dataset is split into training (82K images and 443K questions), validation (40K images and 214K questions), and testing (81K images and 448K questions) sets. The task is to predict a correct answer A given a corresponding image-question pair (I, Q). As a main advantage with respect to version 1.0 [9] , for every question VQA-2.0 includes complementary images that lead to different answers, reducing language bias by forcing the model to use the visual information. Visual Genome: The Visual Genome (VG) dataset [12] contains 108077 images, with an average of 17 QA pairs per image. We follow the processing scheme from<cite> [6]</cite> , where non-informative words in the questions and answers such as \"a\" and \"is\" are removed. Afterwards, (I, Q, A) triplets with answers to be single keyword and overlapped with VQA-2.0 dataset are included in our training set.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_8",
  "x": "Each region/object is annotated by one sentence/phrase description and bounding box coordinates. VQA-HAT: VQA-HAT dataset [5] contains 58475 human visual attention heat (HAT) maps for (I, Q, A) triplets in VQA-1.0 training set. Annotators were shown a blurred image, a (Q, A) pair and were asked to \"scratch\" the image until they believe someone else can answer the question by looking at the blurred image and the sharpened area. The ---------------------------------- **RANK CORRELATION** Accuracy/% VQA-HAT VQA-X VQA-2.0 Human [5] 0.623 -80.62 PJ-X [17] 0.396 0.342 -MCB<cite> [6]</cite> 0 authors also collect 1374 \u00d7 3 = 4122 HAT maps for VQA-1.0 validation sets, where each of the 1374 (I, Q, A) were labeled by three different annotators, so one can compare the level of agreement among labels. We use VQA-HAT to evaluate visual grounding performance, by comparing the rank-correlation between human attention and model attention, as in [5, 17] . VQA-X: VQA-X dataset [17] contains 2000 labeled attention maps in VQA-2.0 validation sets. In contrast to VQA-HAT, VQA-X attention maps are in the form of instance segmentations, where annotators were asked to segment objects and/or regions that most prominently justify the answer.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_0",
  "x": "This paper seeks to fill this gap by providing a thorough analysis on the contributions of lexical resources for cross-lingual PoS tagging in neural times. Our base tagger is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016) with a rich word encoding model which consists of a character-based bi-LSTM representation cw paired with pre-trained word embeddings w. Sub-word and especially character-level modeling is currently pervasive in top-performing neural sequence taggers, owing to its capacity to effectively capture morphological features that are useful in labeling out-ofvocabulary (OOV) items. Sub-word information is often coupled with standard word embeddings to mitigate OOV issues. Specifically, i) word embeddings are typically built from massive unlabeled datasets and thus OOVs are less likely to be encountered at test time, while ii) character embeddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. ---------------------------------- **INTRODUCTION** In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_1",
  "x": "Sub-word information is often coupled with standard word embeddings to mitigate OOV issues. Specifically, i) word embeddings are typically built from massive unlabeled datasets and thus OOVs are less likely to be encountered at test time, while ii) character embeddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. ---------------------------------- **INTRODUCTION** In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).",
  "y": "motivation"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_2",
  "x": "We use linguistic resources that are user-generated and available for many languages. The first is WIKTIONARY, a word type dictionary that maps words to one of the 12 Universal PoS tags (Li et al., 2012; Petrov et al., 2012) . The second resource is UNIMORPH, a morphological dictionary that provides inflectional paradigms for 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from Li et al. (2012) . UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_3",
  "x": "The first is WIKTIONARY, a word type dictionary that maps words to one of the 12 Universal PoS tags (Li et al., 2012; Petrov et al., 2012) . The second resource is UNIMORPH, a morphological dictionary that provides inflectional paradigms for 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from Li et al. (2012) . UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon. A property here is a possible PoS tag (for Wiktionary) or a morphological feature (for Unimorph).",
  "y": "extends"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_4",
  "x": "In an ideal setup, the dictionaries contain no disjoint tag sets, and larger amounts of equal tag sets or superset of the treebank data. This is particularly desirable for approaches that take lexical information as type-level supervision. ---------------------------------- **EXPERIMENTAL SETUP** In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_5",
  "x": "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings. Language-specific hyperparameters could lead to optimized models for each language. However, we use identical settings for each language which worked well and is less expensive, following Bohnet et al. (2018) . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_6",
  "x": "This is particularly desirable for approaches that take lexical information as type-level supervision. ---------------------------------- **EXPERIMENTAL SETUP** In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_7",
  "x": "**EXPERIMENTAL SETUP** In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_8",
  "x": "We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ---------------------------------- **RESULTS** Table 1 presents our replication results, i.e., tagging accuracy for the 21 individual languages, with means over all languages and language families (for which at least two languages are available). There are several take-aways. ---------------------------------- **INCLUSION OF LEXICAL INFORMATION** Combining the best of two worlds results in the overall best tagging accuracy, confirming<cite> Plank and Agi\u0107 (2018)</cite> : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages).",
  "y": "differences"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_9",
  "x": "However, we use identical settings for each language which worked well and is less expensive, following Bohnet et al. (2018) . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ---------------------------------- **RESULTS** Table 1 presents our replication results, i.e., tagging accuracy for the 21 individual languages, with means over all languages and language families (for which at least two languages are available). There are several take-aways. ----------------------------------",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_10",
  "x": "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ---------------------------------- **RESULTS** Table 1 presents our replication results, i.e., tagging accuracy for the 21 individual languages, with means over all languages and language families (for which at least two languages are available). There are several take-aways. ---------------------------------- **INCLUSION OF LEXICAL INFORMATION** Combining the best of two worlds results in the overall best tagging accuracy, confirming<cite> Plank and Agi\u0107 (2018)</cite> : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages). On 15 out of 21 languages, DSDS is the best performing model.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_11",
  "x": "Here we dig deeper into the effect of including lexical information by a) examining learning curves with increasing dictionary sizes, b) relating tag set properties to performance, and finally c) having a closer look at model internal representations, by comparing them to the representations of the base model that does not include lexical information. We hypothesize that when learning from dictionary-level supervision, information is propagated through the representation layers so as to generalize beyond simply relying on the respective external resources. ---------------------------------- **LEARNING CURVES** The lexicons we use so far are of different sizes (shown in Table 1 of<cite> Plank and Agi\u0107 (2018)</cite> ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries. In a low-resource setup, large dictionaries might not be available. It is thus interesting to examine how tagging accuracy is affected by dictionary size. We examine two cases: randomly sampling dictionary entries and sampling by word frequency, over increasing dictionary sizes: 50, 100, 200, 400, 800, 1600 word types. The latter is motivated by the fact that an informed dictionary creation (under limited resources) might be more beneficial. We estimate word frequency by using the UD training data sets (which are otherwise not used).",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_12",
  "x": "To test this hypothesis, we train the base tagger on high-quality gold training data (effectively, the UD training data sets), with and without freezing the word embeddings layer. We find that updating the word embedding layer is in fact beneficial in the high-quality training data regime: on average +0.4% absolute improvement is obtained (mean over 21 languages). This is in sharp contrast to the noisy training data regime, in which the baseline accuracy drops by as much as 1.2% accuracy. Therefore, we train the tagger with pre-trained embeddings on projected WTC data and freeze the word embeddings lookup layer during training. In recent years, natural language processing has witnessed a move towards deep learning approaches, in which automatic representation learning has become the de facto standard methodology (Collobert et al., 2011; Manning, 2015) . One of the first works that combines neural representations with semantic symbolic lexicons is the work on retrofitting (Faruqui et al., 2015) . The main idea is to use the relations defined in semantic lexicons to refine word embedding representations, such that words linked in the lexical resource are encouraged to be closer to each other in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_13",
  "x": "They create nonsensical construction so that the RNN cannot rely on lexical or semantic clues, showing that RNNs still capture syntactic properties in sentence embeddings across the four tested languages while obfuscating lexical information. There is also more theoretical work on investigating the capabilities of recurrent neural networks, e.g., Weiss et al. (2018) show that specific types of RNNs (LSTMs) are able to use counting mechanisms to recognize specific formal languages. Finally, linguistic resources can also serve as proxy for evaluation. As recently shown (Agi\u0107 et al., 2017) , type-level information from dictionaries approximates PoS tagging accuracy in the absence of gold data for cross-lingual tagger evaluation. Their use of high-frequency word types inspired parts of our analysis. ---------------------------------- **CONCLUSIONS** We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner. We replicated the results of<cite> Plank and Agi\u0107 (2018)</cite> , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting. By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size.",
  "y": "uses extends"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_0",
  "x": "In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model -the <cite>relaxed hybrid tree</cite> model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. ---------------------------------- **INTRODUCTION** This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) . One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured.",
  "y": "motivation differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_1",
  "x": "Our system is available for download from http://statnlp.org/research/sp/. ---------------------------------- **INTRODUCTION** This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) . One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_2",
  "x": "This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) . One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations. Thus, <cite>the model</cite> is unable to identify joint representations for certain sentence-semantics pairs during the training process, and is unable to produce desired outputs for certain inputs during the evaluation process. In this work, we propose a solution addressing the above limitation, which makes our model more robust. Through experiments, we demonstrate that our improved discriminative model for semantic parsing, even when simpler features are used, is able to obtain new state-of-the-art results on standard datasets.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_3",
  "x": "Semantic parsing has recently attracted a significant amount of attention in the community. In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_4",
  "x": "This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. ---------------------------------- **<cite>RELAXED HYBRID TREES</cite>** We briefly discuss our previously proposed <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> in this section.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_6",
  "x": "We briefly discuss our previously proposed <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> in this section. <cite>The model</cite> is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008) . Both systems are publicly available 1 . Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. <cite>The model</cite> defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach: where \u039b is the set of parameters (weights of features) used by the model. its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_7",
  "x": "its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se- In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_8",
  "x": "where \u039b is the set of parameters (weights of features) used by the model. its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se- In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a .",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_9",
  "x": "In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_10",
  "x": "---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_11",
  "x": "---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h .",
  "y": "motivation differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_13",
  "x": "For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_14",
  "x": "When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h . To eliminate <cite>relaxed hybrid trees</cite> consisting of an infinite number of nodes, pattern X is disallowed in the <cite>relaxed hybrid trees</cite> model <cite>(Lu, 2014)</cite> . However, disallowing pattern X has led to other issues. Specifically, for certain semanticssentence pairs, it is not possible to find <cite>relaxed hybrid trees</cite> that jointly represent them. In the example semantics-sentence pair given in Figure 3 (a) , it is not possible to find any <cite>relaxed hybrid tree</cite> that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word. On the other hand, it is still possible to find a hybrid tree representation for both the sentence and the semantics where pattern X is allowed (see Figure 3 (c) ). In practice, we can alleviate this issue by extending the lengths of the sentences. For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the <cite>relaxed hybrid trees</cite> to be constructed for certain sentence-semantics pairs with short sentences.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_17",
  "x": "In the example semantics-sentence pair given in Figure 3 (a) , it is not possible to find any <cite>relaxed hybrid tree</cite> that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word. On the other hand, it is still possible to find a hybrid tree representation for both the sentence and the semantics where pattern X is allowed (see Figure 3 (c) ). In practice, we can alleviate this issue by extending the lengths of the sentences. For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the <cite>relaxed hybrid trees</cite> to be constructed for certain sentence-semantics pairs with short sentences. However, such an approach does not resolve the theoretical limitation of <cite>the model</cite>. ---------------------------------- **CONSTRAINED SEMANTIC FORESTS** To address this limitation, we allow pattern X to be included when building our new discriminative semantic parsing model. However, as mentioned above, doing so will lead to latent structures (<cite>relaxed hybrid tree</cite> representations) of infinite heights. To resolve such an issue, we instead add an additional constraint -limiting the height of a semantic representation to a fixed constant c, where c is larger than the maximum height of all the trees appearing in the training set.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_18",
  "x": "To address this limitation, we allow pattern X to be included when building our new discriminative semantic parsing model. However, as mentioned above, doing so will lead to latent structures (<cite>relaxed hybrid tree</cite> representations) of infinite heights. To resolve such an issue, we instead add an additional constraint -limiting the height of a semantic representation to a fixed constant c, where c is larger than the maximum height of all the trees appearing in the training set. Table 1 summarizes the list of patterns that our model considers. This is essentially the same as those considered by the hybrid tree model. Our new objective function is as follows: where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible <cite>relaxed hybrid tree</cite> representations where the pattern X is allowed. The main challenge now becomes the computation of the denominator term in Equation 2, as the set M is still very large. To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees.",
  "y": "motivation uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_19",
  "x": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS** Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012) . The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_20",
  "x": "To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS** Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012) .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_21",
  "x": "The main challenge now becomes the computation of the denominator term in Equation 2, as the set M is still very large. To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS** Our experiments were conducted on the publicly available multilingual GeoQuery dataset.",
  "y": "similarities"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_22",
  "x": "Our experiments were conducted on the publicly available multilingual GeoQuery dataset. Various previous works on semantic parsing used this dataset for evaluations (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Jones et al., 2012) . The dataset consists of 880 natural language sentences where each sentence is coupled with a formal tree-structured semantic representation. The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006) , and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 .",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_23",
  "x": "The early version of this dataset was annotated with English only (Wong and Mooney, 2006; Kate and Mooney, 2006) , and Jones et al. (2012) released a version that is annotated with three additional languages: German, Greek and Thai. To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 . We compared our system's performance against those of several previous works. The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_24",
  "x": "Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 . We compared our system's performance against those of several previous works. The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_25",
  "x": "In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_26",
  "x": "The WASP system (Wong and Mooney, 2006 ) is based on statistical machine translation technique while the HY-BRIDTREE+ system (Lu et al., 2008 ) is based on the generative hybrid tree model augmented with a discriminative re-ranking stage where certain global features are used. UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_27",
  "x": "UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_28",
  "x": "Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_29",
  "x": "Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_30",
  "x": "Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_31",
  "x": "It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ---------------------------------- **CONCLUSION** In this work, we presented an improved discriminative approach to semantic parsing. Our approach does not have the theoretical limitation associated with our previous state-of-the-art approach. We demonstrated through experiments that our new model was able to yield new stateof-the-art results on a standard dataset across four different languages, even though simpler features were used. Since our new model involves simpler features, including unigram features defined over individual semantic unit -word pairs, we believe our new model would aid the joint modeling of both distributional and logical semantics (Lewis and Steedman, 2013 ) within a single framework.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_0",
  "x": "Browns Cambridge is a pub, also it is a moderately priced italian place near Adriatic, also it is family friendly, you know and it's in the city centre. In subsequent work, we showed that we could augment the E2E training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training<cite> [5]</cite> . Here, instead, we explore whether a model that is trained to achieve a single stylistic personality target can produce outputs that combine stylistic targets, to yield a novel style that is significantly different than what was seen in training, while still maintaining high semantic correctness. We first train each stylistic model with a single latent variable for supervision, for five different personality models, or voices, based on the Big Five theory of personality, namely the personality trait styles of EXTRAVERT, AGREEABLE, DISAGREEABLE, CONSCI-ENTIOUS, and UNCONSCIENTIOUS. Then, at generation time, we provide the model with combinations of the stylistic variables, i.e. we instruct the NNLG to generate multivoice outputs that combine EXTRAVERT with DISAGREEABLE, where such combined outputs never occurred in the training data. We first describe how we set up our dataset and neural models in Section 2, and then present our results in Section 3. We evaluate the multivoice outputs for both semantic fidelity and for similarities to and differences from the linguistic features that characterize the original training style. We hypothesize that controlling multiple stylistic parameters is more difficult and will lead to more semantic errors, so we examine in detail the interaction of stylistic variation and semantic fidelity, as well as quantifying stylistic fidelity. We leave a discussion of related work until Section 4 where we also conclude. ----------------------------------",
  "y": "background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_1",
  "x": "We leave a discussion of related work until Section 4 where we also conclude. ---------------------------------- **DATA AND MODELS** There is a long tradition in AI of using slightly synthetic tasks and datasets in order to test the ability of particular models to achieve these tasks [7, 8] . The PERSONAGE corpus<cite> [5]</cite> provides a controlled environment for testing different models of neural generation and style generation. It consists of 88,500 restaurant domain utterances whose style varies according to models of personality, which were generated by an existing statistical NLG engine that has the capability of manipulating 67 different stylistic parameters [9] . Table 2 shows sample utterances that are output for the singlevoice models and for each of our multi- Table 2 : MultiVoice generation output and comparable singlevoice outputs for DISAGREEABLE, EXTRAVERT and CONSCIENTIOUS for the meaning representation in Figure 1 . We count the frequency of periods (Period Agg.) and expletives (Explet. Prag) for multivoice models that utilize DISAGREEABLE). voice models (described below) for the same MR. Each output corresponding to each single voice personality is controlled by a set of sentence planning parameters that vary for each personality.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_2",
  "x": "There are 3,784 unique MRs in the training set, and the corpus contains 17,771 MR/training utterance pairs for each of the existing models for the personality traits of AGREEABLE, DISAGREEABLE, CONSCIENTIOUSNESS, UN-CONSCIENTIOUSNESS, and EXTRAVERT, for a total training set of 88,855 utterances. This guarantees a wide range of variation in parameter combinations. The test set consists of 278 unique MRs. The frequencies of longer utterances (more attribute MRs) vary across train and test with test MRs not seen during training. The training data has more smaller MRs, while the test set is more challenging, with more larger MRs. Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation. This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> . Figure 1 summarizes the model architecture.",
  "y": "differences background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_3",
  "x": "There are 3,784 unique MRs in the training set, and the corpus contains 17,771 MR/training utterance pairs for each of the existing models for the personality traits of AGREEABLE, DISAGREEABLE, CONSCIENTIOUSNESS, UN-CONSCIENTIOUSNESS, and EXTRAVERT, for a total training set of 88,855 utterances. This guarantees a wide range of variation in parameter combinations. The test set consists of 278 unique MRs. The frequencies of longer utterances (more attribute MRs) vary across train and test with test MRs not seen during training. The training data has more smaller MRs, while the test set is more challenging, with more larger MRs. Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation. This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> . Figure 1 summarizes the model architecture.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_4",
  "x": "2 The system is based on the seq2seq generation method with attention [14, 1<cite>5]</cite> , and uses a sequence of LSTMs [16] for the encoder and decoder, combined with beamsearch and an n-best list reranker for output tuning. The inputs to the model are dialog acts for each system action (such as inform) and a set of attribute slots (such as rating) and their values (such as high for attribute rating). To prepro- 2 We refer the reader to TGen publications [11, 13] for model details. cess the corpus of MR/utterance pairs, attributes that take on proper-noun values are delexicalized during training i.e. name and near. We encode personality as an additional dialog act, of type CONVERT with personality as the key and the target personality as the value (see Figure 1) . For every input MR and a personality, we train the model with the corresponding PER-SONAGE generated sentence. Our model differs from the TO-KEN model used in our previous work<cite> [5]</cite> because it is trained on unsorted inputs to allow us to add multiple CONVERT tags to the MR at generation time. Note that we do not train on multiple personalities, instead, we train one model that uses all the data, where each distinct single personality has a corresponding CONVERT(PERSONALITY = X) in the training instance. At generation time, we generate singlevoice data for all the test MRs (1,390 total realizations, 278 unique MRs, realized for each of 5 personalities). For the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test MRs, since the order of the CONVERT tags matters.",
  "y": "differences"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_5",
  "x": "We carefully evaluate the multivoice outputs for both semantic fidelity and for similarities to and differences from the linguistic features that characterize the original training style. We show that contrary to our predictions, the learned models do not always simply interpolate model parameters, but rather produce styles that are distinct, and novel from the personalities they were trained on. ---------------------------------- **INTRODUCTION** Natural language generators for task-oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics. The use of neural natural language generation (NNLG) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance. However, there has been little investigation of NNLGs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation. Instead, work on stylistic transfer has focused on tasks where only coarse-grained semantic fidelity is needed, such as controlling the sentiment of the utterance (positive or negative), or the topic or entity under discussion [1, 2, 3] . Consider for example a training instance for the restaurant domain consisting of a meaning representation (MR) from the End-to-End (E2E) Generation Challenge 1 and a sample output from one of our neural generation models in Figure 1 [4, <cite>5]</cite> . Systems using the training set of 50K crowdsourced utterances from the E2E task achieved high semantic correctness, e.g. the BLEU score for our best system on the dev set was 0.72 [6] .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_0",
  "x": "Apart from the natural conclusion that beam-search reduces error propagation compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many interesting questions remain unanswered. For example, the contribution of global learning in improving the accuracies has not been separately studied. It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of Zhang and Nivre (2011) to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar (Zhang and Nivre, 2011) by using the same range of rich feature definitions. In this paper, we answer the above questions empirically. First, we separate out global learning and beam-search, and study the effect of each technique by comparison with a local greedy baseline. Our results show that significant improvements are achieved only when the two are jointly applied. Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of Zhang and Nivre (2011) . Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by<cite> McDonald and Nivre (2007)</cite> .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_1",
  "x": "We follow<cite> McDonald and Nivre (2007)</cite> and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data. Our results show that beam-search im-proves the precision on long sentences and dependencies compared to greedy search, while the advantage of transition-based parsing on short dependencies is preserved. Under particular measures, such as precision for arcs at different levels of the trees, ZPar shows characteristics surprisingly similar to MSTParser. ---------------------------------- **ANALYZING THE EFFECT OF GLOBAL LEARNING AND BEAM-SEARCH** In this section we study the effects of global learning and beam-search on the accuracies of transition-based dependency parsing. Our experiments are performed using the Penn Treebank (PTB). We follow the standard approach to split PTB3 into training (sections 2-21), development (section 22) and final testing (section 23) sections. Bracketed sentences from the treebank are transformed into dependency structures using the Penn2Malt tool. 1 POS-tags are assigned using a perceptron tagger (Collins, 2002) , with an accuracy of 97.3% on a standard Penn Treebank test.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_2",
  "x": "In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005 ) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees. MSTParser produces non-projective trees from projective trees by scorebased rearrangements of arcs. ---------------------------------- **ERROR DISTRIBUTIONS** We take a range of different perspectives to characterize the errors of ZPar, comparing them with those of MaltParser and MSTParser by measuring the accuracies against various types of metrics, including the size of the sentences and dependency arcs, the distance to the root of the dependency tree, and the number of siblings.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_3",
  "x": "For further evidence, we add rich non-local features in the same increments as Zhang and Nivre (2011) to both ZPar and MaltParser, and evaluate UAS on the same development data set. Original settings are applied to both parsers, with ZPar using global learning and beam-search, and MaltParser using local learning and greedy search. Table 2 shows that while ZPar's accuracy consistently improves with the addition of each new set of features, there is very little impact on MaltParser's accuracy and in some cases the effect is in fact negative, indicating that the locally trained greedy parser cannot benefit from the rich non-local features. Yet another evidence for the support of more complex models by global learning and beamsearch is the work of Bohnet and Nivre (2012) , where non-projective parsing using online reordering (Nivre, 2009 ) and rich features led to significant improvements over greedy search (Nivre, 2009) , achieving state-of-the-art on a range of typologically diverse languages. 3 Characterizing the errors ---------------------------------- **THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_4",
  "x": "Table 2 shows that while ZPar's accuracy consistently improves with the addition of each new set of features, there is very little impact on MaltParser's accuracy and in some cases the effect is in fact negative, indicating that the locally trained greedy parser cannot benefit from the rich non-local features. Yet another evidence for the support of more complex models by global learning and beamsearch is the work of Bohnet and Nivre (2012) , where non-projective parsing using online reordering (Nivre, 2009 ) and rich features led to significant improvements over greedy search (Nivre, 2009) , achieving state-of-the-art on a range of typologically diverse languages. 3 Characterizing the errors ---------------------------------- **THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_5",
  "x": "Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root. Although the vast majority of sentences have only one root-modifier, there is no global control for the number of root-modifiers in the greedy shift-reduce process, and each action is made locally and independently. As a result, MaltParser tends to over-predict root modifiers, leading to the comparatively low precision. Surprisingly, the precision curve of ZPar is much more similar to that of MSTParser than that of MaltParser, although ZPar is based on the same shift-reduce parsing process, and even has a similar default single-root mechanism as MaltParser. This result is perhaps the most powerful demonstration of the effect of global learning and beam-search compared to local learning and greedy search. The model which scores whole sequences of shift-reduce actions, plus the reduction of search error propagation, lead to significantly reduced over-prediction of rootmodifiers. In addition, rich features used by ZPar, such as the valency (number of modifiers for a head) and set of modifier labels for a head, can also be useful in reducing over-prediction of modifiers. Because of these, ZPar effectively pushes the predictions of difficult arcs down the tree, which is exactly the behavior of MSTParser.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_6",
  "x": "Another important factor is the use of rich non-local features by ZPar, which is a likely reason for its precision to drop slower even than that of MSTParser when the arc size increases from 1 to 8. Interestingly, the precision of ZPar is almost indistinguishable from that of MaltParser for size 1 arcs (arcs between neighbouring words), showing that the wider range of features in ZPar is the most helpful in arcs that take more than one, but not too many shiftreduce actions to build. The recall curves of the three parsers are similar, with ZPar having higher recall than MSTParser and MaltParser, particularly when the dependency size is greater than 2. This shows that particular gold-standard dependencies are hard for all parsers to build, but ZPar is better in recovering hard gold dependencies probably due to its rich features. To take another perspective, we compare the performance of the three parsers at different levels of a dependency tree by measuring accuracies for arcs relative to their distance to the root. Here the distance of an arc to the root is defined as the number of arcs in the path from the root to the modifier in the arc. Figure 4 shows the precision and recall of each system for arcs of varying distances to the root. Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root.",
  "y": "background similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_0",
  "x": "First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010) . However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V\u00e9ronis, 1998; Murray and Green, 2004; <cite>Erk et al., 2009</cite>; Passonneau et al., 2012b) . Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. This paper provides the following contributions. First, we demonstrate that the choice in annotation setup can significantly improve IAA and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. ---------------------------------- **RELATED WORK** Given the potential utility of a sense-labeled corpus, multiple studies have examined how to efficiently gather high quality sense annotations.",
  "y": "background motivation"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_1",
  "x": "Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. This paper provides the following contributions. First, we demonstrate that the choice in annotation setup can significantly improve IAA and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. ---------------------------------- **RELATED WORK** Given the potential utility of a sense-labeled corpus, multiple studies have examined how to efficiently gather high quality sense annotations. Snow et al. (2008) had MTurk workers, referred to as Turkers, disambiguate uses of \"president.\" While they reported extremely high IAA (0.952), their analysis was only performed on a single word. Biemann and Nygaard (2010) and Biemann (2012) construct a sense-labeled corpus by concurrently constructing the sense inventory itself. Turkers used a lexical substitution task to identify valid substitutions of a target word.",
  "y": "similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_3",
  "x": "They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. ---------------------------------- **ANNOTATION METHODOLOGIES** We consider three methodologies for gathering sense labels: (1) the methodology of<cite> Erk et al. (2009)</cite> for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of<cite> Erk et al. (2009)</cite> which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012) . We propose a two-stage strategy that aims to reduce the complexity of the annotation task, referred to as Select and Rate (S+R).",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_4",
  "x": "Furthermore, as a pragmatic benefit, removing inapplicable senses reduces the visual space required for displaying the questions on the MTurk platform, which can improve annotation throughput. MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word's senses and then asked to select (1) the sense option that best matches the mean-add.v ask.v win.v argument.n interest.n paper.n different.a important.a<cite> Erk et al. (2009)</cite> ing in the example context and (2) the sense option that least matches (Louviere, 1991) . In our setting, we presented three options at a time for words with fewer than seven senses, and four options for those with seven senses. For a single context, multiple subsets of the senses are presented and then their relative ranking is used to produce the numeric rating. The final applicability ratings were produced using a modification of the counting procedure of Orme (2009) . First, all sense ratings are computed as the number of times the sense was rated best minus the number of times rated least. Second, all negativelyrated senses are assigned score of 1, and all positively ratings are normalized to be (1, 5]. ---------------------------------- **EXPERIMENTS** For measuring the difference in methodologies, we propose three experiments based on different analyses of comparing Turker and non-Turker annotations on the same dataset, the latter of which we refer to as the reference labeling.",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_5",
  "x": "Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of<cite> Erk et al. (2009)</cite> , where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions. 1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word.",
  "y": "uses"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_0",
  "x": "Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007) , or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%). Our experiments will show our multitask model can make significant improvement on the full training set. In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning. Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_1",
  "x": "Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%). Our experiments will show our multitask model can make significant improvement on the full training set. In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning. Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly. Fu et al. (2017) applied domain adversarial networks (Ganin and Lempitsky, 2015) to relation extraction and obtained improvement on out-of-domain evaluation. Inspired by the adversarial training, we attempt to use it as a regularization tool in our multi-task model and find some improvement. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_2",
  "x": "Fu et al. (2017) applied domain adversarial networks (Ganin and Lempitsky, 2015) to relation extraction and obtained improvement on out-of-domain evaluation. Inspired by the adversarial training, we attempt to use it as a regularization tool in our multi-task model and find some improvement. ---------------------------------- **SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_3",
  "x": "In terms of the regularization to the representation, Duong et al. (2015) used l2 regularization between the parameters of the same part of two models in multi-task learning. Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly. Fu et al. (2017) applied domain adversarial networks (Ganin and Lempitsky, 2015) to relation extraction and obtained improvement on out-of-domain evaluation. Inspired by the adversarial training, we attempt to use it as a regularization tool in our multi-task model and find some improvement. ---------------------------------- **SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_4",
  "x": "The encoder is a bidirectional RNN with attention and the decoder is one hidden fully connected layer followed by a softmax output layer. In the input layer, we convert word tokens into word embeddings with pretrained word2vec (Mikolov et al., 2013) . For each token, we convert the distance to the two arguments of the example to two position embeddings. We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by Zeng et al. (2014) . The entity embedding (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i \u2208 [0, T ), T is the length of the sentence. A wide range of encoders have been proposed for relation extraction. Most of them fall into categories of CNN (Zeng et al., 2014) , RNN (Zhou et al., 2016) and TreeRNN (Miwa and Bansal, 2016) . In this work, we follow Zhou et al. (2016) to use Bidirectional RNN with attention (BiRNN), which works well on both of the datasets we are going to evaluate on.",
  "y": "similarities"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_5",
  "x": "Given input w i from the input layer, the encoder is defined as the following: We use GRU (Cho et al., 2014) as the RNN cell. W v and b v are the weights for the projection v i . v w is the word context vector, which works as a query of selecting important words. The importance of the word is computed as the similarity between v i and v w . The importance weight is then normalized through a softmax function. Then we obtain the high level summarization \u03c6(x) for the relation example. The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014; Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) and a softmax output layer. We use the same structure which can be formalized as the following:",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_6",
  "x": "**EXPERIMENTS** ---------------------------------- **DATASETS** To apply the multi-task learning, we need at least two datasets. We pick ACE05 and ERE for our case study. The ACE05 dataset provides a cross-domain evaluation setting . It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_7",
  "x": "With syntactic features as (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data. A common way to do so is pre-training. As a baseline, we pre-train the encoder of the supervised model on ERE and then fine-tune on ACE05, and vice versa (row \"Pretraining\" in Table 1 ). We observe improvement on both fine-tuned datasets. This shows the similarity between the encoders of the two datasets. However, if we fix the encoder trained from one dataset, and only train the decoder on the other dataset, we will actually obtain a much worse model. This indicates that neither dataset contains enough data to learn a universal feature representation layer for classification. This leaves the possibility to further improve the representation by learning a better encoder.",
  "y": "future_work"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_0",
  "x": "****DYNAMIC ENTITY REPRESENTATION WITH MAX-POOLING IMPROVES MACHINE READING**** **ABSTRACT** We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset<cite> (Hermann et al., 2015)</cite> , our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network ---------------------------------- **INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_1",
  "x": "---------------------------------- **INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a \u2026! \" [X] \" star @entity0 presents a young child with a bionic arm! Figure 1 : A document-query-answer triple constructed from a news article and its bullet point summary. An entity in the summary (Robert Downey Jr.) is replaced by the placeholder [X] to form a query. All entities are anonymized to exclude world knowledge and focus on reading comprehension.",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_2",
  "x": "com/soskek/der-network ---------------------------------- **INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a \u2026! \" [X] \" star @entity0 presents a young child with a bionic arm! Figure 1 : A document-query-answer triple constructed from a news article and its bullet point summary. An entity in the summary (Robert Downey Jr.) is replaced by the placeholder [X] to form a query.",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_3",
  "x": "For example, in Figure 1 , a reader reading the sentence \"Robert Downey Jr. may be Iron Man . . . \" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori. Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2). Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ----------------------------------",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_4",
  "x": "For example, in Figure 1 , a reader reading the sentence \"Robert Downey Jr. may be Iron Man . . . \" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori. Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2). Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_5",
  "x": "We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2). Evaluation of our model, DER Network, exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity representations, which is intended to model the accumulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. ---------------------------------- **MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as: in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>:",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_6",
  "x": "---------------------------------- **MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as: in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>: in which a vector u(D, q) is learned to represent the status of a reader after reading a document and a query, and this vector is used to retrieve an answer by coupling with the answer vector v(a). 1 Factorization (2) relies on the hypothesis that there exists a fixed vector for each candidate answer representing its meaning. However, as we argued in Section 1, an entity surface does not possess meaning; rather, it serves as an anchor to link pieces of information about it. Therefore, we hypothesize that the meaning representation v(e; D, q) of an entity e should be dynamically constructed from its surrounding contexts, and the meanings are \"accumulated\" through the reader reading the document D. We explain the construction of v(e; D, q) in Section 2.1, and propose a max-pooling process for modeling information accumulation in Section 2.2.",
  "y": "differences"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_7",
  "x": "**3 EVALUATION** We use the CNN-QA dataset<cite> (Hermann et al., 2015)</cite> for evaluating our model's ability to answer questions about named entities. The dataset consists of (D, q, e)-triples, where the document D is taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document (Figure 1) . The training set has 90k articles and 380k queries, and both validation and test sets have 1k articles and 3k queries. An average article has about 25 entities and 700 word tokens. One trains a machine reading system on the data by maximizing likelihood of correct answers. We use Chainer 5 (Tokui et al., 2015) to implement our model 6 . Experimental Settings Named entities in CNN-QA are already recognized. For preprocessing, we segment sentences at punctuation marks \".\", \"!\", and \"?\". 7 We train our model 8 with hyper-parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_8",
  "x": "7 Text in CNN-QA are tokenized without any sentence segmentations. 8 Training process takes roughly a week (3-5 passes of the training data) on a 6-core 2.4GHz Xeon CPU. 9 Vector dimension: 300, Dropout: 0.3, Batch: 50, Optimization: RMSProp with momentum (Tieleman and Hinton, 2012; Graves, 2013) (momentum: 0.9, decay: 0.95), Learning rate: 1e-4 divided by 2.0 per epoch, Gradient clipping factor: 10. We initialize word vectors by uniform distribution [-0.05, 0.05] , and other matrix parameters by Gaussians of mean 0 and variance 2/(# rows + # columns). Hermann et al. (2015) and * * from Hill et al. (2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself .! \u2026! @entity7 received his robotic arm in the summer , then later had it upgraded to resemble a \" @entity26 \" arm .! this past saturday , @entity7 received an even more impressive gift , from \" @entity2 \" himself .! \u2026! the actor showed the child two arms , one from @entity0 's movies and one for @entity7 : a real , working robotic @entity2 arm .! clear effects, suggesting that the attention mechanism plays a key role in our model. Combining these two techniques helps more. Further, we note that initializing our model with pre-trained word vectors 10 is helpful, though world knowledge of entities has been prevented by the anonymization process. This suggests that pre-trained word vectors may still bring extra linguistic knowledge encoded in ordinary words. Finally, we note that our model, full DER Network, shows the best results compared to several previous reader models<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) , endorsing our approach as promising.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_0",
  "x": "**INTRODUCTION** Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017) , end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018) , because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge- * Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; <cite>Madotto et al., 2018</cite>; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019) . Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015) ) and copy mechanism (Gu et al., 2016 ) into a sequential generative architecture. However, most models tended to confound the dialog history with KB tuples and simply stored them into one memory. A shared memory forces the memory reader to reason over the two different types of data, which makes the task harder, especially when the memory is large. To explore this problem, Reddy et al. (2019) very recently proposed to separate memories for modeling dialog context and KB results. In this paper, we adopt working memory to interact with two longterm memories. Furthermore, compared to Reddy et al. (2019) , we leverage the reasoning ability of MemNNs to instantiate the external memories.",
  "y": "background"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_1",
  "x": "The WM consists of a Short-Term Storage system (STS) and a Central-EXE including an Attention Controller (Attn-Ctrl) and a rule-based word selection strategy. The Attn-Ctrl dynamically generates the attention control vector to query and reason over the two long memories and then stores three \"activated\" distributions into STS. Finally a generated token is selected from the STS under the word selection strategy at each decoder step. ---------------------------------- **MODEL DESCRIPTION** The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set. P T RE supervised information for copying words in dialog history P T RS = {ptrS,1, \u00b7 \u00b7 \u00b7 , ptrS,m}, KB pointer index set. P T RS supervised information for copying entities in KB tuples Table 1 : Notation Table. where xb z \u2208 X or B is the dialog history or KB tuples according to the subscript (E or S) and n xb + 1 is the sentinel position index as n xb is equal to the dialog history length n or the number of KB triples l. The idea behind Eq. 1 is that we can obtain the positions of where to copy by matching the target text with the dialog history or KB information.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_2",
  "x": "Furthermore, we augment E-MemNN and S-MemNN with copy mechanism from where we need to copy tokens or entities. The encoder encodes the dialog history to obtain the high-level signal, a distributed intent vector. The WM consists of a Short-Term Storage system (STS) and a Central-EXE including an Attention Controller (Attn-Ctrl) and a rule-based word selection strategy. The Attn-Ctrl dynamically generates the attention control vector to query and reason over the two long memories and then stores three \"activated\" distributions into STS. Finally a generated token is selected from the STS under the word selection strategy at each decoder step. ---------------------------------- **MODEL DESCRIPTION** The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_3",
  "x": "where h i is the context-aware representation, and \u03c6 e is a trainable embedding function. We combine MemNNs with TRANS(\u00b7) to alleviate the OOV problem when reasoning about memory contents. ---------------------------------- **WORKING MEMORY DECODER** Inspired by the studies on the working memory, we design our decoder as an attentional control system for dialog generation which consists of the working memory and two long-term memories. As shown in Figure 1 , we adopt the E-MemNN to memorize the dialog history X as described in Section 2.1, and then store KB tuples into the S-MemNN without TRANS(\u00b7). We also incorporate additional temporal information and speaker information into dialog utterances as <cite>(Madotto et al., 2018)</cite> and adopt a (subject, relation, object) representation of KB information as (Eric and Manning, 2017b) . More details can be found in the supplementary material. Having written dialog history and KB tuples into E-MemNN and S-MemNN, we then use the WM to interact with them (to query and reason over them) to generate the response. At each decoder step, the Attn-Ctrl, instantiated as a GRU, dynamically generates the query vector q t as follows:",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_4",
  "x": "At each decoder step, the Attn-Ctrl, instantiated as a GRU, dynamically generates the query vector q t as follows: Here, query q t is used to access E-MemNN activating the final query q E = o K E , vocabulary distribution P vocab by Eq. 9 and copy distribution for dialog history P E\u00b7ptr . When querying S-MemNN, we consider the dialog history by using query q t = q E + q t and then obtain the copy distribution for KB entities P S\u00b7ptr . The two copy distributions are obtained by augmenting MemNNs with copy mechanism that is P E\u00b7ptr = p K E,t and P S\u00b7ptr = p K S,t . Now, three distributions, P vocab , P E\u00b7ptr and P S\u00b7ptr , are activated and moved into the STS, and then a proper word is generated from the activated distributions. We here use a rule-based word selection strategy by extending the sentinel idea in <cite>(Madotto et al., 2018)</cite> , which is shown in Figure 1 . If the expected word is not appearing either in the episodic memory or the semantic memory, the two copy pointers are trained to produce the sentinel token and our WMM2Seq generates the token from P vocab ; otherwise, the token is generated by copying from either the dialog history or KB tuples and this is done by comparing the two copy distributions. We always select the other distribution if one of the two distributions points to the sentinel or select to copy the token corresponding to the biggest probability of the two distributions. Hence, during the training stage, all the parameters are jointly learned by minimizing the sum of three standard cross-entropy losses with the corresponding targets (Y , P T R E and P T R S ). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_5",
  "x": "The shared size of embedding and hidden units is selected from [64, 512] and the default hop K = 3 is used for all MemNNs. The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. ---------------------------------- **RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 . We can see that our model does much better on the OOV situation and is on par with the best results on T5.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_6",
  "x": "The learning rate is simply fixed to 0.001 and the dropout ratio is sampled from [0.1, 0.4]. Furthermore, we randomly mask some memory cells with the same dropout ratio to simulate the OOV situation for both episodic and semantic memories. The hyper-parameters for best models are given in the supplementary material. ---------------------------------- **RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 . We can see that our model does much better on the OOV situation and is on par with the best results on T5. Moreover, our model can perfectly issue API calls (task 1), update API calls (task 2) and provide extra information (task 4).",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_7",
  "x": "And this reasoning ability is also proved by the performance improvements on the DSTC2 dataset according to several metrics in Table 3 . Especially, a significant improvement on entity F1 scores indicates that our model can choose the right entities and incorporate them into responses more naturally (with highest BLEU scores). Furthermore, there is no significant difference between the two kinds of the transformation TRANS(\u00b7). Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) .",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_8",
  "x": "Especially, a significant improvement on entity F1 scores indicates that our model can choose the right entities and incorporate them into responses more naturally (with highest BLEU scores). Furthermore, there is no significant difference between the two kinds of the transformation TRANS(\u00b7). Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory.",
  "y": "extends differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_9",
  "x": "Especially, a significant improvement on entity F1 scores indicates that our model can choose the right entities and incorporate them into responses more naturally (with highest BLEU scores). Furthermore, there is no significant difference between the two kinds of the transformation TRANS(\u00b7). Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_10",
  "x": "Firstly, our model generates a different but still correct response as the customer wants a moderately priced restaurant in the west and does not care about the type of food. Secondly, the generated response has tokens from the vocabulary (e.g. \"is\" and \"a\"), dialog history (e.g. \"west\" and \"food\") and KB information (e.g. \"saint johns chop house\" and \"british\"), indicating that our model learns to interact well with the two long-term memories by two sentinels. Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ---------------------------------- **CONCLUSION** We leverage the knowledge from the psychological studies and propose our WMM2Seq for dialog response generation.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_11",
  "x": "Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ---------------------------------- **CONCLUSION** We leverage the knowledge from the psychological studies and propose our WMM2Seq for dialog response generation. First, the storage separation of the dialog history and KB information is very important and we explore two context-sensitive perceptual processes for the word-level representations of the dialog history. Second, working memory is adopted to interact with the long-term memories and then generate the responses.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_12",
  "x": "Attention Visualization: As an intuitive way to show the model's dynamics, attention weight visualization is also used to understand how the Central-EXE controls the access to the two long-term memories (E-MemNN and S-MemNN). Figure 2 shows the episodic and semantic memory attention vectors at the last hop for each generated token. Firstly, our model generates a different but still correct response as the customer wants a moderately priced restaurant in the west and does not care about the type of food. Secondly, the generated response has tokens from the vocabulary (e.g. \"is\" and \"a\"), dialog history (e.g. \"west\" and \"food\") and KB information (e.g. \"saint johns chop house\" and \"british\"), indicating that our model learns to interact well with the two long-term memories by two sentinels. Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_0",
  "x": "**INTRODUCTION** BLEU (Papineni et al., 2002 ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved. Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation. It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003) , independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Gardent, 2014, 2016) .",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_1",
  "x": "1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. ---------------------------------- **INTRODUCTION** BLEU (Papineni et al., 2002 ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_2",
  "x": "It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003) , independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Gardent, 2014, 2016) . For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010) , respectively (Narayan and Gardent, 2016) . Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018) , in which the automatic metric used is BLEU. For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by<cite> Xu et al. (2016)</cite> , evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion. 2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting. The first setting allows assessing whether BLEU with the standard reference set is a reliable metric on systems that perform splitting.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_3",
  "x": "---------------------------------- **BLEU IN TS.** While BLEU is standardly used for TS evaluation (e.g.,<cite> Xu et al., 2016</cite>; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017 ), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy. T-BLEU (\u0160tajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment. Xu et al. (2016) focused on lexical simplification, finding that BLEU obtains reasonable correlation for grammaticality and meaning preservation but fails to capture simplicity, even when multiple references are used. To our knowledge, no previous work has examined the behavior of BLEU on sentence splitting, which we investigate here using a manually compiled gold standard. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_4",
  "x": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of<cite> Xu et al. (2016)</cite> . 3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) . We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning. The guidelines include two sentence splitting examples. 4 In Set 2, annotators are encouraged to split only in cases where it simplifies the original sentence. That is, simplicity is implicit in Set 1 and explicit in Set 2. In both sets, the annotators are instructed to leave the source unchanged if splitting violates grammaticality, fluency or meaning preservation.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_5",
  "x": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of<cite> Xu et al. (2016)</cite> . 3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) . We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning. The guidelines include two sentence splitting examples. 4 In Set 2, annotators are encouraged to split only in cases where it simplifies the original sentence. That is, simplicity is implicit in Set 1 and explicit in Set 2. In both sets, the annotators are instructed to leave the source unchanged if splitting violates grammaticality, fluency or meaning preservation.",
  "y": "extends"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_6",
  "x": "The last row presents the average scores of the 4 HSplit corpora. ---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_7",
  "x": "**EXPERIMENTAL SETUP** Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_8",
  "x": "Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism. 9 We explore two settings. In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", \u00a74.3), we use HSplit as the reference set. Systems. For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\"). Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017) , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (Koehn et al., 2007) and SBMT-SARI<cite> (Xu et al., 2016)</cite> , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs).",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_9",
  "x": "6 Wilicoxon's signed rank test, p = 1.6 \u00b7 10 \u22125 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism. 9 We explore two settings. In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", \u00a74.3), we use HSplit as the reference set. Systems. For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\").",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_10",
  "x": "In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", \u00a74.3), we use HSplit as the reference set. Systems. For \"Standard Reference Setting\", we consider both a case where evaluated systems do not perform any splittings on the test set (\"Systems/Corpora without Splits\"), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\"All Systems/Corpora\"). Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017) , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (Koehn et al., 2007) and SBMT-SARI<cite> (Xu et al., 2016)</cite> , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (Sulem et al., 2018b) . Human Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of<cite> Xu et al. (2016)</cite> , and extend it to apply to HSplit as well.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_11",
  "x": "Indeed, BLEU-1ref obtains 59.85 for the input and 43.90 for the HSplit corpora (averaged over the 4 HSplit corpora). BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit. 12 The high scores obtained for Identity, also observed by<cite> Xu et al. (2016)</cite> , indicate that BLEU is a not a good predictor for relative simplicity to the input. The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited. For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores. They are described in the following paragraph. Correlation with Human Evaluation. The system-level Spearman correlations between the rankings of the automatic metrics and the human judgments (see \u00a74.1) are presented in Table 2 . We find that in all cases BLEU and iBLEU negatively correlate with S and StS, indicating that they fail to capture simplicity and structural simplicity. Where gold standard splits are evaluated as well, BLEU's and iBLEU's failure to capture StS is even more pronounced.",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_0",
  "x": "A novel, deep, classificationspecific attention mechanism improves the performance of the RNN further, and can also highlight suspicious words for free, without including highlighted words in the training data. We consider both fully automatic and semi-automatic moderation. ---------------------------------- **INTRODUCTION** User comments play a central role in social media and online discussion fora. News portals and blogs often also allow their readers to comment to get feedback, engage their readers, and build customer loyalty. 1 User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) (Cheng et al., 2015) . Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts. 2<cite> Wulczyn et al. (2017)</cite> estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions. News portals also suffer from abusive user comments, which damage their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_1",
  "x": "They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments. 3 Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments sections entirely. We examine how deep learning (Goodfellow et al., 2016; Goldberg, 2016 Goldberg, , 2017 can be employed to moderate user comments. We experiment with a new dataset of approx. 1.6M manually moderated (accepted or rejected) user comments from a Greek sports news portal (called Gazzetta), which we make publicly available. 4 This is one of the largest publicly available datasets of moderated user comments. We also provide word embeddings pre-trained on 5.2M comments from the same portal. Furthermore, we experiment on the 'attacks' dataset of<cite> Wulczyn et al. (2017)</cite> , approx. 115K English Wikipedia talk page comments labeled as containing personal attacks or not.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_3",
  "x": "There are approx. 1.45M training comments (covering Jan. 1, 2015 to Oct. 6, 2016 in the Gazzetta dataset; we call them G-TRAIN-L (Table 1) . Some experiments use only the first 100K comments of G-TRAIN-L, called G-TRAIN-S. An additional set of 60,900 comments (Oct. 7 to Nov. 11, 2016) was split to development (G-DEV, 29,700 comments), large test (G-TEST-L, 29,700), and small test set (G-TEST-S, 1,500). Gazzetta's moderators (2 full-time, plus journalists occasionally helping) are occasionally instructed to be stricter (e.g., during violent events). To get a more accurate view of performance in normal situtations, we manually re-moderated (labeled as 'accept' or 'reject') the comments of G-TEST-S, producing G-TEST-S-R. The reject ratio is approx. 30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R. Each G-TEST-S-R comment was re-moderated by five annotators. Krippendorff's (2004) alpha was 0.4762, close to the value (0.45) reported by<cite> Wulczyn et al. (2017)</cite> for the Wikipedia 'attacks' dataset. Using Cohen's Kappa (Cohen, 1960) , the mean pairwise agreement was 0.4749. The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%. Cohen's Kappa and Krippendorff's alpha lead to lower scores, because they account for agreement by chance, which is high when there is class imbalance (22% reject, 78% accept in G-TEST-S-R).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_4",
  "x": "The Wikipedia 'attacks' dataset <cite>(Wulczyn et al., 2017)</cite> contains approx. 115K English Wikipedia talk page comments, which were labeled as containing personal attacks or not. Each comment was labeled by at least 10 annotators. Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff's (2004) alpha, was 0.45. The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject). Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels. 7 The dataset is split in three parts (Table 1) : training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_5",
  "x": [
   "7 The dataset is split in three parts (Table 1) : training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens). Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not. Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space."
  ],
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_6",
  "x": "The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not. Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space. ---------------------------------- **METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists. ---------------------------------- **DETOX**",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_7",
  "x": "Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space. ---------------------------------- **METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists. ---------------------------------- **DETOX** DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_8",
  "x": "---------------------------------- **DETOX** DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_9",
  "x": "DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it. An API for Perspective is available at https://www.perspectiveapi. com/, but we did not have access to the API at the time the experiments of this paper were carried out. for the MLP on W-ATT-DEV. 9 The tuning also selected probabilistic labels for Wikipedia, as in the work of Wulczyn et al.",
  "y": "differences"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_12",
  "x": "The decisions of the systems are always probabilistic. Table 2 : Comment classification results. Scores reported by<cite> Wulczyn et al. (2017)</cite> are shown in brackets. always better than CNN and DETOX; there is no clear winner between CNN and DETOX. Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to Table 2 . Also, da-CENT is always worse than a-RNN and RNN, confirming that the hidden states (intuitively, context-aware word embeddings) of the RNN chain are important, even with the attention mechanism. Increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAIN-L) significantly improves the performance of all methods. The implementation of DETOX could not handle the size of G-TRAIN-L, which is why we do not report DETOX results for G-TRAIN-L. Notice, also, that the Wikipedia dataset is easier than the Gazzetta one (all methods perform better on Wikipedia comments, compared to Gazzetta). Figure 5 shows F 2 (P reject , P accept ) on G-TEST-L and W-ATT-TEST, when t a , t r are tuned on G-DEV, W-ATT-DEV for varying coverage. For G-TEST-L, we show results training on G-TRAIN-S (solid lines) and G-TRAIN-L (dotted).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_13",
  "x": "Again, a-RNN and RNN are better than CNN and DETOX. All three deep learning methods benefit from the larger training set (dotted). In Wikipedia, a-RNN obtains P accept , P reject \u2265 0.94 for all coverages (Fig. 5, call-outs) . On the more difficult Gazzetta dataset, a-RNN still obtains P accept , P reject \u2265 0.85 when tuned for 50% coverage. When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high P accept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (0.59). On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%. We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets).",
  "y": "background uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_14",
  "x": "All three deep learning methods benefit from the larger training set (dotted). In Wikipedia, a-RNN obtains P accept , P reject \u2265 0.94 for all coverages (Fig. 5, call-outs) . On the more difficult Gazzetta dataset, a-RNN still obtains P accept , P reject \u2265 0.85 when tuned for 50% coverage. When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high P accept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (0.59). On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%. We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_15",
  "x": [
   "Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best results were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender. Warner and Hirschberg (2012) aimed to detect anti-semitic speech, experimenting with 9K paragraphs and a linear SVM. Their features consider windows of at most 5 tokens, examining the tokens of each window, their order, POS tags, Brown clusters etc., following Yarowsky (1994) . Cheng et al. (2015) aimed to predict which users would be banned from on-line communities. Their best system used a random forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). Sood et al. (2012a; 2012b) experimented with 6.5K comments from Yahoo Buzz, moderated via crowdsourcing."
  ],
  "y": "similarities uses"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_0",
  "x": "Therefore, to ensure the clarity and understandability of the written information, it is crucial to measure its readability. The significance of this measurement is apparent from its applications in different fields such as education [1, 2] , medical instructions [3] [4] [5] [6] [7] [8] , social media communications [9] , marketing and advertising [10] [11] [12] , and in some related fields of research like text simplification [13] [14] [15] . However, readability assessment entails some challenges. The first attempt to quantify the readability of the text was the manual intuition-based evaluation, which was done by human readability experts. Such evaluation is not standardized or globally correct; hence, researchers such as Flesch [16] has developed readability measurement formulas. These formulas use simple and manually calculable properties of the text, such as the number of syllables, words, or sentences in the text to assess its readability. These formulas become so popular that they are even widely used nowadays. Nonetheless, the low accuracy of such formulas and their language dependency made way for more advanced and accurate readability assessment methods, which involve machine learning techniques. These models are highly accurate for their use of sophisticated NLP features and machine intelligence to associate the extracted features to a proper readability level. Models proposed by Vajjala and Meurers [17] , Xia et al. <cite>[18]</cite> , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_1",
  "x": "Different models are required to assess the readability of English texts for the second language readers as a different set of characteristics of text is influential on its readability level for second language readers [35] . Xia et al. <cite>[18]</cite> has published a thorough study on second language text readability assessment. Similar to the study done by Vajjala and Meurers [17] , Xia has used an SVM classifier, and a set of NLP features consists of traditional, lexico-semantic, parse tree syntactic, language modeling, and discourse-based features. Many comparable studies have been carried out to create automated text readability assessment models for languages such as French [36] Russian [37] Germen [38] Chinese [39] Arabic [40] Portuguese [41] . This study is concentrated on the English and Persian languages as our test case for multilingual text readability assessment. The only known machine learning based text readability assessment model for the Persian language is the model proposed by Mohammadi and Khasteh [19] , which also uses an SVM model. In conclusion, machine learning models can obtain higher accuracies in contrast to the traditional formulas while being more straightforward to construct, assuming the existence of a useful dataset. In contrast, their use of a large number of sophisticated features makes them time-consuming and costly to implement, language-dependent, and less interpretable. The focus of this study is to reduce the need for intricate feature engineering and language dependency of text readability assessment models. In order to overcome these problems, the current advances in deep learning, reinforcement learning, and their mixture, deep reinforcement learning became advantageous.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_2",
  "x": "A sum of 3145 texts is used for evaluation purposes, which is similar to the original Weebit paper [17] . Prior to this study, distinct models have been used to assess the readability of English texts for second language readers. Since the DRL model eliminates the need for specific feature engineering for different types of text readability assessment, the proposed model can be applied to second language datasets without any modifications. To examine the proposed DRL model regarding this ability, it is applied to the Cambridge Exams dataset <cite>[18]</cite> . This dataset contains texts from the reading section of Cambridge English Exams, which is targeted for students at five readability levels (A2 to C2) of the Common European Framework of Reference (CEFR). This dataset contains 331 texts, which makes it a small dataset in comparison to the Weebit dataset. The automated feature extraction ability of the proposed model has also given the model the ability to be readily applied to other languages. As GloVe and language models are language-specific, the only necessary change is the use of the GloVe and frequency language model of the target language. These features are readily and freely available on the internet for many languages. The proposed model is evaluated on the Persian text readability dataset [19] .",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_0",
  "x": "Knowledge graphs, for example, consist of entities and relations between them [1, 2, 3, 4] . Representation learning [5, 6, 7, 8] and reasoning [9, 10, 11, 12] with such structured representations is an important and active area of research. Most previous work on knowledge representation and reasoning relies on a pipeline of natural language processing systems, often consisting of named entity extraction [13] , entity resolution and coreference [14] , relationship extraction [5] , and knowledge graph inference [15] . While this cascaded approach of using NLP systems can be effective at reasoning with knowledge bases at scale, it also leads to a problem of compounding of the error from each component sub-system. The importance of each of these sub-component on a particular downstream application is also not very clear. For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning.",
  "y": "background motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_1",
  "x": "Most previous work on knowledge representation and reasoning relies on a pipeline of natural language processing systems, often consisting of named entity extraction [13] , entity resolution and coreference [14] , relationship extraction [5] , and knowledge graph inference [15] . While this cascaded approach of using NLP systems can be effective at reasoning with knowledge bases at scale, it also leads to a problem of compounding of the error from each component sub-system. The importance of each of these sub-component on a particular downstream application is also not very clear. For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_2",
  "x": "For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_3",
  "x": "In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks. ---------------------------------- **RELNET MODEL** We describe the RelNet model in this section. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory.",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_4",
  "x": "We describe the RelNet model in this section. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network.",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_5",
  "x": "The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] : Dynamic Relational Memory This is the main component of an end-to-end reasoning pipeline, where we need to process the information contained in the text such that it can be used to reason about the entities, their properties and the relationships among them. The memory consists of two parts: entity memory and relational memory. The entity memory is organized as a key-value memory network [12] , where the keys are global embeddings updated during training time but not during inference, and the value memory slot is a dynamic memory for each example (document, question) whose values are updated while reading the document. The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K).",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_6",
  "x": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] :",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_7",
  "x": "The entity memory is organized as a key-value memory network [12] , where the keys are global embeddings updated during training time but not during inference, and the value memory slot is a dynamic memory for each example (document, question) whose values are updated while reading the document. The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K). At time t, after reading the sentence t into a vector representation s t , a gating mechanism decides the set of memories to be updated (< \u00b7, \u00b7 > denotes inner product): Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities. The update in (1) essentially does a soft selection of memory slots based on cosine distance in the embedding space. Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> . After selecting the set of memories, there is an update step which stores information in the corresponding memory slots: where PReLU is a parametric Rectified linear unit [21] , and U , V and W are k \u00d7 k parameter matrices.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_8",
  "x": "Dynamic Relational Memory This is the main component of an end-to-end reasoning pipeline, where we need to process the information contained in the text such that it can be used to reason about the entities, their properties and the relationships among them. The memory consists of two parts: entity memory and relational memory. The entity memory is organized as a key-value memory network [12] , where the keys are global embeddings updated during training time but not during inference, and the value memory slot is a dynamic memory for each example (document, question) whose values are updated while reading the document. The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K). At time t, after reading the sentence t into a vector representation s t , a gating mechanism decides the set of memories to be updated (< \u00b7, \u00b7 > denotes inner product): Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities. The update in (1) essentially does a soft selection of memory slots based on cosine distance in the embedding space. Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> .",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_9",
  "x": "First, a gating mechanism decides the set of active relational memories: where g m i , g m j select the relational memory slot based on the active entity slots and the last sigmoid gate decides whether the corresponding relational memory needs to be updated based on the current input sentence. After selecting the set of active relational memory, we update the contents of the relational memory:r ij \u2190 P ReLU (Ar ij + Bs t ) r ij \u2190 r ij + g r ij \u2299r ij (4) where again A, B are k \u00d7 k parameter matrices. Note that for updates (3)- (4) we use a different encoding mask to obtain the sentence representation for relations. Similar to [<cite>18</cite>] , we normalize the memories after each update step (that is after reading each sentence). This acts as a forget step and does not cause the memory to explode. The full memory consists of the entity memory slots {h j } and the relational memory slots {r ij }. Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] . The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_10",
  "x": "Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] . The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer: where y is the predicted answer, and C, H, Z are parameter matrices. ---------------------------------- **RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] .",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_11",
  "x": "The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer: where y is the predicted answer, and C, H, Z are parameter matrices. ---------------------------------- **RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] . As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_12",
  "x": "The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer: where y is the predicted answer, and C, H, Z are parameter matrices. ---------------------------------- **RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] . As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_14",
  "x": "Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_15",
  "x": "**EXPERIMENTS** We evaluate the model's performance on the bAbI tasks [19] , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We Task <cite>EntNet</cite> [<cite>18</cite>] compare the performance with the <cite>Recurrent Entity Networks model (EntNet)</cite> <cite>[18]</cite> . Performance is measured in terms of mean percentage error on the tasks. Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> .",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_16",
  "x": "embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ---------------------------------- **CONCLUSION** We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering.",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_17",
  "x": "embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ---------------------------------- **CONCLUSION** We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_0",
  "x": "---------------------------------- **INTRODUCTION** Multi-task learning (MTL) is an important machine learning paradigm that aims at improving the generalization performance of a task using other related tasks. Such framework has been widely studied by Thrun (1996) ; Caruana (1997) ; Evgeniou & Pontil (2004) ; Ando & Zhang (2005) ; Argyriou et al. (2007) ; Kumar & III (2012) , among many others. In the context of deep neural networks, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015) , to vision (Donahue et al., 2014) , and speech (Heigold et al., 2013; Huang et al., 2013) . Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_1",
  "x": "Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1 . We also explore two unsupervised learning objectives, sequence autoencoders (Dai & Le, 2015) and skip-thought vectors , and reveal their interesting properties in the MTL setting: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",
  "y": "motivation"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_2",
  "x": "Such framework has been widely studied by Thrun (1996) ; Caruana (1997) ; Evgeniou & Pontil (2004) ; Ando & Zhang (2005) ; Argyriou et al. (2007) ; Kumar & III (2012) , among many others. In the context of deep neural networks, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015) , to vision (Donahue et al., 2014) , and speech (Heigold et al., 2013; Huang et al., 2013) . Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_3",
  "x": "Recent work such as Jean et al., 2015a; Luong et al., 2015a; <cite>Vinyals et al., 2015a)</cite> has found that it is crucial to empower seq2seq models with the attention mechanism. ---------------------------------- **MULTI-TASK SEQUENCE-TO-SEQUENCE LEARNING** We generalize the work of Dong et al. (2015) to the multi-task sequence-to-sequence learning setting that includes the tasks of machine translation (MT), constituency parsing, and image caption generation. Depending which tasks involved, we propose to categorize multi-task seq2seq learning into three general settings. In addition, we will discuss the unsupervised learning tasks considered as well as the learning process. ---------------------------------- **ONE-TO-MANY SETTING** This scheme involves one encoder and multiple decoders for tasks in which the encoder can be shared, as illustrated in Figure 2 . The input to each task is a sequence of English words.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_5",
  "x": "Following Luong et al. (2015a) , we use the 50K most frequent words for each language from the training corpus. 1 These vocabularies are then shared with other tasks, except for parsing in which the target \"language\" has a vocabulary of 104 tags. We use newstest2013 (3000 sentences) as a validation set to select our hyperparameters, e.g., mixing coefficients. For testing, to be comparable with existing results in (Luong et al., 2015a) For the unsupervised tasks, we use the English and German monolingual corpora from WMT'15. 4 Since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the same validation and test sets as the accompanied translation tasks. For constituency parsing, we experiment with two types of corpora: 1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by<cite> Vinyals et al. (2015a)</cite> . The two parsing tasks, however, are evaluated on the same validation (section 22) and test (section 23) sets from the PTB data. Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> . Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b) .",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_6",
  "x": "The two parsing tasks, however, are evaluated on the same validation (section 22) and test (section 23) sets from the PTB data. Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> . Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b) . ---------------------------------- **TRAINING DETAILS** In all experiments, following Sutskever et al. (2014) and Luong et al. (2015b) , we train deep LSTM models as follows: (a) we use 4 LSTM layers each of which has 1000-dimensional cells and embeddings, 5 (b) parameters are uniformly initialized in [-0.06, 0.06] , (c) we use a mini-batch size of 128, (d) dropout is applied with probability of 0.2 over vertical connections (Pham et al., 2014) , (e) we use SGD with a fixed learning rate of 0.7, (f) input sequences are reversed, and lastly, (g) we use a simple finetuning schedule -after x epochs, we halve the learning rate every y epochs. The values x and y are referred as finetune start and finetune cycle in Table 1 together with the number of training epochs per task. As described in Section 3, for each multi-task experiment, we need to choose one task to be the reference task (which corresponds to \u03b1 1 = 1). The choice of the reference task helps specify the number of training epochs and the finetune start/cycle values which we also when training that reference task alone for fair comparison. To make sure our findings are reliable, we run each experimental configuration twice and report the average performance in the format mean (stddev).",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_7",
  "x": "**LARGE TASKS WITH SMALL TASKS** In this setting, we want to understand if a small task such as PTB parsing can help improve the performance of a large task such as translation. Since the parsing task maps from a sequence of English words to a sequence of parsing tags<cite> (Vinyals et al., 2015a)</cite> , only the encoder can be shared with an English\u2192German translation task. As a result, this is a one-to-many MTL scenario ( \u00a73.1). To our surprise, the results in Table 2 suggest that by adding a very small number of parsing minibatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we can improve the translation quality substantially. More concretely, our best multi-task model yields a gain of +1.5 BLEU points over the single-task baseline. It is worth pointing out that as shown in Table 2 , our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a) . Larger mixing coefficients, however, overfit the small PTB corpus; hence, achieve smaller gains in translation quality. For parsing, as<cite> Vinyals et al. (2015a)</cite> have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneficial for parsing, yielding an improvement of up to +8.9 F 1 points over the baseline.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_8",
  "x": "It is worth pointing out that as shown in Table 2 , our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a) . Larger mixing coefficients, however, overfit the small PTB corpus; hence, achieve smaller gains in translation quality. For parsing, as<cite> Vinyals et al. (2015a)</cite> have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneficial for parsing, yielding an improvement of up to +8.9 F 1 points over the baseline. 6 It would be interesting to study how MTL can be useful with the presence of the attention mechanism, which we leave for future work. ---------------------------------- **TASK** ---------------------------------- **LARGE TASKS WITH MEDIUM TASKS** We investigate whether the same pattern carries over to a medium task such as image caption generation.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_9",
  "x": "---------------------------------- **LARGE TASKS WITH LARGE TASKS** Our first set of experiments is almost the same as the one-to-many setting in Section 4.3.1 which combines translation, as the reference task, with parsing. The only difference is in terms of parsing Table 2 . Reference tasks are in italic with mixing ratios in parentheses. The average results of 2 runs are in mean (stddev) format. data. Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-confidence (HC) corpus, which is provided by<cite> Vinyals et al. (2015a)</cite> . As highlighted in Table 4 , the trend is consistent; MTL helps boost translation quality by up to +0.9 BLEU points. Table 4 : English\u2192German WMT'14 translation -shown are perplexities (ppl) and BLEU scores of various translation models.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_10",
  "x": "Mixing ratios are in parentheses and the average results over 2 runs are in mean (stddev) format. Best results are bolded. ---------------------------------- **TASK TRANSLATION** The second set of experiments shifts the attention to parsing by having it as the reference task. We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_11",
  "x": "**TASK TRANSLATION** The second set of experiments shifts the attention to parsing by having it as the reference task. We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_12",
  "x": "Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models. For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_13",
  "x": "Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models. For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_0",
  "x": "Semantic Role Labeling (SRL) is the process of extracting simple event structures, i.e., \"who\" did \"what\" to \"whom\", \"when\" and \"where\". Current systems usually perform SRL in two pipelined steps: argument identification and argument classification. While identification is mostly syntactic, classification requires semantic knowledge to be taken into account. Semantic information is usually captured through lexicalized features on the predicate and the head-word of the argument to be classified. Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL-2004 (Carreras and M\u00e0rquez, 2005 observed that all systems showed a significant performance degradation (\u223c10 F 1 points) when applied to test data from a different genre of that of the training set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words.",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_1",
  "x": "In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models. ---------------------------------- **SPS FOR SR CLASSIFICATION** SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_2",
  "x": "The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models. ---------------------------------- **SPS FOR SR CLASSIFICATION** SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification.",
  "y": "motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_3",
  "x": "(2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models. ---------------------------------- **SPS FOR SR CLASSIFICATION** SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ).",
  "y": "motivation background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_4",
  "x": "SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_5",
  "x": "Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible. Distributional similarity: Following (<cite>Zapirain et al., 2009</cite>) we considered both first order and second order similarity. In first order similarity, the similarity of two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words.",
  "y": "uses"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_6",
  "x": "In first order similarity, the similarity of two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad\u00f3 and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad\u00f3 and Lapata, 2007, p. 179 ). We will refer to these similarities as sim cos and sim Jac , respectively. In contrast, second order similarity uses vectors of similar words, i.e., the similarity of two words was computed using the cosine (or Jaccard measure) between the thesaurus entries of those words in Lin's thesaurus (Lin, 1998) . We refer to these as sim 2 cos and sim 2 Jac . Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase).",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_7",
  "x": "We refer to these as sim 2 cos and sim 2 Jac . Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP). For NPs, which tend to be core arguments 2 , we use the SPs of the verb (as formalized above). For PPs, which have an even distribution between core and adjunct arguments, we use the SPs of the prepositions alone, ignoring the verbs. Implementation wise, this means that in Eq. (1), we change v for p, where p is the preposition heading the PP.",
  "y": "extends differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_0",
  "x": "The models that are generally used are often slow to train, and have a large number of parameters. <cite>Dyer et al. (2013)</cite> present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4. While this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power. For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely. However, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences. For example, when aligning to Dutch, it may be common for one verb to be aligned near the end of the sentence that would be at the beginning in English. This would mean most of the other words in the sentence would also align slightly away from the diagonal in one direction. Figure 1 shows an example sentence in which this happens. Here, a circle denotes an alignment, and darker squares are more likely under the alignment model. In this case the modified Model 2 would simply make both directions equally likely, where we would really like for only one direction to be more likely.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_1",
  "x": "Here, a circle denotes an alignment, and darker squares are more likely under the alignment model. In this case the modified Model 2 would simply make both directions equally likely, where we would really like for only one direction to be more likely. In some cases it could be that the prior probability for a word alignment should be off the diagonal. Furthermore, it is common in word alignment to take word classes into account. This is commonly implemented for the HMM alignment model as well as Models 4 and 5. Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is. However, using the reparameterization in<cite> (Dyer et al., 2013)</cite> would leave the model simple enough even with a relatively large amount of word classes. word class, and so can have different gradients for alignment probability over the english words. If the model has learned that prepositions and nouns are more likely to align to words later in the sentence, it could have a lower lambda for both word classes, resulting in a less steep slope.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_2",
  "x": "Finally, instead of just having one side of the diagonal less steep than the other, it may be useful to instead move the peak of the alignment probability function off the diagonal, while keeping it equally likely. In Figure 2 , this is done for the past participle 'gezien'. We will present a simple model for adding the above extensions to achieve the above (splitting the parameter, adding an offset and conditioning the parameters on the POS tag of the target word) in section 2, results on a set of experiments in section 3 and present our conclusions in section 4. ---------------------------------- **METHODS** We make use of a modified version of Model 2, from <cite>Dyer et al. (2013)</cite> , which has an alignment model that is parameterised in its original form solely on the variable \u03bb. Specifically, the probability of a sentence e given a sentence f is given as: here, m is the length of the target sentence e, n the same for source sentence f , \u03b4 is the alignment model and \u03b8 is the translation model. In this paper we are mainly concerned with the alignment model \u03b4. In the original formulation (with a minor tweak to ensure symmetry through the center), this function is defined as:",
  "y": "extends"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_3",
  "x": "we change the definition of h(\u00b7) to the following instead: + \u03c9 otherwise j \u2193 is the point closest to or on the diagonal here, calculated as: Here, \u03c9 can range from \u22121 to 1, and thus the calculation for the diagonal j \u2193 is clamped to be in a valid range for alignments. As the partition function (Z(\u00b7)) used in<cite> (Dyer et al., 2013)</cite> consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute \u03b3 for the geometric series calculations in order to use different parameters for each: where j \u2191 is j \u2193 + 1. ---------------------------------- **OPTIMIZING THE PARAMETERS** As in the original formulation, we need to use gradient-based optimisation in order to find good values for \u03bb, \u03b3 and \u03c9. Unfortunately, optimizing \u03c9 would require taking the derivative of h(\u00b7), and thus the derivative of the absolute value. This is unfortunately undefined when the argument is 0, however we work around this by choosing a subgradient of 0 at that point.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_4",
  "x": "And similar for finding the first derivative with respect to \u03b3, but summing from j \u2191 to n instead. The first derivative with respect to \u03c9 then, is: Where h (\u00b7) is the first derivative of h(\u00b7) with respect to \u03c9. For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in<cite> (Dyer et al., 2013)</cite> . Conditioning on the POS of the target words then becomes as simple as using a different \u03bb, \u03b3, and \u03c9 for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag. A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. ---------------------------------- **EMPIRICAL RESULTS** The above described model is evaluated with experiments on a set of 3 language pairs, on which AER scores and BLEU scores are computed.",
  "y": "differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_5",
  "x": "A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. ---------------------------------- **EMPIRICAL RESULTS** The above described model is evaluated with experiments on a set of 3 language pairs, on which AER scores and BLEU scores are computed. We use similar corpora as used in<cite> (Dyer et al., 2013)</cite> : a French-English corpus made up of Europarl version 7 and news-commentary corpora, the ArabicEnglish parallel data consisting of the non-UN portions of the NIST training corpora, and the FBIS Chinese-English corpora. The models that are compared are the original reparameterization of Model 2, a version where \u03bb is split around the diagonal (split), one where pos tags are used, but \u03bb is not split around the diagonal (pos), one where an offset is used, but parameters aren't split about the diagonal (offset), one that's split about the diagonal and uses pos tags used as in<cite> (Dyer et al., 2013)</cite> , with stepsize for updates to \u03bb and \u03b3 during gradient ascent is 1000, and that for \u03c9 is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both \u03bb and \u03b3 are initialised to 6, and \u03c9 is initialised to 0. For these experiments the pos and pos & split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000) , using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003) . For the comparisons in AER, the corpora are used as-is, but for the BLEU comparisons, sentences longer than 50 words are filtered out.",
  "y": "similarities"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_0",
  "x": "One of the main advantages of the corpora techniques is that they can build lexicons that are tailored to a specific application simply by using a specific corpus. Currently, only anecdotal observations and data from other areas of language processing plead in favour of the utility of specific corpora (Bestgen, 2002 (Bestgen, , 2006 . This research aims at testing this hypothesis explicitly. ---------------------------------- **DETERMINING THE AFFECTIVE VALENCE OF WORDS FROM SMALL CORPORA** To my knowledge, the first researchers to propose an automatic procedure to determine the valence of words on the basis of corpora are Hatzivassiloglou and McKeown (1997) . Their algorithm aims to infer the semantic orientation of adjectives on the basis of an analysis of their co-occurrences with conjunctions. The main limitation of their algorithm is that it was developed specifically for adjectives and that the question of its application to other grammatical categories has not been solved <cite>(Turney & Littman, 2003)</cite> . If several other techniques have been proposed to determine affective valence from corpora, only a few of them have been designed to work with relatively small corpora (ten million words or fewer), a necessary property for building specific affective lexicons. Two techniques that fulfil this condition are described below.",
  "y": "motivation"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_1",
  "x": "Two techniques that fulfil this condition are described below. ---------------------------------- **SO-LSA** The technique proposed by<cite> Turney and Littman (2003)</cite> tries to infer semantic orientation from semantic association in a corpus. It is based on the semantic proximity between a target word and fourteen benchmarks: seven with positive valence and seven with negative valence (see Table 1 ). A word is considered as positive if it is closer to the positive benchmarks and further away from the negative benchmarks. Turney and Littman proposed two techniques for estimating the strength of the semantic association between words on the basis of corpora. The first technique estimates the semantic proximity between a word and a benchmark on the basis of the frequency with which they co-occur. Its main limitation is that it requires a very large corpus to be effective. Turney and Littman (2003)<cite> Turney and Littman (2003)</cite> .",
  "y": "background"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_3",
  "x": "The inter-rater agreement, computed by means of Cronbach's alpha, was very high (0.93). The average correlation between the ratings of one participant and the average ratings of all the other participants was 0.75 (the leave-one-out technique). The average correlation between the ratings provided by two participants was 0.60. A detailed presentation of the procedure used to build the materials is given in Bestgen, Fairon and Kevers (2004) . ---------------------------------- **METHOD** The two techniques described above were used in this experiment. The fourteen SO-LSA benchmarks chosen by<cite> Turney and Littman (2003)</cite> were translated into 2 Each sentence was automatically modified so as to replace the name and the description of the function of every individual by a generic first name of adequate sex (Mary, John, etc.) in order to prevent the judges being influenced by their prior positive or negative opinion about these people. French (bon, gentil, excellent, positif, heureux, correct et sup\u00e9rieur: mauvais, m\u00e9chant, m\u00e9diocre, n\u00e9gatif, malheureux, faux et inf\u00e9rieur) . For DI-LSA, a French lexicon made up of 3000 words evaluated on the pleasant-unpleasant scale was used (Hogenraad et al., 1995) .",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_0",
  "x": "****A DEEP DIVE INTO WORD SENSE DISAMBIGUATION WITH LSTM**** **ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_1",
  "x": "****A DEEP DIVE INTO WORD SENSE DISAMBIGUATION WITH LSTM**** **ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_2",
  "x": "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available. ---------------------------------- **INTRODUCTION** Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998) .",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_3",
  "x": "Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available. ---------------------------------- **INTRODUCTION** Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998) . Many approaches have been proposed -the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010) , SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch\u00fctze, 2017) , and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015) . In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016) . These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_4",
  "x": "Many approaches have been proposed -the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010) , SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch\u00fctze, 2017) , and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015) . In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016) . These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique.",
  "y": "motivation"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_5",
  "x": "Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique. This investigation aimed at understanding how sensitive the WSD approach is w.r.t. the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets. The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community.",
  "y": "extends"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_6",
  "x": "the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets. The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings. Finally, we show that the limited sense coverage in the annotated datasets is a major limitation, as shown by the fact that resulting model does not have a representation for more than 30% of the meanings which should have been considered for disambiguating the test sets.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_7",
  "x": "The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings. Finally, we show that the limited sense coverage in the annotated datasets is a major limitation, as shown by the fact that resulting model does not have a representation for more than 30% of the meanings which should have been considered for disambiguating the test sets. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_8",
  "x": "One example is provided by Iacobacci et al. (2016) , who investigated the role of word embeddings as features in a WSD system. Four methods (concatenation, average, fractional decay, and exponential decay) are used to extract features from the sentential context using word embeddings. The features are then added to the default feature set of IMS (Zhong and Ng, 2010) . Moreover, Raganato et al. (2017b) present a number of end-to-end neural WSD architectures. The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels). Melamud et al. (2016) also make use of unannotated data to train a BLSTM. The work by <cite>Yuan et al. (2016)</cite> , which we consider in this paper, belongs to this last category. Different from Melamud et al. (2016) , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated. We describe this approach in more detail in the following section. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_9",
  "x": "The method proposed by <cite>Yuan et al. (2016)</cite> performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997 ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (Sutskever et al., 2014; Dyer et al., 2015; He et al., 2017, among others) . Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 \u2264 k \u2264 n) by a special token $. The model takes this new sentence as input and produces a context vector c of dimensionality p (see Figure 1 ).",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_10",
  "x": "Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 \u2264 k \u2264 n) by a special token $. The model takes this new sentence as input and produces a context vector c of dimensionality p (see Figure 1 ). 1 Each word w in the vocabulary V is associated with an embedding \u03c6 o (w) of the same dimensionality. The model is trained to predict the omitted word, minimizing the softmax loss over a big collection D of sentences. After the model is trained, we can use it to extract context embeddings, i.e., latent numerical representations of the sentence surrounding a given word. Calculating Sense Embeddings. The model produced by the LSTM network is meant to capture the \"meaning\" of words in the context they are mentioned.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_11",
  "x": "Then, the procedure looks up the corresponding sense embeddings s 1 , . . . , s n computed in the previous step. 3. The procedure invokes a subroutine to choose one of the n senses for the context vector c t . It selects the sense whose vector is closest to c t using cosine as the similarity function. Label Propagation. <cite>Yuan et al. (2016)</cite> argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior. For this reason, they propose to use label propagation for inference as an alternative to averaging. Label propagation (Zhu and Ghahramani, 2002 ) is a classic semi-supervised algorithm that has been employed in WSD (Niu et al., 2005) and other NLP tasks (Chen et al., 2006; Zhou, 2011) .",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_12",
  "x": "---------------------------------- **REPRODUCTION STUDY: METHODOLOGY** Before we report the results of our experiments, we describe the datasets used and give some details regarding our implementation. Training data. The 100-billion-token corpus used in the original publication is not publicly available. Therefore, for the training of the LSTM models, we used the English Gigaword Fifth Edition (Linguistic Data Consortium (LDC) catalog number LDC2011T07). The corpus consists of 1.8 billion tokens in 4.1 million documents, originated from four major news agencies. We leave the study of bigger corpora for future work. For the training of the sense embeddings, we use the same two corpora used by <cite>Yuan et al. (2016)</cite>: 1. SemCor (Miller et al., 1993 ) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (Francis and Kucera, 1979) and cover various genres.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_13",
  "x": "4 The articles contain Table 1 : Performance of our implementation compared to already published results. We report the model/method used to perform WSD, the used annotated dataset and scorer, and F1 for each test set. In the naming of our models, LSTM indicates that the averaging technique was used for the sense assignment, while LSTMLP refers to the results obtained using label propagation (see Section 3). The datasets following T: indicate the annotated corpus used to represent the senses while U:OMSTI stands for using OMSTI as unlabeled sentences in case label propagation is used. P: SemCor indicates that sense distributions from SemCor are used in the system architecture. Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by <cite>Yuan et al. (2016)</cite> while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_14",
  "x": "**RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU. The whole training process took four months. We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_15",
  "x": "1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%. ---------------------------------- **RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU. The whole training process took four months.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_16",
  "x": "The whole training process took four months. We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared. However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how <cite>Yuan et al. (2016)</cite> handled these cases. In the WSD evaluation framework (Moro et al., 2014 ) that we selected for evaluation, these cases were either re-annotated or removed. Thus, our F 1 on senseval2 cannot be directly compared with the F 1 in the original paper. From a first glance at Table 1 , we observe that if we use SemCor to train the synset embeddings, then our results come close to the state-of-the-art on senseval2 (0.720 vs. 0.733).",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_17",
  "x": "We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared. However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how <cite>Yuan et al. (2016)</cite> handled these cases. In the WSD evaluation framework (Moro et al., 2014 ) that we selected for evaluation, these cases were either re-annotated or removed. Thus, our F 1 on senseval2 cannot be directly compared with the F 1 in the original paper. From a first glance at Table 1 , we observe that if we use SemCor to train the synset embeddings, then our results come close to the state-of-the-art on senseval2 (0.720 vs. 0.733). On semeval2013, we achieve results comparable to other embeddings-based approaches (Raganato et al., 2017b; Iacobacci et Table 2 : Performance of our implementation with respect to MFS and LFS recall.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_19",
  "x": "However, the gap with the graph-based approach of Weissenborn et al. (2015) is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets. Most-vs. less-frequent-sense instances. The original paper only analyses the performance on the whole test sets. We extend this analysis by looking at the performance for disambiguating the most frequent-sense (MFS) and less-frequent-sense (LFS) instances. The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones. This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (Postma et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_20",
  "x": "Thus, our F 1 on senseval2 cannot be directly compared with the F 1 in the original paper. From a first glance at Table 1 , we observe that if we use SemCor to train the synset embeddings, then our results come close to the state-of-the-art on senseval2 (0.720 vs. 0.733). On semeval2013, we achieve results comparable to other embeddings-based approaches (Raganato et al., 2017b; Iacobacci et Table 2 : Performance of our implementation with respect to MFS and LFS recall. R mfs and R lfs are the recall on the mostfrequent-sense and least-frequent-sense instances respectively. n represents the number of considered instances. al., 2016; Melamud et al., 2016) . However, the gap with the graph-based approach of Weissenborn et al. (2015) is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_21",
  "x": "Table 2 shows that the method by <cite>Yuan et al. (2016)</cite> does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them). On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI. For comparison, the default system IMS (Zhong and Ng, 2010) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (Postma et al., 2016) and only reaches 0.33 with a large amount of annotated data. Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS. When we compare the results of the averaging technique using SemCor and OMSTI versus when we use label propagation, we notice an increase in the MFS recall (from 0.85 to 0.91), whereas the LFS recall drops from 0.40 to 0.32. Meaning coverage in annotated datasets. The WSD procedure depends on an annotated corpus to compose its sense representations, making missing annotations an insurmountable obstacle. In fact, annotated datasets only contain annotations for a proper subset of the possible candidate synsets listed in WordNet. We analyze this phenomenon using four statistics: 1. Candidate Coverage: For each test set, we performed a lookup in WordNet to determine the unique candidate synsets of all target lemmas.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_23",
  "x": "Impact of unannotated data and model size. Since unannotated data is abundant, it is tempting to use more and more data to train language models, hoping that better word embeddings would translate into improved WSD performance. The fact that <cite>Yuan et al. (2016)</cite> used a 100-billion-token corpus only reinforces this intuition. We empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train LSTM models and measure the corresponding WSD performance. More in particular, the size of the training data was set at 1%, 10%, 25%, and 100% of the GigaWord corpus (which contains 1.8 \u00d7 10 7 , 1.8 \u00d7 10 8 , 4.5 \u00d7 10 8 and 1.8 \u00d7 10 9 words, respectively). Figure 2a shows the effect of unannotated data volume on WSD performance. The data points at 100 billion (10 11 ) tokens correspond to <cite>Yuan et al. (2016)</cite> 's reported results. As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD. However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale). Extrapolating from this graph, to get a performance of 0.8 F 1 by adding more unannotated data, one would need a corpus of 10 12 tokens.",
  "y": "similarities uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_24",
  "x": "We, therefore, believe random fluctuation does not affect the interpretation of the results. ---------------------------------- **CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances. In addition, we identified that the limited sense coverage in annotated dataset places a potentially upper bound for the overall performance. The code with detailed replication instructions is available at: https://github.com/ cltl/wsd-dynamic-sense-vector and the trained models at: https://figshare.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_25",
  "x": "We, therefore, believe random fluctuation does not affect the interpretation of the results. ---------------------------------- **CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances. In addition, we identified that the limited sense coverage in annotated dataset places a potentially upper bound for the overall performance. The code with detailed replication instructions is available at: https://github.com/ cltl/wsd-dynamic-sense-vector and the trained models at: https://figshare.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_0",
  "x": "This paper proposes a new unsupervised technique for clustering a collection of documents written by distinct individuals into authorial components. We highlight the importance of utilizing syntactic structure to cluster documents by author, and demonstrate experimental results that show the method we outline performs on par with state-of-the-art techniques. Additionally, we argue that this feature set outperforms previous methods in cases where authors consciously emulate each other's style or are otherwise rhetorically similar. ---------------------------------- **INTRODUCTION** Unsupervised authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship. Nothing is assumed about each document except that it was written by a single author. Koppel et al. (2011) formulated this problem in a paper focused on clustering five books from the Hebrew Bible. They also consider a 'multi-author document' version of the problem: decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors. Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model. We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present.",
  "y": "background"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_1",
  "x": "We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present. Furthermore, this formulation precludes results dependent on a random document generation procedure. In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style. We demonstrate a general technique that can accurately discern multiple authors contained within the Books of Ezekiel and Jeremiah. Prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of Ezekiel and Jeremiah were written by multiple individuals. To cluster documents by true authorship, we propose that considering part-of-speech (POS) ngrams as features most distinctly identifies an individual writer. The use of syntactic structure in authorial research has been studied before. Baayen et al. (1996) introduced syntactic information measures for authorship attribution and Stamatatos (2009) argued that POS information could reflect a more reliable authorial fingerprint than lexical information. Both Zheng et al. (2006) and Layton et al. (2013) propose that syntactic feature sets are reliable predictors for authorial attribution, and Tschuggnall and Specht (2014) demonstrates, with modest success, authorial decomposition using pq-grams extracted from sentences' syntax trees. We found that by combining the feature set of POS n-grams with a clustering approach similar to the one presented by Akiva (2013) , our method of decomposition attains higher accuracy than Tschuggnall's method, which also considers grammatical style.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_2",
  "x": "Furthermore, this formulation precludes results dependent on a random document generation procedure. In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style. We demonstrate a general technique that can accurately discern multiple authors contained within the Books of Ezekiel and Jeremiah. Prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of Ezekiel and Jeremiah were written by multiple individuals. To cluster documents by true authorship, we propose that considering part-of-speech (POS) ngrams as features most distinctly identifies an individual writer. The use of syntactic structure in authorial research has been studied before. Baayen et al. (1996) introduced syntactic information measures for authorship attribution and Stamatatos (2009) argued that POS information could reflect a more reliable authorial fingerprint than lexical information. Both Zheng et al. (2006) and Layton et al. (2013) propose that syntactic feature sets are reliable predictors for authorial attribution, and Tschuggnall and Specht (2014) demonstrates, with modest success, authorial decomposition using pq-grams extracted from sentences' syntax trees. We found that by combining the feature set of POS n-grams with a clustering approach similar to the one presented by Akiva (2013) , our method of decomposition attains higher accuracy than Tschuggnall's method, which also considers grammatical style. Additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by Akiva (2013) and <cite>Aldebei (2015)</cite> , which both rely on word occurrences as features.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_3",
  "x": "---------------------------------- **CLARIFYING DETAILS WITH NYT COLUMNS** We shall describe a clustering of New York Times columns to clarify our framework. The NYT cor- pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> . The corpus is comprised of texts from four columnists: Gail Collins (274 documents), Maureen Dowd (298 documents), Thomas Friedman (279 documents), and Paul Krugman (331 documents). Each document is approximately the same length and the columnists discuss a variety of topics. Here we consider the binary (k = 2) case of clustering the set of 629 Dowd and Krugman documents into two groups. In step one, the documents are converted into their 'POS-translated' form as previously outlined. Each document is represented as a frequency vector that reflects all 3, 4, and 5-grams that appear in the 'POS-translated' corpus. This range of ngrams was determined through validation of different values for n across several datasets.",
  "y": "similarities"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_4",
  "x": "We found instead that our method performed exceptionally poorly, clustering these speeches with only 74.2% accuracy. Indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> -by replacing POS n-grams with ordinary word occurrences in step one -our framework performed very well, clustering at 95.3%. Similarly, our framework performed poorly on the Books of Ezekiel and Jeremiah from the Hebrew Bible. Using the English-translated King James Version, and considering each chapter as an individual document, our framework clusters the 48 chapters of Ezekiel and the 52 chapters of Jeremiah at 54.7%. Aldebei et al. (2015) reports 98.0% on this dataset, and when considering the original English text instead of the POS-translated text, our framework achieves 99.0%. The simultaneous success of word features and failure of POS features on these two datasets seemed to completely contradict our previous results. We propose two explanations. First, perhaps too much syntactic structure is lost during translation. This could certainly be a factor, but does not explain the Obama-McCain results. The second explanation comes from the wide consensus among biblical scholars that there was no single 'Ezekiel' or 'Jeremiah' entirely responsible for each book.",
  "y": "similarities"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_0",
  "x": "Detecting abuse on Twitter can be challenging, particularly because the text is often noisy. Abuse can also have different facets. [10] released one of the initial data sets from Twitter with the goal of identifying what constitutes racism and sexism. <cite>[9]</cite> in <cite>their work</cite> pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language. Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from [10] data set demonstrating online abuse They find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive. [4] introduced a large, hand-coded corpus of online harassment data for studying the nature of harassing comments and the culture of trolling. Keeping these motivations in mind, we make the following salient contributions: \u2022 We build a deep context-aware attention-based model for abusive behavior detection on Twitter . To the best of our knowledge ours is the first work that exploits context aware attention for this task. \u2022 Our model is robust and achieves consistent performance gains in all the three abusive data sets \u2022 We show how context aware attention helps in focusing on certain abusive keywords when used in specific context and improve the performance of abusive behavior detection .",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_1",
  "x": "<cite>[9]</cite> use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech. [2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by [10] . Their best-reported F1-score is achieved using Long Short Term Memory Networks (LSTM) + Gradient Boosting. On the data set released by [10] , [5] experiment with a two-step approach of detecting abusive language first and then classifying them into specific types i.e. racist, sexist or none. They achieve best results using a Hybrid Convolution Neural Network (CNN) with the intuition that character level input would counter the purposely or mistakenly misspelled words and made-up vocabularies. [6] in their work ran experiments on the Gazetta dataset and the DETOX system ( [12] ) and show that a Recurrent Neural Network (RNN) coupled with deep, classification-specific attention outperforms the previous state of the art in abusive comment moderation. In their more recent work [7] explored how user embeddings, user-type embeddings, and user type biases can improve their previous RNN based model on the Gazetta dataset. Attentive neural networks have been shown to perform well on a variety of NLP tasks ( [13] , [11] ). [13] use hierarchical contextual attention for text classification (i.e attention both at word and sentence level) on six large scale text classification tasks and demonstrate that the proposed architecture outperform previous methods by a substantial margin. We primarily focus on word level attention because most of the tweets are single sentence tweets.",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_3",
  "x": "---------------------------------- **DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter. [3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_4",
  "x": "---------------------------------- **DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter. [3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_5",
  "x": "The network is trained at a learning rate of 0.001 for 10 epochs, with a dropout of 0.2 to prevent over-fitting. The results are averaged over 10-fold cross-validations for D1 and D3 and 5 fold cross-validations for <cite>D2</cite> because <cite>[9]</cite> reported results using 5 fold CV. Because of class imbalance in all our data sets, we report weighted F1 scores. Table 3 shows our results in detail. We compare our model with the best models reported in each paper. Because [4] is a data set paper, we cannot fill the corresponding row. * denotes the numbers from baseline papers. All the results were reproducible except for the one marked red. For (Waseem and Hovy, 2016 ) data set, (Badjatiyaet al., 2017) claim that using Gradient Boosting with LSTM embeddings obtained from random word embeddings boosted their performance by 12 F1 from 81.0 to 93.0. When we tried to reproduce the result, we did not find any significant improvement over 81.",
  "y": "motivation"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_6",
  "x": "We also share some examples from the three data sets in Figure  2 which our BiLSTM attention model could not classify correctly. On closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle. The first tweet is a tweet from [10] , the second tweet is a tweet from from <cite>[9]</cite> data set and the third from the [4] ---------------------------------- **WHY CONTEXTUAL ATTENTION?** Attention mechanism enables our neural network to focus on the relevant parts of the input more than the irrelevant parts while performing a prediction task. But the relevance is often dependant on the context and so the importance of words is highly context dependent. For example, the word islam may appear in the realm of Racism as well as in any normal conversation. The top tweet in Figure 3 belongs to None class while the bottom tweet belongs to Racism class. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_0",
  "x": "****STRUCTURAL CORRESPONDENCE LEARNING FOR PARSE DISAMBIGUATION**** **ABSTRACT** The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. ---------------------------------- **INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_1",
  "x": "****STRUCTURAL CORRESPONDENCE LEARNING FOR PARSE DISAMBIGUATION**** **ABSTRACT** The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. ---------------------------------- **INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_2",
  "x": "---------------------------------- **INTRODUCTION** Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain (Gildea, 2001; Daum\u00e9 III, 2007) , we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001 ). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III and Marcu, 2006; Daum\u00e9 III, 2007;<cite> Blitzer et al., 2006</cite>; McClosky et al., 2006; . We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum\u00e9 III, 2007) : supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III, 2007) , besides the labeled source data, we have access to a comparably small, but labeled amount of target data.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_3",
  "x": "We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum\u00e9 III, 2007) : supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III, 2007) , besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation<cite> (Blitzer et al., 2006</cite>; McClosky et al., 2006; is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult. Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are \"surprisingly difficult to beat\" (Daum\u00e9 III, 2007) . Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daum\u00e9 III, 2007; Plank and van Noord, 2008) . ---------------------------------- **MOTIVATION AND PRIOR WORK** While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006;<cite> Blitzer et al., 2006</cite>; .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_5",
  "x": "---------------------------------- **MOTIVATION AND PRIOR WORK** While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006;<cite> Blitzer et al., 2006</cite>; . Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\". In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007 ). The system just ended up at rank 7 out of 8 teams.",
  "y": "background motivation"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_6",
  "x": "Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\". In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007 ). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets and a bug in their system (Shimizu and Nakagawa, 2007) , their results are inconclusive. 1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007) , i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996) . Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006 ). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_7",
  "x": "We examine the effectiveness of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006) . For our empirical eval-uation we explore Wikipedia as primary test and training collection. In the sequel, we first introduce the parsing system. Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices. In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study. 3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006 ) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000) .",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_8",
  "x": "They are used to train the baseline model on the given labeled source data. ---------------------------------- **STRUCTURAL CORRESPONDENCE LEARNING** ---------------------------------- **SCL** (Structural Correspondence Learning)<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as \"boring\" and \"repetitive\" are common ways to express negative sentiment in A, while \"not working\" or \"defective\" are specific to B. If there are features across the domains, e.g. \"don't buy\", with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_9",
  "x": "Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as \"boring\" and \"repetitive\" are common ways to express negative sentiment in A, while \"not working\" or \"defective\" are specific to B. If there are features across the domains, e.g. \"don't buy\", with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> . They are inspired by auxiliary problems from Ando and Zhang (2005) . Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available)<cite> (Blitzer et al., 2006)</cite> . The outline of the algorithm is given in Figure 1 . The first step is to identify m pivot features occurring frequently in the unlabeled data of both 4.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_10",
  "x": "Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> . They are inspired by auxiliary problems from Ando and Zhang (2005) . Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available)<cite> (Blitzer et al., 2006)</cite> . The outline of the algorithm is given in Figure 1 . The first step is to identify m pivot features occurring frequently in the unlabeled data of both 4. Apply SVD to W : :h,:] are the h top left singular vectors of W . 5. Apply projection x s \u03b8 and train a predictor on the original and new features obtained through the projection. domains. Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: \"Does pivot feature l occur in this instance?\".",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_11",
  "x": "domains. Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: \"Does pivot feature l occur in this instance?\". The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features. In this way, we obtain a weight vector w for each pivot predictor. Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature. Step 3 is to arrange the m weight vectors in a matrix W , where a column corresponds to a pivot predictor weight vector. Applying the projection W T x (where x is a training instance) would give us m new features, however, for \"both computational and statistical reasons\"<cite> (Blitzer et al., 2006</cite>; Ando and Zhang, 2005 ) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let \u03b8 = U T h\u00d7n be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features). The resulting \u03b8 is a projection onto a lower dimensional space R h , parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x, \u03b8x .",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_12",
  "x": "If \u03b8 contains meaningful correspondences, then the pre-dictor trained on the augmented data should transfer well to the new domain. ---------------------------------- **SCL FOR PARSE DISAMBIGUATION** A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input. So far, pivot features on the word level were used<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) , e.g. \"Does the bigram not buy occur in this document?\" (Blitzer, 2008) . Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand. For the task of parse disambiguation based on a conditional model this is not the case. Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data. In this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data (for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n-best).",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_13",
  "x": "As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types). In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred. We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold. In all our experiments, we set t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W , we disregard all negative entries in W and compute the SVD (W = U DV T ) on the resulting non-negative sparse matrix.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_14",
  "x": "**SELECTION OF PIVOT FEATURES** As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types). In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred. We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold. In all our experiments, we set t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_15",
  "x": "Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W , we disregard all negative entries in W and compute the SVD (W = U DV T ) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. ---------------------------------- **MATRIX AND SVD** ---------------------------------- **FURTHER PRACTICAL ISSUES OF SCL** In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006;<cite> Blitzer et al., 2006</cite>; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection \u03b8, in order to \"allow them to receive more weight from a regularized discriminative learner\".",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_16",
  "x": "When training the supervised model on the augmented feature space x, \u03b8x ,<cite> Blitzer et al. (2006)</cite> only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008) . Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2 ), and then to apply separate projection for every type. Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **EXPERIMENTAL DESIGN** The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_17",
  "x": "An extension suggested in Ando and Zhang (2005) is to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2 ), and then to apply separate projection for every type. Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **EXPERIMENTAL DESIGN** The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens. For parameter estimation of the disambiguation model, in all reported experiments we use the TADM 2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (\u03c3 2 =1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002) . For training the binary pivot predictors, we use the MegaM 3 Optimization Package with the socalled \"bernoulli implicit\" input format. To compute the SVD, we use SVDLIBC.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_18",
  "x": "In our empirical setup, we followed<cite> Blitzer et al. (2006)</cite> and tried to balance the size of source and target data. Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages. We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\"). Alternatively, we might have used a filter mechanism that excludes certain pages directly. In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory. Of course, the page itself is not included in that dataset. With regard to supercategories, we usually included all pages having a category c \u2208 super categories(p), unless stated otherwise. ---------------------------------- **TEST COLLECTION** Our testset consists of a selection of Wikipedia articles that have been manually corrected in the course of the D-Coi/LASSY project.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_19",
  "x": "This confirms the intuition that this specific subdomain is the \"hardest\", given that mathematical expressions might emerge in the data (e.g. \"Wet der distributiviteit : a(b+c) = ab+ac\" -distributivity law). Table 4 shows the results of our instantiation of SCL for parse disambiguation, with varying h parameter (dimensionality parameter; h = 25 means that applying the projection x\u03b8 resulted in adding 25 new features to every source domain instance). Table 4 : Results of our instantiation of SCL (with varying h parameter and no feature normalization). ---------------------------------- **SCL RESULTS** The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in \u03c6 measure (absolute error reduction). This corresponds to a relative error reduction of up to 7.29%. Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4) , which is in line with previous findings (Ando and Zhang, 2005;<cite> Blitzer et al., 2006)</cite> . Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time.",
  "y": "similarities"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_20",
  "x": "Feature normalization. We also tested feature normalization (as described in Section 4.2). While<cite> Blitzer et al. (2006)</cite> found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. A look at \u03b8 To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of \u03b8. Recall that applying a row of the projection matrix \u03b8 i to a training instance x gives us a new real-valued feature. If features from different domains have similar entries (scores) in the projection row, they are assumed to correspond (Blitzer, 2008) . Figure 4 shows example of correspondences that SCL found in the Prince dataset. The first column represents the score of a feature. The labels wiki and alp indicate the domain of the features, respectively.",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_21",
  "x": "The paper presents an application of Structural Correspondence Learning (SCL) to parse disam- Figure 5 : Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed. biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> . We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation. On the three examined datasets, SCL slightly but constantly outperformed the baseline. Applying SCL involves many design choices and practical issues, which we tried to depict here in detail. A novelty in our application is that we first actually parse the unlabeled data from both domains. This allows us to get a possibly noisy, but more abstract representation of the underlying data on which the pivot predictors are trained. In the near future, we plan to extend the work on semi-supervised domain adaptation for parse disambiguation, viz. (1) further explore/refine SCL (block SVDs, varying amount of target domain data, other testsets, etc.), and (2) examine selftraining.",
  "y": "motivation background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_22",
  "x": "While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> . We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation. On the three examined datasets, SCL slightly but constantly outperformed the baseline. Applying SCL involves many design choices and practical issues, which we tried to depict here in detail. A novelty in our application is that we first actually parse the unlabeled data from both domains. This allows us to get a possibly noisy, but more abstract representation of the underlying data on which the pivot predictors are trained. In the near future, we plan to extend the work on semi-supervised domain adaptation for parse disambiguation, viz. (1) further explore/refine SCL (block SVDs, varying amount of target domain data, other testsets, etc.), and (2) examine selftraining. Studies on the latter have focused mainly on generative, constituent based, i.e. data-driven parsing systems. Furthermore, from a machine learning point of view, it would be interesting to know a measure of corpus similarity to estimate the success of porting an NLP system from one domain to another.",
  "y": "differences"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_0",
  "x": "Relation extraction can also benefit from incorporating paraphrase generation into its processing pipeline (Romano et al., 2006) . Manually annotating translation references is expensive, and automatically generating references through paraphrasing has been shown to be effective for evaluation of machine translation (Zhou et al., 2006; Kauchak and Barzilay, 2006) . Datasets used for paraphrase generation include QUORA 1 , TWITTER (Lan et al., 2017) and MSCOCO (Lin et al., 2014) . Previous work on paraphrase generation that used these datasets (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics. In this paper, we find that simply using the input sentence as output in an unsupervised manner (i.e. fully parroting the input) significantly outperforms the state-of-the-art on two metrics for TWITTER, and on one metric for QUORA. Even after changing part of the input sentence (i.e. partially parroting the input), state-of-the-art metric scores can still be surpassed. Consequently, for future paraphrase generation research which achieve good evaluation scores, we suggest investigating whether their methods or models act differently from simple parroting behavior. ---------------------------------- **METHOD DESCRIPTION** Given an input sentence i, the goal of paraphrase generation is to generate an output sentence o which is semantically identical to i, but contain variations in lexicon or syntax.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_1",
  "x": "After processing the dataset, there are 149,650 unique sentences that have reference paraphrases. <cite>Gupta et al. (2018)</cite> sampled 4K sentences as their test set, but did not specify which sentences they used. Li et al.(2018) sampled 30K sentences as their test set, also not specifying which sentences they used. To avoid selecting a subset of data that is biased in favor of our method, we perform evaluation on the entire QUORA dataset. Although we evaluate on the entire dataset, the size of our training set is zero due to the fully unsupervised nature of full and partial parroting. We group sentences by the number of reference paraphrases they have, and plot the relative counts in Appendix A. It can be seen that over 64% of entries have only a single reference paraphrase, which is problematic because even if a paraphrase of good quality is generated for any one of these entries, BLEU, METEOR and TER scores could still be inferior if the generated paraphrase differs too much from the single reference paraphrase. Previous paraphrase generation work on QUORA (Gupta et al., 2018; Li et al., 2018) did not mention removing these entries, thus we include them in our experiments for fair comparison. However, we strongly recommend future work which wishes to use BLEU, METEOR and TER as evaluation metrics to only consider entries that have multiple reference paraphrases. TWITTER. There are 114,025 paraphrase sentence pairs in TWITTER, which were acquired by collecting tweets which contain identical URLs (Lan et al., 2017) .",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_2",
  "x": "We follow the same data processing steps as QUORA, and plot the number of reference paraphrases in Appendix A. MSCOCO. This is an image captioning dataset, with multiple captions provided for a single image (Lin et al., 2014) . There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) . The training and testing sets are available, containing 331,163 and 162,016 input sentences respectively. However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed. This makes the use of MSCOCO as a paraphrase generation dataset questionable. We plot the number of reference paraphrases in Appendix A. ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_3",
  "x": "As with QUORA, prior paraphrase generation work on this dataset (Li et al., 2018) did not provide their sampled test set sentences, so we evaluate parroting on the entire dataset to avoid bias. We follow the same data processing steps as QUORA, and plot the number of reference paraphrases in Appendix A. MSCOCO. This is an image captioning dataset, with multiple captions provided for a single image (Lin et al., 2014) . There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) . The training and testing sets are available, containing 331,163 and 162,016 input sentences respectively. However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed. This makes the use of MSCOCO as a paraphrase generation dataset questionable. We plot the number of reference paraphrases in Appendix A. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_4",
  "x": "Parroting performance on these sampled test sets can be found in Table 4 . It can be observed that the average metric scores for QUORA are similar to the scores in Table 1 , whereas the average scores for TWITTER are noticeably better than those in Table 2 . Furthermore, the score deviation between different samples is small. Consequently, although the exact test sets used by<cite> (Gupta et al., 2018)</cite> and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets. Partial parroting. We also introduce lexical variation into our parroting method by replacing or cutting words of the input sentence. For replacement, we substitute input words with an outof-vocabulary word not found in any of the input sentence's reference paraphrases. Paraphrase generation models are usually allowed to generate words which exist in reference paraphrases; we purposely use out-of-vocabulary words to give harsher scores to our method. words from the start of input sentences. For QUORA, when over 10% of the input sentence has been modified by being cut off, partial parroting underperforms the state-of-the-art by only 3.8% on METEOR.",
  "y": "future_work"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_0",
  "x": "State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011) . The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i , yielding the following coverage maximization objective, subject to the appropriate constraints: In proposing bigrams as concepts for their system,<cite> Gillick and Favre (2009)</cite> explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_1",
  "x": "Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities. We show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain. We evaluate coverage maximization incorporating syntactic and semantic concepts across three different domains: newswire, legal judgments, and Wikipedia articles. ---------------------------------- **CONCEPT COVERAGE MAXIMIZATION FOR EXTRACTIVE SUMMARIZATION** In extractive summarization, the unsupervised version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as possible. In the maximum coverage objective, concepts are considered as independent of each other.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_2",
  "x": "We evaluate coverage maximization incorporating syntactic and semantic concepts across three different domains: newswire, legal judgments, and Wikipedia articles. ---------------------------------- **CONCEPT COVERAGE MAXIMIZATION FOR EXTRACTIVE SUMMARIZATION** In extractive summarization, the unsupervised version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as possible. In the maximum coverage objective, concepts are considered as independent of each other. Concepts are weighted by the number of times they appear in a document. Moreover, due the NP-hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints. Bigrams.<cite> Gillick and Favre (2009)</cite> proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) by the frequency with which they occur in the document. Some pre-processing is first carried out to these bigrams: all bigrams consisting uniquely of stop-words are removed from consideration, and each word is stemmed. They also require bigrams to occur with a minimal frequency (cf. Section 3.2).",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_4",
  "x": "WIKIPEDIA consists of 992 Wikipedia articles (all labeled \"good article\" 5 ) from a comprehensive dump of English language Wikipedia articles 6 . We use the Wikipedia abstracts (the leading paragraphs before the table of contents) as summaries. The (document,summary) pairs were split into training, development and test sets, consisting of 784, 97, and 111 pairs, respectively. In the training set (pruning sentences of length less than 5), the average document length is around 8918 words or 339 sentences. The average summary length is 335 words or 13 sentences. For both documents and summaries, the average sentence length is around 26 words. In our main experiments, we use unsupervised summarization techniques, and we only use the training summaries (and not the documents) to determine output summary lengths. ---------------------------------- **BASELINE AND SYSTEMS** Our baseline is the bigram-based extraction summarization system of<cite> Gillick and Favre (2009)</cite> , icsisumm 7 .",
  "y": "uses"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_5",
  "x": "We note that we also tried replacing glpk by gurobi 9 , for which no time limit was necessary, but found poorer results on the development set of the ECHR data. The original system takes several important input parameters. 1. Summary length, for TAC08, is specified by the TAC 2008 conference guidelines as 100 words. For WIKIPEDIA and ECHR, we have access to training sets which gave an average summary length of around 335 and 805 words respectively, which we take as the standard output summary length. 2. Concept count cut-off is the minimum frequency of concepts from the document (set) that qualifies them for consideration in coverage maximization. For bigrams of the original system on TAC08, there are two types of document sets: 'A' and 'B'. For 'A' type documents,<cite> Gillick and Favre (2009)</cite> set this threshold to 3 and for 'B' type documents, they set this to 4. For WIKIPEDIA and ECHR, we take the bigram threshold to be 4. In our extension of the system to other concepts, we do not use any threshold.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_6",
  "x": "We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in<cite> (Gillick and Favre, 2009</cite> ) (see Table 1 , BIGRAMS): 0.081 compared to 0.110 respectively. On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence,<cite> Gillick and Favre (2009)</cite> were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset. However, when we change domain to the legal judgments or Wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the ROUGE metrics. For ECHR, replacing bigrams by frame names (FRAME) results in an increase of +0.1 in ROUGE-1, +0.031 in ROUGE-2 and +0.046 in ROUGE-SU4. We note that FrameNet 1.5 covers the legal domain quite well, which may explain why these concepts are particularly useful for the ECHR dataset. However, labeled (LDEP) and unlabeled (UDEP) dependencies also significantly outperform the baseline. For WIKIPEDIA, replacing bigrams by labeled or unlabeled syntactic dependencies results in significant improvements: an increase of +0.088 for ROUGE-1, +0.015 for ROUGE-2, and +0.03 for ROUGE-SU4. Interestingly, the NER system also yields significantly better performance over the baseline, which may reflect the nature of Wikipedia articles, often being about historical figures, famous places, organizations, etc. We observe in Table 2 , that for concept combination systems as well, ROUGE scores on TAC08 do not indicate any improvement in performance.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_0",
  "x": "Macro Acc STA EVE REP GENI GENA QUE IMP CRF<cite> (Friedrich et al., 2016)</cite> 66 ---------------------------------- **IMPACT OF GENRE** Considering that MASC+Wiki is rich in written genres, we additionally conduct cross-genre classification experiments, where we use one genre of documents for testing and the other genres of documents for training. The purpose of cross-genre experiment is to see whether the model can work robustly across genres. Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system<cite> (Friedrich et al., 2016)</cite> and the baseline model by a large margin. As shown in Table 5, benefited from modeling wider contexts and common SE label patterns, our full paragraphlevel model improves performance across almost all the genres. The high performance in the crossgenre setting demonstrates the robustness of our paragraph-level model across genres. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_1",
  "x": "**INTRODUCTION** Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_2",
  "x": "**INTRODUCTION** Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_3",
  "x": "Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river: (1): [Today, the Amazon river is experiencing a crisis of overfishing. ] STATE [Both subsistence fishers and their commercial rivals compete in netting large quantities of pacu,] GENERALIZING [which bring good prices at markets in Brazil and abroad.] GENERALIZING If we ignore the wider context, the second clause can be wrongly recognized as GENERIC easily since \"fishers\" usually refer to one general class rather than specific individuals.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_4",
  "x": "Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river:",
  "y": "background motivation"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_5",
  "x": "For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals. We recognize the latter two clauses in the following paragraph as GENERALIZING because both clauses describe situations related to the Amazon river: (1): [Today, the Amazon river is experiencing a crisis of overfishing. ] STATE [Both subsistence fishers and their commercial rivals compete in netting large quantities of pacu,] GENERALIZING [which bring good prices at markets in Brazil and abroad.] GENERALIZING If we ignore the wider context, the second clause can be wrongly recognized as GENERIC easily since \"fishers\" usually refer to one general class rather than specific individuals. However, considering the background introduced in first clause, \"fishers\" here actually refer to the fishers who fish on Amazon river which become specific individuals immediately. Therefore, we aim to build context-aware clause representations dynamically which are informed by their paragraph-wide contexts. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph. Our paragraph-level model derive clause representations by modeling interdependencies between clauses within a paragraph. In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_6",
  "x": "Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph. Our paragraph-level model derive clause representations by modeling interdependencies between clauses within a paragraph. In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. ---------------------------------- **RELATED WORK** ---------------------------------- **LINGUISTIC CATEGORIES OF SE TYPES**",
  "y": "extends"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_7",
  "x": "Our paragraph-level model derive clause representations by modeling interdependencies between clauses within a paragraph. In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. ---------------------------------- **RELATED WORK** ---------------------------------- **LINGUISTIC CATEGORIES OF SE TYPES** The situation entity types annotated in the MASC+Wiki corpus<cite> (Friedrich et al., 2016)</cite> were initially introduced by Smith (2003) , which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b) .",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_8",
  "x": "Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) . This model is different from our approach that processes one paragraph (with a sequence of clauses) at a time and extensively models inter-dependencies of clauses. Other related tasks include predicting aspectual classes of verbs (Friedrich and Palmer, 2014a) , classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_9",
  "x": "The type GENERALIZING is for habitual utterances that refer to ongoing actions or properties of specific individuals (e.g., Audubon educates the public.). \u2022 Speech Acts (QUESTION and IMPERA-TIVE): for clauses expressing two types of speech acts (Searle, 1969) . ---------------------------------- **SITUATION ENTITY (SE) TYPE CLASSIFICATION** Although situation entities have been well-studied in linguistics, there were only several previous works focusing on data-driven SE type classification using computational methods. Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_10",
  "x": "\u2022 Speech Acts (QUESTION and IMPERA-TIVE): for clauses expressing two types of speech acts (Searle, 1969) . ---------------------------------- **SITUATION ENTITY (SE) TYPE CLASSIFICATION** Although situation entities have been well-studied in linguistics, there were only several previous works focusing on data-driven SE type classification using computational methods. Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively.",
  "y": "background differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_11",
  "x": "---------------------------------- **FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph. Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both the training and decoding procedures of the CRF layer can be conducted efficiently using the Viterbi algorithm. With the CRF layer, the model jointly assigns a sequence of SE labels, one label per clause, by considering individual clause representations as well as common SE label patterns. ---------------------------------- **PARAMETER SETTINGS AND MODEL TRAINING**",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_12",
  "x": "For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph. Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both the training and decoding procedures of the CRF layer can be conducted efficiently using the Viterbi algorithm. With the CRF layer, the model jointly assigns a sequence of SE labels, one label per clause, by considering individual clause representations as well as common SE label patterns. ---------------------------------- **PARAMETER SETTINGS AND MODEL TRAINING** We finalized hyperparameters based on the best performance with 10-fold cross-validation on the training set. The word vectors were fixed during model training. Both word representations and clause representations in the model are of 300 dimensions, and all the Bi-LSTM layers contain 300 hidden units as well.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_13",
  "x": "All our proposed models were implemented with Pytorch 6 and converged to the best result within 40 epochs. Note that to diminish the effects of randomness in training neural network models and report stable experimental results, we ran each of the proposed models as well as our own baseline models ten times and reported the averaged performance across the ten runs. ---------------------------------- **EVALUATION** ---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses. In figure 4 , the horizontal axis shows the distribution of paragraphs based on the number of clauses in a paragraph.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_14",
  "x": "We chose the standard cross-entropy loss function for training our neural network models and adopted Adam (Kingma and Ba, 2014) optimizer with the initial learning rate of 0.001 and the batch size 5 of 128. All our proposed models were implemented with Pytorch 6 and converged to the best result within 40 epochs. Note that to diminish the effects of randomness in training neural network models and report stable experimental results, we ran each of the proposed models as well as our own baseline models ten times and reported the averaged performance across the ten runs. ---------------------------------- **EVALUATION** ---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_17",
  "x": "Macro Acc CRF<cite> (Friedrich et al., 2016)</cite> 69.3 74.7 GRU (Becker et al., 2017) 68. notators' annotation as \"gold labels\". It has been reported that labeling SE types is a nontrivial task even for humans. In addition, we implemented a clause-level Bi-LSTM model as our own baseline, which takes a single clause as its input. Since there is only one clause, the upper Bi-LSTM layer shown in Figure  1 is meaningless and removed in the clause-level Bi-LSTM model. ---------------------------------- **EXPERIMENTAL RESULTS** Following the previous work<cite> (Friedrich et al., 2016)</cite> on the same task and dataset, we report accuracy and macro-average F1-score across SE types on the test set of MASC+Wiki. The first section of Table 3 shows the results of the previous works. The second section shows the result of our implemented clause-level Bi-LSTM baseline, which already outperforms the previous best model.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_18",
  "x": "**ANALYSIS** ---------------------------------- **10-FOLD CROSS-VALIDATION** We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results. Consistently, our clause-level baseline model already outperforms the previous best model. By exploiting paragraph-wide contexts, the basic paragraph-level model obtains consistent performance improvements across all the classes compared with the baseline clause-level prediction model, especially for the classes GENERIC and GENERALIZING, where the improvements are significant. After using the CRF layer to fine-tune the predicted SE label sequence, slight performance improvements were observed on the four small classes. Overall, the full paragraphlevel neural network model achieves the best macro-average F1-score of 77.8% in predicting SE types, which not only outperforms all previous approaches but also reaches human-like performance on some classes.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_19",
  "x": "Compared with the baseline clause-level Bi-LSTM model, the basic paragraphlevel model achieves 3.5% and 3.3% of performance gains in macro-average F1-score and accuracy respectively. Building on top of the basic paragraph-level model, the CRF layer further improves the SE type prediction performance slightly by 0.4% and 0.7% in macro-average F1-score and accuracy respectively. Therefore, our full model with the CRF layer achieves the state-of-the-art performance on the MASC+Wiki corpus. ---------------------------------- **ANALYSIS** ---------------------------------- **10-FOLD CROSS-VALIDATION** We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results.",
  "y": "uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_0",
  "x": "In practice, this is a realistic situation in which transliterations from other languages can help. For example, Wikipedia contains articles on guitarist John Petrucci in English and Japanese, but not in Hindi. If we wanted to automatically generate a stub (skeleton) article in Hindi, we would need to transliterate his name into Hindi. Since a Japanese version already exists, we could extract from it additional information to help with the transliteration process. Importantly, since our article is about an American guitarist, we would explicitly want to start with the English (original) version of the name, and treat other languages as extra data, rather than vice versa. In order to effectively incorporate the otherlanguage data, we apply SVM re-ranking in a manner that has previously been shown to provide significant improvement for grapheme-to-phoneme conversion (Bhargava and Kondrak, 2011) . This method is flexible enough to incorporate multiple languages; it employs features based on character alignments between potential outputs and existing transliterations from other languages, as well as scores of these alignments, which serve as a measure of similarity. We apply this approach on top of the same DIRECTL+ system as submitted last year <cite>(Jiampojamarn et al., 2010b)</cite> for English-to-Hindi machine transliteration. Compared to the base DI-RECTL+ performance, we are able to achieve significantly better results, with a relative performance increase of over 10%. We also achieve improvements without supplemental transliterations by simply apply the same approach with another system's output as extra data.",
  "y": "background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_1",
  "x": "Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration <cite>(Jiampojamarn et al., 2010b)</cite> . For re-ranking, note that training a re-ranker requires training data where the base system scores are representative of unseen data so that the re-ranker does not simply learn to follow the base system; we therefore split the training data into ten folds and perform a sort-of cross validation with DIRECTL+. This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system. We also repeat the cross-validation process for training the re-ranker. We also apply the SVM re-ranking approach to system combination. In this case, we additionally train another system-here we use SE-QUITUR (Bisani and Ney, 2008 )-for English-toHindi transliteration. During test time, we feed the input into both DIRECTL+ and SEQUITUR, and use the top SEQUITUR output as supplemental data. We expect that sometimes SEQUITUR will provide a correct answer where DIRECTL+ does not; the hope is that the SVM re-ranking approach will be able to learn when this is the case based on the n-gram and score features.",
  "y": "background uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_2",
  "x": "This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system. We also repeat the cross-validation process for training the re-ranker. We also apply the SVM re-ranking approach to system combination. In this case, we additionally train another system-here we use SE-QUITUR (Bisani and Ney, 2008 )-for English-toHindi transliteration. During test time, we feed the input into both DIRECTL+ and SEQUITUR, and use the top SEQUITUR output as supplemental data. We expect that sometimes SEQUITUR will provide a correct answer where DIRECTL+ does not; the hope is that the SVM re-ranking approach will be able to learn when this is the case based on the n-gram and score features. ---------------------------------- **HINDI ROMANIZATION**",
  "y": "extends"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_4",
  "x": "We hypothesize that the complexity of English-to-Chinese mappings is better captured by the alignments that map longer sequences of English letters to single Chinese characters. making it difficult to generalize to new data. Finally, we observe very good overall accuracy in the English-to-Japanese results (which also only use base DIRECTL+), which further confirm the effectiveness of DIRECTL+ when applied to machine transliteration. ---------------------------------- **PREVIOUS WORK** There are three lines of research that are relevant to the work we have presented in this paper: (1) DI-RECTL+ and SEQUITUR for machine transliteration; (2) applying multiple languages; and (3) system combination. For the NEWS 2009 and 2010 Shared Tasks, the discriminative DIRECTL+ system that incorporates many-to-many alignments, online maxmargin training and a phrasal decoder was shown to function well as a general string transduction tool; while originally designed for grapheme-tophoneme conversion, it produced excellent results for machine transliteration (Jiampojamarn et al., 2009;<cite> Jiampojamarn et al., 2010b)</cite> , leading us to re-use it here. Finch and Sumita (2010) also submitted a top-performing system that was based in part on SEQUITUR, which is a generative system based on joint n-gram modelling (Bisani and Ney, 2008) . In this paper, we applied multiple transliteration languages to a single transliteration task. While our method is based on SVM re-ranking with similar features as to those used in the base system (Bhargava and Kondrak, 2011) , there have been other explorations into incorporating other language data, particularly when data are scarce.",
  "y": "background uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_0",
  "x": "Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional.",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_1",
  "x": "**RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score.",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_2",
  "x": "---------------------------------- **RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.**",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_3",
  "x": "In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_4",
  "x": "The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_5",
  "x": "In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ---------------------------------- **MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_6",
  "x": "In order to capture synonymbased matches, we optionally look for synonyms of the component word in the definition, 2 and expand our notion of lexical overlap to include these synonyms. For example, for the MWE china clay, the definition is kaolin, which includes neither of the components. However, we find the component word clay in the definition for kaolin, as shown below. A fine clay, rich in kaolinite, used in ceramics, paper-making, etc. This method is compatible with the three definition-based similarity methods described above, and indicated by the +SYN suffix (e.g. FIRSTDEF+SYN is FIRSTDEF with synonymbased expansion). ---------------------------------- **TRANSLATIONS** A third information source in Wiktionary that can be used to predict compositionality is sense-level translation data. Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_7",
  "x": "A third information source in Wiktionary that can be used to predict compositionality is sense-level translation data. Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation. Unlike the definition and synonym-based approach, the translation-based approach will produce real rather than binary values. To combine the two approaches, we discretise the scores given by the translation approach. In the case of disagreement between the two approaches, we label the given MWE as non-compositional. This results in higher recall and lower precision for the task of detecting compositionality. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_8",
  "x": "**DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE. In our case, the task setup is a binary classification task relative to each of the two components of the MWE. The ENC dataset was originally constructed by Reddy et al. (2011) , and annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun. The sampling was random in an attempt to make the dataset balanced, with 48% of compositional English noun compounds, of which 51% are compositional in the first component and 60% are compositional in the second component. We generate discrete labels by discretising the component-wise compositionality scores based on the partitions [0, 2.5] and (2.5, 5]. On average, each NC in this dataset has 1.4 senses (definitions) in Wiktionary. The EVPC dataset was constructed by Bannard (2006) , and manually annotated for compositionality on a binary scale for each of the head verb and particle. For the 160 EVPCs, 76% are verb-compositional and 48% are particlecompositional.",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_9",
  "x": "A dictionary-based method is only as good as the dictionary it is applied to. In the case of MWE compositionality analysis, our primary concern is lexical coverage in Wiktionary, i.e., what proportion of a representative set of MWEs is contained in Wiktionary. We measure lexical coverage relative to the two datasets used in this research (described in detail in Section 4), namely 90 English noun compounds (ENCs) and 160 English verb particle constructions (EVPCs). In each case, we calculated the proportion of the dataset that is found in Wiktionary, Wiktionary+Wikipedia (where we back off to a Wikipedia document in the case that a MWE is not found in Wiktionary) and WordNet (Fellbaum, 1998) . The results are found in Table 1 , and indicate perfect coverage in Wiktionary+Wikipedia for the ENCs, and very high coverage for the EVPCs. In both cases, the coverage of WordNet is substantially lower, although still respectable, at around 90%. ---------------------------------- **DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE.",
  "y": "differences similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_10",
  "x": "---------------------------------- **EXPERIMENTS** The baseline for each dataset takes the form of looking for a user-annotated idiom tag in the Wiktionary lexical entry for the MWE: if there is an idiomatic tag, both components are considered to be non-compositional; otherwise, both components are considered to be compositional. We expect this method to suffer from low precision for two reasons: first, the guidelines given to the annotators of our datasets might be different from what Wiktionary contributors assume to be an idiom. Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are compositional (e.g. from the Wiktionary guidelines for idiom inclusion, 3 computer chess, basketball player, telephone box). We also compare our method with: (1) \"LCS\", the string similarity-based method of<cite> Salehi and Cook (2013)</cite> , in which 54 languages are used; (2) \"DS\", the monolingual distributional similarity method of Salehi et al. (2014) ; (3) \"DS+DSL2\", the multilingual distributional similarity method of Salehi et al. (2014) , including supervised language selection for a given dataset, based on crossvalidation; and (4) \"LCS+DS+DSL2\", whereby the first three methods are combined using a supervised support vector regression model. In each case, the continuous output of the model is equal-width discretised to generate a binary classification. We additionally present results for the combination of each of the six methods proposed in this paper with LCS, DS and DSL2, using a linear-kernel support vector machine (represented with the suffix \" COMB(LCS+DS+DSL2) \" for a given method). The results are based on cross-3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD validation, and for direct comparability, the partitions are exactly the same as Salehi et al. (2014) . Tables 2 and 3 provide the results when our proposed method for detecting non-compositionality is applied to the ENC and EVPC datasets, respectively.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_11",
  "x": "Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods. It is important to reinforce that the proposed methods make no language-specific assumptions and are therefore applicable to any type of MWE and any language, with the only requirement being that the MWE of interest be listed in the Wiktionary for ---------------------------------- **ERROR ANALYSIS** We analysed all items in each dataset where the system score differed from that of the human annotators. For both datasets, the majority of incorrectly-labelled items were compositional but predicted to be non-compositional by our system, as can be seen in the relatively low precision scores in Tables 2 and 3 . In many of these cases, the prediction based on definitions and synonyms was compositional but the prediction based on translations was non-compositional. In such cases, we arbitrarily break the tie by labelling the instance as non-compositional, and in doing so favour recall over precision.",
  "y": "differences uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_12",
  "x": "In each case, the continuous output of the model is equal-width discretised to generate a binary classification. We additionally present results for the combination of each of the six methods proposed in this paper with LCS, DS and DSL2, using a linear-kernel support vector machine (represented with the suffix \" COMB(LCS+DS+DSL2) \" for a given method). The results are based on cross-3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD validation, and for direct comparability, the partitions are exactly the same as Salehi et al. (2014) . Tables 2 and 3 provide the results when our proposed method for detecting non-compositionality is applied to the ENC and EVPC datasets, respectively. The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods. For reasons of space, results without translation data are therefore omitted from the paper. Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods. It is important to reinforce that the proposed methods make no language-specific assumptions and are therefore applicable to any type of MWE and any language, with the only requirement being that the MWE of interest be listed in the Wiktionary for",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_0",
  "x": "A popular technique for doing this is resampling the training collection by multiplying the number of instances of the preferred class by the cost ratio. Also, the unpreferred class can be downsampled by eliminating some instances. The software package we use for our experiments applies both methods depending on the algorithm tested. We have tested four learning algorithms: Naive Bayes (NB), C4.5, PART and k-nearest neighbor (kNN), all implemented in the Weka package (Witten and Frank, 1999) . The version of Weka used in this work is Weka 3.0.1. The algorithms used can be biased to prefer the mistake of classify a UCE message as not UCE to the opposite, assigning a penalty to the second kind of errors. Following<cite> (Androutsopoulos et al., 2000)</cite> , we have assigned 9 and 999 (9 and 999 times more important) penalties to the missclassification of legitimate messages as UCE. This means that every instance of a legitimate message has been replaced by 9 and 999 instances of the same message respectively for NB, C4.5 and PART. However, for kNN the data have been downsampled. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_1",
  "x": "---------------------------------- **EVALUATION AND RESULTS** The experiments results are summarized in the Table 1 , 2 and 3. The learning algorithms Naive Bayes (NB), 5-Nearest Neighbor (5NN), C4.5 and PART were tested on words (-W), heuristic features (-H), and both (-WH). The kNN algorithm was tested with values of k equal to 1, 2, 5 and 8, being 5 the optimal number of neighbors. We present the weighted accuracy (wacc), and also the recall (rec) and precision (pre) for the class UCE. Weighted accuracy is a measure that weights higher the hits and misses for the preferred class. Recall and precision for the UCE class show how effective the filter is blocking UCE, and what is its effectiveness letting legitimate messages pass the filter, respectively<cite> (Androutsopoulos et al., 2000)</cite> . In Table 1 , no costs were used. Tables 2 and  3 show the results of our experiments for cost ratios of 9 and 999.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_2",
  "x": "Its numbers are comparable to those shown in a commercial study by the top performing Brightmail filtering system (Mariano, 2000) , which reaches a UCE recall of 0.73, and a precision close to 1.0, and it is manually updated. Naive Bayes has not shown high variability with respect to costs. This is probably due to the sampling method, which only slightly affects to the estimation of probabilities (done by approximation to a normal distribution). In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words. The improvement shown in our experiments is modest, due to the heuristics used. We are not able to add other heuristics in this case because the Spambase collection comes in a preprocessed fashion. For future experiments, we will use the collection from<cite> (Androutsopoulos et al., 2000)</cite> , which is in raw form. This fact will enable us to search for more powerful heuristics.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_3",
  "x": "Its numbers are comparable to those shown in a commercial study by the top performing Brightmail filtering system (Mariano, 2000) , which reaches a UCE recall of 0.73, and a precision close to 1.0, and it is manually updated. Naive Bayes has not shown high variability with respect to costs. This is probably due to the sampling method, which only slightly affects to the estimation of probabilities (done by approximation to a normal distribution). In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words. The improvement shown in our experiments is modest, due to the heuristics used. We are not able to add other heuristics in this case because the Spambase collection comes in a preprocessed fashion. For future experiments, we will use the collection from<cite> (Androutsopoulos et al., 2000)</cite> , which is in raw form. This fact will enable us to search for more powerful heuristics.",
  "y": "future_work"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_0",
  "x": "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated. ---------------------------------- **INTRODUCTION** Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data<cite> (Bolukbasi et al., 2016</cite>; Caliskan et al., 2017) . In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways.",
  "y": "motivation background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_1",
  "x": "Gender bias has been shown to affect several realworld applications relying on automatic language analysis, including online news (Ross and Carter, 2011) , advertisements (Sweeney, 2013) , abusive language detection (Park et al., 2018) , machine translation (Font and Costa-juss\u00e0, 2019; Vanmassenhove et al., 2018) , and web search (Kay et al., 2015) . In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017) . For word representations,<cite> Bolukbasi et al. (2016)</cite> and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors.",
  "y": "motivation background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_2",
  "x": "Gender bias has been shown to affect several realworld applications relying on automatic language analysis, including online news (Ross and Carter, 2011) , advertisements (Sweeney, 2013) , abusive language detection (Park et al., 2018) , machine translation (Font and Costa-juss\u00e0, 2019; Vanmassenhove et al., 2018) , and web search (Kay et al., 2015) . In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017) . For word representations,<cite> Bolukbasi et al. (2016)</cite> and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors.",
  "y": "background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_3",
  "x": "---------------------------------- **GEOMETRY OF GENDER** Next, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one<cite> (Bolukbasi et al., 2016)</cite> . The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender). To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a) . Each sentence in the corpus contains one gendered pronoun and two occupation words, such as \"The developer corrected the secretary because she made a mistake\" and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (blue dots) and female (orange dots) on the two principal components from the PCA analysis.",
  "y": "differences"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_4",
  "x": "(1) a train-time data augmentation approach and (2) a test-time neutralization approach. Zhao et al. (2018a) propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task. Data augmentation is performed by replacing gender revealing entities in the OntoNotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data. In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from<cite> Bolukbasi et al. (2016)</cite> . We evaluate the performance of both aspects of this approach. ---------------------------------- **DATA AUGMENTATION** Neutralization We also investigate an approach to mitigate bias induced by ELMo embeddings without retraining the coreference model. Instead of augmenting training corpus by swapping gender words, we generate a gender-swapped version of the test instances. We then apply ELMo to obtain contextualized word representations of the original and the gender-swapped sentences and use their average as the final representations.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_0",
  "x": "We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier <cite>(Socher et al., 2013)</cite> to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets. In particular, we address the following research questions: 1. How does off-the-shelf MT on Arabic social data influence SA performance? 2. Can MT-based approaches be a viable alternative to improve sentiment classification performance on Arabic tweets? 3. Given the linguistic resources currently available for Arabic and its dialects, is it more effective to adapt an MT-based approach instead of building a new system from scratch? ---------------------------------- **RELATED WORK** There are currently two main approaches to automatic sentiment analysis: using a sentiment lexicon or building a classifier using machine learning. Lexicon-based approaches, on the one hand, utilise sentiment lexica to retrieve and annotate sentiment bearing word tokens for their sentiment orientation and then utilise a set of rules to assign the overall sentiment label (Taboada et al., 2011) .",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_1",
  "x": "The authors argue that despite the difficulties associated with MT, e.g. information loss, the translated text still maintains a sufficient level of captured sentiments for their purposes. This work differs from our work in terms of domain and in measuring summary consistency rather than SA accuracy. Balahur and Turchi (2013) investigate the use of an MT system (Google) to translate an annotated corpus of English tweets into four European languages in order to obtain an annotated training set for learning a classifier. The authors report an accuracy score of 64.75% on the English held-out test set. For the other languages, reported accuracy scores ranged between 60 -62%. Hence, they conclude that it is possible to obtain high quality training data using MT, which is an encouraging result to motivate our approach. Wan (2009) proposes a co-training approach to tackle the lack of Chinese sentiment corpora by employing Google Translate as publicly available machine translation (MT) service to translate a set of annotated English reviews into Chinese. Using a held-out test set, the best reported accuracy score was at 81.3% with SVM on binary classification task: positive vs negative. Our approach differs from the ones described, in that we use automatic MT to translate Arabic tweets into English and then perform SA using a stateof-the-art SA classifier for English <cite>(Socher et al., 2013)</cite> . Most importantly, we empirically benchmark its performance towards previous SA approaches, including lexicon-based, fully supervised and distant supervision SA.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_2",
  "x": "The data-set was manually labeled with gold-standard sentiment orientation by two native speakers of Arabic, obtaining a Kappa score of 0.81, which indicates highly reliable annotations. Table 1 summarises the data set and its distribution of labels. For SA, we perform binary classification using positive and negative tweets. We apply a number of common preprocessing steps following Go et al. (2009) and Pak and Paroubek (2010) to account for noise introduced by Twitter. The data set will be released as part of this submission. ---------------------------------- **MT-BASED APPROACH** In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_3",
  "x": "In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments. Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 . The authors observe that the recursive models work well on shorter text while BOW features with NB and SVM perform well only on longer sentences. Using<cite> Socher et al. (2013)</cite> 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 . ---------------------------------- **BASELINE SYSTEMS** We benchmark the MT-approach against three baseline systems representing current standard approaches to SA: a lexicon-based approach, a fully supervised machine learning approach and a distant supervision approach (also see Section 2). The lexicon-based baseline combines three sentiment lexica.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_4",
  "x": "\u2022 MT-based SA approaches in general have a problem of identifying positive tweets (low recall and precision), often misclassifying them as negative. The reverse it true for the DS and fully supervised baselines, which find it hard to identify negative tweets. This is in line with results reported by Refaee and Rieser (2014b) which evaluate DS approaches to Arabic SA. Only the lexiconapproach is balanced between the positive and negative class. Note that our ML baseline systems as well as the English SA classifier by<cite> Socher et al. (2013)</cite> are trained on balanced data sets, i.e. we can assume no prior bias towards one class. ---------------------------------- **PLANNED CONTRASTS** ---------------------------------- **ERROR ANALYSIS** The above results highlight the potential of an MTbased approach to SA for languages that lack a large Table 4 : Examples of misclassified tweets training data-set annotated for sentiment analysis, such as Arabic.",
  "y": "similarities"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_5",
  "x": "5. Example 5 represents a case of correctly translated sentiment-bearing words (love, life), but failed to translate surrounding text ('Ashan' and 'Amtlat'). Bautin et al. (2008) point out that this type of contextual information loss is one of the main challenges of MT-based SA. 6. Example 6 represents a case of a correctly translated tweet, but with an incorrectly assigned sentiment label. We assume that this is due to changes in sentence structure introduced by the MT system. Balahur and Turchi (2013) state that word ordering is one of the most prominent causes of SA misclassification. In order to confirm this hypothesis, we manually corrected sentence structure before feeding it into the SA classifier. This approach led to the correct SA label, and thus, confirmed that the cause of the problem is word-ordering. Note that the Stanford SA system pays particular attention to sentence structure due to its \"deep\" architecture that adds to the model the feature of being sensitive to word ordering <cite>(Socher et al., 2013)</cite> . In future work, we will verify this by comparing these results to other high performing English SA tools (see for example Abbasi et al. (2014) In sum, one of the major challenges of this approach seems to be the use of Arabic dialects in social media, such as Twitter. In order to confirm this hypothesis, we automatically label Dialectal Arabic (DA) vs. Modern Standard Arabic (MSA) using AIDA (Elfardy et al., 2014) and analyse the performance of MT-based SA.",
  "y": "future_work"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_6",
  "x": "However, with more resources being released for informal Arabic and Arabic dialects, e.g. (Cotterell and Callison-Burch, 2014; Refaee and Rieser, 2014a) , we assume that off-the-shelf MT systems will improve their performance in the near future. ---------------------------------- **CONCLUSION** This paper is the first to investigate and empirically evaluate the performance of Machine Translation (MT)-based Sentiment Analysis (SA) for Arabic Tweets. In particular, we make use of off-theshelf MT tools, such as Google and Microsoft MT, to translate Arabic Tweets into English. We then use the Stanford Sentiment Classifier <cite>(Socher et al., 2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. In contrast to previous work, we benchmark this approach on a gold-standard test set of 937 manually annotated tweets and compare its performance to standard SA approaches, including lexicon-based, supervised and distant supervision approaches. We find that MT approaches reach a comparable performance or significantly outperform more resourceintense standard approaches. As such, we conclude that using off-the-shelf tools to perform SA for under-resourced languages, such as Arabic, is an effective and efficient alternative to building SA classifiers from scratch. Future directions of this work include quantifying the impact of the used off-the-shelf tools, e.g. by using alternative high performing English SA tools.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_0",
  "x": "In such cases, we may first try to examine other usages of the same expression in the text, in order to infer its meaning from this context. Failing to do so, we may consult a dictionary, and in the case of polysemous words, choose an appropriate meaning based on the context. Acquiring novel word senses via dictionary definitions is known to be more effective than contextual guessing (Fraser, 1998; Chen, 2012) . However, very often, hand-crafted dictionaries do not contain definitions for rare or novel phrases/words, and we eventually give up on un- derstanding them completely, leaving us with only a shallow reading of the text. There are several natural language processing (NLP) tasks that can roughly address this problem of unfamiliar word senses, all of which are incomplete in some way. Word sense disambiguation (WSD) can basically only handle words (or senses) that are registered in a dictionary a priori. Paraphrasing can suggest other ways of describing a word while keeping its meaning, but those paraphrases are generally context-insensitive and may not be sufficient for understanding. To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_1",
  "x": "To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_2",
  "x": "Paraphrasing can suggest other ways of describing a word while keeping its meaning, but those paraphrases are generally context-insensitive and may not be sufficient for understanding. To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_3",
  "x": "However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows:",
  "y": "differences"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_4",
  "x": "p(y t |y 1 , ..., y t\u22121 , X, X trg ). (1) 3 LOG-CaD: Local & Global Context-aware Description Generator We propose LOG-CaD, a neural model that generates the description of a given phrase or word by using its local and global contexts. In the rest of this section, we first describe our idea of utilizing local and global contexts in the description generation task, then present the details of our model. Local & Global Contexts for Description Generation In this paper, we refer to the explicit contextual information included in a single sentence as \"local context,\" and the implicit contextual information in the word/phrase embedding trained in an unsupervised manner on largescale corpora as \"global context. \" Previous work on the definition generation task<cite> (Noraset et al., 2017)</cite> has shown that global contexts can be useful clues when generating definitions of unknown words. The intuition behind their method is that words with similar meanings tend to have similar definitions in a dictionary. This can be seen as an extension of the Distributional Hypothesis (Harris, 1954; Firth, 1957) , which states words that share semantic meanings tend to appear in similar contexts. Additionally, work on the WSD task (Navigli, 2009) , novel sense detection (Erk, 2006; Lau et al., 2014) , and the non-standard word explanation task (Ni and Wang, 2017) have revealed that local contexts surrounding the word can help disambiguate its sense. Based on these studies, we propose to incorporate both local and global contexts to describe an unknown expression.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_5",
  "x": "The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a GATE function<cite> (Noraset et al., 2017)</cite> to dynamically control how the global and local contexts influence the generation of the description. We use bi-directional and uni-directional LSTMs (Hochreiter and Schmidhuber, 1997) as our context encoder and description decoder (Figure 1 ), respectively. Given a sentence X and a phrase X trg , the context encoder generates a sequence of continuous vectors where x i denotes the word embedding of word x i . Then, the description decoder computes the conditional probability of a description Y with Eq. (1), which can be approximated with another LSTM as where s t is a hidden state of the decoder LSTM, and y t\u22121 is a jointly-trained word embedding of the previous output word y t\u22121 . Considering the fact that the local context can be relatively long (e.g. around 20 words on average in the Wikipedia dataset that will be introduced in the next section) it is hard for a decoder to focus on important words in local contexts. In order to deal with this problem, ATTENTION(\u00b7) function in Eq. (4) decides which words in the local context X to focus on at each time step. d t can be computed with an attention mechanism (Luong and Manning, 2016) as",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_6",
  "x": "where x i denotes the word embedding of word x i . Then, the description decoder computes the conditional probability of a description Y with Eq. (1), which can be approximated with another LSTM as where s t is a hidden state of the decoder LSTM, and y t\u22121 is a jointly-trained word embedding of the previous output word y t\u22121 . Considering the fact that the local context can be relatively long (e.g. around 20 words on average in the Wikipedia dataset that will be introduced in the next section) it is hard for a decoder to focus on important words in local contexts. In order to deal with this problem, ATTENTION(\u00b7) function in Eq. (4) decides which words in the local context X to focus on at each time step. d t can be computed with an attention mechanism (Luong and Manning, 2016) as where U h and U s are matrices that map the encoder and decoder hidden states into a common space, respectively. In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_7",
  "x": "We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> . First, we feed phrase embedding x trg to initialize the decoder as Here, phrase embedding x trg is calculated by simply summing up all the embeddings of words that consistute the phrase X trg . Note that we use a random-initialized vector if no pre-trained embedding is available for the words in X trg . As described in the previous section, we use both local and global contexts. In order to capture the interaction between two contexts and the description decoder, we adopt a GATE(\u00b7) function (Eq. (6) Table 2 : Domains, expressions to be described, and the coverage of pre-trained word embeddings of the four datasets. context d t , and character-level information c trg as where \u03c3(\u00b7), \u2299 and ; denote sigmoid function, element-wise multiplication, and vector concatenation, respectively. Ws and bs are weight matrices and bias terms. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_8",
  "x": "6 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as ground-truth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet 7 using the dict-definition 8 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the word. We split this dataset to obtain Train, Validation, and Test sets. If a word has multiple definitions/examples, we treat them as different entries. Note that the words are mutually exclusive across the three sets. The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of (Noraset et al., 2017 ) (see Table 1 ). In addition to WordNet, we use the Oxford Dictionary following (Gadetsky et al., 2018) , the Urban Dictionary following (Ni and Wang, 2017) and our Wikipedia dataset described in the previous section.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_9",
  "x": "In order to control the experiments on four datasets, we use the same pre-trained CBOW 9 vectors as global context following (Noraset et al., 6 Dataset will be made available upon publication. 7 https://wordnet.princeton.edu/ 8 https://github.com/NorThanapon/dict-definition 9 GoogleNews-vectors-negative300.bin.gz at https://code.google.com/archive/p/word2vec/ Table 3 : Hyperparameters of the models 2017). If the expression to be described consists of multiple words, its phrase embedding is calculated by simply summing up all the CBOW vectors of words in the phrase, such as \"sonic\" and \"boom.\" (See Figure 1) . If pre-trained CBOW embeddings are unavailable, we instead use a special [UNK] vector (which is randomly initialized with a uniform distribution) as word embeddings. Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ). Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS** We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in<cite> (Noraset et al., 2017)</cite> . It can access the embedding (global context) of the phrase to be described, but has no ability to read the usage examples (local context).",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_10",
  "x": "Although several studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider sub-sentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) have proposed a definition generation method that works with polysemous words in dictionaries. They present a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context. While their method use local context only for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_11",
  "x": "They present a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context. While their method use local context only for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ---------------------------------- **CONCLUSIONS** This paper sets up a task of generating a natural language description for a word/phrase with a specific context, aiming to help us acquire unknown word senses when reading text. We approached this task by using a variant of encoder-decoder models that capture the given local context by an encoder and global contexts by the target word's embedding induced from massive text.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_12",
  "x": "While their method use local context only for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ---------------------------------- **CONCLUSIONS** This paper sets up a task of generating a natural language description for a word/phrase with a specific context, aiming to help us acquire unknown word senses when reading text. We approached this task by using a variant of encoder-decoder models that capture the given local context by an encoder and global contexts by the target word's embedding induced from massive text. Experimental results on three existing datasets and one novel dataset built from Wikipedia dataset confirmed that the use of both local and global contexts is the key to generating appropriate contextsensitive description in various situations.",
  "y": "uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_0",
  "x": "This paper describes an incremental alignment method to build confusion networks based on the translation edit rate (TER) algorithm. This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBN's submission to the WMT08 shared translation task. ---------------------------------- **INTRODUCTION** Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a \"skeleton\" hypothesis. The skeleton hypothesis defines the word order of the decoding output.",
  "y": "background"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_1",
  "x": "**INTRODUCTION** Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a \"skeleton\" hypothesis. The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons. Using the pair-wise hypothesis alignment, the confusion networks are built in two steps. First, all hypotheses are aligned against the skeleton independently.",
  "y": "extends"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_2",
  "x": "The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons. Using the pair-wise hypothesis alignment, the confusion networks are built in two steps. First, all hypotheses are aligned against the skeleton independently. Second, the confusion networks are created from the union of these alignments. The incremental hypothesis alignment algorithm combines these two steps. All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses. As in <cite>(Rosti et al., 2007)</cite> , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models. System weights and language model weights are tuned to optimize the quality of the decoding output on a development set. This paper is organized as follows.",
  "y": "similarities uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_3",
  "x": "The NULL word arc confidences are adjusted as in the case of a match or a substitution depending on whether the NULL word arc exists or not. Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network. The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc. After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment. each set of two consecutive nodes. Other scores for the word arc are set as in <cite>(Rosti et al., 2007)</cite> . ---------------------------------- **BENEFITS OVER PAIR-WISE TER ALIGNMENT** The incremental hypothesis alignment guarantees that insertions between a hypothesis and the current confusion network are always considered when aligning the following hypotheses. This is not the case in any pair-wise hypothesis alignment algorithm.",
  "y": "similarities uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_0",
  "x": "Therefore, we combine GermaNet with Wikipedia, and yield substantial improvements over measures operating on a single knowledge source. ---------------------------------- **DATASETS** Several German datasets for evaluation of SS or SR have been created so far (see Table 1 ). <cite>Gurevych (2005)</cite> conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965) , but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004) . However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_1",
  "x": "Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. ---------------------------------- **SEMANTIC RELATEDNESS MEASURES** Semantic wordnet based measures Lesk (1986) introduced a measure (Les) based on the number of word overlaps in the textual definitions (or glosses) of two terms, where higher overlap means higher similarity. As GermaNet does not contain glosses, this measure cannot be employed. <cite>Gurevych (2005)</cite> proposed an alternative algorithm (PG) generating surrogate glosses by using a concept's relations within the hierarchy. Following the description in Budanitsky and Hirst (2006) , we further define several measures using the taxonomy structure. PL is the taxonomic path length l(c 1 , c 2 ) between two concepts c1 and c2.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_2",
  "x": "We compute the overlap between article texts based on (i) the first paragraph, as it usually contains a short gloss, and (ii) the full article text. As Wikipedia articles do not form a taxonomy, path based measures have to be adapted to the Wikipedia category graph (see the right part of Figure 2 ). We define C 1 and C 2 as the set of categories assigned to article a i and a j , respectively. We compute the SR value for each category pair (c k , c l ) with c k \u2208 C 1 and c l \u2208 C 2 . We use two different strategies to combine the resulting SR values: First, we choose the best value among all pairs (c k , c l ), i.e., the minimum for path based, and the maximum for information content based measures. As a second strategy, we average over all category pairs. Table 2 gives an overview of our experimental results on three German datasets. Best values for each dataset and knowledge source are in bold. We use the P G measure in optimal configuration as reported by <cite>Gurevych (2005)</cite> . For the Les measure, we give the results for considering: (i) only the first paragraph (+First) and (ii) the full text (+Full).",
  "y": "uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_3",
  "x": "This comparison is fairer, because article titles in Wikipedia are usually nouns. Table 2 also gives the inter annotator agreement for each subset. It constitutes an upper bound of a measure's performance. ---------------------------------- **EXPERIMENTS & RESULTS** Our results on Gur65 using GermaNet are very close to those published by <cite>Gurevych (2005)</cite> , ranging from 0.69-0.75. For Gur350, the performance drops to 0.38-0.50, due to the lower upper bound, and because GermaNet does not model SR well. These findings are endorsed by an even more significant performance drop on ZG222. The measures based on Wikipedia behave less uniformly. Les yields acceptable results on Gur350, but is generally not among the best performing measures.",
  "y": "similarities"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_0",
  "x": "**INTRODUCTION** Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016;<cite> Vaswani et al., 2017)</cite> trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017) . Recent studies (Tiedemann and Scherrer, 2017; Wang et al., 2017; have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017) . Most of these methods use an additional encoder Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder. More recently, have shown that a cache-based memory network performs better than the above encoder-based methods. The cache-based memory keeps past context as a set of words, where each cell corresponds to one unique word keeping the hidden representations learned by the NMT while translating it. However, in this method, the word representations are stored irrespective of the sentences where they occur, and those vector representations are disconnected from the original NMT network. We propose to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentence-level abstractions.",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_1",
  "x": "More recently, have shown that a cache-based memory network performs better than the above encoder-based methods. The cache-based memory keeps past context as a set of words, where each cell corresponds to one unique word keeping the hidden representations learned by the NMT while translating it. However, in this method, the word representations are stored irrespective of the sentences where they occur, and those vector representations are disconnected from the original NMT network. We propose to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentence-level abstractions. In contrast to the hierarchical recurrent neural network (HRNN) used by (Wang et al., 2017) , here the attention allows dynamic access to the context by selectively focusing on different sentences and words for each predicted word. In addition, we integrate two HANs in the NMT model to account for target and source context. The HAN encoder helps in the disambiguation of source-word representations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integration is done by (i) re-using the hidden representations from both the encoder and decoder of previous sentence translations and (ii) providing input to both the encoder and decoder for the current translation. This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_2",
  "x": "In addition, we integrate two HANs in the NMT model to account for target and source context. The HAN encoder helps in the disambiguation of source-word representations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integration is done by (i) re-using the hidden representations from both the encoder and decoder of previous sentence translations and (ii) providing input to both the encoder and decoder for the current translation. This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-chitecture<cite> (Vaswani et al., 2017)</cite> and show significant improvement over two strong baselines on multiple data sets. (iii) We perform an ablation study of the contribution of each HAN configuration, showing that contextual information obtained from source and target sides are complementary. ---------------------------------- **THE PROPOSED APPROACH**",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_3",
  "x": "The function f w is a linear transformation to obtain the query q w . We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: Figure 1 : Integration of HAN during encoding at time step t,h t is the context-aware hidden state of the word x t . Similar architecture is used during decoding. where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ). Each layer is followed by a normalization layer (Lei Ba et al., 2016) . ---------------------------------- **CONTEXT GATING** We use a gate to regulate the information at sentence-level h t and the contextual information at document-level d t .",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_4",
  "x": "where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ). Each layer is followed by a normalization layer (Lei Ba et al., 2016) . ---------------------------------- **CONTEXT GATING** We use a gate to regulate the information at sentence-level h t and the contextual information at document-level d t . The intuition is that different words require different amount of context for translation: where W h , W p are parameter matrices, and h t is the final hidden representation for a word x t or y t . ---------------------------------- **INTEGRATED MODEL** The context can be used during encoding or decoding a word, and it can be taken from previously encoded source sentences, previously decoded target sentences, or from previous alignment vectors (i.e. context vectors (Bahdanau et al., 2015) ).",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_5",
  "x": "Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002 ) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007) ). ---------------------------------- **MODEL CONFIGURATION AND TRAINING** As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by , with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention. The target and source vocabulary size is 30K.",
  "y": "similarities uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_6",
  "x": "The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention. The target and source vocabulary size is 30K. The optimization and regularization methods were the same as proposed by<cite> Vaswani et al. (2017)</cite> . Inspired by we trained the models in two stages. First we optimize the parameters for the NMT without the HAN, then we proceed to optimize the parameters of the whole network. We use k = 3 previous sentences, which gave the best performance on the development set. Table 1 shows the BLEU scores for different models. The baseline NMT transformer already has better performance than previously published results on these datasets, and we replicate previous previous improvements from the cache method over the this stronger baseline.",
  "y": "similarities uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_0",
  "x": "There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b; Levy et al., 2015) . Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings. This paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings. Specifically, we ask \"what is the impact of representing each dimension of a dense representation with significantly fewer bits than the standard 64 bits?\" Moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used. The results we present are quite surprising. We show that it is possible to reduce the memory consumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits.",
  "y": "background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_1",
  "x": "In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide two ways to train existing algorithms<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b ) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16. The truncated word embeddings are available from the papers web page at https://cogcomp.cs.illinois. edu/page/publication_view/790. ---------------------------------- **RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) .",
  "y": "extends"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_2",
  "x": "The truncated word embeddings are available from the papers web page at https://cogcomp.cs.illinois. edu/page/publication_view/790. ---------------------------------- **RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) . However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009 ). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension. They have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks. Moreover, Faruqui et al. (Faruqui et al., 2015) showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.",
  "y": "motivation background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_4",
  "x": "Then the real precision is = 2 1\u2212m r . For example, if r = 10 \u22124 and m = 8, then the numerical precision is 7.8 \u00b7 10 \u22127 which can capture much higher precision than the SGD update values have. When the cumulated values in the auxiliary update vectors are greater than the original numerical precision , e.g., = 2 \u22127 for 8 bits, we update the original vector and clear the value in the auxiliary vector. In this case, we can have final n-bit values in word embedding vectors as good as the method presented in Section 3.1. ---------------------------------- **EXPERIMENTS ON WORD/PHRASE SIMILARITY** In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algorithms, word2vec<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) , based on the Oct. 2013 Wikipedia dump. 1 We first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_5",
  "x": "**ANALYSIS OF BITS NEEDED** We ran both CBOW and skipgram with negative sampling<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) on the Wikipedia dump data, and set the window size of context to be five. Then we performed value truncation with 4 bits, 6 bits, and 8 bits. The results are shown in Fig. 1 , and the numbers of the averaged results are shown in Table 1 . We also used the binarization algorithm (Guo et al., 2014) to truncate each dimension to three values; these experiments are is denoted using the suffix \"binary\" in the figure. For both CBOW and skipgram models, we train the vectors with 25 and 200 dimensions respectively. The representations used in our experiments were trained using the whole Wikipedia dump. A first observation is that, in general, CBOW performs better than the skipgram model. When using the truncation method, the memory required to store the embedding is significantly reduced, while the performance on the test datasets remains almost the same until we truncate down to 4 bits. When comparing CBOW and skipgram models, we again see that the drop in performance with 4-bit values for the skipgram model is greater than the one for the CBOW model.",
  "y": "uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_0",
  "x": "****CAPTURING SEMANTIC SIMILARITY FOR ENTITY LINKING WITH CONVOLUTIONAL NEURAL NETWORKS**** **ABSTRACT** A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . ---------------------------------- **1** ---------------------------------- **INTRODUCTION**",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_1",
  "x": "**1** ---------------------------------- **INTRODUCTION** One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation's government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking.",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_2",
  "x": "One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation's government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system <cite>(Durrett and Klein, 2014)</cite> . Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets.",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_3",
  "x": "All pairs of these vectors between the source and the target are then compared using cosine similarity, as shown in the middle of Figure 1 . This yields the vector of features f C (s, t e ) which indicate the different types of similarity; this vector can then be combined with other sparse features and fed into a final logistic regression layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. ---------------------------------- **INTEGRATING WITH A SPARSE MODEL** The dense model presented in Section 2.1 is effective at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011) , which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following<cite> Durrett and Klein (2014)</cite> , we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve.",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_4",
  "x": "Query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. This processes generates on average 9 queries for each mention. Conveniently, this set of queries also defines the set of candidate entities that we consider linking a mention to: each query generates a set of potential entities based on link counts, whose unions are then taken to give on the possible entity targets for each mention (including the null link). In the example shown in Figure 1 , the query phrases are Pink Floyd and Floyd, which generate Pink Floyd and Gavin Floyd as potential link targets (among other options that might be derived from the Floyd query). Our final model has the form P (t|x) = q P (t, q|x). We parameterize P (t, q|x) in a loglinear way with three separate components: f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) .",
  "y": "similarities extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_5",
  "x": "f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> . f Q only impacts which query is selected and not the disambiguation to a title. It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_6",
  "x": "f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> . f Q only impacts which query is selected and not the disambiguation to a title. It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> . ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_7",
  "x": "\u2022 ACE (NIST, 2005; Bentivogli et al., 2010) : This corpus was used in Fahrni and Strube (2014) and<cite> Durrett and Klein (2014)</cite> . \u2022 CoNLL-YAGO (Hoffart et al., 2011) : This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities. \u2022 WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia. \u2022 Table 2 : Performance of the system in this work (Full) compared to two baselines from prior work and two ablations. Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit. We use standard train-test splits for all datasets except for WP, where no standard split is available. In this case, we randomly sample a test set. For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2.",
  "y": "similarities uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_8",
  "x": "\u2022 Table 2 : Performance of the system in this work (Full) compared to two baselines from prior work and two ablations. Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit. We use standard train-test splits for all datasets except for WP, where no standard split is available. In this case, we randomly sample a test set. For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2. Table 2 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN features (Full). We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) . We can also compare to two ablations: using just the sparse features (a system which is a direct extension of<cite> Durrett and Klein (2014)</cite> ) or using just the CNNderived features.",
  "y": "differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_9",
  "x": "Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit. We use standard train-test splits for all datasets except for WP, where no standard split is available. In this case, we randomly sample a test set. For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2. Table 2 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN features (Full). We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) . We can also compare to two ablations: using just the sparse features (a system which is a direct extension of<cite> Durrett and Klein (2014)</cite> ) or using just the CNNderived features. 5 Our CNN features generally outperform the sparse features and improve even further when stacked with them.",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_10",
  "x": "In the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. This suggests that the system of<cite> Durrett and Klein (2014)</cite> has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct. By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so. ---------------------------------- **MULTIPLE GRANULARITIES OF CONVOLUTION** One question we might ask is how much we gain by having multiple convolutions on the source and target side. Table 3 compares our full suite of CNN features, i.e. the six features specified in Figure 1 , with two specific convolutional features in isolation. Using convolutions over just the source document (s doc ) and target article text (t doc ) gives a system 6 that performs, in aggregate, comparably to using convolutions over just the mention (s ment ) and the entity title (t title ). These represent two extremes of the system: consuming the maximum amount of context, which might give the most robust representation of topic semantics, and consuming the minimum amount of context, which gives the most focused representation of topics semantics (and which more generally might allow the system to directly memorize train-test pairs observed in training). However, neither performs as well as the combination of all CNN features, showing that the different granularities capture complementary destroying missiles . spy planes has died of his wounds him which was more depressing and destroying missiles .",
  "y": "background motivation"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_0",
  "x": "For example the morpheme over occurs in words like hold+over, lay+over, and skip+over. 1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980) , supervised (Ruokolainen et al., 2013) , semi-supervised (Gr\u00f6nroos et al., 2014) , and unsupervised (Creutz and Lagus, 2002) . <cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems.",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_1",
  "x": "<cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems. We propose an explanation for this surprising result. ---------------------------------- **METHODS**",
  "y": "extends"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_2",
  "x": "**METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (Tsochantaridis et al., 2005) . Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (Altun et al., 2003) . A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance. The test instances are tagged using the Viterbi decoding algorithm on the basis of the weighted features. ---------------------------------- **BASE SYSTEM** Each training instance is represented as a sequence of feature vectors, with the tags following the \"Numbered NB\" tagging scheme, which was found to produce the best results.",
  "y": "uses"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_3",
  "x": "We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings. We find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead. On the other hand, relying on a fully-supervised system appears to be much less robust, even though it generates more accurate morphological segmentations than the unsupervised systems. We propose an explanation for this surprising result. ---------------------------------- **METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification.",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_5",
  "x": "---------------------------------- **BASELINE SYLLABIFICATION** As a baseline, we replicate the experiments of <cite>Bartlett et al. (2008)</cite> , and extend them to lowresource settings. Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 . We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples. ---------------------------------- **MORPHOLOGICALLY-INFORMED SYLLABIFICATION** Our main set of experiments concerns the incorporation of the morphological information obtained from methods described in Section 2.2 into the baseline syllabification system. As seen in Table 3, the accuracy of the baseline syllabification system trained on a large number of instances is already very high, so the gains introduced by mor- For the sake of clarity, we omit some of the methods from the graphs. The unsupervised methods are represented by Morfessor FlatCat.",
  "y": "uses extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_0",
  "x": "Sentence simplification maps a sentence to a simpler, more readable one approximating its content. Typically, a simplified sentence differs from a complex one in that it involves simpler, more usual and often shorter, words (e.g., use instead of exploit); simpler syntactic constructions (e.g., no relative clauses or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996) , summarisation (Knight and Marcu, 2000) , sentence fusion (Filippova and Strube, 2008 ) and semantic role labelling (Vickrey and Koller, 2008) . It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999) , for low literacy readers (Watanabe et al., 2009 ) and for non native speakers (Siddharthan, 2002) . There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) .",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_1",
  "x": "First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches. Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981) ) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving. ---------------------------------- **RELATED WORK** Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010) . While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_2",
  "x": "As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996) , summarisation (Knight and Marcu, 2000) , sentence fusion (Filippova and Strube, 2008 ) and semantic role labelling (Vickrey and Koller, 2008) . It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999) , for low literacy readers (Watanabe et al., 2009 ) and for non native speakers (Siddharthan, 2002) . There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) . In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches.",
  "y": "extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_3",
  "x": "Using the parallel dataset formed by Simple English Wikipedia (SWKP) 1 and traditional English Wikipedia (EWKP) 2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001 ). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim-pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011) , simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_4",
  "x": "Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim-pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011) , simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach.",
  "y": "background uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_5",
  "x": "In (Wubben et al., 2012; Coster and Kauchak, 2011) , simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach. Similarly, Wubben et al. (2012) use Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) systems. ---------------------------------- **SIMPLIFICATION FRAMEWORK** We start by motivating our approach and explaining how it relates to previous proposals w.r.t., the four main operations involved in simplification namely, splitting, deletion, substitution and reordering. We then introduce our framework.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_6",
  "x": "While splitting opportunities have a clear counterpart in syntax (i.e., splitting often occurs whenever a relative, a subordinate or an appositive clause occurs in the complex sentence), completion i.e., the reconstruction of the shared element in the second simpler clause, is arguably semantically governed in that the reconstructed element corefers with its matching phrase in the first simpler clause. While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2),<cite> Zhu et al. (2010)</cite> fails to copy the shared argument \"The judge\" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (\"he\" or \"she\") for the antecedent \"The judge\". (2) C. The judge ordered that Chapman should receive psychiatric treatment in prison and sentenced him to twenty years to life. S1. The judge ordered that Chapman should get psychiatric treatment. In prison and sentenced him to twenty years to life. (Zhu et al., 2010) S2. The judge ordered that Chapman should receive psychiatric treatment in prison.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_7",
  "x": "The judge ordered that Chapman should get psychiatric treatment. In prison and sentenced him to twenty years to life. (Zhu et al., 2010) S2. The judge ordered that Chapman should receive psychiatric treatment in prison. It sentenced him to twenty years to life. (Woodsend and Lapata, 2011) Deletion. By handling deletion using a probabilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors).",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_8",
  "x": "The judge ordered that Chapman should receive psychiatric treatment in prison. It sentenced him to twenty years to life. (Woodsend and Lapata, 2011) Deletion. By handling deletion using a probabilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_9",
  "x": "**ESTIMATING THE PARAMETERS** We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and<cite> Zhu et al. (2010)</cite> ; and build training graphs (Figure 2 ) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). Each deletion candidate creates a deletion O-node marking successful or failed deletion of the candidate and a result M-node. The deletion process continues on the result M-node until there is no deletion candidate left to process. The governing criteria for the construction of the training graph is that, at each step, it tries to minimize the Levenshtein edit distance between the complex and the simple sentences. Moreover, for the splitting operation, we introduce a split only if the reference sentence consists of several sentences (i.e., there is a split in the training data); and only consider splits which maximises the overlap between split and simple reference sentences. We initialize our probability tables Table 1 and  Table 2 with the uniform distribution, i.e., 0.5 because all our features are binary. The EM algorithm iterates over training graphs counting model features from O-nodes and updating our probability tables.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_10",
  "x": "To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_11",
  "x": "---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer produces a DRS for 96 of the 100 input sentences.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_12",
  "x": "The DRS associated with the final M-node D f in is then mapped to a simplified sentence s \u2032 f in which is further simplified using the phrase-based machine translation system to produce the final simplified sentence s simple . ---------------------------------- **EXPERIMENTS** We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_13",
  "x": "PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 . We then parse all complex sentences in PWKP using Boxer 9 to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer produces a DRS for 96 of the 100 input sentences. These input are simplified using our simplification system namely, the DRS-SM model and the phrase-based machine translation system (Section 3.2). For the remaining four complex sentences, Boxer fails to produce DRSs. These four sentences are directly sent to the phrase-based machine translation system to produce simplified sentences. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_0",
  "x": "We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. ---------------------------------- **INTRODUCTION** Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . <cite>McClosky et al. (2006a</cite>; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
  "y": "background"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_2",
  "x": "The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . <cite>McClosky et al. (2006a</cite>; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of <cite>McClosky et al. (2006a</cite>; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_0",
  "x": "Due to the vigorous development of social media in recent years, more and more user-generated sentiment data have been shared on the Web. It is a useful means to understand the opinion of the masses, which is a major issue for businesses. However, they exist in the forms of comments in a live webcast, opinion sites, or social media, and often contain considerable amount of noise. Such characteristics pose obstacles to those who intend to collect this type of information efficiently. It is the reason why opinion mining has recently become a topic of interest in both academia and business institutions. Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values<cite> (Yu et al., 2015)</cite> . The categorical approach aims at classifying the sentiment into polarity classes (such as positive, neutral, and negative,) or Ekman's six basic emotions, i.e., anger, happiness, fear, sadness, disgust, and surprise (Ekman, 1992 ). This approach is extensively studied because it can provide a desirable outcome, which is an overall evaluation of the sentiment in the material that is being analyzed. For instance, a popular form of media in recent years is live webcasting. This kind of applications usually provide viewers with the ability to comment immediately while the stream is live.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_1",
  "x": "Such characteristics pose obstacles to those who intend to collect this type of information efficiently. It is the reason why opinion mining has recently become a topic of interest in both academia and business institutions. Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values<cite> (Yu et al., 2015)</cite> . The categorical approach aims at classifying the sentiment into polarity classes (such as positive, neutral, and negative,) or Ekman's six basic emotions, i.e., anger, happiness, fear, sadness, disgust, and surprise (Ekman, 1992 ). This approach is extensively studied because it can provide a desirable outcome, which is an overall evaluation of the sentiment in the material that is being analyzed. For instance, a popular form of media in recent years is live webcasting. This kind of applications usually provide viewers with the ability to comment immediately while the stream is live. Categorical sentiment analysis can immediately classify each response as either positive or negative, thus helping the host to quickly summarize every period of their broadcast. On the other hand, the dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valencearousal space (Markus and Kitayama, 1991) , as shown in Fig. 1 gree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement. According to the twodimensional representation, any affective state can be represented as a point in the valence-arousal space by determining the degrees of valence and arousal of given words (Wei et al., 2011; <cite>Yu et al., 2015)</cite> or texts (Kim et al., 2010) .",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_2",
  "x": "In order to cope with the problem of unknown words, we separate words in WVA into 4,184 characters with valence-arousal ratings, called CVA. The valence-arousal score of the unknown word can be obtained by averaging the matched CVA. Moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence-arousal methods<cite> (Yu et al., 2015)</cite> . Thus, we use two sets of methods for the prediction of valence: (1) prediction based on WVA and CVA, and (2) a kNN valence prediction method. The results of these two methods are averaged as the final valence score. First, we describe the prediction of valence values. As shown in Fig. 3 , the \"\u5b8c\u6210\" of the test data exists in the WVA, so we can directly obtain its valence value of 7.0. However, another word \"\u901a\u77e5\" does not exist in the WVA, so we search in CVA and calculate a valence value of 5.6. Additionally, we propose another prediction method of the valence value, as shown in Fig. 4 , based on kNN. We begin by computing the similarity between words using word embeddings (Mikolov et al., 2013) .",
  "y": "motivation background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_0",
  "x": "****MORPHO-SYNTACTIC REGULARITIES IN CONTINUOUS WORD REPRESENTATIONS: A MULTILINGUAL STUDY**** **ABSTRACT** We replicate the syntactic experiments of <cite>Mikolov et al. (2013b)</cite> on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. ---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen \u2248 king \u2212 man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked (Figure 1 ).",
  "y": "uses extends"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_1",
  "x": "**ABSTRACT** We replicate the syntactic experiments of <cite>Mikolov et al. (2013b)</cite> on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. ---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen \u2248 king \u2212 man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked (Figure 1 ). They report state-of-the-art results on a set of analogy questions of the form \"a is to b as c is to \", where the variables represent various English word forms.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_2",
  "x": "They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen \u2248 king \u2212 man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked (Figure 1 ). They report state-of-the-art results on a set of analogy questions of the form \"a is to b as c is to \", where the variables represent various English word forms. Our work is motivated by two observations regarding Mikolov et al.'s experiments: first, the syntactic analogies that they test correspond to morphological inflections, and second, the tests only evaluate English, a language with little morphological complexity. In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. ---------------------------------- **REPLICATION EXPERIMENTS** In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS**",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_3",
  "x": "They report state-of-the-art results on a set of analogy questions of the form \"a is to b as c is to \", where the variables represent various English word forms. Our work is motivated by two observations regarding Mikolov et al.'s experiments: first, the syntactic analogies that they test correspond to morphological inflections, and second, the tests only evaluate English, a language with little morphological complexity. In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. ---------------------------------- **REPLICATION EXPERIMENTS** In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_4",
  "x": "---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus. <cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model. 1 Rather than use an RNN model to learn our own vectors, we employ the far simpler skip-gram model. Mikolov et al. (2013a) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train. The skip-gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words.",
  "y": "motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_5",
  "x": "Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus. <cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model. 1 Rather than use an RNN model to learn our own vectors, we employ the far simpler skip-gram model. Mikolov et al. (2013a) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train. The skip-gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words. The WORD2VEC package includes code for learning skip-gram models from very large corpora. 2 We train 640-dimensional vectors using the skip-gram model with a hierarchical softmax, a context window of 10, sub-sampling of 1e-3, and a minimum frequency threshold of 10. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_7",
  "x": "We therefore modify the nominal test set to only include questions that contain the singular vs. plural distinction. We make no changes to the adjectival and verbal analogy sets. The adjectival set contains analogies between the comparative and the superlative, the comparative and the base, and the superlative and the base. The verbal set includes comparisons between the preterite, the infinitive, and the 3rd person singular present, but not the past and present participles. ---------------------------------- **RESULTS** In Table 1 , we report two numbers for each part of speech. The first, labeled as M13, is the result of applying the vectors of <cite>Mikolov et al. (2013b)</cite> to their test set. The results match the results reported in their paper, except for the nominal results, which reflect our modifications described in Section 2.2. The removal of the possessives improves the accuracy from 25.2% reported in the original paper to 40.1%.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_8",
  "x": "The second column, labeled as Ours, reports the results for our vectors, which were trained using WORD2VEC on the English data described in Section 2.1. Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of <cite>Mikolov et al. (2013b)</cite> , but they are not far off. For nouns, however, we obtain higher accuracy than Mikolov et al. The tokenization method that removes possessives from consideration may produce better vectors for singular and plural forms, as it increases the frequency of these types. ---------------------------------- **MULTILINGUAL EXPERIMENTS** Our second set of experiments examine to what extent the syntactic regularities are captured by word vectors in four other languages: Dutch, French, German, and Spanish. ---------------------------------- **TRAINING CORPORA FOR WORD VECTORS** As in the previous experiment, our training corpora are from the Polyglot project. We limit each corpus to the first 320M lowercased tokens, except for the Dutch corpus, which has only 180M tokens.",
  "y": "similarities differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_9",
  "x": "In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of <cite>Mikolov et al. (2013b)</cite> in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by TREETAGGER (Schmid, 1994) . We identify inflections using manually constructed inflection tables from several sources. Spanish and German verbal inflections, as well as German nominal inflections, are from a Wiktionary data set introduced by Durrett and DeNero (2013) . 4 Dutch verbal inflections and English verbal and nominal inflections are from the CELEX database (Baayen et al., 1995) . French verbal inflections are from Verbiste, an online French conjugation dictionary. 5 Whereas Mikolov et al. create analogies from various inflectional forms, we require the analogies to always include the base dictionary form: the infinitive for verbs, and the nominative singular for nouns. In other words, all analogies are limited to 4 We exclude Finnish because of its high morphological complexity and the small size of the corresponding Polyglot corpus. 5 http://perso.b2b2c.ca/sarrazip/dev/verbiste.html comparisons between the base form and an inflected form.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_0",
  "x": "* Much of the work presented here was carried out while the first author was at the University of Stuttgart. Phrase-based systems employ a simple and effective machinery by learning larger chunks of translation called phrases 1 . Memorizing larger units enables the phrase-based model to learn local dependencies such as short reorderings, idioms, insertions and deletions, etc. The model however, has the following drawbacks: i) it makes independence assumptions over phrases ignoring the contextual information outside of phrases ii) it has issues handling long-distance reordering iii) it has the spurious phrasal segmentation problem which allows multiple derivations of a bilingual sentence pair having different model scores for each segmentation. Modeling with minimal translation units helps address some of these issues. The N-gram-based SMT framework is based on tuples. Tuples are minimal translation units composed of source and target cepts 2 . N-gram-based models are Markov models over sequences of tuples or operations encapsulating tuples<cite> (Durrani et al., 2011)</cite> . This mechanism has several useful properties. Firstly, no phrasal independence assumption is made.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_1",
  "x": "Firstly, no phrasal independence assumption is made. The model has access to both source and target context outside of phrases. Secondly the model learns a unique derivation of a bilingual sentence given its alignment, thus avoiding the spurious segmentation problem. Using minimal translation units, however, results in a higher number of search errors because of i) 1 A phrase-pair in PBSMT is a sequence of source and target words that is translation of each other, and is not necessarily a linguistic constituent. Phrases are built by combining minimal translation units and ordering information. 2 A cept is a group of words in one language that is translated as a minimal unit in one specific context (Brown et al., 1993). poor translation selection, ii) inaccurate future-cost estimates and iii) incorrect early pruning of hypotheses that would produce better model scores if allowed to continue. In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; , or a higher beam size is used in decoding<cite> (Durrani et al., 2011)</cite> . The ability to memorize and produce larger translation chunks during decoding, on the other hand, gives a distinct advantage to the phrase-based system during search. Phrase-based systems i) have access to uncommon translations, ii) do not require higher beam sizes, iii) have more accurate future-cost estimates because of the availability of phrase-internal language model context before search is started.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_2",
  "x": "A higher beam is required to prevent it from getting pruned. Phrase-based systems, on the other hand, are likely to have access to the phrasal unit \"scho\u00df ein Tor -scored a goal\" and can generate it in a single step. Moreover, a more accurate future-cost estimate can be computed because of the available context internal to the phrase. In this work, we extend the N-gram model, based on operation sequences<cite> (Durrani et al., 2011)</cite> , to use phrases during decoding. The main idea is to study whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above-mentioned problems. The remainder of this paper is organized as follows. The next two sections review phrase-based and N-gram-based SMT. Section 2 provides a comparison of phrase-based and N-gram-based SMT. Section 3 summarizes the operation sequence model (OSM), the main baseline for this work. Section 4 analyzes the search problem when decoding with minimal units.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_3",
  "x": "There are three drawbacks of this approach. Firstly, lexical generation and reordering are decoupled. Search is only performed on a small number of reorderings, pre-calculated using the source side and completely ignoring the targetside. And lastly, the POS-based rules face data sparsity problems especially in the case of long distance reorderings. <cite>Durrani et al. (2011)</cite> recently addressed these problems by proposing an operation sequence Ngram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules. Representing bilingual sentences as a sequence of operations enables them to memorize phrases and lexical reordering triggers like PBSMT. However, using minimal units during decoding and searching over all possible reorderings means that hypotheses can no longer be arranged in 2 m stacks. The problem of inaccurate future-cost estimates resurfaces resulting in more search errors. A higher beam size of 500 is therefore used to produce translation units in comparison to phrase-based systems. This, however, still does not eliminate all search errors.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_4",
  "x": "This is similar to N-gram-based SMT except that the tuples in the N-gram-based model are generated monotonically, whereas in this case lexical generation and reordering information is strongly coupled in an operation sequence. Consider the phrase pair: The model memorizes it through the sequence: . . , o j\u22121 be a sequence of operations as hypothesized by the translator to generate the bilingual sentence pair F, E with an alignment function A. The translation model is defined as: where n indicates the amount of context used. The translation model is implemented as an N-gram model of operations using SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing. A 9-gram model is used. Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework<cite> (Durrani et al., 2011)</cite> . ---------------------------------- **SEARCH 4.1 OVERVIEW OF DECODING FRAMEWORK**",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_5",
  "x": "The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004a) . The decoder uses beam search to build up the translation from left to right. The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words. The ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence. The overall process can be roughly divided into the following steps: i) extraction of translation units ii) future-cost estimation, iii) hypothesis extension iv) recombination and pruning. During the hypothesis extension each extracted phrase is translated into a sequence of operations. The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations. The model backs-off to the smaller n-grams of operations if the full history is unknown.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_6",
  "x": "---------------------------------- **SEARCH 4.1 OVERVIEW OF DECODING FRAMEWORK** The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004a) . The decoder uses beam search to build up the translation from left to right. The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words. The ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence. The overall process can be roughly divided into the following steps: i) extraction of translation units ii) future-cost estimation, iii) hypothesis extension iv) recombination and pruning. During the hypothesis extension each extracted phrase is translated into a sequence of operations. The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ).",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_7",
  "x": "In the last section we discussed the disadvantages of using cepts during search in a left-to-right decoding framework. We now define a method to empirically study the mentioned drawbacks and whether using information available in phrase-pairs during decoding can help improve search accuracy and translation quality. ---------------------------------- **TRAINING** We extended the training steps in <cite>Durrani et al. (2011)</cite> to extract a phrase lexicon from the parallel data. We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments. Only continuous phrases as used in a traditional phrase-based system are extracted thus allowing only inside-out (Wu, 1997) type of alignments. The future cost of each feature component used in the log-linear model is calculated. The operation sequence required to hypothesize each phrase is generated and its future cost is calculated. The future costs of other features such as language models, lexicalized probability features, etc. are also estimated.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_8",
  "x": "We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments. Only continuous phrases as used in a traditional phrase-based system are extracted thus allowing only inside-out (Wu, 1997) type of alignments. The future cost of each feature component used in the log-linear model is calculated. The operation sequence required to hypothesize each phrase is generated and its future cost is calculated. The future costs of other features such as language models, lexicalized probability features, etc. are also estimated. The estimates of the countbased reordering penalties (gap penalty and open gap penalty) and the distance-based features (gapwidth and reordering distance) could not be estimated previously with cepts but are available when using phrases. ---------------------------------- **DECODING** We extended the decoder developed by <cite>Durrani et al. (2011)</cite> and tried three ideas. In our primary experiments we enabled the decoder to use phrases instead of cepts.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_9",
  "x": "**EXPERIMENTAL SETUP** We initially experimented with two language pairs: German-to-English (G-E) and French-to-English (F-E). We trained our system and the baseline systems on most of the data 6 made available for the translation task of the Fourth Workshop on Statistical Machine Translation. 7 We used 1M bilingual sentences, for the estimation of the translation model and 2M sentences from the monolingual corpus (news commentary) which also contains the English part of the bilingual corpus. Word alignments are obtained by running GIZA++ (Och and Ney, 2003) with the grow-diag-final-and (Koehn et al., 2005) symmetrization heuristic. We follow the training steps described in <cite>Durrani et al. (2011)</cite> , consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models. ---------------------------------- **SEARCH ACCURACY RESULTS** We divided our evaluation into two halves. In the first half we carried out experiments to measure search accuracy and translation quality of our decoders against the baseline cept-based OSM (cept.500) that uses minimal translation units with a stack size of 500.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_10",
  "x": "In order to make a fair comparison, both the phrase-based and the baseline cept-based decoders should be allowed to run for the same amount of time. We therefore reduced the stack size in the phrase-based decoder so that it runs in the same amount of time as the cept-based decoder. We found that using a stack size of 200 10 for the phrase-based decoder was comparable in speed to using a stacksize of 500 in the cept-based decoding. We first tuned the baseline on dev 11 to obtain an optimized weight vector. We then ran the baseline and our decoders as discussed in Section 5.2 on the dev-test. Then we repeated this experiment by tuning the weights with our phrase-based decoder (using a stack size of 100) and ran all the decoders again using the new weights. Table 1 shows the average search accuracies and BLEU scores of the two experiments. Using phrases during decoding in the G-E experiments resulted in a statistically significant 12 0.69 BLEU points gain comparing our best system phrase.200 with the baseline system cept.500. We mark a result as sig-8 Discontinuous source-side units did not lead to any improvements in<cite> (Durrani et al., 2011)</cite> and increased the decoding times by multiple folds. We also found these to be less useful.",
  "y": "differences"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_0",
  "x": "While multiple intelligent writing assistants have been developed (Writelab, 2015; Draft, 2015; Turnitin, 2016) , they typically focus on the quality of the current essay instead of the revisions that have been made. For example, Turnitin identifies weak points of the essay and gives suggestions on how to improve them; it also assigns an overall score to the essay so students can get a coarse-grained feedback on whether they are making progress in their revisions. However, without explicit feedback on each change, students may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions \"better\". Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author's revision purpose is not correctly recognized, it indicates that the effect of the writer's change might have not met the writer's expectation, which suggests that the writer should revise their revisions. The framework also connects the automatic analyzer with an interface for the instructor to manually correct the analysis results.",
  "y": "background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_1",
  "x": "In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author's revision purpose is not correctly recognized, it indicates that the effect of the writer's change might have not met the writer's expectation, which suggests that the writer should revise their revisions. The framework also connects the automatic analyzer with an interface for the instructor to manually correct the analysis results. As a side benefit, it also sets up an annotation pipeline to collect further data to improve the underlying automatic analyzer. ---------------------------------- **SYSTEM OVERVIEW** The design of ArgRewrite aims to encourage students to concentrate on revision improvement: to iteratively refine the essay based on the feedback of the automatic system or the writing instructor. Our framework consists of three components, arranged in a server client model.",
  "y": "background extends"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_2",
  "x": "The student can choose to view the analysis results immediately after the completion of automatic revision analysis or wait until the analysis results were corrected by the instructor. After receiving the analysis feedback, the student can choose to continue with the cycle of essay revising until the revisions are satisfactory. aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. Revision classification. Following the argumentative revision definition in our prior work<cite> (Zhang and Litman, 2015)</cite> , revisions are first categorized to Content (Text-based) and Surface 3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence, and Other content changes (General Content). The Surface revisions include Fluency (Wordusage/Clarity), Reordering (Organization) and Errors (Conventions/Grammar/Spelling). On the basis of later work, the system includes the two new categories Precision 4 and Unknown 5 . Using the corpora and features defined in our prior work, a multiclass Random Forest classifier was trained to automatically predict the revision purpose type for each extracted revision.",
  "y": "uses"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_3",
  "x": "Our rewriting assistant interface is designed with several principles in mind. 1) Because the revision classification taxonomy goes beyond the binary textual versus surface distinction, we want to make sure that users don't get lost distinguishing different categories; 2) We want to encourage users to think about their revisions holistically, not always just focusing on low-level details; 3) We want to encourage users to continuously re-evaluate whether they succeeded in making changes between drafts (rather than focusing on generating new contents). Thus, we have designed an interface that offers multiple views of the revision changes. As demonstrated in Figure 2 , the interface includes a revision overview interface for the overview of the authors' revisions and a revision detail interface that allows the author to access the details of their essays and revisions. Inspired by works on learning analytics (Liu et al., 2013; Verbert et al., 2013) , we design the revision overview interface which displays the statistics of the revisions. Following design principle #1, the revision purposes are color coded and each purpose corresponds to a specific color. Our prior work<cite> (Zhang and Litman, 2015)</cite> demonstrates that only Text-based revisions are significantly correlated with the writing improvement. To inspire the writers to focus more on the important Text-based revisions, cold colors are chosen for the Surface revisions and warm colors are chosen for the Text-based revisions. The statistics and the pie chart provide a quantitative summary of the writer's revision efforts. For example, in Figure 2 , the writer makes many changes on the Fluency (15) of sentences but makes no change on the Thesis/Ideas (0).",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_0",
  "x": "On the one hand we are inspired by the synchronic work on predicting the degree of compositionality of compounds by comparing the vector-based representations of the parts to the vector-based representations of the compound as a whole. On the other hand, we rely on methods designed for detecting semantic change, such as presented in Hamilton et al. (2018) , to study compositionality in compounds from a diachronic viewpoint. ---------------------------------- **RELATED WORK** From a synchronic perspective, <cite>Reddy et al. (2011</cite> ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations. However, Schulte im Walde et al. (2013) use German data and do not investigate diachronic changes. They report a Spearman's \u03c1 of 0.65 for predicting the compositionality of compounds based on the features of their semantic space and find that the modifiers mainly influence the compositionality of the whole compound, contrary to their expectation that the head should be the main source of influence. This is true for both the human annotation and their vector space model. Schulte im Walde et al. (2016a) further investigate the role of heads and modifiers on the prediction of compositionality and report \u03c1 values between 0.35 and 0.61 for their models on German and English data. Reddy et al. (2011) also report Spearman's \u03c1 between their surveyed compositionality values and word vectors.",
  "y": "similarities background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_1",
  "x": "This is true for both the human annotation and their vector space model. Schulte im Walde et al. (2016a) further investigate the role of heads and modifiers on the prediction of compositionality and report \u03c1 values between 0.35 and 0.61 for their models on German and English data. Reddy et al. (2011) also report Spearman's \u03c1 between their surveyed compositionality values and word vectors. They achieve \u03c1 values of around 0.68, depending on the model. From a diachronic perspective, we follow the general methodological approach of Hamilton et al. (2018) , who use PPMI, SVD and word2vec based vector spaces to investigate a shift in meaning for chosen words with a known semantic change (gay, broadcast, etc.). They use time series to detect a significant change-point for two words, using cosine similarity and Spearman's \u03c1. They also compute the displacement for a single word embedding by calculating the cosine similarity between a point in time t and a later point in time t + \u2206. We adapt this methodology and make use of the same corpus (Google Books Ngram). ---------------------------------- **METHODS AND DATA** Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009;<cite> Reddy et al., 2011</cite>; Schulte im Walde et al., 2016b) .",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_2",
  "x": "That means that a bigram (say the compound gold mine) could occur in four different positions in the 5-grams (1-2, 2-3, 3-4 and finally 4-5). We then capture the contexts for each of these positions, in order to enrich the representation of a compound and its constituents (which similarly have five such positions, as they are unigrams). Ideally, we would validate our diachronic model on diachronic test data. However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by<cite> Reddy et al. (2011)</cite> (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general. Reddy et al. (2011) compiled a list of 90 English compounds and asked annotators to rate the compositionality of these compounds on a scale from 0 to 5. They provide three mean values of their ratings for the compounds (compound-mean), heads (head-mean) and modifiers (modifier-mean). We make use of REDDY in order to verify that our methods are capable of capturing compositionality (synchronically) and use the diachronic data of Google Books Ngram to investigate the temporal change of compositionality. A common challenge in building semantic spaces on a diachronic scale is that when building the spaces for individual spans of time, the spaces need to be aligned later on in order to compare models (see e.g. Kutuzov et al., 2018, Section 3.3) . We circumvent this problem by jointly learning the spaces for the target words. To do this, we take the sparse representations of the compounds and their constituents and jointly learn their dense representations using SVD. Similar to Hamilton et al. (2018) we also choose the dimensions of our embeddings to be 300.",
  "y": "uses"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_3",
  "x": "They were inspired by research on N-N compounds in Dutch that suggests that constituents such asmolen '-mill' in pepermolen 'peppermill' are separately stored as abstract combinatorial structures rather than understood on the basis of their independent constituents (De Jong et al., 2002) . We hence adopt the compound-centric setting. ---------------------------------- **CORRELATION** We first carry out a quantitative experiment, to see if our features bolster the prediction of compositionality in noun-noun compounds. To do so, we calculate correlation scores between our proposed metrics and the annotated compositionality ratings of REDDY. Like<cite> Reddy et al. (2011)</cite> and Schulte im Walde et al. (2013), we opt for Spearman's \u03c1. To find the best configuration of a time span and cut-off for the regression models, we use the R 2 metric. Table 1 presents our findings; we will discuss them in the following Section 5. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_4",
  "x": "Contrary to Schulte im Walde et al. (2013) we do not find a strong correlation between modifiers and the REDDY ratings. Interestingly, PPMI is always weakly negatively correlated with the ratings. This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100. A few other observations could be made: In general the cut-off seems to improve the R 2 metric and the time spans of 10 and 20 years appear to be the most informative and stable for the cut-off values. Also, using temporal information almost always outperforms the setup that ignores all temporal information. For our following experiment, we choose to use the configuration with the highest R 2 value: a time span of 20 years and a cut-off of 100. Since LMI achieved the highest \u03c1 values, we also choose LMI over the other features.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_5",
  "x": "As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100. A few other observations could be made: In general the cut-off seems to improve the R 2 metric and the time spans of 10 and 20 years appear to be the most informative and stable for the cut-off values. Also, using temporal information almost always outperforms the setup that ignores all temporal information. For our following experiment, we choose to use the configuration with the highest R 2 value: a time span of 20 years and a cut-off of 100. Since LMI achieved the highest \u03c1 values, we also choose LMI over the other features. We group the compounds of REDDY into three groups based on the human ratings they obtained: low (0-1), med (2-3) and high (4-5). Each group contains around 30 compounds. We then plot the LMI values of these three groups with their confidence interval across the time step of 20 years, shown in Figure 1 .",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_6",
  "x": "Both figures based on cosine based features largely confound the three groups of compounds across time, reinforcing our previous findings. 6 Future Work Our current work was limited to English compounds from<cite> Reddy et al. (2011)</cite> . We plan to expand our models to other languages for which compositionality ratings are available, such as German. We would also like to further investigate the fact that the information theory based measures outperform the ones based on cosine similarity. We intend to do so by incorporating more compounds and their compositionality ratings, as well as by using larger corpora. Lastly, we will seek to find ways to harvest proxies for compositionality ratings of compounds over time. A possible avenue could be to use the information available in dictionaries. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_0",
  "x": "****AN IMPROVED TAG DICTIONARY FOR FASTER PART-OF-SPEECH TAGGING**** **ABSTRACT** Abstract Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed<cite> (Moore, 2014)</cite> makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992) . Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens.",
  "y": "motivation background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_1",
  "x": "We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ---------------------------------- **** introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. ---------------------------------- **OVERVIEW** In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_2",
  "x": "While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. ---------------------------------- **OVERVIEW** In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992) . Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair. 2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data.",
  "y": "motivation differences"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_3",
  "x": "Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ---------------------------------- **TAG DICTIONARIES AND TAGGING SPEED** A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t 1 , . . . , t n given a sequence of words w 1 , . . . , w n . The tag sequence assigned the highest score by the model for a given word sequence is selected as the tagging for the word sequence. If T is the set of possible tags, and there are no restrictions on the form of the model, then the time to find the highest scoring tag sequence is potentially O(n|T | n ) or worse, which would be intractable. To make tagging practical, models are normally defined to be factorable in a way that reduces the time complexity to O(n|T | k ), for some small integer k. For models in which all tagging decisions are independent, or for higher-order mod-els pruned by fixed-width beam search, k = 1, so the time to find the highest scoring tag sequence is O(n|T |). But this linear dependence on the size of the tag set means that reducing the average number of tags considered per token should further speed up tagging, whatever the underlying model or tagger may be.",
  "y": "motivation differences"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_4",
  "x": "But this linear dependence on the size of the tag set means that reducing the average number of tags considered per token should further speed up tagging, whatever the underlying model or tagger may be. ---------------------------------- **RATNAPARKHI'S METHOD** For each word observed in an annotated training set, Ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found <cite>(Moore, 2014</cite> ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%. ---------------------------------- **OUR PREVIOUS METHOD**",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_5",
  "x": "---------------------------------- **RATNAPARKHI'S METHOD** For each word observed in an annotated training set, Ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found <cite>(Moore, 2014</cite> ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%. ---------------------------------- **OUR PREVIOUS METHOD** We previously presented<cite> (Moore, 2014)</cite> a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold T .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_6",
  "x": "---------------------------------- **THE TAGGING MODEL** The model structure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper<cite> (Moore, 2014)</cite> . The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001 ), using stochastic subgradient descent as described by Zhang (2004) , with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time. Our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not exceed the score of the highest scoring incorrect tag by a specified margin. In our new version of this procedure, we use the KN-smoothed tag dictionary described in Section 1.3. to speed up finding the highest scoring incorrect tag. Recall that the KN-smoothed tag dictionary estimates a non-zero probability p(t|w) for every possible word/tag pair, and that the possible tags for a given word are determinted by setting a threshold T on this probability.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_7",
  "x": "We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Section 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the methods described in our previous work <cite>(Moore, 2014</cite>, Section 3.1) . Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. ---------------------------------- **EXTRACTING THE TAG DICTIONARY** We extracted a Ratnaparkhi-like tag dictionary for the 957,819 words with 10 or more occurrences in our corpus. Tokens of all other words in the corpus were treated as unknown word tokens and used to define a set of 24 tags 4 to be used for words not explicitly listed in the dictionary. To allow pruning the dictionary as described in Section 1.4, for each word (including the unknown word), we computed a probability distribution p(t|w) using unsmoothed relative frequencies. As noted above, we treated all digits as indistinguishable in constructing and applying the dictionary. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_8",
  "x": "Additional experiments showed that we could prune this dictionary with a threshold on p(t|w) as high as T = 0.0024 without decreasing development set accuracy. In addition to applying this threshold to the tag probabilities for all listed words, we also applied it to the tag probabilities for unknown words, leaving 13 possible tags 5 for those. Tagging the WSJ development set with these two dictionaries is compared in Table 1 to tagging with our previous pruned KN-smoothed dictionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens-it can simply output the single possible tag. The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described<cite> (Moore, 2014)</cite> , in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2) .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_9",
  "x": "Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2) . The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work<cite> (Moore, 2014)</cite> . The results of these tests are shown in Table 2 Table 2 : WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_10",
  "x": "For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work<cite> (Moore, 2014)</cite> . The results of these tests are shown in Table 2 Table 2 : WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported. The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data. ---------------------------------- **CONCLUSIONS** Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art<cite> (Moore, 2014)</cite> for a tag dictionary that does not degrade tagging accuracy.",
  "y": "differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_0",
  "x": "Semantic roles capture commonalities between different realizations of the same underlying predicate-argument structures. For example, present will still be A1 in sentence \"John gave a nice present to his wonderful wife\", despite different surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph.",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_1",
  "x": "Consider Figure 1 , the predicate gave has three arguments: 1 John (semantic role A0, 'the giver'), wife (A2, 'an entity given to') and present (A1, 'the thing given'). Semantic roles capture commonalities between different realizations of the same underlying predicate-argument structures. For example, present will still be A1 in sentence \"John gave a nice present to his wonderful wife\", despite different surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by .",
  "y": "similarities background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_2",
  "x": "Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation). These linguistically-aware word representations are used within a neural encoder. Although recent research has shown that neural architectures are able to learn some linguistic phenomena without explicit linguistic supervision (Linzen et al., 2016; Vaswani et al., 2017) , informing word representations with linguistic structures can provide a useful inductive bias. We apply GCNs to the semantic dependency graphs and experiment on the English-German language pair (WMT16).",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_3",
  "x": "As we use exactly the same modeling approach as in the syntactic method of <cite>Bastings et al. (2017)</cite> , we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics). We observe that when using full WMT data we obtain better results with semantics than with syntax (23.9 BLEU for syntactic GCN). Using syntactic and semantic GCN together, we obtain a further gain (24.9 BLEU) that suggests the complementarity of syntax and semantics. ---------------------------------- **MODEL** ---------------------------------- **ENCODER-DECODER MODELS** We use a standard attention-based encoderdecoder model (Bahdanau et al., 2015) as a starting point for constructing our model. In encoderdecoder models, the encoder takes as input the source sentence x and calculates a representation of each word x t in x. The decoder outputs a translation y relying on the representations of the source sentence. Traditionally, the encoder is parametrized as a Recurrent Neural Network (RNN), but other architectures have also been successful, such as Convolutional Neural Networks (CNN) (Gehring et al., 2017) and hierarchical selfattention models (Vaswani et al., 2017) , among others.",
  "y": "motivation similarities uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_4",
  "x": "Once the sentence has been encoded, the decoder takes as input the induced sentence representation and generates the target sentence y. The target sentence y is predicted word by word using an RNN decoder. At each step, the decoder calculates the probability of generating a word y t conditioning on a context vector c t and the previous state of the RNN decoder. The context vector c t is calculated based on the representation of the source sentence computed by the encoder, using an attention mechanism (Bahdanau et al., 2015) . Such a model is trained end-to-end on a parallel corpus to maximize the conditional likelihood of the target sentences. ---------------------------------- **GRAPH CONVOLUTIONAL NETWORKS** Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ).",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_5",
  "x": "**GRAPH CONVOLUTIONAL NETWORKS** Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges. Each node v \u2208 V is represented by a feature vector x v \u2208 R d , where d is the latent space dimensionality. The GCN induces a new representation h v \u2208 R d of a node v while relying on representations h u of its neighbors: where N (v) is the set of neighbors of v, W dir(u,v) \u2208 R d\u00d7d is a direction-specific parameter matrix. There are three possible directions (dir(u, v) \u2208 {in, out, loop}): self-loop edges were added in order to ensure that the initial representation of node h v directly affects its new representation h v .",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_6",
  "x": "The GCN induces a new representation h v \u2208 R d of a node v while relying on representations h u of its neighbors: where N (v) is the set of neighbors of v, W dir(u,v) \u2208 R d\u00d7d is a direction-specific parameter matrix. There are three possible directions (dir(u, v) \u2208 {in, out, loop}): self-loop edges were added in order to ensure that the initial representation of node h v directly affects its new representation h v . The vector b lab(u,v) \u2208 R d is an embedding of a semantic role label of the edge (u, v) (e.g., A0). The functions g u,v are scalar gates which weight the importance of each edge. Gates are particularly useful when the graph is predicted BiRNN Baseline<cite> (Bastings et al., 2017)</cite> 23.3 +Sem 24.5 +Syn<cite> (Bastings et al., 2017)</cite> 23.9 +Syn + Sem 24.9 and thus may contain errors, i.e., wrong edges. In this scenario gates can down weight the influence of such edges. \u03c1 is a non-linearity (ReLU). 2 As with CNNs, GCN layers can be stacked in order to incorporate higher order neighborhoods. In our experiments, we used GCNs on top of a standard BiRNN encoder and a CNN encoder (Figure 2) .",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_7",
  "x": "We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002) . The settings and the framework (Neural Monkey (Helcl and Libovick\u00fd, 2017) ) used for experiments are the ones used in <cite>Bastings et al. (2017)</cite> , which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014) . We now discuss the impact that different architectures and linguistic information have on the translation quality. ---------------------------------- **RESULTS AND DISCUSSION** First, we start with experiments with the smaller News Commentary training set (See Table 1 ). As in <cite>Bastings et al. (2017)</cite> , we used the standard attention-based encoder-decoder model as a baseline. We tested the impact of semantic GCNs when used on top of CNN and BiRNN encoders. As expected, BiRNN results are stronger than CNN ones.",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_8",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** First, we start with experiments with the smaller News Commentary training set (See Table 1 ). As in <cite>Bastings et al. (2017)</cite> , we used the standard attention-based encoder-decoder model as a baseline. We tested the impact of semantic GCNs when used on top of CNN and BiRNN encoders. As expected, BiRNN results are stronger than CNN ones. In general, for both encoders we observe the same trend: using semantic GCNs leads to an improvement over the baseline model. The improvements is 0.7 BLEU for BiRNN and 0.8 for CNN. This is slightly surprising as the potentially non-local semantic information should in principle be more beneficial within a less powerful and local CNN encoder. The syntactic GCNs<cite> (Bastings et al., 2017)</cite> appear stronger than semantic GCNs.",
  "y": "uses"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_0",
  "x": "Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 We follow the definitions in Antske Fokkens' guest blog post \"replication (obtaining the same results using the same experiment) as well as reproduction (reach the same conclusion through different means)\" from http://coling2018. org/slowly-growing-offspring-zigglebottom-anno-2017-guest-post/ are evaluated on the SemEval dataset (Pontiki et al., 2014) but not all. Datasets can vary by domain (e.g. product), type (social media, review), or medium (written or spoken), and to date there has been no comparative evaluation of methods from these multiple classes. Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2016; Liu and Zhang, 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017) . In some cases, the code was initially made available, then removed, and is now back online <cite>(Tang et al., 2016a)</cite> . Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_1",
  "x": "This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing (Wang et al., 2017) , and RNN <cite>(Tang et al., 2016a)</cite> , as well as having been applied largely to different datasets. At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created. ---------------------------------- **RELATED WORK** Reproducibility and replicability have long been key elements of the scientific method, but have been gaining renewed prominence recently across a number of disciplines with attention being given to a 'reproducibility crisis'.",
  "y": "motivation"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_2",
  "x": "The closest related work to our reproducibility study is that of Marrese-Taylor and Matsuo (2017) which they replicate three different syntactic based aspect extraction methods. They found that parameter tuning was very important however using different pre-processing pipelines such as Stanford's CoreNLP did not have a consistent effect on the results. They found that the methods stated in the original papers are not detailed enough to replicate the study as evidenced by their large results differential. Dashtipour et al. (2016) undertook a replication study in sentiment prediction, however this was at the document level and on different datasets and languages to the originals. In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002) . Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) <cite>(Tang et al., 2016a)</cite> , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF .",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_3",
  "x": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN (Dong et al., 2014) , TDLSTM <cite>(Tang et al., 2016a)</cite> , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. ---------------------------------- **DATASETS USED IN OUR EXPERIMENTS** We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums. As highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results. In this paper, we do not consider the quality or inter-annotator agreement levels of these datasets but it has been noted that some datasets may have issues here. For example, Pavlopoulos and Androutsopoulos (2014) point out that the Hu and Liu (2004) dataset does not state their inter-annotator agreement scores nor do they have aspect terms that express neutral opinion.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_4",
  "x": "For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" <cite>(Tang et al., 2016a)</cite> as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE and Glove Twitter vectors 11 (Pennington et al., 2014) . As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop. As it has been shown by Reimers and Gurevych (2017) that the random seed statistically significantly changes the results of experiments we ran each model over each word embedding thirty times, using a different seed value but keeping the same stratified train and validation split, and reported the results on the same test data as the original paper. As can be seen in Figure 4 , the initial seed value makes a large difference more so for the smaller embeddings. In table 5, we show the difference between our mean and maximum result and the original result for each model using the 200 dimension Glove Twitter vectors. Even though the mean result is quite different from the original the maximum is much closer. Our results generally agree with their results on the ranking of the word vectors and the embeddings.",
  "y": "differences extends"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_5",
  "x": "Our results generally agree with their results on the ranking of the word vectors and the embeddings. Overall, we were able to reproduce the results of all three papers. However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ---------------------------------- **MASS EVALUATION** For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) . To make the experiments quicker and computationally less expensive, we filtered out all words from the word vectors that did not appear in the train and test datasets, and this is equivalent with respect to word coverage as using all words. Finally we only reported results for the LSTM methods with one seed value and not multiple due to time constraints.",
  "y": "extends differences"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_6",
  "x": "Our results generally agree with their results on the ranking of the word vectors and the embeddings. Overall, we were able to reproduce the results of all three papers. However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ---------------------------------- **MASS EVALUATION** For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) . To make the experiments quicker and computationally less expensive, we filtered out all words from the word vectors that did not appear in the train and test datasets, and this is equivalent with respect to word coverage as using all words. Finally we only reported results for the LSTM methods with one seed value and not multiple due to time constraints.",
  "y": "similarities uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_0",
  "x": "Link prediction methods in knowledge graphs (see (Nickel et al., 2016) for an overview) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities (such as homophily). Lao and Cohen (2010) introduced a new method that predicts direct links based on paths that connect the source and target nodes. Such paths are not only useful for link prediction (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) . These approaches rely on the structure of the knowledge graph, which is inherently incomplete. This incompleteness can affect the process in different ways, e.g. it leads to representations for nodes with few connection that are not very informative, it can miss relevant patterns/paths (or derive misleading patterns/paths). In this paper we investigate whether a higherlevel view of a graph -an abstract graph that captures an intensional view of the original extensional graph -can help derive more robust and informative patterns. Such patterns are paths (i.e. sequences of relations) that could be used not only for link prediction, but also for targeted information extraction for completing the graph with external information. This abstract graph will contain only one edge for each relation type, that will connect a node representing the relation's domain (or source) to its range (or target). Additional edges will link the nodes to capture set relations (intersection, subset, superset) information between the different relations' domains and ranges. This step drastically reduces the graph size, making many different graph processing approaches more tractable.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_1",
  "x": "We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using<cite> Gardner et al. (2014)</cite> 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL. Our analysis shows that we find different and much fewer paths than the PRA method does (mostly because the abstract paths do not contain back-and-forth sequences of generalizing or type relations). The paths found in the abstract graphs lead to better performance on NELL than the PRA paths, which could be explained by the fact that NELL's relation inventory was designed to capture interdependencies (Carlson et al., 2010a) . On Freebase the results we obtain are lower, but this could be due to a different negative sampling process. Inspection of the paths produced reveal that they seem to capture legitimate dependencies. ---------------------------------- **RELATED WORK** Representing facts in a knowledge graph has multiple advantages: (i) they provide knowledge in an easily accessible and machine-friendly format; (ii) they facilitate various ways of encoding this information and deriving representations for nodes and edges that reflect their connectivity in the graph; (iii) they allow for the discovery of connectivity patterns, and possibly more. In recent years, projecting the knowledge graph in an n-dimensional vector space, or learning embeddings for predicting missing facts has attracted a lot of interest. Embedding models aim to map entities, relations and triples to vector space such that additional facts can be inferred from known facts using notions of vector similarity.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_2",
  "x": "Representing facts in a knowledge graph has multiple advantages: (i) they provide knowledge in an easily accessible and machine-friendly format; (ii) they facilitate various ways of encoding this information and deriving representations for nodes and edges that reflect their connectivity in the graph; (iii) they allow for the discovery of connectivity patterns, and possibly more. In recent years, projecting the knowledge graph in an n-dimensional vector space, or learning embeddings for predicting missing facts has attracted a lot of interest. Embedding models aim to map entities, relations and triples to vector space such that additional facts can be inferred from known facts using notions of vector similarity. A class of embedding models that aim to factorize the graph are termed as latent factor models. Neural network based models such as ER-MLP (Dong et al., 2014) , NTN (Socher et al., 2013) , RNNs (Neelakantan et al., 2015; Das et al., 2016) and Graph CNNs (Schlichtkrull et al., 2018) are examples of embedding models while RESCAL (Nickel et al., 2012) , DistMult (Yang et al., 2015) , TransE (Bordes et al., 2013) , ComplEx (Trouillon et al., 2017) are examples of latent factor models. Lao and Cohen (2010) introduced a novel way to exploit information in knowledge graphs: using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into \"meta-paths\". The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) .",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_3",
  "x": "The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into \"meta-paths\". The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) . Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task. We focus here on discovering useful and explanatory paths, not on optimizing or further improving the KGC task. Using paths can lead to interpretable models because the paths can help explain the predicted fact. Meng et al. (2015) present a method to automate the induction of metapaths in large heterogeneous information networks (a.k.a. knowledge graphs) for given node pairs, even if the given node pairs are not connected by a direct relation. Path information is also found to improve performance since paths help the model learn logical rules.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_4",
  "x": "To overcome this limitation (Das et al., 2017) proposed deep reinforcement learning and (Chen et al., 2018) proposed RNNS for generating paths. However, many datasets suffer from paths sparsity, lack of enough paths connecting source target pairs, resulting in poor performance for many relations. Wang et al. (2013) have a different approachthey start with patterns in the form of first-order probabilistic rules, which they then ground in a small subgraph of a large knowledge graph. The approach we present here combines different elements of these previous approaches in a novel way: we build an abstract graph to find pat-terns that would be similar to those used by (Wang et al., 2013) . To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to<cite> (Gardner et al., 2014)</cite> . ---------------------------------- **ABSTRACT GRAPHS AND ABSTRACT PATHS** Knowledge graphs are incomplete in an imbalanced way. Figures 1a-1b show how much the relation and node frequencies for Freebase 15k and NELL vary, and the fact that numerous nodes and edges have very low frequency (each data point corresponds to a node/relation, and the value is the degree of the node/frequency of the relation respectively). Freebase and NELL have a helpful characteristic: they have strongly typed relations, i.e. the source and target of a relation have a very specific type.",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_5",
  "x": "we build the abstract graph where: the source node of relation r i in the abstract graph is the set of source nodes (the domain) of relation r i in KG: the target node of relation r i in the abstract graph is the set of target nodes (the range) of r i in KG: R is the set of relation types of KG, R set = {intersection, subset, superset} 1 . weighted edges where the weight of a set relation between KG A 's nodes quantifies the overlap between the two sets: Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by<cite> (Gardner et al., 2014)</cite> and in this paper). Every data point is the degree of a node (top plots), or frequency of a relation (bottom plots). The data points are ordered monotonically, the x axis is just an index. Building such a graph makes sense only for knowledge repositories that have strongly typed relations -like Freebase and NELL -but we do not require knowledge of the types of the relations' domains and ranges. Such information is not finegrained enough: for example, the relation capital has a type City as a domain, but capital cities are a very small subset of the set of all cities.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_6",
  "x": "The data points are ordered monotonically, the x axis is just an index. Building such a graph makes sense only for knowledge repositories that have strongly typed relations -like Freebase and NELL -but we do not require knowledge of the types of the relations' domains and ranges. Such information is not finegrained enough: for example, the relation capital has a type City as a domain, but capital cities are a very small subset of the set of all cities. Using an \"atomic\" node to represent the domain/range of a relation would not allow us to make finer grained connections and distinctions between the domains and ranges of the existing relations. Figure 2 shows a subset of the abstract graph built from the Freebase dataset. The blue edges are set relations -intersection, superset, subsetbetween the domains and ranges of a subset of the relations in the dataset. The black edges correspond to the actual relations in the dataset. ---------------------------------- **ABSTRACT PATHS** The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by<cite> (Gardner et al., 2014)</cite> , and their abstract versions associating a value for each added path.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_7",
  "x": "Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next. An abstract path, just like a meta-path extracted by previous work, is a sequence of relation types: \u03c0 j =< r j,1 , r j,2 , ...r j,m >, some of which are \"proper\" relations, some are set relations. Because of the more general view of the graph, we lose information about individual paths (i.e. instances of a path in the original graph). Because of this, the paths we extract are hypothetical, but will have associated a confidence score based on the frequency of occurrence of relations in the original KG, and the strength of the connection of the range of one relation on the path with the domain of the next one. The weight of an abstract path \u03c0 j is computed as: In our experiments we used k = 5 where the weight w(r j,i ) of an individual relation is defined based on whether r i,j is a \"proper\" relation or a set relation as:",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_8",
  "x": "Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next. An abstract path, just like a meta-path extracted by previous work, is a sequence of relation types: \u03c0 j =< r j,1 , r j,2 , ...r j,m >, some of which are \"proper\" relations, some are set relations. Because of the more general view of the graph, we lose information about individual paths (i.e. instances of a path in the original graph). Because of this, the paths we extract are hypothetical, but will have associated a confidence score based on the frequency of occurrence of relations in the original KG, and the strength of the connection of the range of one relation on the path with the domain of the next one. The weight of an abstract path \u03c0 j is computed as: In our experiments we used k = 5 where the weight w(r j,i ) of an individual relation is defined based on whether r i,j is a \"proper\" relation or a set relation as:",
  "y": "background differences"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_9",
  "x": "**GROUNDED PATHS** The abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple. They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_10",
  "x": "---------------------------------- **GROUNDED PATHS** The abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple. They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_11",
  "x": "They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_12",
  "x": "A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG.",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_13",
  "x": "where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_14",
  "x": "The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG. The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_15",
  "x": "A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG.",
  "y": "background uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_16",
  "x": "We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG. The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets. The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types. We will skip this graph variation in the rest of the experiments presented here.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_17",
  "x": "The abstract graph approach we present here does not fit well the combination of the knowledge base (Freebase or NELL) with unstructured SVO triples, because we rely on strongly typed relations to build node sets. The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types. We will skip this graph variation in the rest of the experiments presented here. Gardner et al. (2014) use these graphs to generate paths for augmenting the representation of node pairs, for link prediction, for a subset of 24 relation types from Freebase's inventory, and 10 relations from NELL. Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 . Negative sampling The number of negative instances used in<cite> (Gardner et al., 2014)</cite> is not clearly stated. Both the number and methods of generating the negative samples can impact the results (Kotnis and Nastase, 2018) . We use (up to) 200 negative samples for each positive pair: for a pair (s, t) in the provided training or test sets for each relation r, we make 100 negative samples by corrupting the source s, and 100 negative samples by corrupting the target t. The corrupted s and t are chosen from r's domain V r,s and range V r,t respectively, such that these corrupted triples are not part of the training, test or graph.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_18",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ). Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples). Freebase has 4200+ relation types, and NELL 500+. More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_19",
  "x": "If a relation instance does not have a grounding for an abstract path, the values of these features will be 0. ---------------------------------- **RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ). Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples). Freebase has 4200+ relation types, and NELL 500+. More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_20",
  "x": "More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration). It is not surprising that overall the results for NELL are higher -NELL has been designed on the principle of coupled learning, where connections between different relations are the basis of the resource and its continuous growth (Carlson et al., 2010b) . It also has more training data for each relation (see table in Section 4.1). There is no consistent trend -for some relations using the paths extracted with this approach leads to better results, for others it does not (although, as we frequently mentioned, the fact that we used different negative sampling methods, the results are not directly comparable). A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_21",
  "x": "More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration). It is not surprising that overall the results for NELL are higher -NELL has been designed on the principle of coupled learning, where connections between different relations are the basis of the resource and its continuous growth (Carlson et al., 2010b) . It also has more training data for each relation (see table in Section 4.1). There is no consistent trend -for some relations using the paths extracted with this approach leads to better results, for others it does not (although, as we frequently mentioned, the fact that we used different negative sampling methods, the results are not directly comparable). A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973).",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_22",
  "x": "stract paths. The overlap between the sets of paths discovered with the two methods is very small: for Freebase the average overlap with respect to PRA is around 0.004 (for the different graph configurations), and with respect to the abstract paths around 0.2; for NELL around 0.003 relative to PRA and 0.27 relative to the abstract paths. We note that overall, the system found more paths than what could be grounded for the given training instances for both Freebase and NELL. Another general observation is that relations for which we found the most patterns (AthletePlaysForTeam and StateHasLake for NELL, /medicine/disease/symptoms and /film/film/rating for Freebase) do not necessarily perform the best. NELL The results for each relation in terms of average precision are presented in Table 4 . We include the best result on PRA (on any variation of the graph), as reported by<cite> (Gardner et al., 2014)</cite> , although since we used different negative instances the results are not directly comparable. Several of the NELL target relations have interesting patterns in the abstract graph, in particular StadiumLocatedInCity, TeamPlaysInLeague. In several cases, the algorithm has discovered \"parallel\" relations. For the relation WriterWroteBook, the most useful feature is the relation AgentCreated, which connects many of the source-target pairs in the WriterWroteBook relation. We found a similar situation with the relation JournalistWritesForPublication, which has WorksFor paralleling it in the graph.",
  "y": "differences uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_0",
  "x": "Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001;<cite> Sudo et al., 2003</cite>; Yangarber, 2003) . In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_1",
  "x": "For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001;<cite> Sudo et al., 2003</cite>; Yangarber, 2003) . In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models. Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_2",
  "x": "In dependency analysis (Mel'\u010duk, 1987 ) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of node X. in the dependency tree shown in Figure 1 .",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_3",
  "x": "Finally, Section 6 discusses the conclusions which may be drawn from this work. ---------------------------------- **IE PATTERN MODELS** In dependency analysis (Mel'\u010duk, 1987 ) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the relation between the head and modifier (e.g. subject, object). An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> .",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_4",
  "x": "Linked Chains: The linked chains model represents extraction patterns as a pair of chains which share the same verb but no direct descendants. Example linked chains are shown in Figure 2 . This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 . Subtrees: The final model to be considered is the subtree model<cite> (Sudo et al., 2003)</cite> . In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another. Single nodes are not considered to be subtrees. The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree. Each of the previous models form a proper subset of the subtrees. By choosing an appropriate subtree it is possible to link together any pair of nodes in a tree and consequently this model can ----------------------------------",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_5",
  "x": "<cite>Sudo et al. (2003)</cite> compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not. They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model. However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events. Stevenson and Greenwood (2006) compared the four pattern models described in Section 2 in terms of their complexity and ability to represent relations found in text. The complexity of each model was analysed in terms of the number of patterns which would be generated from a given dependency parse. This is important since several of the algorithms which have been proposed to make use of dependency-based IE patterns use iterative learning (e.g. (Yangarber et al., 2000; Yangarber, 2003; ) and are unlikely to cope with very large sets of candidate patterns. The number of patterns generated therefore has an effect on how practical computations using that model may be. It was found that the number of patterns generated for the SVO model is a linear function of the size of the dependency tree. The number of chains and linked chains is a polynomial function while the number of subtrees is exponential.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_6",
  "x": "However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ---------------------------------- **EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task. In this context \"relevant\" means that the document contains the information we are interested in identifying. D and R are such that D = R \u222aR and R\u2229R = \u2205. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall. ---------------------------------- **RANKING PATTERNS**",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_7",
  "x": "(The documents used were taken from newswire texts and biomedical journal articles.) They found that the SVO and chain model could only represent a small proportion of the relations in the corpora. The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model. They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns. There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ---------------------------------- **EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task.",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_8",
  "x": "The score for each pattern, p, is given by: where tf p is the number of times pattern p appears in relevant documents, N is the total number of documents in the corpus and df p the number of documents in the collection containing the pattern p. Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus). Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred. <cite>Sudo et al. (2003)</cite> found that it was important to find the appropriate balance between these two factors. They introduced the \u03b2 parameter as a way of controlling the relative contribution of the inverse document frequency. \u03b2 is tuned for each extraction task and pattern model combination. Although simple, this approach has the advantage that it can be applied to each of the four pattern models to provide a direct comparison. ---------------------------------- **EXTRACTION SCENARIO**",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_9",
  "x": "We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since <cite>Sudo et al. (2003)</cite> showed that adequate knowledge about document relevance could be obtained automatically using an IR system. ---------------------------------- **PATTERN GENERATION** The texts used for these experiments were parsed using the Stanford dependency parser (Klein and Manning, 2002) . The dependency trees were processed to replace the names of entities belonging to specific semantic classes with a general token. Three of these classes were used for the management succession domain (PERSON, ORGANISA-TION and POST). For example, in the dependency analysis of \"Smith will became CEO next year\", \"Smith\" is replaced by PERSON and \"CEO\" by POST. This process allows more general patterns to be extracted from the dependency trees.",
  "y": "differences"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_10",
  "x": "At each stage the known trees are extended by the addition of a single node. In order to avoid duplication the extension is restricted to allowing nodes only to be added to the nodes on the rightmost path of the tree. Applying the process recursively creates a search space in which all subtrees are enumerated with minimal duplication. The rightmost extension algorithm is most suited to finding subtrees which occur multiple times and, even using this efficient approach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_11",
  "x": "For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. ---------------------------------- **PARAMETER TUNING** The value of \u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by <cite>Sudo et al. (2003)</cite> .",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_12",
  "x": "Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models. Previous analysis (see Section 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. ---------------------------------- **PARAMETER TUNING** The value of \u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by <cite>Sudo et al. (2003)</cite> . To generate this additional text we used the Reuters Corpus (Rose et al., 2002 ) which consists of a year's worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus. Unlike the MUC-6 corpus, items belonging to the required semantic classes are not annotated in the Reuters Corpus.",
  "y": "uses"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_0",
  "x": "**ABSTRACT** The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. While several previous works show that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly a Transformer model with more than 12 encoder/decoder layers fails to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation which changes the computation order of residual connection and layer normalization can effectively ease the optimization of deep Transformers. In addition, we deeply compare the subtle difference in computation order, and propose a parameter initialization method which simply puts Lipschitz restriction on the initialization of Transformers but can effectively ensure their convergence. We empirically show that with proper parameter initialization, deep Transformers with the original computation order can converge, which is quite in contrast to all previous works, and obtain significant improvements with up to 24 layers. Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders. ---------------------------------- **INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_1",
  "x": "In this paper, we first empirically demonstrate that a simple modification made in the official implementation which changes the computation order of residual connection and layer normalization can effectively ease the optimization of deep Transformers. In addition, we deeply compare the subtle difference in computation order, and propose a parameter initialization method which simply puts Lipschitz restriction on the initialization of Transformers but can effectively ensure their convergence. We empirically show that with proper parameter initialization, deep Transformers with the original computation order can converge, which is quite in contrast to all previous works, and obtain significant improvements with up to 24 layers. Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders. ---------------------------------- **INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently. The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; .",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_2",
  "x": "In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016) . However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers. show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which weighted combines outputs of all encoder layers as encoded representation. However, the TA mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back-propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep Transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers' outputs for each layer instead of at the end of encoding. Wu et al. (2019) research on incremental increasing the depth of the Transformer Big by freezing pre-trained shallow layers. In concurrent work, Zhang et al. (2019) also point out the same issue as in this work, but there are differences between. In contrast to all previous works, we empirically show that with proper parameter initialization, deep Transformers with the original computation order can converge. The contributions of our work are as follows: We empirically demonstrate that a simple modification made in the Transformer's official implementation which changes the computation order of residual connection and layer normalization can effectively ease its optimization; We deeply analyze how the subtle difference of computation order affects the convergence deep Transformer models, and propose to initialize deep Transformer models under Lipschitz restriction; Our simple approach effectively ensures the convergence of deep Transformers with up to 24 layers, and bring +1.50 and +0.92 BLEU improvements in the WMT 14 English to German task and the WMT 15 Czech to English task;",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_3",
  "x": "The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k. We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_4",
  "x": "**CONVERGENCE OF DIFFERENT COMPUTATION ORDER** In our research we focus on training problems of deep Transformers which prevent them from convergence (as opposed to other important issues such as over-fitting on the training set). To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016 ) are adopted. The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k. We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; .",
  "y": "extends differences"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_5",
  "x": "Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach. \u2020 and \u2021 mean p < 0.01 and p < 0.05 while comparing between v1 and v2 of the same number of layers in significance test. ---------------------------------- **ANALYSIS AND LIPSCHITZ RESTRICTED PARAMETER INITIALIZATION** Since the subtle change of computation order results in huge differences in convergence, we analyze the differences between the computation orders to figure out how they affect convergence. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_6",
  "x": "We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach. \u2020 and \u2021 mean p < 0.01 and p < 0.05 while comparing between v1 and v2 of the same number of layers in significance test. ---------------------------------- **ANALYSIS AND LIPSCHITZ RESTRICTED PARAMETER INITIALIZATION** Since the subtle change of computation order results in huge differences in convergence, we analyze the differences between the computation orders to figure out how they affect convergence.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_7",
  "x": "**EFFECTS OF DEEPER ENCODER AND DEEPER DECODER** Previous approaches only increases the depth of encoder, while we suggest that deep decoders should also be helpful. We analyzed the influence of deep encoders and decoders separately and results are shown in Table 4 . Table 4 shows that the deep decoder can benefit the performance in addition to the deep encoder, especially on the Czech to English task. ---------------------------------- **CONCLUSION** In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_8",
  "x": "Previous approaches only increases the depth of encoder, while we suggest that deep decoders should also be helpful. We analyzed the influence of deep encoders and decoders separately and results are shown in Table 4 . Table 4 shows that the deep decoder can benefit the performance in addition to the deep encoder, especially on the Czech to English task. ---------------------------------- **CONCLUSION** In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization. Our experiments demonstrate the effectiveness of our simple approach on the convergence of deep Transformers, and brings significant improvements on the WMT 14 English to German and the WMT 15 Czech to English news translation tasks.",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_0",
  "x": "**ABSTRACT** ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the<cite> Weeds et al. (2014)</cite> datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015) . The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_1",
  "x": "Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9. On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_2",
  "x": "**INTRODUCTION** Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_3",
  "x": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9. On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_4",
  "x": "Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) . The ablation test has shown that four out of thirteen features were actually not contributing to the system's performance, and they were therefore removed, turning ROOT13 into ROOT9. On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets.",
  "y": "background extends"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_5",
  "x": "In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) . The test consisted in providing to the trained model switched hypernyms (e.g. from \"dog HYPER animal\" to \"dog RANDOM fruit\"), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training. ---------------------------------- **RELATED WORK** Since the pioneering work of Hearst (1992) , who used a pattern based approach for the \"automatic acquisition of hyponyms from large text corpora\", a large number of distributional methods were applied to the identification of hypernyms.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_6",
  "x": "Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) . The test consisted in providing to the trained model switched hypernyms (e.g. from \"dog HYPER animal\" to \"dog RANDOM fruit\"), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training. ---------------------------------- **RELATED WORK** Since the pioneering work of Hearst (1992) , who used a pattern based approach for the \"automatic acquisition of hyponyms from large text corpora\", a large number of distributional methods were applied to the identification of hypernyms. These methods relied on interpretations of the Distributional Hypothesis (Harris, 1954) , according to which the meaning of a linguistic expression can be inferred from its distribution in text corpora, so that linguistic expressions occurring in similar contexts are likely to be similar.",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_7",
  "x": "Lenci and Benotto (2012) adapted this measure to check not only to which extent the features of the narrower term are included in the features of the broader, but also how the features of the broader are not included in the features of the narrower. Kotlerman et al. (2010) combined Average Precision (AP) with the balancing approach of Szpektor and Dagan (2008) , outperforming the above mentioned methods. Herbelot and Ganesalingam (2013) measured the Kullback-Leibler (KL) divergence between the probability distribution over context words for a term, and the background probability distribution, based on the idea that the smaller such KL was, the less informative the word was (and therefore more likely to be a hypernym). Rimmel (2014) considered the top features in a context vector as topics and used a Topic Coherence (TC) measure. Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts. In their evaluation, the authors have shown that hypernyms' most typical contexts are in fact less informative than hyponyms' ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation (after having tried also subtraction and division) of the vectors. Roller et al. (2014) used the vectors' difference, while<cite> Weeds et al. (2014)</cite> implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_8",
  "x": "---------------------------------- **EVALUATION** ---------------------------------- **TASKS** We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and -in particularagainst the best performant models in<cite> Weeds et al. (2014)</cite> ; iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt. For what concerns the ablation test, we performed it on a tree-classes classification task (hypernyms, co-hyponyms and randoms), removing each feature at a time and measuring the loss/gain (F1 score is used for the evaluation on a 10-fold cross validation). Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_9",
  "x": "For what concerns the ablation test, we performed it on a tree-classes classification task (hypernyms, co-hyponyms and randoms), removing each feature at a time and measuring the loss/gain (F1 score is used for the evaluation on a 10-fold cross validation). Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> . These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_10",
  "x": "Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> . These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_11",
  "x": "Considering instead only the second word, we have 3,665 terms (1,945 nouns, 860 verbs and 862 adjectives). In the third task, we have extended this dataset randomly switching the 3,200 hypernymy pairs (e.g. from \"car HYPER vehicle\" to \"car RANDOM mammal\") to verify whether ROOT9 was able to classify them as randoms. ---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation). In this dataset, ontological information cannot be learnt and re-used, and indeed the random vectors cannot perform well.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_12",
  "x": "---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation). In this dataset, ontological information cannot be learnt and re-used, and indeed the random vectors cannot perform well. Unfortunately our DSM did not cover the whole datasets, because of the chosen frequency threshold (in Table 1 , we report the size of our subsets in comparison to the original datasets). However,<cite> Weeds et al. (2014)</cite>",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_13",
  "x": "Considering instead only the second word, we have 3,665 terms (1,945 nouns, 860 verbs and 862 adjectives). In the third task, we have extended this dataset randomly switching the 3,200 hypernymy pairs (e.g. from \"car HYPER vehicle\" to \"car RANDOM mammal\") to verify whether ROOT9 was able to classify them as randoms. ---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation). In this dataset, ontological information cannot be learnt and re-used, and indeed the random vectors cannot perform well.",
  "y": "background uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_15",
  "x": "---------------------------------- **BASELINES AND OTHER MODELS** For our internal tests, we have implemented two baselines, which can be used as reference for evaluating the performance of ROOT9: COSINE and RANDOM13. The first baseline simply uses the vector cosine (COSINE) with a Random Forest classifier in the default settings (i.e. 100 trees, 1 seed, and maxDepth and numFeatures initialized to 0). This baseline is supposed to perform particularly well in discriminating similar words (i.e. hypernyms and co-hyponyms) from randoms. In fact, this measure has been extensively used to identify word similarity in vector spaces (Turney and Pantel, 2010) because it verifies the normalized correlation between the vectors of 1 and 2 : where is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_16",
  "x": "This baseline is supposed to perform particularly well in discriminating similar words (i.e. hypernyms and co-hyponyms) from randoms. In fact, this measure has been extensively used to identify word similarity in vector spaces (Turney and Pantel, 2010) because it verifies the normalized correlation between the vectors of 1 and 2 : where is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech. Also, the performance of three main unsupervised methods is reported as a reference: cosine (see above in this section), balAPinc (Kotlerman et al., 2010) and invCL (Lenci and Benotto, 2012) . A threshold p empirically found in a training set was used in these methods for the decision, Table 2 . Ablation test, F1 scores on a 10-fold cross validation and loss/gain values.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_17",
  "x": "In a similar way, Diff Entr and Diff Freq can be seen as redundant in respect to the features Entr1,2 and Freq1,2. Perhaps surprisingly, Cooc does not contribute to the final score, and instead penalizes it. Removing the four redundant features (we removed Shared but kept APSyn), ROOT13 turns into ROOT9. This system outperforms all the baselines (COSINE, RANDOM13) and ROOT13. For the sake of completeness, in Table 2 we also report the performance of ROOT9 using Logistic Regression (Cessie, 1992) and SMO (Keerthi et al., 2001) classifiers. As it can be seen, the Random Forest version largely outperforms the other classifiers in this dataset. However, it is worth noticing here that such difference disappears with the WN datasets proposed by<cite> Weeds et al. (2014)</cite> . See section 6, and -in particular - Table 3 . F1 scores on a 10-fold cross validation for binary classification tasks. Scores are in percent.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_18",
  "x": "---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms. The SVM on the difference (svmDIFF) and on the second PPMI vector (svmSINGLE) is instead particularly good at identifying hypernyms, while it performs bad at identifying co-hyponyms. Among the unsupervised methods, we report the results for the cosine and the methods of Lenci and Benotto (2012; invCL) and Kotlerman et al. (2010; balAPinc Table 4 . F1 scores, in percent, on a 10-fold cross validation (state of the art models are evaluated on a 5-fold cross validation). {bold= best results vs. ROOT9; italics = other classifiers}.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_19",
  "x": "Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms. The SVM on the difference (svmDIFF) and on the second PPMI vector (svmSINGLE) is instead particularly good at identifying hypernyms, while it performs bad at identifying co-hyponyms. Among the unsupervised methods, we report the results for the cosine and the methods of Lenci and Benotto (2012; invCL) and Kotlerman et al. (2010; balAPinc Table 4 .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_20",
  "x": "Scores are in percent. Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms. The SVM on the difference (svmDIFF) and on the second PPMI vector (svmSINGLE) is instead particularly good at identifying hypernyms, while it performs bad at identifying co-hyponyms.",
  "y": "differences background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_21",
  "x": [
   "Finally, we have tried to test Levy et al. (2015) 's claim by evaluating the classifier on a dataset containing 3,200 hypernyms and 3,200 switched hypernyms (e.g. apple RANDOM animal and dog RANDOM fruit). In this evaluation, we have noticed that a large number of the switched hypernyms were indeed misclassified as hypernyms (up to 100% of them, if the words in the testing switched pairs were exactly the same used as hypernyms in the training set). In the attempt of correcting the behavior of the classifier, we extended the original 9,600 pairs dataset with other 3,200 switched hypernyms pairs labeled as randoms. It is relevant to notice that the switched hypernyms (tagged as randoms) contain the same words used in for the real hypernyms, and that in this new dataset, the size of the random class is double the others, including a total of 6,400 pairs. The new 10-fold cross validation test on the three classes registered a significant loss, passing from 90.7% to 84%. However, only 576 out of 6,400 randoms (most of which are likely to be the switched pairs) were misclassified as hypernyms. ---------------------------------- **CONCLUSIONS** In this paper, we have described ROOT9, a classifier for hypernyms, co-hyponyms and random words that is derived from an optimization of ROOT13 (Santus et al., 2016b) . The classifier, based on the Random Forest algorithm, uses only nine unsupervised corpus-based features, which have been described, and their contribution assessed."
  ],
  "y": "uses future_work"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_0",
  "x": "This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory<cite> (Wachsmuth et al., 2017a)</cite> and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a) . In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). We find that assessments of overall argumentation quality largely match in theory and practice. Nearly all phrased reasons are adequately represented in theory. However, some theoretical quality dimensions seem hard to separate in practice. Most importantly, we provide evidence that the observed relative quality differences are reflected in absolute quality ratings. Still, our study underpins the fact that the theory-based argumentation quality assessment remains complex. Our results do not generally answer the question of what view of argumentation quality is preferable, but they clarify where theory can learn from practice and vice versa. In particular, practical approaches indicate what to focus on to simplify theory, whereas theory seems beneficial to guide quality assessment in practice.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_2",
  "x": "Strength may mean cogency but also rhetorical effectiveness (Perelman and Olbrechts-Tyteca, 1969) . Rhetoric has been studied since Aristotle (2007) who developed the notion of the means of persuasion (logos, ethos, pathos) and their linguistic delivery in terms of arrangement and style. Dialectical quality dimensions resemble those of cogency, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004) . <cite>Wachsmuth et al. (2017a)</cite> point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. 5-1 B is attacking / abusive. 5-2 B has language/grammar issues, or uses humour or sarcasm. 5-3 B is unclear / hard to follow. 6-1 B has no credible evidence / no facts. 6-2 B has less or insufficient reasoning.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_3",
  "x": "Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension. by crowd workers (UKPConvArg2). These pairs represent the practical view in our experiments. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_4",
  "x": "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension. by crowd workers (UKPConvArg2). These pairs represent the practical view in our experiments. ---------------------------------- **MATCHING THEORY AND PRACTICE** We now report on experiments that we performed to examine to what extent the theory and practice of argumentation quality assessment match. 1",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_5",
  "x": "---------------------------------- **CORRELATIONS OF DIMENSIONS AND REASONS** For Hypotheses 1 and 2, we consider all 736 pairs of arguments from Habernal and Gurevych (2016a) where both have been annotated by <cite>Wachsmuth et al. (2017a)</cite> . For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie. This way, we can correlate each dimension with all reason labels in Table 2 including Conv. In particular, we compute Kendall's \u03c4 based on all argument pairs given for each label. 2  Table 3 presents all \u03c4 -values. The phrasing of a reason can be assumed to indicate a clear quality difference-this is underlined by the generally high correlations. Analyzing the single values, we find much evidence for Hypothesis 1: Most notably, label 5-1 perfectly correlates with global acceptability, fitting the intuition that abuse is not acceptable. The high \u03c4 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_6",
  "x": "For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie. This way, we can correlate each dimension with all reason labels in Table 2 including Conv. In particular, we compute Kendall's \u03c4 based on all argument pairs given for each label. 2  Table 3 presents all \u03c4 -values. The phrasing of a reason can be assumed to indicate a clear quality difference-this is underlined by the generally high correlations. Analyzing the single values, we find much evidence for Hypothesis 1: Most notably, label 5-1 perfectly correlates with global acceptability, fitting the intuition that abuse is not acceptable. The high \u03c4 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense. Only the comparably low \u03c4 of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected. Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from <cite>Wachsmuth et al. (2017a)</cite> given for each reason label (Habernal and Gurevych, 2016a) . The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_7",
  "x": "Thus, argumentation quality assessment seems to match in theory and practice to a broad extent. ---------------------------------- **ABSOLUTE RATINGS FOR RELATIVE DIFFERENCES** The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from <cite>Wachsmuth et al. (2017a)</cite> with a particular reason label from Habernal and Gurevych (2016a) . As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings. Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4). For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3. 3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from Habernal and Gurevych (2016a) both arguments are strong or weak respectively. high values.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_8",
  "x": "3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from Habernal and Gurevych (2016a) both arguments are strong or weak respectively. high values. Vice versa, especially 8-5 (more credible) and 9-4 (well thought through) are reflected in high ratings, whereas 9-2 (sticks to topic) does not have much positive impact. ---------------------------------- **ANNOTATING THEORY IN PRACTICE** The results of Section 3 suggest that theory may guide the assessment of argumentation quality in practice. In this section, we evaluate the reliability of a crowd-based annotation process. ---------------------------------- **ABSOLUTE QUALITY RATINGS BY THE CROWD** We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_9",
  "x": "---------------------------------- **ABSOLUTE QUALITY RATINGS BY THE CROWD** We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment. In particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 (or choose \"cannot judge\"). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. ---------------------------------- **AGREEMENT OF THE CROWD WITH EXPERTS** First, we checked to what extent lay annotators and experts agree in terms of Krippendorff's \u03b1. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of <cite>Wachsmuth et al. (2017a)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_0",
  "x": "Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990) ). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance <cite>(Downey et al., 2007)</cite> . Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be-to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate. In a large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques. The experiments show that accuracy over unlabeled data can be used for selecting among SLMs-for an ADS approach utilizing Hidden Markov Models, this results in an average error reduction of 26% over previous results in extraction and type-checking tasks. ---------------------------------- **EXTRACTION ASSESSMENT WITH LANGUAGE MODELS** We begin by formally defining the extraction and typechecking tasks we consider, then discuss statistical language models and their utilization for extraction assessment. The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances S R , and a list of candidate extractions U R , the task is to order elements of U R such that correct instances for R are ranked above extraction errors.",
  "y": "differences"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_1",
  "x": "---------------------------------- **EXTRACTION ASSESSMENT WITH LANGUAGE MODELS** We begin by formally defining the extraction and typechecking tasks we consider, then discuss statistical language models and their utilization for extraction assessment. The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances S R , and a list of candidate extractions U R , the task is to order elements of U R such that correct instances for R are ranked above extraction errors. Let U Ri denote the set of the ith arguments of the extractions in U R , and let S Ri be defined similarly for the seed set S R . For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction <cite>(Downey et al., 2007)</cite> . The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors. As an example, the extraction Founded(Bill Gates, Oracle) is type correct, but is not correct for the extraction task. ---------------------------------- **STATISTICAL LANGUAGE MODELS**",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_2",
  "x": "We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990) . Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS <cite>(Downey et al., 2007)</cite> . An HMM models a sentence w as a sequence of observations w i each generated by a hidden state variable t i . Here, hidden states take values from {1, . . . , T }, and each hidden state variable is itself generated by some number k of previous hidden states. Formally, the joint distribution of a word sequence w given a corresponding state sequence t is: The distributions on the right side of Equation 1 are learned from the corpus in an unsupervised manner using Expectation-Maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989) . ---------------------------------- **PERFORMING ADS WITH SLMS** The Assessment by Distributional Similarity (ADS) technique is to rank extractions in U R in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs <cite>(Downey et al., 2007)</cite> and adapt it to also apply to n-gram models, as detailed below.",
  "y": "background"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_3",
  "x": "The distributions on the right side of Equation 1 are learned from the corpus in an unsupervised manner using Expectation-Maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989) . ---------------------------------- **PERFORMING ADS WITH SLMS** The Assessment by Distributional Similarity (ADS) technique is to rank extractions in U R in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs <cite>(Downey et al., 2007)</cite> and adapt it to also apply to n-gram models, as detailed below. Define a context of an extraction argument e i to be a string containing the m words preceding and m words following an occurrence of e i in the corpus. Let C i = {c 1 , c 2 , ..., c |C i | } be the union of all contexts of extraction arguments e i and seed arguments s i for a given relation R. We create a probabilistic context vector for each extraction e i where the j-th dimension of the vector is the probability of the context surrounding given the extraction, P (c j |e i ), computed from the language model. 1 We rank the extractions in U R according to how similar their arguments' contextual distributions, P (c|e i ), are to those of the seed arguments. Specifically, extractions are ranked according to: where KL represents KL Divergence, and the outer sum is taken over arguments e i of the extraction e. For HMMs, we alternatively rank extractions using the HMM state distributions P (t|e i ) in place of the probabilistic context vectors P (c|e i ).",
  "y": "extends"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_4",
  "x": "We evaluated performance on three distinct data sets. The first two data sets evaluate ADS for unsupervised information extraction, and were taken from <cite>(Downey et al., 2007)</cite> . The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged). The 10 most frequent extractions served as bootstrapped seeds. The two test sets contained 361 and 265 extractions, respectively. The third data set, Wikipedia, evaluates ADS on weaklysupervised extraction, using seeds and extractions taken from Wikipedia 'List of' pages (Pantel et al., 2009) . Seed sets of various sizes (5, 10, 15 and 20) were randomly selected from each list, and we present results averaged over 10 random samplings. Other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors. The data set included 2264 extractions across 36 unary relations, including Composers and US Internet Companies. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_5",
  "x": "---------------------------------- **MODEL SELECTION** Different language models can be configured in different ways: for example, HMMs require choices for the hyperparameters k and T . Here, we show that SLM perplexity can be used to select a high-quality model configuration for ADS using only unlabeled data. We evaluate on the Unary and Binary data sets, since they have been employed in previous work on our corpora. Figure 2 shows that for HMMs, ADS performance increases as perplexity decreases across various model configurations (a similar relationship holds for n-gram models). A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results. As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in <cite>(Downey et al., 2007)</cite> by 26%, on average. The experiments also reveal an important difference between the HMM and n-gram approaches. While KN3 is more accurate in SLM than our HMM models, it performs worse in ADS on average.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_0",
  "x": "However, na\u00efvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_1",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_2",
  "x": "In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_3",
  "x": "In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_4",
  "x": "We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_5",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_6",
  "x": "Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) .",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_7",
  "x": "The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_8",
  "x": "**RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_10",
  "x": "**PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION** (PRA-ODA) Training: Let s and t be any two KB entities and let s (n) and t (n) be their corresponding noun phrase representations or aliases. We search for bridging entities x 1 , x 2 , ..x n by performing limited depth first search (DFS) starting with s n such that we obtain a path s \u2212\u2192 t, where v i are verbs present in the corpus graph. This is done for all n \u2264 d max \u2212 1, where d max is the maximum depth of DFS.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_11",
  "x": "Let F = {\u03c0 1 , \u03c0 2 , ..., \u03c0 k } be the set of all path types. For predicting the existence of relation r between entities s and t, the logistic regression classifier outputs a score which is a measure of the confidence that r exists between s and t. It does so by first assigning weights to the features in the training phase. The score is given by where \u03b8 r \u03c0 is the weight learned by the logistic regression classifier during training specially for relation r and path type \u03c0. During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. ---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_12",
  "x": "During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. ---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION** (PRA-ODA) Training: Let s and t be any two KB entities and let s (n) and t (n) be their corresponding noun phrase representations or aliases. We search for bridging entities x 1 , x 2 , ..x n by performing limited depth first search (DFS) starting with s n such that we obtain a path s",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_13",
  "x": "The negative dataset is generated using the closed world assumption by performing a random walk. After augmenting the KB, we run the training phase of the PRA algorithm to obtain the feature (path) weights computed by the logistic regression Table 2 : Comparison of Mean Reciprocal Rank (MRR) metric for 10 relations from NELL (higher is better). PRA-SVO, <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Improvements in PRA-ODA over PRA-SVO is statistically significant with p < 0.007, with PRA-SVO as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time. We use all the entities included in the range of the relation being predicted as candidate target entities. For example, if the relation is riverFlowsThroughCity, the candidate target set would include entities in the KB that are cities. The DFS is now performed starting from source entities as during training, but this time only restricting to paths with positive weights learned during training.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_14",
  "x": "Any path (along with bridging entities) found during this search are added to the KB, and the PRA algorithm is now run over this augmented graph. ---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_15",
  "x": "For example, if the relation is riverFlowsThroughCity, the candidate target set would include entities in the KB that are cities. The DFS is now performed starting from source entities as during training, but this time only restricting to paths with positive weights learned during training. Any path (along with bridging entities) found during this search are added to the KB, and the PRA algorithm is now run over this augmented graph. ---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_16",
  "x": "We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_17",
  "x": "Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_18",
  "x": "Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_19",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_20",
  "x": "Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_21",
  "x": "and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_22",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "similarities"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_23",
  "x": "The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ---------------------------------- **CONCLUSION** In this paper, we investigated the usefulness of adding paths to a Knowledge Base for improving its connectivity by mining bridging entities from an external corpus.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_24",
  "x": "In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ---------------------------------- **CONCLUSION** In this paper, we investigated the usefulness of adding paths to a Knowledge Base for improving its connectivity by mining bridging entities from an external corpus. While previous KB augmentation methods focused only on augmentation using mined surface verbs while keeping the node set fixed, we extended these approaches by also adding bridging entities in an online fashion. We used a large corpus of 500 million web text corpus to mine these additional edges and bridging entities.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_25",
  "x": "In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ---------------------------------- **CONCLUSION** In this paper, we investigated the usefulness of adding paths to a Knowledge Base for improving its connectivity by mining bridging entities from an external corpus. While previous KB augmentation methods focused only on augmentation using mined surface verbs while keeping the node set fixed, we extended these approaches by also adding bridging entities in an online fashion. We used a large corpus of 500 million web text corpus to mine these additional edges and bridging entities.",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_0",
  "x": "We apply transfer learning and make a LSTM based model for hate speech classification. This model surpasses the performance shown by the current best models to establish itself as the state-of-the-art in the unexplored domain of Hinglish offensive text classification. We also release our model and the embeddings trained for research purposes. ---------------------------------- **INTRODUCTION** With the penetration of internet among masses, the content being posted on social media channels has uptaken. Specifically, in the Indian subcontinent, number of Internet users has crossed 500 mi 1 , and is rising rapidly due to inexpensive data 2 . With this rise, comes the problem of hate speech, offensive and abusive posts on social media. Although there are many previous works which deal with Hindi and English hate speech (the top two languages in India), but very few on the code-switched version (Hinglish) of the two (<cite>Mathur et al. 2018</cite>) . This is partially due to the following reasons: (i) Hinglish consists of no-fixed grammar and vocabulary.",
  "y": "motivation"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_1",
  "x": "Moreover, the word yatra here, can have phonetic variations, which would result in multiple spellings of the word as yatra, yaatra, yaatraa, etc. Also, the problem of hate speech has been rising in India, and according to the policies of the government and the various social networks, one is not allowed to misuse his right to speech to abuse some other community or religion. Due to the various difficulties associated with the Hinglish language, it is challenging to automatically detect and ban such kind of speech. Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (<cite>Mathur et al. 2018</cite>) . We also release the embeddings and the model trained. ---------------------------------- **METHODOLOGY** Our methodology primarily consists of these steps: Preprocessing of the dataset, training of word embeddings, training of the classifier model and then using that on HEOT dataset. ---------------------------------- **PRE-PROCESSING**",
  "y": "differences"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_2",
  "x": "Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (<cite>Mathur et al. 2018</cite>) . We also release the embeddings and the model trained. ---------------------------------- **METHODOLOGY** Our methodology primarily consists of these steps: Preprocessing of the dataset, training of word embeddings, training of the classifier model and then using that on HEOT dataset. ---------------------------------- **PRE-PROCESSING** In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (<cite>Mathur et al. 2018</cite>) . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary 3 and translation of each word to English using Hindi to English dictionary 4 .",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_3",
  "x": "An overview of the model is given in the Figure 1 . The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. Categorical crossentropy loss was used for the last layer due to the presence of multiple classes. We use Adam optimizer along with L2 regularisation to prevent overfitting. As indicated by the Figure 1 , the model was initially trained on the dataset provided by (Davidson et al. 2017) , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search. Results Table 3 shows the performance of our model (after getting trained on (Davidson et al. 2017) ) with two types of embeddings in comparison to the models by (<cite>Mathur et al. 2018</cite>) and (Davidson et al. 2017 ) on the HEOT dataset averaged over three runs. We also compare results on pre-trained embeddings. As shown in the table, our model when given Glove embeddings performs better than all other models.",
  "y": "differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_0",
  "x": "While plenty of works exist for sentiment analysis for different languages including analysis of social media data for sentiment characteristics (Al Sallab et al., 2015; Baly et al., , 2017b , few works focused on emotion recognition from text. Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons. For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective. In this paper, we focus on expanding coverage of existing emotion lexicon, namely DepecheMood, using the synonymy semantic relation available in English WordNet. We decide to expand DepecheMood since it is one of the largest emotion lexicon publicly available, and since its terms are aligned with EWN, thus allowing us to benefit from powerful semantic relations in EWN. The paper is organized as follows. In section 2, we conduct a brief literature survey on existing emotion lexicons. In section 3, we describe the expansion approach to build EmoWordNet.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_1",
  "x": "Emotion recognition models have been extensively explored based on different modalities such as human computer interaction (Cowie et al., 2001; Pantic and Rothkrantz, 2003; Fragopanagos and Taylor, 2005; Jaimes and Sebe, 2007; Hibbeln et al., 2017; Patwardhan and Knapp, 2017; Constantine et al., 2016) and facial images and expressions (Goldman and Sripada, 2005; Gunes and Piccardi, 2007; Trad et al., 2012; Wegrzyn et al., 2017) . Recently, special attention has been given to emotion recognition from text (Wu et al., 2006; Alm et al., 2005; Shaheen et al., 2014; AbdulMageed and Ungar, 2017; Badaro et al., 2018b,a) . In fact, a tremendous amount of opinionated and emotionally charged text data is nowadays available on the Internet due to the increase of number of users of social networks such as Twitter and Facebook. For instance, Facebook reached more than 2 billion users on September 2017. 1 Recognizing emotions from text has several applications: first, it helps companies and businesses in shaping their marketing strategies based on consumers' emotions (Bougie et al., 2003) ; second, it allows improving typical collaborative filtering based recommender systems (Badaro et al., 2013 (Badaro et al., , 2014c ) in terms of products or advertisements recommendations (Mohammad and Yang, 2011) ; third, politicians can learn how to adapt their political speech based on people emotions (Pang et al., 2008) and last but not least emotion classification helps in stock market predictions (Bollen et al., 2011) . While plenty of works exist for sentiment analysis for different languages including analysis of social media data for sentiment characteristics (Al Sallab et al., 2015; Baly et al., , 2017b , few works focused on emotion recognition from text. Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons. For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_2",
  "x": "EmoLex has around 10K terms annotated for emotions as well as for sentiment polarities. They evaluated the annotation quality using different techniques such as computing inter-annotator agreement and comparing a subsample of EmoLex with existing gold data. AffectNet (Cambria et al., 2012) , part of the SenticNet project, includes also around 10K terms extracted from ConceptNet (Liu and Singh, 2004) and aligned with WordNet Affect. They extended WordNet Affect using the concepts in ConceptNet. While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood<cite> (Staiano and Guerini, 2014)</cite> include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. Affect database extends SentiFul and covers around 2.5K words presented in their lemma form along with the corresponding part of speech (POS) tag. DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions. <cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com. The articles are accompanied by Rappler's Mood Meter, which allows readers to express their emotions about the article they are reading. DepecheMood includes around 37K lemmas along with their part of speech tags and the lemmas are aligned with EWN.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_3",
  "x": "DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions. <cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com. The articles are accompanied by Rappler's Mood Meter, which allows readers to express their emotions about the article they are reading. DepecheMood includes around 37K lemmas along with their part of speech tags and the lemmas are aligned with EWN. Staiano and Guerini also evaluated DepecheMood in emotion regression and classification tasks in unsupervised settings. They claim that although they utilized a na\u00efve unsupervised model, they were able to outperform existing lexicons when applied on SemEval 2007 dataset (Strapparava and Mihalcea, 2007) . Since DepecheMood is aligned with EWN, is publicly available and has a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using EWN semantic relations as described below in section 3. ---------------------------------- **LITERATURE REVIEW** To summarize, there are mainly two approaches that have been followed for building emotion lexicons for English.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_4",
  "x": "On the other hand, the second approach is cheap and results in large scale emotion lexicons but with lower accuracy compared to manually developed emotion lexicons in terms of accurately representing the emotion of the term. ---------------------------------- **EMOWORDNET** In this section, we describe the approach we followed in order to expand DepecheMood and build EmoWordNet. DepecheMood consists of 37,771 lemmas along with their corresponding POS tags where each entry is appended with scores for 8 emotion labels: afraid, amused, angry, annoyed, don't care, happy, inspired and sad. Three variations of score representations exist for DepecheMood. We select to expand the DepecheMood variation with normalized scores since this variation performed best according to the presented results in<cite> (Staiano and Guerini, 2014)</cite> . In Fig. 1 , we show an overview of the steps followed to expand DepecheMood. Step 1: EWN synsets that include lemmas of DepecheMood were retrieved. A score was then computed for each retrieved synset, s. Let S denotes the set of all such synsets.",
  "y": "differences extends"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_5",
  "x": "We evaluate regression as well as classification of emotions in unsupervised settings using similar techniques used for evaluating DepecheMood. ---------------------------------- **DATASET & COVERAGE** We utilized the dataset provided publicly by SemEval 2007 task on Affective text (Strapparava and Mihalcea, 2007) . The dataset consists of one thousand news headlines annotated with six emotion scores: anger, disgust, fear, joy, sadness and surprise. For the regression task, a score between 0 and 1 is provided for each emotion. For the classification task, a threshold is applied on the emotion scores to get a binary representation of the emotions: if the score of a certain emotion is greater than 0.5, the corresponding emotion label is set to 1, otherwise it is 0. The emotion labels used in the dataset correspond to the six emotions of the Ekman model (Ekman, 1992) while those in EmoWordNet, as well as DepecheMood, follow the ones provided by Rappler Mood Meter. We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_6",
  "x": "Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> . One important aspect of the extrinsic evaluation was checking the coverage of EmoWordNet against SemEval dataset. In order to compute coverage, we performed lemmatization of the news headlines using WordNet lemmatizer available through Python NLTK package. We excluded all words with POS tags different than noun, verb, adjective and adverb. EmoWordNet achieved a coverage of 68.6% while DepecheMood had a coverage of 67.1%. An increase in coverage was expected but since the size of the dataset is relatively small, the increase was only around 1.5%. In terms of headline coverage, only one headline (\"Toshiba Portege R400\") was left without any emotion scores when using both EmoWordNet and DepecheMood since none of its terms were found in any of the two lexicons. ---------------------------------- **REGRESSION AND CLASSIFICATION RESULTS** We followed an approach similar to the one presented for evaluating DepecheMood.",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_7",
  "x": "As stated in<cite> (Staiano and Guerini, 2014)</cite> paper, 'Disgust' emotion was excluded since there was no corresponding mapping in EmoWordNet/DepecheMood. The first evaluation consisted of measuring Pearson Correlation between the scores computed using the lexicons and those provided in SemEval. The results are reported in Table 1. We could see that the results are relatively close to each other: EmoWordNet slightly outperformed DepecheMood for the five different emotions. It was expected to have close results given that the coverage of EmoWordNet is very close to DepecheMood. Given the slight improvement, we expect EmoWordNet to perform much better on larger datasets. For the classification task, we first transformed the numerical emotion scores of the headlines to a binary representation. We applied min-max normalization on the computed emotion scores per headline, and then assigned a '1' for the emotion label with score greater than '0.5', and a '0' otherwise. We used F1 measure for evaluation. Results are shown in Table 2 .",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_0",
  "x": "**INTRODUCTION** For the last decade, fast, accurate and wide-coverage parsing for real-world text has been pursued in sophisticated grammar formalisms, such as headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) , combinatory categorial grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982) . They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Malouf and van Noord, 2004; Kaplan et al., 2004; . Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; . An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_1",
  "x": "An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. <cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) . However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> . In the case of supertagging of Weighted CDG , parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_2",
  "x": "However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> . In the case of supertagging of Weighted CDG , parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG. The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model. We compared our model with the probabilistic model for phrase structures . This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging. Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in <cite>(Ninomiya et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_4",
  "x": "This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging. Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in <cite>(Ninomiya et al., 2006)</cite> . The probabilities of <cite>their model</cite> are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but <cite>their model</cite> was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions. ---------------------------------- **HPSG AND PROBABILISTIC MODELS** HPSG (Pollard and Sag, 1994 ) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_8",
  "x": "Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures.",
  "y": "uses"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_9",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_10",
  "x": "combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_11",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_12",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_13",
  "x": "This is an extension of 's model by replacing the unigram reference distribution with an n-gram reference distribution. Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_14",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_15",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_16",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_17",
  "x": "Our models increased the parsing accuracy. 'our model 1' was around 2.6 times faster and had around 2.65 points higher F-score than 's model. 'our model 2' was around 2.3 times slower but had around 2.9 points higher F-score than 's model. We must admit that the difference between our models and <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite> was not as great as the difference from 's model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007) 's model. Matsuzaki et al. (2007) ---------------------------------- **PRO-** The terms \u03ba and \u03b4 are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_18",
  "x": "The parameters increase for each iteration by the terms prefixed by \u2206, and parsing finishes when the parameters reach the terms with suffixes last. Details of the parameters are written in . The beam thresholding parameters for 'our model 2' are \u03b1 0 = 18, \u2206\u03b1 = 6, \u03b1 last = 42, \u03b2 0 = 9.0, \u2206\u03b2 = 3.0, \u03b2 last = 21.0, \u03b4 0 = 18, \u2206\u03b4 = 6, \u03b4 last = 42, \u03ba 0 = 9.0, \u2206\u03ba = 3.0, \u03ba last = 21.0. In 'our model 2', the global thresholding was not used. posed a technique for efficient HPSG parsing with supertagging and CFG filtering. Their results with the same grammar and servers are also listed in the lower half of Table 4 . They achieved drastic improvement in efficiency. Their parser ran around 6 times faster than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.' Instead, our models achieved better accuracy. 'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score. Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_19",
  "x": "The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module. Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser. On the other hand, <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite> uses the supertagger as an external module. Once the parser acquires the supertagger's outputs, the n-gram information never goes in and out of the kernel. This advantage of <cite>Ninomiya et al. (2006)</cite><cite>'s model</cite> can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser. We estimate that the overhead of the interface is around from 50 to 80 ms/sentence. We think that re-implementation of the parser will improve the parsing speed as estimated. In Figure 3 , the line of our model crosses the line of <cite>Ninomiya et al. (2006)</cite> <cite>'s model</cite>. If the estimation is correct, our model will be faster and more accurate so that the lines in the figure do not cross. Speed-up in our model is left as a future work.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_23",
  "x": "Speed-up in our model is left as a future work. ---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_24",
  "x": "Speed-up in our model is left as a future work. ---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_25",
  "x": "Speed-up in our model is left as a future work. ---------------------------------- **CONCLUSION** We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_0",
  "x": "These features are often specific to each entity mention and candidate entity, covering a wide range of linguistic and/or structured representations such as lexical and part-of-speech tags of context words, dependency paths, topical features, KB infoboxes (Bunescu and Pasca, 2006; Mendes et al., 2011; Cassidy et al., 2011; Ji and Grishman, 2011; Shen et al., 2014) etc. Although the local approach can exploit a rich set of discrete structures for EL, its limitation is twofold: (i) The independent ranking mechanism in the local approach overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 2011; Hoffart et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) . (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities. The central idea is that the referent entities of some mentions in a document might in turn introduce useful information to link other mentions in that document due to the semantic relatedness among them. For example, the appearances of \"Manchester\" and \"Chelsea\" as the football clubs in a document would make it more likely that the entity mention \"Liverpool\" in the same document is also a football club. Unfortunately, the coherence assumption of the global approach does not hold in some situations, necessitating the discrete/coarse features in the local approach as a mechanism to compensate for the potential exceptions of the coherence assumption Hoffart et al., 2011; Sil et al., 2012; Durrett and Klein, 2014; Pershina et al., 2015) .",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_1",
  "x": "Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a) . In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture. This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities. Given a document, we simultaneously perform linking for every entity mention from the beginning to the end of the document. For each entity mention, we utilize convolutional neural networks (CNN) to obtain the distributed representations for the entity mention as well as its target candidates.",
  "y": "motivation background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_2",
  "x": "This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a) . In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture. This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities. Given a document, we simultaneously perform linking for every entity mention from the beginning to the end of the document. For each entity mention, we utilize convolutional neural networks (CNN) to obtain the distributed representations for the entity mention as well as its target candidates. These distributed representations are then used for two purposes: (i) computing the local similarities for the entity mention and target candidates, and (ii) functioning as the input for the recurrent neural networks (RNN) that runs over the entity mentions in the documents. The role of the RNNs is to accumulate information about the previous entity mentions and target entities, and provide them as the global constraints for the linking process of the current entity mention.",
  "y": "background similarities"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_3",
  "x": "**ENCODING** Let x be some context word sequence of the entity mentions or target candidates (i.e, x \u2208 {s i , c i , . In order to obtain the distributed representation for x, we first transform each word x i \u2208 x into a real-valued, h-dimensional vector w i using the word embedding table E (Mikolov et al., 2013) : . This essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed-length sequence of vectors w = (w 1 , w 2 , . . . , w n ) of length n. In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function<cite> (Francis-Landau et al., 2016)</cite> . Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b) ), we utilize the set L of multiple window sizes to parameterize the convolution operation. Each window size l \u2208 L corresponds to a convolution matrix M l \u2208 R v\u00d7lh of dimensionality v. Eventually, the concatenation vectorx of the resulting vectors for each window size in L would be used as the distributed representation for where is the concatenation operation over the window set L and w i:(i+l\u22121) is the concatenation vector of the given word vectors.",
  "y": "background uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_4",
  "x": "We employ the local similarities \u03c6 local (m i , p ij ) from<cite> (Francis-Landau et al., 2016)</cite> , the state-of-the-art neural network model for EL. In particular: In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively. F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_5",
  "x": "F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **GLOBAL SIMILARITIES** In order to encapsulate the coherence among the entity mentions and their target entities, we run recurrent neural networks over the sequences of the representation vectors for the entity mentions (i.e, the vector sequences for the surface strings (s 1 ,s 2 , . . . ,s k ) and for the immediate contexts (c 1 ,c 2 , . . . ,c k )) and the target entities (i.e, the vector sequences for the page titles (t * 1 ,t * 2 , . . . ,t * k ) and for the body contents Given the hidden vector sequence, when predicting the target entity for the entity mention m i , we ensure that the target entity is consistent with the global information stored in h b i\u22121 . This is achieved by using the cosine similarities between h b i\u22121 and the representation vectors of each target candidate p ij of m i , (i.e, cos(h b i\u22121 ,t ij ) and cos(h b i\u22121 ,b ij )) as the global features for the ranking score.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_6",
  "x": "This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **GLOBAL SIMILARITIES** In order to encapsulate the coherence among the entity mentions and their target entities, we run recurrent neural networks over the sequences of the representation vectors for the entity mentions (i.e, the vector sequences for the surface strings (s 1 ,s 2 , . . . ,s k ) and for the immediate contexts (c 1 ,c 2 , . . . ,c k )) and the target entities (i.e, the vector sequences for the page titles (t * 1 ,t * 2 , . . . ,t * k ) and for the body contents Given the hidden vector sequence, when predicting the target entity for the entity mention m i , we ensure that the target entity is consistent with the global information stored in h b i\u22121 . This is achieved by using the cosine similarities between h b i\u22121 and the representation vectors of each target candidate p ij of m i , (i.e, cos(h b i\u22121 ,t ij ) and cos(h b i\u22121 ,b ij )) as the global features for the ranking score. We can repeat this process for the other representation vector sequences in both the entity mention side and the target entity side.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_7",
  "x": "**EXPERIMENTS** ---------------------------------- **DATASETS** Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST. It is also used in (Fahrni and Strube, 2014) and (Durrett and Klein, 2014) . ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English. iii) WP (Heath and Bizer, 2011) : This dataset consists of short snippets from Wikipedia. iv) WIKI : This dataset contains 10,000 randomly sampled Wikipedia articles. The task is to disambiguate the links in each article 4 . For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison<cite> (Francis-Landau et al., 2016)</cite>.",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_8",
  "x": "**DATASETS** Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST. It is also used in (Fahrni and Strube, 2014) and (Durrett and Klein, 2014) . ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English. iii) WP (Heath and Bizer, 2011) : This dataset consists of short snippets from Wikipedia. iv) WIKI : This dataset contains 10,000 randomly sampled Wikipedia articles. The task is to disambiguate the links in each article 4 . For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison<cite> (Francis-Landau et al., 2016)</cite>. ---------------------------------- **PARAMETERS AND RESOURCES**",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_9",
  "x": "We employ the English Wikipedia dump from June 2016 as our reference knowledge base. Regarding the input contexts for the entity mentions and the target candidates, we utilize the window size of 10 for the immediate context c i , and only extract the first 100 words in the documents for d i and b ij . Finally, we pre-train the word embedings on the whole English Wikipedia dump using the word2vec toolkit (Mikolov et al., 2013) . The training parameters are set to the default values in this toolkit. The dimensionality of the word embeddings is 300. Note that every parameter and resource in this work is either taken from the previous work (Nguyen and Grishman, 2016b;<cite> Francis-Landau et al., 2016)</cite> or selected by the development data. ---------------------------------- **EVALUATING THE GLOBAL FEATURES** In this section, we evaluate the effectiveness of the global features for EL. In particular, we differentiate two types of global features based on the side of information we expect to enforce the coherence.",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_10",
  "x": "The most important observation from the table is that the global features, in general, help to improve the performance of the model on different datasets. This is substantial on the ACE and CoNLL datasets when only one type of the global features (either global-mention or global-entity) is integrated into the model. The combination of global-mention and global-entity is not very effective as it is actually worse than the performance of the individual global feature types. This suggests that global-mention and global-entity might cover overlapping information and their combination would inject redundancy into the model. The best performance is achieved by the global-entity features that would be used in all the evaluations below. ---------------------------------- **COMPARING TO THE PREVIOUS WORK** This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_11",
  "x": "This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump. We also include the performance of the systems on the December 2014 Wikipedia dump that was used and provided by<cite> (Francis-Landau et al., 2016)</cite> for further and compatible comparison. ---------------------------------- **SYSTEMS** Wikipedia 2014 Wikipedia 2016 ACE CoNLL WP WIKI ACE CoNLL WP WIKI DK2014 (Durrett and Klein, 2014) 79 First, we see that the performance of the systems drop significantly when we switch from Wikipedia 2014 to Wikipedia 2016 (especially for the datasets ACE and CoNLL). This is can be partly explained by the inclusion of new entities (pages) into Wikipedia from 2014 to 2016 that has made the entity mentions in the datasets more ambiguous 7 . Second and more importantly, Global-RNN significantly outperforms the all the compared models (except for the ACE dataset on Wikipedia 2014 and the WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_12",
  "x": "**DOMAIN ADAPTATION EXPERIMENTS** The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a) . Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a) . These invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others. For EL, we hypothesize that the global coherence is an effective domain-independent feature that would help to improve the crossdomain performance of the models. The intuition is that the entities mentioned in a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach<cite> (Francis-Landau et al., 2016)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_13",
  "x": "Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen et al., 2015c; Gormley et al., 2015) , we use news (the union of bn and nw) as the source domain and bc, cts, wl, un as four different target domains. We take half of bc as the development set and use the remaining data for testing. We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013) . Table 3 compares Global-RNN with the neural network EL model in<cite> (Francis-Landau et al., 2016)</cite> , the best reported model on the ACE dataset in the literature 8 . In this table, the models are trained on the source domain news, and evaluated on news itself (in-domain performance) (via 5-fold cross validation) as well as on the 4 target domains bc, cts, wl, un (out-of-domain performance The first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in-domain performance. In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL. Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ---------------------------------- **EVALUATION**",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_14",
  "x": "We take half of bc as the development set and use the remaining data for testing. We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013) . Table 3 compares Global-RNN with the neural network EL model in<cite> (Francis-Landau et al., 2016)</cite> , the best reported model on the ACE dataset in the literature 8 . In this table, the models are trained on the source domain news, and evaluated on news itself (in-domain performance) (via 5-fold cross validation) as well as on the 4 target domains bc, cts, wl, un (out-of-domain performance The first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in-domain performance. In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL. Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ---------------------------------- **EVALUATION** ----------------------------------",
  "y": "background differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_15",
  "x": "**RELATED WORK** Entity linking or disambiguation has been studied extensively in NLP research, falling broadly into two major approaches: local and global disambiguation. Both approaches share the goal of measuring the similarities between the entity mentions and the target candidates in the reference KB. The local paradigm focuses on the internal structures of each separate mention-entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on (Bunescu and Pasca, 2006; Milne and Witten, 2008; Zheng et al., 2010; Ji and Grishman, 2011; Mendes et al., 2011; Cassidy et al., 2011; Shen et al., 2014) . In contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence. Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007) , Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) , Point-wise Mutual Information measures , integer linear programming (Cheng and Roth, 2013) , PageRank (Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) , stacked generalization (He et al., 2013a) , to name a few. The entity linking techniques and systems have been actively evaluated at the NIST-organized Text Analysis Conference (Ji et al., 2014) . Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoising Auto-encoders. Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while <cite>Francis-Landau et al. (2016)</cite> combine CNN-based representations with sparse features to improve the performance.",
  "y": "differences background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_0",
  "x": "For instance, topic models have been used to identify scientific communities working on related problems in different disciplines, e.g., work on cancer funded by multiple Institutes within the NIH (Talley et al., 2011) . While vocabulary mismatch occurs within the realm of one language, naturally this mismatch occurs across different languages. Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (Mimno et al., 2009;<cite> Platt et al., 2010)</cite> . Aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc. The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to find close matches. Many widely used techniques do not scale efficiently, however, as the size of the document collection grows. Posterior inference by Gibbs sampling, for instance, may make thousands of passes through the data. For the task of comparing topic distributions, recent work has also resorted to comparing all pairs of documents (Talley et al., 2011) . This paper presents efficient methods for both of these steps and performs empirical evaluations on the task of detected translated document pairs embedded in a large multilingual corpus. Unlike some more exploratory applications of topic models, translation detection is easy to evaluate.",
  "y": "motivation"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_1",
  "x": "Most current topic models are based on Latent Dirichlet Allocation (LDA) . In some early work on the subject, showed the usefulness of LDA on the task of automatic annotation of images. Hall et al. (2008) used LDA to analyze historical trends in the scientific literature; Wei and Croft (2006) showed improvements on an information retrieval task. More recently Eisenstein et al. (2010) modeled geographic linguistic variation using Twitter data. Aside from their widespread use on monolingual text, topic models have also been used to model multilingual data (Boyd-Graber and Blei, 2009;<cite> Platt et al., 2010</cite>; Jagarlamudi and Daum\u00e9, 2010; Fukumasu et al., 2012) , to name a few. In this paper, we focus on the Polylingual Topic Model, introduced by Mimno et al. (2009) . Given a multilingual set of aligned documents, the PLTM assumes that across an aligned multilingual document tuple, there exists a single, tuple-specific, distribution across topics. In addition, PLTM assumes that for each language-topic pair, there exists a distribution over words in that language \u03b2 l . As such, PLTM assumes that the multilingual corpus is created through a generative process where first a document tuple is generated by drawing a tuple-specific distribution over topics \u03b8 1 which, as it is the case with LDA, is drawn from a Dirichlet prior \u03b8 \u223c Dir (\u03b1) . For each of the languages l in the tuple and for each of the N words w l n in the document the generative process: first chooses a topic assignment z l n \u223c M ultinomial (\u03b8) which is then followed by choosing a word w l n from a multinomial distribution conditioned on the topic assignment and the language specific topics distribution over words \u03b2 l \u223cDir (\u03b7 l ).",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_2",
  "x": "In this figure we emphasize the lower left section of the plot where the nearest neighbors (i.e., likely translations) reside, and the relationship between JS and Hellinger is much tighter than the theoretical bounds and from pratical perspective as we will show in the next section. As a summary for the reader, using the above approaches, we will approximate JS divergence by using the Euclidean based representation of the Hellinger distance. As stated earlier, the Euclidean based representation is computed using well established approximation approaches and in our case we will use two such approaches: the Exact Euclidean LSH (E2LSH) (Andoni et al., 2005) ---------------------------------- **EFFICIENT APPROXIMATE TRANSLATION DETECTION** Mapping multilingual documents into a common, language-independent vector space for the purpose of improving machine translation (MT) and performing cross-language information retrieval (CLIR) tasks has been explored through various techniques. Mimno et al. (2009) introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently,<cite> Platt et al. (2010)</cite> proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI). Both the PLTM and PLSI represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions. While the nature of both works was exploratory, results shown on fairly large collections of bilingual documents (less than 20k documents) offer convincing argument of their potential. Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex.",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_3",
  "x": "Both the PLTM and PLSI represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions. While the nature of both works was exploratory, results shown on fairly large collections of bilingual documents (less than 20k documents) offer convincing argument of their potential. Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex. While there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as Krstovski and Smith (2011) which utilizes LSH for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen-tences. In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_4",
  "x": "Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex. While there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as Krstovski and Smith (2011) which utilizes LSH for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen-tences. In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_5",
  "x": "In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> . That paper used the Europarl (Koehn, 2005) (Mimno et al., 2009) , these performance comparisons are not done on the same training and test sets-a gap that we fill below. We train PLTM models with number of topics T set to 50, 100, 200, and 500.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_6",
  "x": "**EVALUATION TASK AND RESULTS** Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one. This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection. Shown in Table 1 are results across the four different measurements for all four PLTM models. When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations. When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%. We believe that this is due to the small amount of error in the search introduced by ANN, due to its approximate nature, which for this task yields positive results. On the same data set, <cite>(Platt et al., 2010)</cite> report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_7",
  "x": "We measured the total time that it takes to perform exhaustive all-pairs comparison using JS divergence, the LSH and kdtrees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2.66GHz on each core and a total of 8GB of memory. Since the time performance of the E2LSH depends on the radius R of data set points considered for each query point (Indyk and Motwani, 1998) , we performed measurements with different values of R. For this task, the all-pairs JS code implementation first reads both source and target sets of documents and stores them in hash tables. We then go over each entry in the source table and compute divergence against all target table entries. We refer to this code implementation as hash map implementation. ---------------------------------- **EVALUATION TASK AND RESULTS** Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one. This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_8",
  "x": "When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations. When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%. We believe that this is due to the small amount of error in the search introduced by ANN, due to its approximate nature, which for this task yields positive results. On the same data set, <cite>(Platt et al., 2010)</cite> report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference. Shown in Table 2 are the relative differences in time between all pairs JS divergence, approximate kd-trees and LSH based Hellinger distance with different value of R. Rather than showing absolute speed numbers, which are often influenced by the processor configuration and available memory, we show relative speed improvements where we take the slowest running configuration as a referent value. In our case we assign the referent speed value of 1 to the configuration with T=500 and allpairs JS computation. Results shown are based on comparing running time of E2LSH and ANN against the all-pairs similarity comparison implementation that uses hash tables to store all documents in the bilingual collection which is significantly faster than the other code implementation. For the approximate, LSH based, Hellinger distance with T=100 we obtain a speed improvement of 24.2 times compared to regular all-pairs JS divergence while maintaining the same performance compared to Hellinger distance metric and insignificant loss over all-pairs JS divergence. From Table 2 it is evident that as we increase the radius R we reduce the relative speed of performance since the range of points that LSH considers for a given query point increases. Also, as the number of topics increases, the speed benefit is reduced for both the LSH and k-d tree techniques.",
  "y": "extends differences"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_0",
  "x": "However, the extensive training data necessary for supervised learning is expensive to obtain and therefore restrictive in a Web-scale relation extraction task. * Equal Contribution to this work \u2020 This research was conducted during the author's research assistantship at Indian Institute of Science To overcome this challenge, [Mintz et al., 2009] proposed a Distant Supervision (DS) method for relation extraction to help automatically generate new training data by taking an intersection between a text corpus and knowledge base. The distant supervision assumption states that for a pair of entities participating in a relation, any sentence mentioning that entity pair in the text corpora is a positive example for the relation fact. This assumption outputs evidence from multiple sentences for multiple relation labels between an entity-pair. Therefore the problem of relation extraction in distantly supervised datasets is posed as a Multi-instance Multi-label (MIML) problem [Surdeanu et al., 2012] , as shown in Figure 1 . However, the DS assumption is too strong and may introduce noise such as false negative samples due to missing facts in the knowledge base. In this paper, we propose relation extraction models and a new dataset to improve RE. We define 'instance' as a sentence containing an entity-pair, and 'instance set' as a set of sentences containing the same entity-pair. It was observed by<cite> [Zeng et al., 2015]</cite> that 50% of the sentences in the Riedel2010 Distant Supervision dataset [Riedel et al., 2010] , a popular DS benchmark dataset, had 40 or more words in them. We note that not all the words in these long sentences contribute towards expressing the given relation.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_1",
  "x": "**PROPOSED METHODS** In this section, we present our attention-based models for distantly supervised relation extraction. We first describe problem background and Piecewise Convolution Neural Network (PCNN), a previous-state-of-the-art model. We then introduce our Entity attention (EA) and Bi-GRU based word attention (BGWA) models. The last subsection describes a simple ensemble approach to combine predictions of various models for robust relation extraction. ---------------------------------- **BACKGROUND** Relation Extraction: A relation is defined as a semantic property between a set of entities {e k }. In our task, we consider binary relations where k \u2208 [1, 2], such as Born In(Barack Obama, Hawaii). Given a set of sentences S = {s i }; i \u2208 [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN:<cite> [Zeng et al., 2015]</cite> proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_2",
  "x": "We modify and adapt their model for the distant supervision setting and propose Entity Attention (EA) which works with a bag of sentences. For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> . The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows: is the concatenation of a word and the entity embedding. A k , r k are learned parameters. Bilinear operator A k determines the relevance of concatenated word & entity embedding for a relation vector r k . Intuitively, attention should choose words which are related to the entity for a given relation.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_3",
  "x": "The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> . The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows: is the concatenation of a word and the entity embedding. A k , r k are learned parameters. Bilinear operator A k determines the relevance of concatenated word & entity embedding for a relation vector r k . Intuitively, attention should choose words which are related to the entity for a given relation. The u i,j,qk are normalized using a softmax function to generate a i,j,qk , the attention scores for a given word. Similar to the PCNN model in Section 2.1, the attention weighted word embeddings are pooled using piecewise pooling method to generate s ea \u2208 R 1\u00d73g dimensional sentence embeddings. The output from the PCNN layer and the entity attention layers are concatenated and then passed through a linear layer to obtain probabilities for each relation. The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original [Shen and Huang, 2016] model (a) The EA processes a set of sentences.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_4",
  "x": "A k , r k are learned parameters. Bilinear operator A k determines the relevance of concatenated word & entity embedding for a relation vector r k . Intuitively, attention should choose words which are related to the entity for a given relation. The u i,j,qk are normalized using a softmax function to generate a i,j,qk , the attention scores for a given word. Similar to the PCNN model in Section 2.1, the attention weighted word embeddings are pooled using piecewise pooling method to generate s ea \u2208 R 1\u00d73g dimensional sentence embeddings. The output from the PCNN layer and the entity attention layers are concatenated and then passed through a linear layer to obtain probabilities for each relation. The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original [Shen and Huang, 2016] model (a) The EA processes a set of sentences. It uses PCNN<cite> [Zeng et al., 2015]</cite> assumption to select the sentence with highest probability of any relation. The selected sentence is used to estimate the relation probabilities for an entity-pair and for back-propagation of the error for the bag-of-sentences. (b) EA uses PCNN instead of CNN to preserve structural features in a sentence.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_5",
  "x": "We partitioned Riedel2010 train set into a new train (80%) and development set (20%). Development set is created to facilitate the learning of an ensemble model and for model selection. This resulting dataset is called Riedel2010-b. Details of the new GDS dataset is described in Section 3. Evaluation Metrics: Following [Lin et al. , 2016], we use held-out evaluation scheme. The performance of each model is evaluated on a test set using Precision-Recall (PR) curve. Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN)<cite> [Zeng et al., 2015]</cite> and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) [Lin et al., 2016] . Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models. Model Parameters: The parameters used for the various models are summarized in Table 4 .",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_6",
  "x": "Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following<cite> [Zeng et al., 2015]</cite> and used author provided results and implementation for NRE baseline. The EA and BGWA models were developed in PyTorch 2 . We use SGD algorithm with dropout [Srivastava et al., 2014] for model learning. The experiments were run on GeForce GTX 1080 Ti using NVIDIA-CUDA. Model selection for all algorithms was done based on the AUC (Area Under the Curve) metric for the precision-recall curve for development dataset. ---------------------------------- **RESULTS** Performance Comparison: Figure 7 and Figure 8 show the precision-recall curve for baseline and proposed algorithms on two datasets, Riedel2010-b (with development set) and the GDS dataset. Please note that the NRE model's PR-curve in Figure 7 is taken from author published results which used combined train+dev set for training.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_7",
  "x": "[Lin et al., 2016] improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction. [ <cite>Zheng et al., 2016]</cite> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction. Recently, work by [Ye et al., 2016] exploit the connections between relation (class ties) to improve relation extraction performance. A few papers propose the addition of background knowledge to reduce noise in training data. [Weston et al., 2013] proposes a joint-embedding model for text and KB entities where the known part of the KB is utilized as part of the supervision signal. [Han and Sun, 2016] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using Markov logic networks. [Nagarajan et al., 2017] uses inter-instance-set couplings for relation extraction in multi-task setup to improve performance. Attention models learn the importance of a feature in the supervised task through back-propogation. Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction<cite> [Zheng et al., 2016]</cite> etc.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_8",
  "x": "[Lin et al., 2016] improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction. [ <cite>Zheng et al., 2016]</cite> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction. Recently, work by [Ye et al., 2016] exploit the connections between relation (class ties) to improve relation extraction performance. A few papers propose the addition of background knowledge to reduce noise in training data. [Weston et al., 2013] proposes a joint-embedding model for text and KB entities where the known part of the KB is utilized as part of the supervision signal. [Han and Sun, 2016] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using Markov logic networks. [Nagarajan et al., 2017] uses inter-instance-set couplings for relation extraction in multi-task setup to improve performance. Attention models learn the importance of a feature in the supervised task through back-propogation. Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction<cite> [Zheng et al., 2016]</cite> etc.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_0",
  "x": "**INTRODUCTION** Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., Bangalore et al., 2006) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) . Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al., 2000) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts. This approach operates on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in a dialogue mode that characterizes the nature of the exchanges between tutor and student. In our model, a dialogue mode is defined by a probability distribution over the observed symbols (e.g., dialogue acts and adjacency pairs). Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al., in press) . Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_1",
  "x": "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) . Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al., 2000) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts. This approach operates on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in a dialogue mode that characterizes the nature of the exchanges between tutor and student. In our model, a dialogue mode is defined by a probability distribution over the observed symbols (e.g., dialogue acts and adjacency pairs). Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al., in press) . Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied. Training an HMM on a sequence of adjacency pairs rather than individual dialogue acts is one way to generate a more descriptive model without increasing model complexity more than is required to accommodate the expanded set of observation symbols. To this end, we apply the approach of Midgley et al. (2006) for empirically identifying significant adjacency pairs within dialogue, and proceed by treating adjacency pairs as atomic units for the purposes of training the HMM.",
  "y": "motivation background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_2",
  "x": "Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study. ---------------------------------- **1** The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & Sacks, 1973) , and adjacency pair analysis has illuminated important phenomena in tutoring as well <cite>(Forbes-Riley et al., 2007)</cite> . For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs. However, as noted in (Midgley et al., 2006) , in order to establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher probability of the second member occurring. For this analysis we utilize a \u03c7 2 test for independence of the categorical variables act i and act i+1 for all two-way combinations of dialogue act tags. Only pairs in which speaker(act i )\u2260speaker(act i+1 ) were considered. Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Section 3. Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance; the subscript indicates tutor (t) or student (s).",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_0",
  "x": "Examples like this are common in tweets discussing topics like politics. As has been demonstrated by the failure of election polls in both referenda and general elections (Burnap et al., 2016) , it is important to understand not only the overall mood of the electorate, but also to distinguish and identify sentiment towards different key issues and entities, many of which are discussed on social media on the run up to elections. Recent developments on target-specific Twitter sentiment classification have explored different ways of modelling the association between target entities and their contexts. Jiang et al. (2011) propose a rule-based approach that utilises dependency parsing and contextual tweets. Dong et al. (2014) , Tang et al. (2016a) and Zhang et al. (2016) have studied the use of different recurrent neural network models for such a task but the gain in performance from the complex neural architectures is rather unclear 1 In this work we introduce the multi-targetspecific sentiment recognition task, building a corpus of tweets from the 2015 UK general election campaign suited to the task. In this dataset, target entities have been semi-automatically selected, and sentiment expressed towards multiple target entities as well as high-level topics in a tweet have been manually annotated. Unlike all existing studies on target-specific Twitter sentiment analysis, we move away from the assumption that each tweet mentions a single target; we introduce a more realistic and challenging task of identifying sentiment towards multiple targets within a tweet. To tackle this task, we propose TDParse, a method that divides a tweet into different segments building on the approach introduced by Vo and Zhang (2015) . TDParse exploits a syntactic dependency parser designed explicitly for tweets (Kong et al., 2014) , and combines syntactic information for each target with its left-right context. We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment<cite> (Dong et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_1",
  "x": "Recently Vargas et al. (2016) analysed the differences between the overall and target-dependent sentiment of tweets for three events containing 30 targets, showing many significant differences between the corresponding overall and target-dependent sentiment labels, thus confirming that these are distinct tasks. Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work<cite> (Dong et al., 2014)</cite> used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees. Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively. While Tang et al (2016a) propose two long shortterm memory (LSTM) models showing competitive performance to Vo and Zhang (2015) , Zhang et al (2016) design a gated neural network layer between the left and right context in a deep neural network structure but require a combination of three corpora for training and evaluation. Results show that conventional neural network models like LSTM are incapable of explicitly capturing important context information of a target (Tang et al., 2016b) .",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_2",
  "x": "Also sentiment classification in formal text such as product reviews 2 The data and code can be found at https://goo.gl/ S2T1GO is very different from that in tweets. Recently Vargas et al. (2016) analysed the differences between the overall and target-dependent sentiment of tweets for three events containing 30 targets, showing many significant differences between the corresponding overall and target-dependent sentiment labels, thus confirming that these are distinct tasks. Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work<cite> (Dong et al., 2014)</cite> used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees. Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively. While Tang et al (2016a) propose two long shortterm memory (LSTM) models showing competitive performance to Vo and Zhang (2015) , Zhang et al (2016) design a gated neural network layer between the left and right context in a deep neural network structure but require a combination of three corpora for training and evaluation.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_3",
  "x": "The tool shows a tweet with the targets highlighted in bold. Possible annotation actions consisted in: (1) marking the sentiment for a target as being positive, negative, or neutral, (2) marking a target as being mistakenly highlighted (i.e. 'doesnotapply') and hence removing it, and (3) highlighting new targets that our preprocessing step had missed, and associating a sentiment value with them. In this way we obtained a corrected list of targets for each tweet, each with an associated sentiment value. We measure inter-annotator agreement in two different ways. On the one hand, annotators achieved \u03ba = 0.345 (z = 92.2, p < 0.0001) (fair agreement) 6 when choosing targets to be added or removed. On the other hand, they achieved a similar score of \u03ba = 0.341 (z = 77.7, p < 0.0001) (fair agreement) when annotating the sentiment of the resulting targets. It is worth noting that the sentiment annotation for each target also involves choosing among not only positive/negative/neutral but also a fourth category 'doesnotapply'. The resulting dataset contains 4,077 tweets, with an average of 3.09 entity mentions (targets) per tweet. As many as 3,713 tweets have more than a single entity mention (target) per tweet, which makes the task different from 2015 Semeval 10 subtask C (Rosenthal et al., 2015) and a target-dependent benchmarking dataset of<cite> Dong et al. (2014)</cite> where each tweet has only one target annotated and thus one sentiment label assigned. The number of targets in the 4,077 tweets to be annotated originally amounted to 12,874.",
  "y": "differences"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_4",
  "x": "We then extract word embedding features from these syntactically dependent tokens [D 1 , ..., D n ] along its dependency path in the parsing tree to the target 7 , as well as from the left-target-right contexts (i.e. L \u2212 T \u2212 R). Feature vectors generated from different contexts are concatenated into a final feature 7 Empirically the proximity/location of such syntactic relations have not made much difference when used in feature weighting and is thus ignored. vector as shown in (2), where P (X) presents a list of k different pooling functions on an embedding matrix X. Not only does this proposed framework make the learning process efficient without labor intensive manual feature engineering and heavy architecture engineering for neural models, it has also shown that complex syntactic and semantic information can be effectively drawn by simply concatenating different types of context together without the use of deep learning (other than pretrained word embeddings). Data set: We evaluate and compare our proposed system to the state-of-the-art baselines on a benchmarking corpus<cite> (Dong et al., 2014</cite> ) that has been used by several previous studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . This corpus contains 6248 training tweets and 692 testing tweets with a sentiment class balance of 25% negative, 50% neutral and 25% positive. Although the original corpus has only annotated one target per tweet, without specifying the location of the target, we expand this notion to consider cases where the target entity may appear more than once at different locations in the tweet, e.g.: \"Nicki Minaj has brought back the female rapper. -really? Nicki Minaj is the biggest parody in popular music since the Lonely Island.\" Semantically it is more appropriate and meaningful to consider both target appearances when determining the sentiment polarity of \"Nicki Minaj\" expressed in this tweet. While it isn't clear if<cite> Dong et al. (2014)</cite> and Tang et al. (2016a) have considered this realistic same-target-multiappearance scenario, Vo et al. (2015) and Zhang et al. (2016) do not take it into account when extracting target-dependent contexts. Contrary to these studies we extend our system to fully incorporate the situation where a target appears multiple times at different locations in the tweet.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_6",
  "x": "**EXPERIMENTAL SETTINGS** To compare our proposed models with Vo & Zhang (2015) , we have used the same pre-trained embedding resources and pooling functions (i.e. max, min, mean, standard deviation and product). For classification we have used LIBLINEAR (Fan et al., 2008) , which approximates a linear SVM. In tuning the cost factor C we perform five-fold cross validation on the training data over the same set of parameter values for both Vo and Zhang (2015) 's implementation and our system. This makes sure our proposed models are comparable with those of Vo and Zhang (2015) . Evaluation metrics: We follow previous work on target-dependent Twitter sentiment classification, and report our performance in accuracy, 3-class macro-averaged (i.e. negative, neutral and positive) F 1 score as well as 2-class macroaveraged (i.e. negative and positive) F 1 score 8 , as used by the Semeval competitions (Rosenthal et al., 2015) for measuring Twitter sentiment classification performance. ---------------------------------- **EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_7",
  "x": "In tuning the cost factor C we perform five-fold cross validation on the training data over the same set of parameter values for both Vo and Zhang (2015) 's implementation and our system. This makes sure our proposed models are comparable with those of Vo and Zhang (2015) . Evaluation metrics: We follow previous work on target-dependent Twitter sentiment classification, and report our performance in accuracy, 3-class macro-averaged (i.e. negative, neutral and positive) F 1 score as well as 2-class macroaveraged (i.e. negative and positive) F 1 score 8 , as used by the Semeval competitions (Rosenthal et al., 2015) for measuring Twitter sentiment classification performance. ---------------------------------- **EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_8",
  "x": "We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well. Among the target-dependent systems neural network baselines have shown varying results. The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ). TD-LSTM and TC-LSTM from Tang et al. (2016a) model left-target-right contexts using two LSTM neural networks and by doing so incorporate target-dependent information. TD-LSTM uses two LSTM neural networks for modeling the left and right contexts respectively. TC-LSTM differs from (and outperforms) TD-LSTM in that it concatenates target word vectors with embedding vectors of each context word. We also test the Gated recurrent neural network models proposed by Zhang et al. (2016) on the same dataset.",
  "y": "background"
 },
 {
  "id": "c57e98c9c07dd5d8653e172136c901_0",
  "x": "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; <cite>Seo et al., 2016)</cite> , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models (Rezende et al., 2014) . The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies are presented to tackle different issues in deep Bayesian learning and understanding. At last, we point out a number of directions and outlooks for future studies. The presentation of this tutorial is arranged into five parts. First of all, we share the current status of researches on natural language understanding, statistical modeling and deep neural network and explain the key issues in deep Bayesian learning for discrete-valued observation data and latent semantics. A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking. Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_0",
  "x": "This approach has achieved promising initial results [6] <cite>[7]</cite> [8] [9] 14] , but many questions remain. Two outstanding questions are the best method of learning verb tensors from a corpus, and the best sentence space for a variety of different tasks. This paper presents work in progress which addresses both of these questions. It compares two methods for learning verb representations, the distributional model of <cite>[7]</cite> in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space. A variety of methods for reducing the noun space and composing the verb, subject, and object representations are investigated. The results show that the plausibility training outperforms the distributional method on a verb disambiguation task, while the purely distributional approach performs better on sentence similarity. ---------------------------------- **METHODS** In the definition of the functional approach to compositional distributional semantics [1] [2] [3] [4] , a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space.",
  "y": "motivation"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_2",
  "x": "This paper presents work in progress which addresses both of these questions. It compares two methods for learning verb representations, the distributional model of <cite>[7]</cite> in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space. A variety of methods for reducing the noun space and composing the verb, subject, and object representations are investigated. The results show that the plausibility training outperforms the distributional method on a verb disambiguation task, while the purely distributional approach performs better on sentence similarity. ---------------------------------- **METHODS** In the definition of the functional approach to compositional distributional semantics [1] [2] [3] [4] , a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_3",
  "x": "It compares two methods for learning verb representations, the distributional model of <cite>[7]</cite> in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space. A variety of methods for reducing the noun space and composing the verb, subject, and object representations are investigated. The results show that the plausibility training outperforms the distributional method on a verb disambiguation task, while the purely distributional approach performs better on sentence similarity. ---------------------------------- **METHODS** In the definition of the functional approach to compositional distributional semantics [1] [2] [3] [4] , a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] .",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_4",
  "x": "All of the plausibility data is used for training and validation, as we employ only the resulting verb matrix to produce a sentence space for transitive verb phrases in the test data. The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping. ---------------------------------- **DISTRIBUTIONAL (DIST)** Following <cite>[7]</cite> , we generate a K \u00d7 K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data: where \u2297 is outer product and N p is the number of positive training examples. The intuition is that the matrix encodes higher weights for contextual features of frequently attested subjects and objects; for example, multiplying by the matrix for eat may yield a higher scalar value when its subject exhibits features common to animate nouns, and its object exhibits features common to edible nouns. ---------------------------------- **TRAINING DATA** In order to generate training data we find plausible SVO triples that occur in an October 2013 dump of Wikipedia.",
  "y": "uses"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_6",
  "x": "**TASKS** We investigate the performance of the regression learning method on two tasks: verb disambiguation, and transitive sentence similarity. In each case the system must compose SVO triples and compare the resulting semantic representations. For the verb disambiguation task we use the GS2011 dataset <cite>[7]</cite> . This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses. For example, the verb draw has senses that correspond to attract and depict. The SVO triple report draw attention has high similarity to report attract attention, but low similarity to report depict attention. Conversely, child draw picture has high similarity to child depict picture, but low similarity to child attract picture. The gold standard consists of human judgements of the similarity between pairs, and we measure the correlation of the system's similarity scores to the gold standard judgements. For the transitive sentence similarity task we use the KS2013 dataset [9] .",
  "y": "uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_0",
  "x": "**ABSTRACT** Despite the success of existing works on single-turn conversation generation, taking the coherence in consideration, human conversing is actually a context-sensitive process. Inspired by the existing studies, this paper proposed the static and dynamic attention based approaches for context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation. ---------------------------------- **INTRODUCTION** Until recently, training open-domain conversational systems that can imitate the way of human conversing is still not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010) , a phrasebased statistical machine translation task (Ritter et al., 2011 ) and a search problem based on the vector space model (Banchs and Li, 2012) , etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017) . Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017;<cite> Xing et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_1",
  "x": "\u2022 First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU<cite> (Xing et al., 2017)</cite> or unidirectional GRU (Tian et al., 2017 ). One is the attention-based approach<cite> (Xing et al., 2017)</cite> , the other is the sequential integration approach (Tian et al., 2017) . \u2022 Utterance Representations: Bidirectional GRU vs. Unidirectional GRU<cite> Xing et al. (2017)</cite> utilized a bidirectional GRU and a word-level attention mechanism to transfer word representations to utterance representations. \u2022 Inter-utterance Representations: Attention vs. Sequential Integration<cite> Xing et al. (2017)</cite> proposed a hierarchical attention mechanism to feed the utterance representations to a backward RNN to obtain contextual representation.",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_2",
  "x": "For utterance representation, we consider the advantages of the two state-of-the-art approaches to encoding contextual information for context-sensitive response generation <cite>(Xing et al., 2017</cite>; Tian et al., 2017) . We utilize a GRU model to obtain utterance representation. For inter-utterance representation, inspired by the above approaches of modeling inter-utterance representations, we proposed two attention mechanisms, namely dynamic and static attention, to weigh the importance of each utterance in a conversation and obtain the contextual representation. Figure 2 shows the framework of the proposed context-sensitive generation model. Drawing the advantages of attention mechanism on Here, u * denotes the * -th utterance in a conversation. weighing the importance of utterances for generating open-domain conversational responses<cite> (Xing et al., 2017)</cite> , we thus model the inter-utterance representation to obtain the context vector in two measures, namely static and dynamic attention, as shown in Figure 2 . We then formally describe the static and dynamic attention for decoding process. \u2022 Static Attention based Decoding As shown in Figure 2 , the static attention mechanism calculates the importance of each utterance as e i or \u03b1 i (i \u2208 {1, ..., s}). Here, h i and h s denote the representations of hidden state of the i-th and the last utterance in a conversation, respectively. V , W and U are parameters.",
  "y": "similarities uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_3",
  "x": "\u2022 Dynamic Attention based Decoding Rather than the static attention mechanism fixes the weights of each utterance before decoding process, the dynamic attention mechanism maintains a weighting matrix and updates the weights of each utterance during decoding process as shown in Figure 2 . The formal illustration of the dynamic attention mechanism for decoding is as follows: Here, V , W and U are also parameters that are independent to those in the static attention. T denotes the transposition operation of V . The e i,t and \u03b1 i,t are calculated in each time step t of decoding. The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows: The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation. Rather than use a hierarchical attention neural network<cite> (Xing et al., 2017)</cite> to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_4",
  "x": "They are VHRED, CVAE, WSI, and HRAN. \u2022 LSTM: Under the sequence-to-sequence framework for generation of conversational responses, the most simple but natural idea is to directly use the LSTM to encode all the utterances in a session word by word and then decode to generate a response. \u2022 HRED: The first hierarchical recurrent encoder-decoder model, which is proposed by Serban et al. (2016a) , for conversational response generation. \u2022 VHRED: The augmented HRED model, which incorporates a stochastic latent variable at utterance level for encoding and decoding, is proposed by Serban et al. (2017b) . \u2022 CVAE: The conditional variational autoencoder based approach, which is proposed by , to learn context diversity for conversational responses generation. \u2022 WSI and HRAN are proposed by Tian et al. (2017) and<cite> Xing et al. (2017)</cite> respectively. We detailed describe and compare the two models in Section 2.1 and 2.2 and their frameworks are shown in Figure 1 . ---------------------------------- **EVALUATION AND RESULTS** ----------------------------------",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_0",
  "x": "****CROSS-LINGUAL PARSING WITH POLYGLOT TRAINING AND MULTI-TREEBANK LEARNING: A FAROESE CASE STUDY**** **ABSTRACT** Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce <cite>multisource projection</cite> <cite>(Tyers et al., 2018)</cite> , in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_1",
  "x": "In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl-edge can be induced from external corpora such as by learning distributed word representations (Mikolov et al., 2013; Al-Rfou' et al., 2013) and more recent contextualized variants Devlin et al., 2019) . This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation. 2 We adopt the same terminology used in Mulcaire et al. (2019) , who use the term cross-lingual transfer to describe methods involving the use of one or more source languages to process a target language. They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages. For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections.",
  "y": "extends"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_2",
  "x": "---------------------------------- **INTRODUCTION** Cross-lingual transfer methods, i. e. methods that transfer knowledge from one or more source languages to a target language, have led to substantial improvements for low-resource dependency parsing (Rosa and Mare\u010dek, 2018; Guo et al., 2015; Lynn et al., 2014; Mc-Donald et al., 2011; Hwa et al., 2005) and part-ofspeech (POS) tagging (Plank and Agi\u0107, 2018) . In low-resource scenarios, there may be not enough data for data-driven models to learn how to parse. In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl-edge can be induced from external corpora such as by learning distributed word representations (Mikolov et al., 2013; Al-Rfou' et al., 2013) and more recent contextualized variants Devlin et al., 2019) . This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_3",
  "x": "They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages. For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections. The former differs from the approach of <cite>Tyers et al. (2018)</cite> , who use multiple discrete, monolingual models to parse the translated sentences, whereas in this work we use a single model trained on multiple source treebanks. The latter differs from training on the target treebank produced by multi-source projection in that the information of the individual projections is still available and training data is not reduced to cases where all source languages provide a projection. In other words, we aim to investigate whether the current state-of-the-art approach for Faroese, which relies on cross-lingual transfer, can be improved upon by adopting an approach based on source-side polyglot learning and/or target-side multi-treebank learning. We hypothesize that a polyglot model can exploit similarities in morphology and syntax across the included source languages, which will result in a better model to provide annotations for projection. On the target side, we expect that combining different sources of information will result in a more robust target model. We evaluated our various models on the Faroese test set and experienced considerable gains for three of the four source languages (Danish, Norwegian Bokm\u00e5l and Swedish) by adopting a polyglot model. However, for Norwegian Nynorsk, a stronger monolingual model was able to outperform the polyglot approach.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_4",
  "x": "The latter differs from training on the target treebank produced by multi-source projection in that the information of the individual projections is still available and training data is not reduced to cases where all source languages provide a projection. In other words, we aim to investigate whether the current state-of-the-art approach for Faroese, which relies on cross-lingual transfer, can be improved upon by adopting an approach based on source-side polyglot learning and/or target-side multi-treebank learning. We hypothesize that a polyglot model can exploit similarities in morphology and syntax across the included source languages, which will result in a better model to provide annotations for projection. On the target side, we expect that combining different sources of information will result in a more robust target model. We evaluated our various models on the Faroese test set and experienced considerable gains for three of the four source languages (Danish, Norwegian Bokm\u00e5l and Swedish) by adopting a polyglot model. However, for Norwegian Nynorsk, a stronger monolingual model was able to outperform the polyglot approach. When we extended multi-treebank learning to the target side, we experienced additional gains for all cases. Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_5",
  "x": "Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments. The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) . The resulting trees make up a synthetic treebank for Faroese which is then used to train a Faroese parsing model. The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> . The approach is compared to a delexicalized baseline, which it outperforms by a large margin. It is also shown that, for Faroese, a combination of the four source languages (multi-source projection) is superior to individual language projection.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_6",
  "x": "Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments. The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) . The resulting trees make up a synthetic treebank for Faroese which is then used to train a Faroese parsing model. The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> . The approach is compared to a delexicalized baseline, which it outperforms by a large margin. It is also shown that, for Faroese, a combination of the four source languages (multi-source projection) is superior to individual language projection. ---------------------------------- **BACKGROUND**",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_7",
  "x": "This method was later used in dependency parsing by Hwa et al. (2005) , who project dependencies to a target language and use a set of heuristics to form dependency trees in the target language. A parser is then trained on the projected treebank and evaluated against gold-standard treebanks. Zeman and Resnik (2008) introduced the idea of delexicalized dependency parsing whereby a parser is trained using only POS information and is then applied to a target language. McDonald et al. (2011) perform delexicalized dependency parsing using direct transfer and show that this approach outperforms unsupervised approaches for grammar induction. Importantly, this approach can be extended to the multi-source case by training on multiple source languages and predicting a target language. In an additional experiment, they combine annotation projection and multi-source transfer. Tiedemann and Agi\u0107 (2016) present a thorough comparison of pre-neural cross-lingual parsing. Various forms of projected annotation methods are compared to delexicalized baselines, and the use of machine translation instead of parallel corpora to produce synthetic treebanks in the target language is explored. In contrast to <cite>Tyers et al. (2018)</cite> , they translate a target sentence and project the source parse tree back to the target during test time instead of using this approach to obtain training data for the target language. leverage massively multilingual parallel corpora such as translations of the Bible and web-scraped data from the Watchtower Online Library website 3 for low-resource POS tagging and dependency parsing using annotation projection.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_8",
  "x": "Interestingly, they found that the best results were achieved by training a model on the various support languages as well as the target data, i. e. their multilingual model. While we do not combine ---------------------------------- **METHOD** We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS**",
  "y": "similarities uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_9",
  "x": "They include three experiments: first, training a monolingual model on a small number of sentences in the target language; second, training a cross-lingual model on related source languages which is then applied to the target data and lastly, training a multilingual model which includes target data as well as data from the related support languages. They found that training a monolingual model on the target data was always superior to training a cross-lingual model. Interestingly, they found that the best results were achieved by training a model on the various support languages as well as the target data, i. e. their multilingual model. While we do not combine ---------------------------------- **METHOD** We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models.",
  "y": "differences similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_10",
  "x": "---------------------------------- **METHOD** We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_11",
  "x": "We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_12",
  "x": "**TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_13",
  "x": "---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_14",
  "x": "Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ---------------------------------- **WORD ALIGNMENTS** We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2. 9 Source Treebanks We use the Universal Dependencies v2.2 treebanks to train our source parsing models.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_15",
  "x": "To ensure that our parser is realistic, we add a pre-trained monolingual word embedding to each monolingual parser, giving a considerable improvement in accuracy on the development sets of the source languages. We use the precomputed Word2Vec embeddings 11 released as part of the 2017 CoNLL shared task on UD parsing (Zeman et al., 2017) which were trained on CommonCrawl and Wikipedia. In order to use pre-trained word embeddings for the polyglot setting, we need to consider that a polyglot model uses a shared vocabulary across all input languages. In our experiments, we simply 10 We observe slightly lower POS tagging scores on fully annotated test sets than UDPipe, which uses gold lemmas, XPOS and morphological features to predict the UPOS label and therefore cannot be applied to the translated text without also building predictors for these features. 11 https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-1989 use the union of the word embeddings and average the word vector for words that occur in more than one language. Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_16",
  "x": "In order to use pre-trained word embeddings for the polyglot setting, we need to consider that a polyglot model uses a shared vocabulary across all input languages. In our experiments, we simply 10 We observe slightly lower POS tagging scores on fully annotated test sets than UDPipe, which uses gold lemmas, XPOS and morphological features to predict the UPOS label and therefore cannot be applied to the translated text without also building predictors for these features. 11 https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-1989 use the union of the word embeddings and average the word vector for words that occur in more than one language. Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_17",
  "x": "In our experiments, we simply 10 We observe slightly lower POS tagging scores on fully annotated test sets than UDPipe, which uses gold lemmas, XPOS and morphological features to predict the UPOS label and therefore cannot be applied to the translated text without also building predictors for these features. 11 https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-1989 use the union of the word embeddings and average the word vector for words that occur in more than one language. Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_18",
  "x": "Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) . The dependency structure is first built by voting over the directed edges. Afterward, dependency labels and POS tags are decided using the same voting procedure.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_19",
  "x": "---------------------------------- **EXPERIMENTS** In this section, we describe our experiments, which include a replication of the main findings of <cite>Tyers et al. (2018)</cite> , using AllenNLP for POS tagging and parsing instead of UDPipe (Straka and Strakov\u00e1, 2017 Figure 3 : Multi-source projection. The source language is listed in brackets. ---------------------------------- **DETAILS** The hyper-parameters of our POS tagging and parsing models are given in Table 1 . For POS tagging, we adopt a standard architecture with a word and character-level Bi-LSTM Graves and Schmidhuber, 2005) to learn contextsensitive representations of our words. These representations are passed to a multilayer perceptron (MLP) classifier followed by a softmax function to choose a tag with the highest probability. For both the POS tagging and parsing models, we use a word embedding dimension of size 100 and a character embedding dimension of size 64.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_20",
  "x": "We test this hypothesis by only including those sentences which contributed to multi-source projection in the single-source synthetic treebanks. The results are given in Table 5. Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences. However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not. In this way, our monolingual models are stronger and likely do not benefit as much from voting. The second result column (MULTI) of Table 4 shows the effect of training a multi-treebank POS tagger and parser on the Faroese treebanks created by each of the four source languages as well as the treebank which is produced by multi-source projection. This experiment is orthogonal to the experiment using a polyglot model on the source side and so we also test a combination of polyglot source side parsing and multi-treebank target side parsing. We see improvements over the single treebank setting for all cases.",
  "y": "differences uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_21",
  "x": "On the source side, the monolingual Norwegian Nynorsk model also performed slightly better than the polyglot model ( guage with the highest LAS (Norwegian Bokm\u00e5l) is also the best choice for projection (in this single target model setting). The multi-source approach was not that effective in our case and some individual better sources were able to surpass this combination approach. One could argue that this may be due to the lower amount of training data when using the multisource treebank. We test this hypothesis by only including those sentences which contributed to multi-source projection in the single-source synthetic treebanks. The results are given in Table 5. Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences. However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not. In this way, our monolingual models are stronger and likely do not benefit as much from voting.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_22",
  "x": "15 Table 6 places our systems in the context of previous results on the same Faroese test set. The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68. Our highest score of 71.51 is achieved through the combination of projecting from strong monolingual source models and then training multi-treebank POS tagging and parsing models on the outputs. ---------------------------------- **CONCLUSION** We have presented parsing results on Faroese, a low-resource language, using annotation projection from multiple monolingual sources versus a single polyglot model. We also extended the idea of multi-treebank learning to the target treebanks.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_23",
  "x": "In this way, our monolingual models are stronger and likely do not benefit as much from voting. The second result column (MULTI) of Table 4 shows the effect of training a multi-treebank POS tagger and parser on the Faroese treebanks created by each of the four source languages as well as the treebank which is produced by multi-source projection. This experiment is orthogonal to the experiment using a polyglot model on the source side and so we also test a combination of polyglot source side parsing and multi-treebank target side parsing. We see improvements over the single treebank setting for all cases. 15 Table 6 places our systems in the context of previous results on the same Faroese test set. The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68. Our highest score of 71.51 is achieved through the combination of projecting from strong monolingual source models and then training multi-treebank POS tagging and parsing models on the outputs.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_0",
  "x": "Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-ofthe-art models on the VLN task. ---------------------------------- **INTRODUCTION** The Vision-and-Language Navigation (VLN) task (Anderson et al., 2018) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror). The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b;<cite> Ma et al., 2019)</cite> have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these go past the couch \u2026 Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach. substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1 ----------------------------------",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_1",
  "x": "Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> . We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_2",
  "x": "Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> . We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks.",
  "y": "motivation"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_3",
  "x": "In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. and find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018 ) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures <cite>(Ma et al., 2019</cite>; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. in a connectivity graph determined by line-of-sight in the physical environment. See the top row of Fig. 1 for a top-down environment illustration. In the VLN task, a virtual agent is placed at a particular viewpoint in an environment, and is given a natural language instruction (written by a human annotator) to follow. At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_4",
  "x": "**RELATED WORK** Vision and Language Navigation. Vision-andLanguage Navigation (VLN) (Anderson et al., 2018; Chen et al., 2019) unites two lines of work: first, of following natural language navigational instructions in an environmental context (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015; Mei et al., 2016; Fried et al., 2018a; Misra et al., 2018) , and second, of vision-based navigation tasks (Mirowski et al., 2017; Yang et al., 2019; Mirowski et al., 2018; Cirik et al., 2018 ) that use visually-rich real-world imagery (Chang et al., 2017) . A number of methods for the VLN task have been recently proposed. Wang et al. (2018) use model-based and model-free reinforcement learning to learn an environmental model and optimize directly for navigation success. Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> .",
  "y": "differences uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_5",
  "x": "See the top row of Fig. 1 for a top-down environment illustration. In the VLN task, a virtual agent is placed at a particular viewpoint in an environment, and is given a natural language instruction (written by a human annotator) to follow. At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop. When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe. In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of<cite> Ma et al. (2019)</cite> . These models obtained stateof-the-art results on the R2R dataset. Both models are based on the encoder-decoder approach (Cho et al., 2014 ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context. Compared to the SF model, the SM model introduces an improved visual-textual co-attention mechanism and a progress monitor component. We refer to the original papers for details on the two models. To analyze the models' visual grounding ability, we focus on their core encoder-decoder components.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_6",
  "x": "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split. For both the SF and SM models, we train two versions of the agents, using either the studentforcing or teacher-forcing approaches of Anderson et al. (2018) 1 , and select the best training snapshot on the val-seen split. 2 The results are shown in Table 1 . In each block, the two rows show the agent's performance (under the specific model architecture and training approach) with or without access to the visual features (\"RN\": ResNet-152 network (He et al., 2016) , \"no vis.\": non-visual). While visual features improve performance on environments seen during training, we see that for the SF architecture the non-visual agent (lines 1 and 3) outperforms the visual agent (lines 2 and 4) on unseen environments under both studentforcing and teacher-forcing training. For SM, the non-visual agent (lines 5 and 7) has a success rate very close to the visual agent (lines 6 and 8).",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_7",
  "x": "Both models are based on the encoder-decoder approach (Cho et al., 2014 ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context. Compared to the SF model, the SM model introduces an improved visual-textual co-attention mechanism and a progress monitor component. We refer to the original papers for details on the two models. To analyze the models' visual grounding ability, we focus on their core encoder-decoder components. In our experiments, we use models trained without data augmentation, and during inference predict actions with greedy search (i.e. without beam search, pragmatic, or progress monitorbased inference). For SF, we use the publicly released code. For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_8",
  "x": "We construct a set of vectors {x obj,j } representing detected objects and their attributes. Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (Pennington et al., 2014) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates. We then use the same visual attention mechanism as in Fried et al. (2018b) and<cite> Ma et al. (2019)</cite> to obtain an attended object representation x obj,att over these {x obj,j } vectors. We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\"). Then we train the SF model or the SM model using this object representation, with results shown in Table 2 . 3 For SF (lines 1-4), object representations substantially improve generalization ability: using either the object representation (\"Obj\") or the combined representation (\"RN+Obj\") obtains higher success rate on unseen environments than using only the ResNet features (\"RN\"), and the combined representation (\"RN+Obj\") obtains the highest overall performance. For SM (lines 5-8), Table 2 : Success rate (SR) of agents with different visual inputs on the R2R dataset (\"RN\": ResNet CNN, \"Obj\": objects, \"no vis.\": no visual representation). Models: Speaker-Follower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> . the model that uses only the object representation achieves the best performance (line 7). Here the success rates across the four settings are closer, and the improvement from object representation is smaller than for SF. However, in Sec. 5 we find that object representation can be combined with other inputs to further improve the performance.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_11",
  "x": "**A DETAILS ON THE COMPARED VLN MODELS** The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model<cite> (Ma et al., 2019)</cite> which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent. Both use an encoder LSTM (Hochreiter and Schmidhuber, 1997 ) to represent the instruction text, and a decoder LSTM to predict actions sequentially. At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction. While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input. The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t\u22121 , and then the attended visual and textual features are used as LSTM inputs to produce h t . The SF model only uses attended visual features as LSTM inputs and then produces textual attention based on updated LSTM state h t . Also, the visual attention weights are calculated with an MLP and batch-normalization in SM, while only a linear dot-product visual attention is used in SF. Empirically these differences produce large performance improvements for the SM model, which may contribute to the smaller gap between the SM model and its non-visual counterparts. B Details on the training mechanisms Anderson et al. (2018) compare two methods for training agents, which subsequent work on VLN has also used.",
  "y": "uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_0",
  "x": "****BERT FOR QUESTION GENERATION.**** **ABSTRACT** In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. And, the second one remedies the first one by restructuring the BERT employment into a sequential manner for taking information from previous decoded results. Our models are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best model yields state-of-the-art performance which advances the BLEU 4 score of existing best models from 16.85 to 21.04. ---------------------------------- **INTRODUCTION** Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_1",
  "x": "---------------------------------- **INTRODUCTION** Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> . The state-of-the-art models mainly adopt neural approaches by training a neural network based on the sequence-to-sequence framework. So far, the best performing result is reported in (Zhao et al., 2018) , which advances the state-of-the-art results from 13.9 to 16.8 (BLEU 4) . The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_2",
  "x": "When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance. For handling long context, the work (Zhao et al., 2018) introduces a maxout pointer mechanism with gated self-attention encoder for processing paragraphlevel input. The work reports state-of-the-art performance for QG tasks. Recently, the NLP community has seen excitement around neural learning models that make use of pre-trained language models (Devlin et al., 2018) (Radford et al., 2018) . The latest development is BERT, which has shown significant performance improvement over various natural language understanding tasks, such as document summarization, document classification, etc. In this study, we investigate the employment of the pretrained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. As will be shown in the experiment, naive employment of BERT offers poor performance, as, by construction, BERT produces all tokens at a time without considering decoding results in previous steps.",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_3",
  "x": "---------------------------------- **DATASETS** The SQuAD dataset contains 536 Wikipedia articles and around 100K reading comprehension questions (and the corresponding answers) posed about the articles. Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks. Table 1 summarizes some statistics for the compared datasets. \u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%). ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_4",
  "x": "The SQuAD dataset contains 536 Wikipedia articles and around 100K reading comprehension questions (and the corresponding answers) posed about the articles. Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks. Table 1 summarizes some statistics for the compared datasets. \u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%). ---------------------------------- **IMPLEMENTATION DETAILS** We use the PyTorch version of BERT 1 to train our BERT-QG and BERT-SQG models.",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_5",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** We use the PyTorch version of BERT 1 to train our BERT-QG and BERT-SQG models. The <cite>(Du et al., 2017)</cite> , and SQuAD 81K is the setting of (Zhao et al., 2018) . SQuAD 73K 73240 11877 10570  SQuAD 81K 81577 8964  8964 pre-trained model uses the officially provided BERT base model (12 layers, 768 hidden dimensions, and 12 attention heads.) with a vocab of 30522 words. Dropout probability is set to 0.1 between transformer layers. The Adamax optimizer is applied during the training process, with an initial learning rate of 5e-5. The batch size for the update is set at 28. All our models use two TITAN RTX GPUs for 5 epochs training. We use Dev.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_6",
  "x": "---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature. The compared models in the experiment are: \u2022 NQG-RC <cite>(Du et al., 2017)</cite> : A seq2seq question generation model based on bidirectional LSTM. \u2022 PLQG (Zhao et al., 2018) : A seq2seq network which contains a gated self-attention encoder and a maxout pointer decoder to enable the capability of handling long text input. PLQG model is the state-of-the-art models for QG tasks. Table 2 shows the comparison results using sentence-level context and Table 3 shows the results on paragraph level context. We compare the models using standard metric BLEU and ROUGE-L ( (Papineni et al., 2002) ). We have the following findings to note about the results.",
  "y": "similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_7",
  "x": "Also, in our BERT-SQG model, we use the Beam Search strategy for sequence decoding. The beam size is set to 3. ---------------------------------- **TRAIN TEST DEV** ---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature. The compared models in the experiment are: \u2022 NQG-RC <cite>(Du et al., 2017)</cite> : A seq2seq question generation model based on bidirectional LSTM. \u2022 PLQG (Zhao et al., 2018) : A seq2seq network which contains a gated self-attention encoder and a maxout pointer decoder to enable the capability of handling long text input.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_0",
  "x": "---------------------------------- **INTRODUCTION** The interpretability of systems based on deep neural network is required to be able to explain the reasoning behind the network prediction(s), that offers to (1) verify that the network works as expected and identify the cause of incorrect decision(s) (2) understand the network in order to improve data or model with or without human intervention. There is a long line of research in techniques of interpretability of Deep Neural networks (DNNs) via different aspects, such as explaining network decisions, data generation, etc. Erhan et al. (2009); Hinton (2012) ; Simonyan et al. (2013) and Nguyen et al. (2016) focused on model aspects to interpret neural networks via activation maximization approach by finding inputs that maximize activations of given neurons. Goodfellow et al. (2014) interprets by generating adversarial examples. However, Baehrens et al. (2010) and Bach et al. (2015) ; Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016) , relation extraction <cite>(Vu et al., 2016a</cite>; Miwa and Bansal, 2016; Gupta et al., 2016 Gupta et al., , 2018c , language modeling (Mikolov et al., 2010; Peters et al., 2018) , slot filling (Mesnil et al., 2015; Vu et al., 2016b) , machine translation (Bahdanau et al., 2014) , sentiment analysis (Wang et al., 2016; Tang et al., 2015) , semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d) . Past works (Zeiler and Fergus, 2014; have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_2",
  "x": "We demonstrate (1) LISA: \"How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response\" (2) Example2pattern: \"How the saliency patterns look like for each category in the data according to the network in decision making\". We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores. For an example sentence that is classified correctly, we identify and extract a saliency pattern (N-grams of words in order learned by the network) that contributes the most in prediction score. Therefore, the term example2pattern transformation for each category in the data. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling (SF) Shared Task (ST) to explain RNN predictions via the proposed LISA and example2pattern techniques. ---------------------------------- **CONNECTIONIST BI-DIRECTIONAL RNN** We adopt the bi-directional recurrent neural network architecture with ranking loss, proposed by<cite> Vu et al. (2016a)</cite> . The network consists of three parts: a forward pass which processes the original sentence word by word (Equation 1); a backward pass which processes the reversed sentence word by word (Equation 2); and a combination of both (Equation 3). The forward and backward passes are combined by adding their hidden layers.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_3",
  "x": "They named the neural architecture as 'Connectionist Bi-directional RNN' (C-BRNN). Figure 1 shows the C-BRNN architecture, where all the three parts are trained jointly. where w t is the word vector of dimension d for a word at time step t in a sentence of length n. <e1> demolition </e1> <e1> demolition </e1> was <e1> demolition </e1> was the <e1> demolition </e1> was the cause <e1> demolition </e1> was the cause of C-BRNN C-BRNN <e1> demolition </e1> was the cause of <e2> <e1> demolition </e1> was the cause of <e2> terror D is the hidden unit dimension. U f \u2208 R d\u00d7D and U b \u2208 R d\u00d7D are the weight matrices between hidden units and input w t in forward and backward networks, respectively; W f \u2208 R D\u00d7D and W b \u2208 R D\u00d7D are the weights matrices connecting hidden units in forward and backward networks, respectively. W bi \u2208 R D\u00d7D is the weight matrix connecting the hidden vectors of the combined forward and backward network. Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN. The ranking scheme offers to maximize the distance between the true label y + and the best competitive label c \u2212 given a data point x. It is defined as- where s \u03b8 (x) y + and s \u03b8 (x) c \u2212 being the scores for the classes y + and c \u2212 , respectively.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_4",
  "x": "<e1> demolition </e1> <e1> demolition </e1> was <e1> demolition </e1> was the <e1> demolition </e1> was the cause <e1> demolition </e1> was the cause of C-BRNN C-BRNN <e1> demolition </e1> was the cause of <e2> <e1> demolition </e1> was the cause of <e2> terror D is the hidden unit dimension. U f \u2208 R d\u00d7D and U b \u2208 R d\u00d7D are the weight matrices between hidden units and input w t in forward and backward networks, respectively; W f \u2208 R D\u00d7D and W b \u2208 R D\u00d7D are the weights matrices connecting hidden units in forward and backward networks, respectively. W bi \u2208 R D\u00d7D is the weight matrix connecting the hidden vectors of the combined forward and backward network. Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN. The ranking scheme offers to maximize the distance between the true label y + and the best competitive label c \u2212 given a data point x. It is defined as- where s \u03b8 (x) y + and s \u03b8 (x) c \u2212 being the scores for the classes y + and c \u2212 , respectively. The parameter \u03b3 controls the penalization of the prediction errors and m + and m are margins for the correct and incorrect classes. Following<cite> Vu et al. (2016a)</cite> , we set \u03b3 = 2, m + = 2.5 and m \u2212 = 0.5. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_5",
  "x": "We use word2vec (Mikolov et al., 2013) embeddings, that are updated during model training. As position features in relation classification experiments, we use position indicators (PI) (Zhang and Wang, 2015) in C-BRNN to annotate target entity/nominals in the word sequence, without necessity to change the input vectors, while it increases the length of the input word sequences, as four independent words, as position indicators (<e1>, </ e1>, <e2>, </e2>) around the relation arguments are introduced. In our analysis and interpretation of recurrent neural networks, we use the trained C-BRNN ( Figure 1 )<cite> (Vu et al., 2016a)</cite> model. ---------------------------------- **LISA AND EXAMPLE2PATTERN IN RNN** There are several aspects in interpreting the neural network, for instance via (1) Data: \"Which dimensions of the data are the most relevant for the task\" (2) Prediction or Decision: \"Explain why a certain pattern\" is classified in a certain way (3) Model: \"How patterns belonging to each category in the data look like according to the network\". In this work, we focus to explain RNN via decision and model aspects by finding the patterns that explains \"why\" a model arrives at a particular decision for each category in the data and verifies that model behaves as expected. To do so, we propose a technique named as LISA that interprets RNN about \"how it accumulates and builds meaningful semantics of a sentence word by word\" and \"how the saliency patterns look like according to the network\" for each category in the data while decision making. We extract the saliency patterns via example2pattern transformation. LISA Formulation: To explain the cumulative nature of recurrent neural networks, we show how does it build semantic meaning of a sentence word by word belonging to a particular category in the data and compute prediction scores for the expected category on different inputs, as shown in Figure 2 .",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_6",
  "x": "Observe that the subsequence '...cause of' is salient enough in decision making (i.e., prediction score=0.77), where the next subsequence '...cause of <e2>' adds in the score to get 0.98. Example2pattern for Saliency Pattern: To further interpret RNN, we seek to identify and extract the most likely input pattern (or phrases) for a given class that is discriminating enough in decision making. Therefore, each example input is transformed into a saliency pattern that informs us about the network learning. To do so, we first compute N-gram for each word w t in the sentence S. For instance, a 3-gram representation of w t is given by w t\u22121 , w t , w t+1 . Therefore, an N-gram (for N=3) sequence S of words is represented as [[w t\u22121 , w t , w t+1 ] n t=1 ], where w 0 and w n+1 are PADDING (zero) vectors of embedding dimension. Following<cite> Vu et al. (2016a)</cite> , we use N-grams (e.g., tri-grams) representation for each word in each subsequence S \u2264k that is input to C-BRNN to compute P (R|S \u2264k ), where the N-gram (N=3) subsequence S \u2264k is given by, for k \u2208 [1, n]. Observe that the 3-gram tri k con- (d) LISA for S4 < e 1 > c a r < / e 1 > l e f t t h e < e 2 > p l a n t < / e 2 > ---------------------------------- **ID**",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_7",
  "x": "In most cases, the context in between the two nominals define the relationship. However,<cite> Vu et al. (2016a)</cite> has shown that the extended context helps. In this work, we focus on the building semantics for a given sentence using relationship contexts between the two nominals. We analyse RNNs for LISA and example2pattern using two relation classification datsets: (1) SemEval10 Shared Task 8 (Hendrickx Input word sequence to C-BRNN pp <e1> 0.10 <e1> demolition 0.25 <e1> demolition </e1> 0.29 <e1> demolition </e1> was 0.30 <e1> demolition </e1> was the 0.35 <e1> demolition </e1> was the cause 0.39 <e1> demolition </e1> was the cause of 0.77 <e1> demolition </e1> was the cause of <e2> 0.98 <e1> demolition </e1> was the cause of <e2> terror 1.00 <e1> demolition </e1> was the cause of <e2> terror </e2> 1.00 Table 2 : Semantic accumulation and sensitivity of C-BRNN over subsequences for sentence S1. Bold indicates the last word in the subsequence. pp: prediction probability in the softmax layer for the relation type. The underline signifies that the pp is sufficient enough (\u03c4 =0.50) in detecting the relation. Saliency patterns, i.e., N-grams can be extracted from the input subsequence that leads to a sudden peak in pp, where pp \u2265 \u03c4 . et al., 2009) (2) TAC KBP Slot Filling (SF) shared task 1 . We demonstrate the sensitiveness of RNN for different subsequences (Figure 2) , input in the same order as in the original sentence.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_8",
  "x": "We demonstrate the sensitiveness of RNN for different subsequences (Figure 2) , input in the same order as in the original sentence. We explain its predictions (or judgments) and extract the salient relationship patterns learned for each category in the two datasets. ---------------------------------- **SEMEVAL10 SHARED TASK 8 DATASET** The relation classification dataset of the Semantic Evaluation 2010 (SemEval10) shared task 8 (Hendrickx et al., 2009) consists of 19 relations (9 directed relations and one artificial class Other), 8,000 training and 2,717 testing sentences. We split the training data into train (6.5k) and development (1.5k) sentences to optimize the C-BRNN Relation 3-gram Patterns 5-gram Patterns 7-gram Patterns </e1> cause <e2> the leading causes of <e2> is one of the leading causes of cause-</e1> caused a the main causes of <e2> is one of the main causes of effect(e1,e2) that cause respiratory </e1> leads to <e2> inspiration </e1> that results in <e2> hardening </e2> which cause acne </e1> that results in <e2> </e1> resulted in the <e2> loss </e2> leading causes of </e1> resulted in the <e2> <e1> sadness </e1> leads to <e2> inspiration caused due to </e1> has been caused by </e1> is caused by a <e2> comet comes from the </e1> are caused by the </e1> however has been caused by the causearose from an </e1> arose from an <e2> </e1> that has been caused by the effect (e2,e1) caused by the </e1> caused due to <e2> that has been caused by the <e2> radiated from a infection </e2> results in an <e1> product </e1> arose from an <e2> in a <e2> </e1> was contained in a </e1> was contained in a <e2> box was inside a </e1> was discovered inside a </e1> was in a <e2> suitcase </e2> contentcontained in a </e1> were in a <e2> </e1> were in a <e2> box </e2> container(e1,e2) hidden in a is hidden in a <e2> </e1> was inside a <e2> box </e2> stored in a </e1> was contained in a </e1> was hidden in an <e2> envelope </e1> released by </e1> issued by the <e2> <e1> products </e1> created by an <e2> product-</e1> issued by </e1> was prepared by <e2> </e1> by an <e2> artist </e2> who produce(e1,e2) </e1> created by was written by a <e2> </e1> written by most of the <e2> by the <e2> </e1> built by the <e2> temple </e1> has been built by <e2> of the <e1> </e1> are made by <e2> </e1> were founded by the <e2> potter </e1> of the </e1> of the <e2> device the <e1> timer </e1> of the <e2> whole(e1, e2) of the <e2> </e1> was a part of </e1> was a part of the romulan componentpart of the </e1> is part of the </e1> was the best part of the </e1> of <e2> is a basic element of </e1> is a basic element of the </e1> on a </e1> is part of a are core components of the <e2> solutions put into a have been moving into the </e1> have been moving back into <e2> released into the was dropped into the <e2> </e1> have been moving into the <e2> entity-</e1> into the </e1> moved into the <e2> </e1> have been dropped into the <e2> destination(e1,e2) moved into the were released into the <e2> </e1> have been released back into the added to the </e1> have been exported to power </e1> is exported to the <e2> </e1> are used </e1> assists the <e2> eye cigarettes </e1> are used by <e2> women used by <e2> </e1> are used by <e2> <e1> telescope </e1> assists the <e2> eye instrument-</e1> is used </e1> were used by some <e1> practices </e1> for <e2> engineers </e2> agency(e1,e2) set by the </e1> with which the <e2> the best <e1> tools </e1> for <e2> </e1> set by readily associated with the <e2> <e1> wire </e1> with which the <e2> The <e1> demolition </e1> was the cause of <e2> terror </e2> and communal divide is just a way of not letting truth prevail. \u2192 cause-effect(e1,e2) The terms demolition and terror are the relation arguments or nominals, where the phrase was the cause of is the relationship context between the two arguments. Table 1 shows the examples sentences (shortened to argument1+relationship context+argument2) drawn from the development and test sets that we employed to analyse the C-BRNN for semantic accumulation in our experiments. We use the similar experimental setup as<cite> Vu et al. (2016a)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_0",
  "x": "**LEVERAGING KNOWLEDGE GRAPH FOR DISTRIBUTED REPRESENTATIONS** The potential of semantic representations of words learned through a neural approach has been introduced in [10, 15] , opening several perspectives in natural language processing and IR tasks. Beyond words, several works focused on the representation of sentences [11] , documents [9] , and also knowledge bases (KBs) [3, 18] . Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_1",
  "x": "While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively. While the relationalbased regularization function characterizes the word relationships which are interpreted as translations in latent semantic space of word embeddings, the categorical-based one aims at minimizing the weighted distance between words with same attributes. With experiments on text mining and NLP tasks, the authors have reported that combining these two regularization functions allows to significantly improve the quality of word representations. In the same mind, Yu et al. [19] propose a relation constrained model (RCM) that extends the CBOW model [10] with a function based on prior relational knowledge issued from an external resource.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_2",
  "x": "The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations. For each word, the retrofitting approach learns its new representation by minimizing both (1) its distance with the representation of all connected words in the semantic graph and (2) its distance with the pre-learned word embedding, namely its initial distributed representation. In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively. While the relationalbased regularization function characterizes the word relationships which are interpreted as translations in latent semantic space of word embeddings, the categorical-based one aims at minimizing the weighted distance between words with same attributes. With experiments on text mining and NLP tasks, the authors have reported that combining these two regularization functions allows to significantly improve the quality of word representations. In the same mind, Yu et al. [19] propose a relation constrained model (RCM) that extends the CBOW model [10] with a function based on prior relational knowledge issued from an external resource. Thus, the final objective of the model is to learn the pure distributed representation in the text corpus and also to capture the semantic relationship between words from external resources. In addition to word similarity tasks, the literature review shows that KBs are also exploited in question-answering tasks.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_3",
  "x": "While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation <cite>[6,</cite> 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching. Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4] . With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB. On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16] . On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding<cite> [6]</cite> or a richer embedding representation of a KB sub-graph, as suggested in [2] . The latter presents the advantage to model the compositionality of concepts within the document. Similarly to previous approaches [8, 16, 17] , the enhanced representations of both document and query would be transformed into low-dimensional semantic feature vectors used within a similarity function. ---------------------------------- **USING KB TRANSLATION MODEL FOR IR** While the first model exploits knowledge bases to enhance the representation of a document-query pair and their similarity score, an alternative approach consists in a ranking model based on the translation role of the knowledge resource.",
  "y": "differences"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_4",
  "x": "As illustrated in Figure 2 , we suggest using a deep neural approach to achieve two levels of representations: 1) an enhanced knowledge-based representation of the document and the query and 2) a distinct representation of the document and the query surrounding by a third KB-based representation aiming at improving the semantic closeness of document and query representations. While in the first approach, a KB is used as a mean of document and query representation enhancement, the KB is exploited in the latter approach as a mean for document-query translation. ---------------------------------- **LEVERAGING ENHANCED REPRESENTATIONS OF TEXT USING KB FOR IR** The first approach that we suggest for integrating KB within a deep neural network focuses on an enhanced representation of documents and queries as illustrated in Figure 2a . While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation <cite>[6,</cite> 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching. Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4] . With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB. On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16] . On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding<cite> [6]</cite> or a richer embedding representation of a KB sub-graph, as suggested in [2] .",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_0",
  "x": "Currently, most of the state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Chen et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) only focus on how to improve accuracy. However, accuracy is not the only metric to score a given VQA model. Robustness is also a crucial property. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017; Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of VQA algorithms after extensive literature review. To Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Regarding the input question of Module 2, it is the direct concatenation of a given main question with 3 basic questions of the main question. Here, \"\u2295\" denotes the direct concatenation of basic questions. the best of our knowledge, this paper is the first work to analyze the robustness of VQA models.",
  "y": "motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_1",
  "x": "Based on this observation, we consider the basic question of given main question as noise to the given main question. That is to say, a basic question with larger similarity score to the main question is considered smaller noise to the main question and a basic question with smaller similarity score is larger noise to the main question. The above interesting concepts inspire us to propose a method, Visual Question Answering by Basic Questions (VQABQ) illustrated in Figure 1 , to measure the robustness of VQA models. In the VQABQ model, there are two modules: the basic question generation module (Module 1) and the visual question answering module (Module 2). We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors , as the input of Module 1. In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ. We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_2",
  "x": "In the VQABQ model, there are two modules: the basic question generation module (Module 1) and the visual question answering module (Module 2). We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors , as the input of Module 1. In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ. We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) . Note that the available pretrained VQA models can be categorized into two main categories, attention-based and non-attention-based VQA models. According to the results of our experiments, we discover that attention-based models not only have the higher accuracy but also the better robustness compared with nonattention-based models. In this work, our main contributions are summarized below:",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_3",
  "x": "---------------------------------- **RELATED WORK** Recently, there are many papers (<cite>Antol et al. 2015</cite>; Shih, Singh, and Hoiem 2016; Chen et al. 2016; Kafle and Kanan 2016; Ma, Lu, and Li 2016; Ren, Kiros, and Zemel 2015; Zhu et al. 2016; Wu et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) have proposed methods to solve the challenging VQA task. Our VQABQ method involves in different areas in Machine Learning, Natural Language Processing (NLP) and Computer Vision. The following, we discuss recent works related to our approach. Sequence Modeling by Recurrent Neural Networks. Recurrent Neural Networks (RNN) can handle the sequences of flexible length. Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) is a particular variant of RNN and in natural language tasks, such as machine translation (Sutskever, Vinyals, and Le 2014; , LSTM is a successful application. In (Ren, Kiros, and Zemel 2015) , they exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar. The input in (Malinowski, Rohrbach, and Fritz 2015; is the concatenation of each word embedding with the same feature vector of the image.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_4",
  "x": "The model has two main parts, Module 1 and Module 2. Regarding Module 1, it will take the encoded MQ as the input and uses the encoded BQ matrix to output the BQ of query question. Then, Module 2 is a VQA model we want to analyze, and it will take the concatenation of the output of Module 1 and MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 also can be referred to Figure 1 . ---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_5",
  "x": "At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well. ---------------------------------- **QUESTION ENCODING** There are many popular text encoders, such as Word2Vec (Mikolov et al. 2013) , GloVe (Pennington, Socher, and Manning 2014) and Skip-Thoughts . In these encoders, Skip-Thoughts not only can focus on the word-toword meaning but also the whole sentence semantic meaning. So, we choose Skip-Thoughts to be our question encoding method.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_6",
  "x": "The model has two main parts, Module 1 and Module 2. Regarding Module 1, it will take the encoded MQ as the input and uses the encoded BQ matrix to output the BQ of query question. Then, Module 2 is a VQA model we want to analyze, and it will take the concatenation of the output of Module 1 and MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 also can be referred to Figure 1 . ---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_7",
  "x": "---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well. ---------------------------------- **QUESTION ENCODING**",
  "y": "similarities uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_8",
  "x": "So, the hidden state h N i can represent the whole question. For convenience, here we drop the index i and iterate the following sequential equations to encode a question: , where U r , U z , W r , W z , U and W are the matrices of weight parameters.h t is the state update at time step t, r t is the reset gate, denotes an element-wise product and z t is the update gate. These two update gates take the values between zero and one. ---------------------------------- **PROBLEM FORMULATION** Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way: (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_9",
  "x": ", where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term. ---------------------------------- **BASIC QUESTION GENERATION** We now describe how to generate the BQ of a query question, illustrated in Figure 1 . According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) by Skip-Thought Vectors, and then we have a matrix of basic question candidates. Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027. Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_10",
  "x": "Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_11",
  "x": "**BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ---------------------------------- **ROBUSTNESS ANALYSIS BY BASIC QUESTIONS** To measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance. The noise can be completely random, have a specific structure and/or be semantically relevant to the final task.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_12",
  "x": "In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ---------------------------------- **ROBUSTNESS ANALYSIS BY BASIC QUESTIONS** To measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance. The noise can be completely random, have a specific structure and/or be semantically relevant to the final task. For VQA the input is an image question pair and therefore the noise should be introduced to both. The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_13",
  "x": "Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_14",
  "x": "For VQA the input is an image question pair and therefore the noise should be introduced to both. The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017;  Table 3 : MUTAN without Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review. Here we propose a novel robustness measure for VQA by introducing semantically relevant noise to the questions where we can control the strength of noisiness. First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa . Then, we append the top ranked k BQs to each of the MQs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it Acc bqd . Finally, we compute the absolute difference Acc dif f = |Acc vqa \u2212 Acc bqd | and we report the robustness score R score .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_15",
  "x": "In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review. Here we propose a novel robustness measure for VQA by introducing semantically relevant noise to the questions where we can control the strength of noisiness. First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa . Then, we append the top ranked k BQs to each of the MQs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it Acc bqd . Finally, we compute the absolute difference Acc dif f = |Acc vqa \u2212 Acc bqd | and we report the robustness score R score . ,where 0 \u2264 t < m \u2264 100. The parameters t and m are the tolerance and the maximum robustness limit, respectively, i.e., the robustness score R score decreases smoothly between 1 and 0 as Acc dif f moves from t to m and remain constant out side this range. The rate of change of this transition is exponentially decreasing from exponential to sublinear in the range [t, m] .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_16",
  "x": "---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_17",
  "x": "**DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one.",
  "y": "background uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_18",
  "x": "---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_19",
  "x": "There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ. Furthermore, although we rank all of the basic question candidates for each MQ, we only collect top 21 BQ to put into our BQD. The most important reason is that the similarity scores are too small after twentyfirst BQ. Regarding the limit of number of words of question input, for most of available pretrained state-of-the-art VQA models they are trained under the condition maximum number of words of input 26 words. Based on the above limitation, we divide each 21 ranked BQs into 7 partitions to do detailed analysis, referring Table 1 to Table 5 , because the total number of words of each MQ with 3 BQs is less than or equal to 26 words. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_20",
  "x": "In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ. Furthermore, although we rank all of the basic question candidates for each MQ, we only collect top 21 BQ to put into our BQD. The most important reason is that the similarity scores are too small after twentyfirst BQ. Regarding the limit of number of words of question input, for most of available pretrained state-of-the-art VQA models they are trained under the condition maximum number of words of input 26 words. Based on the above limitation, we divide each 21 ranked BQs into 7 partitions to do detailed analysis, referring Table 1 to Table 5 , because the total number of words of each MQ with 3 BQs is less than or equal to 26 words. ---------------------------------- **EVALUATION METRICS**",
  "y": "motivation uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_21",
  "x": "Regarding the limit of number of words of question input, for most of available pretrained state-of-the-art VQA models they are trained under the condition maximum number of words of input 26 words. Based on the above limitation, we divide each 21 ranked BQs into 7 partitions to do detailed analysis, referring Table 1 to Table 5 , because the total number of words of each MQ with 3 BQs is less than or equal to 26 words. ---------------------------------- **EVALUATION METRICS** <cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_22",
  "x": "---------------------------------- **EVALUATION METRICS** <cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where N is the total number of examples, I[\u00b7] denotes an indicator function, a i is the predicted answer and T i is an answer set of the i th example. That is, a predicted answer is considered as a correct one if at least 3 annotators agree with it and the score depends on the total number of agreements when the predicted answer is not correct.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_23",
  "x": "However, if we compare the quality of BQ of \u03bb equal to 10 \u22125 with \u03bb equal to 10 \u22126 , we think the BQ quality of \u03bb equal to 10 \u22126 is slightly better than \u03bb equal to 10 \u22125 based on our common sense knowledge. We will put some randomly selected BQ examples from our BQD in the supplementary material for references. Note that Figure 2 : The accuracy of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition. Model LQI HAV HAR MU MUA MLB R score 0.19 0.48 0.45 0.30 0.34 0.36 we use the state-of-the-art question encoder, skip-thoughts , to encode all questions. iii.) Who Is The Robustest VQA Model? According to Table 6 , there are two categories, attention-based and nonattention-based, of VQA models. We can discover that attention-based VQA models are more robust than nonattention-based VQA models. Furthermore, the only difference between MU and MUA is the attention mechanism. Based on the above observation, we can say that attention mechanism can help robustness.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_24",
  "x": "We want to do more advanced analysis on this model, so we claim that if the quality of Figure 3 : The accuracy decrement of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition. score1 score2/score1 score3/score2 avg 0.33 0.61 0.73 std 0.20 0.27 0.21 Table 7 : \"avg\" denotes average and \"std\" denotes standard deviation. BQs are well enough, then even we use the naive concatenation, i.e. direct concatenation, method to concatenate MQ and BQs, it still can directly help the accuracy of VQA models. For justifying our claim, we propose a thresholding criterion, referring to Table 8 , to select the BQ with good quality. v.) Basic Question Concatenation. In this subsection, we propose a thresholding criterion to select the BQ with good quality. In BQD, each MQ has 21 corresponding BQ with scores. We can have the following format, {M Q, (BQ1, score1), ..., (BQ21, score21)}, and these scores are all between 0 and 1 with the following order:",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_26",
  "x": [
   "Although we only have 3.16% of testing questions can benefit from the basic questions, our method still can improve the state-of-the-art accuracy from 60.32% to 60.34%, referring to supplementary material. Then, we have 244302 testing questions, so that means the number of correctly answering questions of our method is more than state-of-the-art method 49 questions. In other words, if we have well enough basic question dataset, we can increase accuracy more, especially in the counting-type question, referring to supplementary material. We conjecture that because the Co-Attention Mechanism is good at localizing, the counting-type question is improved more than others. Accordingly, based on our experiment, we believe that basic questions with well enough quality can directly help accuracy by only using the naive concatenation method. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we propose a novel VQABQ method and Basic Question Dataset (BQD) for robustness analysis of VQA models. The VQABQ method has two main modules, Basic Question Generation Module and VQA Module. The former one can generate the basic questions for the query question, and the latter one can take an image, basic and query questions as the input and then output the text-based answer of the query question about the given image."
  ],
  "y": "uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_0",
  "x": "It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods. ---------------------------------- **INTRODUCTION** Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (Lafferty et al., 2001; Ratinov and Roth, 2009) , where conditional random fields (CRF) (Lafferty et al., 2001) has been proven as an effective framework, by taking discrete features as the representation of input sequence (Sha and Pereira, 2003; Keerthi and Sundararajan, 2007) . With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015;<cite> Ma and Hovy, 2016</cite>; Peters et al., 2017) . Features are extracted automatically through network structures including long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and convolution neural network (CNN) (LeCun et al., 1989) with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies (Collobert et al., 2011; Lample et al., 2016; Peters et al., 2017) . There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++ 2 , CRFSuite (Okazaki, 2007) and FlexCRFs (Phan et al., 2004) , which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_1",
  "x": "There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++ 2 , CRFSuite (Okazaki, 2007) and FlexCRFs (Phan et al., 2004) , which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits. Although many authors released their code along with their sequence labeling papers (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding. In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand- crafted features to CRF framework conveniently. We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer. NCRF++ is: \u2022 Fully configurable: users can design their neural models only through a configuration file without any code work. Figure 1 shows a segment of the configuration file.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_2",
  "x": "It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as <cite>Ma and Hovy (2016)</cite> ), plus POS and Cap features, within 10 lines. This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit. \u2022 Function enriched: NCRF++ extends the Viterbi algorithm (Viterbi, 1967) to enable decoding n best sequence labels with their probabilities.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_3",
  "x": "In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit. \u2022 Function enriched: NCRF++ extends the Viterbi algorithm (Viterbi, 1967) to enable decoding n best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typical examples, we investigate the performance of models built in NCRF++, the influence of humandefined and automatic features, the performance of nbest decoding and the running speed with the batch size. Detail results are shown in Section 3. ---------------------------------- **NCRF++ ARCHITECTURE**",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_4",
  "x": "This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit. \u2022 Function enriched: NCRF++ extends the Viterbi algorithm (Viterbi, 1967) to enable decoding n best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typical examples, we investigate the performance of models built in NCRF++, the influence of humandefined and automatic features, the performance of nbest decoding and the running speed with the batch size.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_5",
  "x": "---------------------------------- **WORD SEQUENCE LAYER** Similar to the character sequence layer, NCRF++ supports both RNN and CNN as the word sequence feature extractor. The selection can be configurated through word seq feature in Figure 1 . The input of the word sequence layer is a word representation, which may include word embeddings, character sequence representations and handcrafted neural features (the combination depends on the configuration file). The word sequence layer can be stacked, building a deeper feature extractor. \u2022 Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (Huang et al., 2015; Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017) . Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both directions on each word are concatenated to represent the corresponding word. \u2022 Word CNN utilizes the same sliding window as character CNN, while a nonlinear function (Glorot et al., 2011) is attached with the extracted features.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_6",
  "x": "Sang and Buchholz, 2000) , data split is following Reimers and Gurevych (2017) . For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, \"Nochar\" suggests a model without character sequence information. \"CLSTM\" and \"CCNN\" represent models using LSTM and CNN to encode character sequence, respectively. Similarly, \"WL-STM\" and \"WCNN\" indicate that the model uses LSTM and CNN to represent word sequence, respectively.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_7",
  "x": "**SETTINGS** To evaluate the performance of our toolkit, we conduct the experiments on several datasets. Sang and Buchholz, 2000) , data split is following Reimers and Gurevych (2017) . For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, \"Nochar\" suggests a model without character sequence information.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_8",
  "x": "As shown in Table 1 , \"WCNN\" based models consistently underperform the \"WLSTM\" based models, showing the advantages of LSTM on capturing global features. Character information can improve model performance significantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to \"CLSTM+WLSTM+CRF\" and \"CCNN+WLSTM+CRF\" of our models) (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017; Peters et al., 2017) . Our implementations can achieve comparable results, with better NER and chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the robustness of our implementation. The full experimental results and analysis are published in Yang et al. (2018) . ---------------------------------- **INFLUENCE OF FEATURES** We also investigate the influence of different features on system performance. Table 2 shows the results on the NER task.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_0",
  "x": "For example, predicting the sentiment of Twitter texts (Kouloumpis et al., 2011) or news articles (Balahur et al., 2013) . However, human language is multimodal in, for instance, face-toface communication and online multimedia opinion sharing. Understanding natural language used in such scenarios is especially important for NLP applications in Human-Computer/Robot Interaction. Thus, in recent years there has been growing interest in multimodal sentiment analysis. The three most widely studied modalities in current multimodal sentiment analysis research are: vocal (e.g., speech acoustics), visual (e.g., facial expressions), and verbal (e.g., lexical content). These are sometimes referred to as \"the three Vs\" of communication (Mehrabian et al., 1971) . Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics). It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance<cite> (Zadeh et al., 2018a)</cite> . While multimodal approaches to sentiment analysis are relatively new in NLP, multimodal emotion recognition has long been a focus of Affective Computing. For example, De Silva and Ng (2000) combined facial expressions and speech acoustics to predict the Big-6 emotion categories (Ekman, 1992) .",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_1",
  "x": "Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005. We use the CMU Multimodal Data Software Development Kit (SDK)<cite> (Zadeh et al., 2018a)</cite> to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets. 1 We implement the sentiment analysis models using the Keras deep learning library (Chollet et al., 2015) . ---------------------------------- **MULTIMODAL FEATURES** For the vocal modality, we use the COVAREP feature set provided by the SDK. These are 74 vocal features including 12 Mel-frequency cepstral coefficients, pitch tracking and voiced/unvoiced segmenting features, glottal source parameters, peak slope parameters, and maxima dispersion quotients. The vocal features are extracted from the audio recordings at a sampling rate of 100Hz. For the visual modality, we use the FACET feature set provided by the SDK. These are 46 visual features including facial indicators of 9 types of emotion (anger, contempt, disgust, fear, joy, sadness, surprise, frustration, and confusion) and movements of 20 facial action units.",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_2",
  "x": "We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion. This is because in previous studies (e.g., <cite>Zadeh et al. (2018a)</cite> ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective. 4 ---------------------------------- **EXPERIMENTS AND RESULTS** Here we report our sentiment score prediction experiments. 2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model. To evaluate the performance of sentiment score prediction, following previous work<cite> (Zadeh et al., 2018a)</cite> , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant. We also include random prediction as a baseline and the human performance reported by .",
  "y": "similarities background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_3",
  "x": "**EXPERIMENTS AND RESULTS** Here we report our sentiment score prediction experiments. 2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model. To evaluate the performance of sentiment score prediction, following previous work<cite> (Zadeh et al., 2018a)</cite> , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant. We also include random prediction as a baseline and the human performance reported by . ---------------------------------- **UNIMODAL EXPERIMENTS** The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ).",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_4",
  "x": "---------------------------------- **UNIMODAL EXPERIMENTS** The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ). This suggests that lexical information remains the most effective for sentiment analysis. On each modality, the best performance is achieved by a multi-task learning model. This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning. In multi-task learning, the main task gains additional information from the auxillary tasks. Compared to the S model, the S+P model has increased focus on the polarity of sentiment, while the S+I model has increased focus on the intensity of sentiment. On the verbal modality, the S+P model achieved the best performance, while on the visual modality the S+I model achieved the best performance.",
  "y": "similarities"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_0",
  "x": "Consequently, we investigate the limitations of variance based embedding post-processing techniques and demonstrate that such post-processing is counterproductive in a number of scenarios such as sentence classification and machine translation tasks. Finally, we offer a few guidelines on variance based embedding post-processing. We have released the source code along with the paper 1 . * equal contribution 1 Code Link: https://github.com/aclsrw/anonymized_code ---------------------------------- **INTRODUCTION** Word embeddings have revolutionized natural language processing by representing words as dense real-valued vectors in a low dimensional space. Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) .",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_1",
  "x": "Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) . Researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from (Yin and Shen, 2018) answering the question of optimal dimensionality selection. In contrast to previous work, we explore the dimensional properties of existing pre-trained word embeddings through their principal components. Specifically, our contributions are as follows: 1. We analyze the word embeddings in terms of their principal components and demonstrate that their performance on both word similarity and sentence classification tasks saturates well before the full dimensionality. 2. We demonstrate that the amount of variance captured by the principal components is a poor representative for the downstream performance of the embeddings constructed using the very same principal components. 3. We investigate the reasons behind the aforementioned result through syntactic information based dimensional linguistic probing tasks and demonstrate that the syntactic information captured by a principal component is independent of the amount of variance it explains. 4. We point out the limitations of applying variance based post-processing <cite>(Mu and Viswanath, 2018)</cite> and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement.",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_2",
  "x": "In contrast to previous work, we explore the dimensional properties of existing pre-trained word embeddings through their principal components. Specifically, our contributions are as follows: 1. We analyze the word embeddings in terms of their principal components and demonstrate that their performance on both word similarity and sentence classification tasks saturates well before the full dimensionality. 2. We demonstrate that the amount of variance captured by the principal components is a poor representative for the downstream performance of the embeddings constructed using the very same principal components. 3. We investigate the reasons behind the aforementioned result through syntactic information based dimensional linguistic probing tasks and demonstrate that the syntactic information captured by a principal component is independent of the amount of variance it explains. 4. We point out the limitations of applying variance based post-processing <cite>(Mu and Viswanath, 2018)</cite> and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement. In Section 2 we discuss dimensional properties of word embeddings. In Section we 3 conduct a variance based analysis by measuring performance of word embeddings on downstream tasks. In Section 4 we move on to dimensional linguistic probing tasks followed by Section 5 where we discuss the post-processing algorithm, and finally conclude in Section 6. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_3",
  "x": "To evaluate the syntactic information contained in each of the principal components, we first construct one-dimensional word embeddings by projecting word vectors onto a single principal component and then use sentence vectors constructed by using these embeddings for solving the TreeDepth and TopConst tasks. Figure 3 depicts the scores (Test accuracy) on TopConst and TreeDepth tasks respectively. Evidently, no single principal component (dimension) achieves a significantly higher score in any of the two tasks and the performance across the dimensions does not have any particular trend (increasing or decreasing). This validates the hypothesis that the principal components do not vary disproportionately in terms of the syntactic information contained. ---------------------------------- **THE POST PROCESSING ALGORITHM (PPA)** In this section, we first describe and then evaluate the post-processing algorithm (PPA) proposed in <cite>(Mu and Viswanath, 2018)</cite> , which achieves high scores on Word and Semantic textual similarity tasks. The algorithm removes the projections of top principal components from each of the word vectors, making the individual word vectors more discriminative (Refer to Algorithm 1 for details). Algorithm 1: Post Processing Algorithm PPA(X, D) Data: Embedding Matrix X, Threshold Parameter D Result: Post-Processed Word Embedding Matrix X 1 X = X -X ; // Subtract Mean Embedding / * Compute PCA Components * / 2 ui = PCA(X), where i = 1, 2 . .",
  "y": "uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_0",
  "x": "In other areas of (aspectbased) sentiment analysis, releasing code for published systems has not been a high priority, e.g. in SemEval 2016 task 5 (Pontiki et al., 2016) only 1 out of 21 papers released their source code. In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002) . Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) <cite>(Dong et al., 2014)</cite> , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF . Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise. Overall, within the field of sentiment analysis there are other granularities such as sentence level (Socher et al., 2013) , topic (Augenstein et al., 2018) , and aspect (Wang et al., 2016; Tay et al., 2017) . Aspect-level sentiment analysis relates to identifying the sentiment of (potentially multiple) topics in the same text although this can be seen as a similar task to TDSA.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_1",
  "x": "Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN). Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative evaluation between the different methods is somewhat difficult. This has serious implications for generalisability of methods. We correct that limitation in our study. There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset <cite>(Dong et al., 2014)</cite> and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_2",
  "x": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. ---------------------------------- **DATASETS USED IN OUR EXPERIMENTS** We are evaluating our models over six different English datasets deliberately chosen to represent a range of domains, types and mediums. As highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results. In this paper, we do not consider the quality or inter-annotator agreement levels of these datasets but it has been noted that some datasets may have issues here. For example, Pavlopoulos and Androutsopoulos (2014) point out that the Hu and Liu (2004) dataset does not state their inter-annotator agreement scores nor do they have aspect terms that express neutral opinion.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_3",
  "x": "First, the time it takes to write parsers and run the models. Second, we only used datasets that contain three distinct sentiments (Wilson (2008) only has two). From the datasets we have used, we have only had issue with parsing Wang et al. (2017) where the annotations for the first set of the data contains the target span but the second set does not. Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset. An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans. As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of<cite> Dong et al. (2014)</cite> and Mitchell et al. (2013) . The only dataset that has a small difference between the number of unique sentiments per sentence is the Wang et al. (2017) ---------------------------------- **REPRODUCTION STUDIES** In the following subsections, we present the three different methods that we are reproducing and how their results differ from the original analysis.",
  "y": "differences"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_4",
  "x": "Vo and Zhang (2015) created the first NP method for TDSA. It takes the word vectors of the left, right, target word, and full tweet/sentence/text contexts and performs max, min, average, standard deviation, and product pooling over these contexts to create a feature vector as input to the Support Vector Machine (SVM) classifier. This feature vector is in affect an automatic feature extractor. They created four different methods: 1. Target-ind uses only the full tweet context, 2. Target-dep-uses left, right, and target contexts, 3. Target-dep Uses both features of Target-ind and Target-dep-, and 4. Target-dep+ Uses the features of Target-dep and adds two additional contexts left and right sentiment (LS & RS) contexts where only the words within a specified lexicon are kept and the rest of the words are zero vectors. All of their experiments are performed on<cite> Dong et al. (2014)</cite> Twitter data set. For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_5",
  "x": "**SCALING AND FINAL MODEL COMPARISON** We test all of the methods on the test data set of<cite> Dong et al. (2014)</cite> and show the difference between the original and reproduced models in figure 2 . Finally, we show the effect of scaling using Max Min and not scaling the data. As stated before, we have been using Max Min scaling on the NP features, however did not mention scaling in their paper. The library they were using, LibLinear (Fan et al., 2008) , suggests in its practical guide (Hsu et al., 2003) to scale each feature to [0, 1] but this was not re-iterated by . We are using scikit-learn's (Pedregosa et al., 2011) LinearSVC which is a wrapper of LibLinear, hence making it appropriate to use here. As can be seen in figure 2, not scaling can affect the results by around one-third. ---------------------------------- **REPRODUCTION OF WANG ET AL. (2017)** Wang et al. (2017) extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_6",
  "x": "The library they were using, LibLinear (Fan et al., 2008) , suggests in its practical guide (Hsu et al., 2003) to scale each feature to [0, 1] but this was not re-iterated by . We are using scikit-learn's (Pedregosa et al., 2011) LinearSVC which is a wrapper of LibLinear, hence making it appropriate to use here. As can be seen in figure 2, not scaling can affect the results by around one-third. ---------------------------------- **REPRODUCTION OF WANG ET AL. (2017)** Wang et al. (2017) extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word. Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2. TDParse the feature of TDParse-and the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the<cite> Dong et al. (2014)</cite> and Wang et al. (2017) Twitter datasets where we train and test on the previously specified train and test splits.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_7",
  "x": "We also scale our features using Max Min scaling before inputting into the SVM. We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets. The results of these experiments can be seen in figure 3 10 . As found with the results of replication, scaling is very important but is typically overlooked when reporting. Tang et al. (2016a) was the first to use LSTMs specifically for TDSA. They created three different models: 1. LSTM a standard LSTM that runs over the length of the sentence and takes no target information into account, 2. TDLSTM runs two LSTMs, one over the left and the other over the right context of the target word and concatenates the output of the two, and 3. TCLSTM same as the TDLSTM method but each input word vector is concatenated with vector of the target word. All of the methods outputs are fed into a softmax activation function. The experiments are performed on the<cite> Dong et al. (2014)</cite> dataset where we train and test on the specified splits.",
  "y": "similarities uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_0",
  "x": "**ABSTRACT** We present a simple method to learn continuous representations of dependency substructures (links), with the motivation of directly working with higher-order, structured embeddings and their hidden relationships, and also to avoid the millions of sparse, template-based word-cluster features in dependency parsing. These link embeddings allow a significantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. ---------------------------------- **INTRODUCTION** Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_1",
  "x": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features). At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_2",
  "x": "While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features). At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly.",
  "y": "motivation differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_3",
  "x": "Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features). At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly. We make our link embeddings publicly available 1 and hope that they will prove useful in various other NLP tasks in future work, e.g., as dense, syntactic features in sentence classification or as linguistically-intuitive, initial units in vectorspace composition.",
  "y": "motivation differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_4",
  "x": "To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec. 2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to <cite>Bansal et al. (2014)</cite> and Levy and Goldberg (2014) . The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc. To this end, we first parse the BLLIP corpus (minus the PTB portion) 3 using the baseline MSTParser (McDonald et al., 2005b) . Next, for each predicted link, we create a tuple, consisting of the parent-child pair p-c (concatenated as a single unit, same as p c) and its various properties such as the N.Y.-Yonkers, Md.-Columbia, N.Y.-Bronx, Va.-Reston, Ky.-Lexington, Mich.-Kalamazoo, Calif.-Calabasas, . .. boost-revenue, tap-markets, take-losses, launch-fight, reduce-holdings, terminate-contract, identify-bidders, ... boosting-bid, meeting-schedules, obtaining-order, having-losses, completing-review, governing-industry, ... says-mean, adds-may, explains-have, contend-has, recalls-had, figures-is, asserted-is, notes-would, ... would-Based, is-Besides, was-Like, is-From, are-Despite, said-Besides, says-Despite, reported-As, ... began-Meanwhile, was-Since, are-Often, would-Now, had-During, were-Over, was-Late, have-Until, ... Catsimatidis-Mr., Swete-Mr., Case-Mr., Montoya-Mr., Byerlein-Mr., Heard-Mr., Leny-Mr., Graham-Mrs., ... only-1.5, about-170, nearly-eight, approximately-10, almost-15, some-80, Only-two, about-23, roughly-50, ... link's dependency relation label l, the grandparent dependency relation label gl, and the signed, binned distance d: We then run the skip-gram model on the the above context tuples (Eq. 1) with a window-size of 2, dimension-size of 100, and a min-count cutoff of 4 to give us a vocabulary of around 92K. 4 We also tried other context settings, e.g., where we add more lexicalized, link-based context to the tuple such as the neighboring grandparent-parent link gp-p: but the setting in Eq. 1 performs slightly better (based on the development set). Clusters: Table 1 shows example clusters obtained by clustering link embeddings via MAT-LAB's linkage + cluster commands, with 1000 clusters. 5 We can see that these link embeddings are able to capture useful groups and subtle distinctions directly at the link level (without having to work with all pairs of word types), e.g., based on syntactic properties like capitalization, verb form, position in sentence; and based on topics like location, time, finance, etc.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_5",
  "x": "**FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_6",
  "x": "Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS** For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_7",
  "x": "Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN). Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS** For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_8",
  "x": "Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS** For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory. Moreover, regarding the preprocessing time taken to generate these various feature types, our Bucket features, which just need the fast word2vec training, take 2-3 orders of magnitude lesser time than the BROWN features (15 mins.",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_9",
  "x": "Next, we again fire a small set of unary indicator features that simply con- sist of the link's bit-string prefix, the prefix-length, and another feature that adds the signed, bucketed distance of that link in the sentence. 6 ---------------------------------- **SETUP AND RESULTS** For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory. Moreover, regarding the preprocessing time taken to generate these various feature types, our Bucket features, which just need the fast word2vec training, take 2-3 orders of magnitude lesser time than the BROWN features (15 mins.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_10",
  "x": "The Bit-string features additionally need hierarchical clustering, but are still at least twice as fast as BROWN features. training and parsing with representations of new domains or languages. Table 3 shows the main UAS (unlabeled attachment score) results on WSJ, where each '+ X' row denotes adding type X features to the MSTParser baseline. All the final test improvements, i.e., Bucket (92.3) and Bit-string (92.6) w.r.t. Baseline (91.9), and BROWN + Bucket (93.0) and BROWN + Bit-string (93.1) w.r.t. BROWN (92.7), are statistically significant at p < 0.01. Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them. We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_11",
  "x": "We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors. This would disallow the classifier to learn useful linear combinations of the various dimensions. Firing D 2 features on all dimension pairs (corresponding to an outer product) would lead to an infeasible number of features. On the other hand, we have a single vector for head+argument, allowing us to fire just D features (one per dimension) and still learn useful dimension combinations in linear space. We also report out-of-domain performance, in Table 4, on the Web treebank (Petrov and McDonald, 2012 ) test sets, directly using the WSJ-trained models. Again, both our Bucket and Bit-string linkembedding features achieve decent improvements over Baseline and they stack over BROWN, while using much fewer features. Moreover, one can hopefully achieve bigger gains by training link embeddings on Web or Wikipedia data (since BLLIP is news-domain). ---------------------------------- **OFF-THE-SHELF: CONSTITUENT PARSING**",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_12",
  "x": "We follow Bansal and Klein (2011) , reranking 50-best lists of the Berkeley parser (Petrov et al., 2006) . We first extract dependency links in each candidate constituent tree based on the head-modifier rules of Collins (2000) . Next, we simply fire our Bit-string features on each link, where the feature again consists of just the prefix bit-string, the prefix length, and the signed, bucketed link distance. 9 Table 5 shows these reranking results, where 1-best and log p(t|w) are the two Berkeley parser baselines, and where Config is the state-of-the-art, nonlocal, configurational feature set of Huang (2008) , which in turn is a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). Again, all our test improvements are statistically significant at p < 0.01: Bit-string (90.9) over both the baselines (90.2, 89.9); and Config + Bit-string (91.4) over Config (91.1). Moreover, the Bit-string result (90.9) is the same (i.e., no statistically significant difference) as the Config result (91.1). Therefore, we can again match the improvements of complex, manually-defined, nonlocal reranking features with a much smaller set of simple, dense, off-the-shelf, link-embedding features, and also complement them statistically significantly. ---------------------------------- **RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_13",
  "x": "9 Table 5 shows these reranking results, where 1-best and log p(t|w) are the two Berkeley parser baselines, and where Config is the state-of-the-art, nonlocal, configurational feature set of Huang (2008) , which in turn is a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). Again, all our test improvements are statistically significant at p < 0.01: Bit-string (90.9) over both the baselines (90.2, 89.9); and Config + Bit-string (91.4) over Config (91.1). Moreover, the Bit-string result (90.9) is the same (i.e., no statistically significant difference) as the Config result (91.1). Therefore, we can again match the improvements of complex, manually-defined, nonlocal reranking features with a much smaller set of simple, dense, off-the-shelf, link-embedding features, and also complement them statistically significantly. ---------------------------------- **RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_14",
  "x": "---------------------------------- **RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on. Moreover, we hope that similar to word embeddings, these link embeddings will also prove useful when imported into various other NLP tasks as dense, continuous features, but now with additional syntactic information. There has also been some recent, useful work on reducing the sparsity of features in dependency parsing, e.g., via low-rank tensors (Lei et al., 2014) and via neural network parsers that learn tag and label embeddings (Chen and Manning, 2014) . In related work, learn dense feature embeddings for dependency parsing; however, they still work with the large number of manuallydefined feature templates from previous work and train embeddings for all those templates, with an aim to discover hidden, shared information among the large set of sparse features. We get similar improvements with a much smaller and simpler set of unary link features; also, our link embeddings are more portable to other NLP tasks than template-based embeddings specific to dependency parsing.",
  "y": "motivation differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_0",
  "x": "****EXEMPLAR-BASED WORD SENSE DISAMBIGUATION: SOME RECENT IMPROVEMENTS**** **ABSTRACT** In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ---------------------------------- **INTRODUCTION** Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\".",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_1",
  "x": "---------------------------------- **INTRODUCTION** Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) . His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_2",
  "x": "**INTRODUCTION** Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) . His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. On the other hand, <cite>our past work</cite> on WSD <cite>(Ng and Lee, 1996)</cite> used an exemplar-based (or nearest neighbor) learning approach.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_3",
  "x": "These parameters include the number of nearest neighbors to use for determining the class of a test example (i.e., k in a k nearest-neighbor classifier), exemplar weights, feature weights, etc. We found that the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven stateof-the-art machine learning algorithms. The rest of this paper is organized as follows. Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm. Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use. Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results. Section 6 gives the conclusion.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_4",
  "x": "The exemplar-based learning algorithm PEBLS contains a number of parameters that must be set before running the algorithm. These parameters include the number of nearest neighbors to use for determining the class of a test example (i.e., k in a k nearest-neighbor classifier), exemplar weights, feature weights, etc. We found that the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven stateof-the-art machine learning algorithms. The rest of this paper is organized as follows. Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm. Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use. Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_5",
  "x": "We only changed the handling of zero probability counts to the method just described. 3 Improvements to Exemplar-Based WSD PEBLS contains a number of parameters that must be set before running the algorithm. These parameters include k (the number of nearest neighbors to use for determining the class of a test example), exemplar weights, feature weights, etc. Each of these parameters has a default value in PEBLS, eg., k = 1, no exemplar weighting, no feature weighting, etc. <cite>We</cite> have used the default values for all parameter settings in <cite>our previous work</cite> on exemplar-based WSD reported in <cite>(Ng and Lee, 1996)</cite> . However, our preliminary investigation indicates that, among the various learning parameters of PEBLS, the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. Cross validation is a well-known technique that can be used for estimating the expected error rate of a classifier which has been trained on a particular data set. For instance, the C4.5 program (Quinlan, 1993) contains an option for running cross validation to estimate the expected error rate of an induced rule set. Cross validation has been proposed as a general technique to automatically determine the parameter settings of a given learning algorithm using a particular data set as training data (Kohavi and John, 1995) . In m-fold cross validation, a training data set is partitioned into m (approximately) equal-sized blocks, and the learning algorithm is run m times.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_6",
  "x": "---------------------------------- **4** Experimental Results Mooney (1996) has reported that the Naive-Bayes algorithm gives the best performance on disambiguating six senses of the word \"line\", among seven state-of-the-art learning algorithms tested. However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus. We tested both algorithms on two test sets from this corpus.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_7",
  "x": "We compute the error rate for each k, and choose the value of k with the minimum error rate. Note that the automatic determination of the best k through 10-fold cross validation makes use of only the training set, without looking at the test set at all. ---------------------------------- **4** Experimental Results Mooney (1996) has reported that the Naive-Bayes algorithm gives the best performance on disambiguating six senses of the word \"line\", among seven state-of-the-art learning algorithms tested. However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_8",
  "x": "This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus. We tested both algorithms on two test sets from this corpus. The first test set, named BC50, consists of 7,119 occurrences of the 191 words appearing in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 words appearing in 6 text files of the WSJ corpus. Both test sets are identical to the ones reported in <cite>(Ng and Lee, 1996)</cite> . Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_9",
  "x": "This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus. We tested both algorithms on two test sets from this corpus. The first test set, named BC50, consists of 7,119 occurrences of the 191 words appearing in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 words appearing in 6 text files of the WSJ corpus. Both test sets are identical to the ones reported in <cite>(Ng and Lee, 1996)</cite> . Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> .",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_10",
  "x": "Let w be the word to be disambiguated, and let 12 ll w rl r2 be the sentence fragment containing w. In the present study, we used seven features in the representation of an example, which are the local collocations of the surrounding 4 words. These seven features are: 12-11, ll-rl, rl-r2, ll, rl, 12 , and r2. The first three features are concatenation of two words. 2 The experimental results obtained are tabulated in Table 1 . The first three rows of accuracy fig-1This corpus is available from the Linguistic Data Consortium (LDC). Contact the LDC at ldc@unagi.cis.upenn.edu for details. 2The first five of these seven features were also used in <cite>(Ng and Lee, 1996)</cite> . , 1996) . The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs (Gale et al., 1992b; Miller et al., 1994) . There are two instantiations of this strategy in our current evaluation.",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_11",
  "x": "Another assignment method is to determine the most frequently occurring sense in the training examples, and to assign this sense to all test examples. We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_12",
  "x": "We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_13",
  "x": "The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm. ---------------------------------- **DISCUSSION**",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_14",
  "x": "Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm. ---------------------------------- **DISCUSSION** To understand why larger values of k are needed, we examined the performance of PEBLS when tested on the WSJ6 test set. During 10-fold cross validation runs on the training set, for each of the 191 words, we compared two error rates: the minimum expected error rate of PEBLS using the best k, and the expected error rate of the most frequent classifter. We found that for 13 words out of the 191 words, the minimum expected error rate of PEBLS using the best k is still higher than the expected error rate of the most frequent classifier.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_15",
  "x": "The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm. ---------------------------------- **DISCUSSION** To understand why larger values of k are needed, we examined the performance of PEBLS when tested on the WSJ6 test set. During 10-fold cross validation runs on the training set, for each of the 191 words, we compared two error rates: the minimum expected error rate of PEBLS using the best k, and the expected error rate of the most frequent classifter. We found that for 13 words out of the 191 words, the minimum expected error rate of PEBLS using the best k is still higher than the expected error rate of the most frequent classifier. That is, for these 13 words, PEBLS will produce, on average, lower accuracy than the most frequent classifier.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_16",
  "x": "However, more sophisticated indexing methods such as that reported in (Friedman et al., 1977) can reduce this to logarithmic expected time, which will significantly reduce testing time. In the present study, we have focused on the comparison of learning algorithms, but not on feature representation of examples. <cite>Our past work</cite> <cite>(Ng and Lee, 1996)</cite> suggests that multiple sources of knowledge are indeed useful for WSD. Future work will explore the addition of these other features to further improve disambiguation accuracy. Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights. Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting. Future work can explore the effect of exemplar weighting and feature weighting on disambiguation accuracy. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_17",
  "x": "<cite>Our past work</cite> <cite>(Ng and Lee, 1996)</cite> suggests that multiple sources of knowledge are indeed useful for WSD. Future work will explore the addition of these other features to further improve disambiguation accuracy. Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights. Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting. Future work can explore the effect of exemplar weighting and feature weighting on disambiguation accuracy. ---------------------------------- **CONCLUSION** In summary, we have presented improvements to the exemplar-based learning approach for WSD. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambignation accuracy on a large sensetagged corpus.",
  "y": "future_work"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_0",
  "x": "**INTRODUCTION** Question Generation over Knowledge Bases (KBQG) aims at generating natural language questions for the corresponding facts on KBs, and it can benefit some real applications. Firstly, KBQG can automatically annotate question answering (QA) datasets. Secondly, the generated questions and answers will be able to augment the training data for QA systems. More importantly, KBQG can improve the ability of machines to actively ask questions on human-machine conversations (Duan et al., 2017; Sun et al., 2018) . Therefore, this task has attracted more attention in recent years (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> . Specifically, KBQG is the task of generating natural language questions according to the input facts from a knowledge base with triplet form, like <subject, predicate, object>. For example, as illustrated in Figure 1 , KBQG aims at generating a question \"Which city is Statue of Liberty located in?\" (Q3) for the input factual triplet Which city is Statue of Liberty located in? Figure 1 : Examples of KBQG. We aims at generating questions like Q3 which expresses (matches) the given predicate and refers to a definitive answer. \"<Statue of Liberty, location/containedby 1 , New York City>\". Here, the generated question is associated to the subject \"Statue of Liberty\" and the predicate fb:location/containedby) of the input fact, and the answer corresponds to the object \"New York City\".",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_1",
  "x": "**PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_2",
  "x": "As depicted by Serban et al. (2016) , KBQG is required to transduce the triplet fact into a question about the subject and predicate, where the object is the correct answer. Therefore, it is a key issue for KBQG to correctly understand the knowledge symbols (subject, predicate and object in the triplet fact) and then generate corresponding text descriptions. More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected. 1 We omit the domain of the predicate for sake of brevity. ---------------------------------- **RI-1:**",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_3",
  "x": "\"<Statue of Liberty, location/containedby 1 , New York City>\". Here, the generated question is associated to the subject \"Statue of Liberty\" and the predicate fb:location/containedby) of the input fact, and the answer corresponds to the object \"New York City\". As depicted by Serban et al. (2016) , KBQG is required to transduce the triplet fact into a question about the subject and predicate, where the object is the correct answer. Therefore, it is a key issue for KBQG to correctly understand the knowledge symbols (subject, predicate and object in the triplet fact) and then generate corresponding text descriptions. More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected. 1 We omit the domain of the predicate for sake of brevity.",
  "y": "background motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_4",
  "x": "1 We omit the domain of the predicate for sake of brevity. ---------------------------------- **RI-1:** The generated question is required to express the given predicate in the fact. For example in Figure 1 , Q1 does not express (match) the predicate (fb:location/containedby) while it is expressed in Q2 and Q3. Previous work<cite> (Elsahar et al., 2018)</cite> usually obtained predicate textual contexts through distant supervision. However, the distant supervision is noisy or even wrong (e.g. \"X is the husband of Y\" is the relational pattern for the predicate fb:marriage/spouse, so it is wrong when \"X\" is a woman). Furthermore, many predicates in the KB have no predicate contexts. We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_5",
  "x": "However, the distant supervision is noisy or even wrong (e.g. \"X is the husband of Y\" is the relational pattern for the predicate fb:marriage/spouse, so it is wrong when \"X\" is a woman). Furthermore, many predicates in the KB have no predicate contexts. We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer. A definitive answer means that one question only associates with a determinate answer rather than alternative answers. As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2.",
  "y": "motivation uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_6",
  "x": "We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously. For the purpose of further making generated questions correspond to definitive answers, we propose the answer-aware loss by optimizing the cross-entropy between the generated question and answer type words, which is beneficial to generate precise questions.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_7",
  "x": "A definitive answer means that one question only associates with a determinate answer rather than alternative answers. As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ).",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_8",
  "x": "Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously. For the purpose of further making generated questions correspond to definitive answers, we propose the answer-aware loss by optimizing the cross-entropy between the generated question and answer type words, which is beneficial to generate precise questions. We conduct experiments on an open public dataset. Experimental results demonstrate that the proposed model using diversified textual contexts outperforms strong baselines (+4.5 BLEU4 score). Besides, it can further increase the BLEU score (+5.16 BLEU4 score) and produce questions associated with more definitive answers by incorporating answer-aware loss. Human evaluations complement that our model can express the given predicate more precisely. In brief, our main contributions are as follows:",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_9",
  "x": "We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously. For the purpose of further making generated questions correspond to definitive answers, we propose the answer-aware loss by optimizing the cross-entropy between the generated question and answer type words, which is beneficial to generate precise questions.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_10",
  "x": "In contrast to general Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) , the input fact is not a word sequence but instead a structured triplet F = (s, p, o). We employ a fact encoder to transform each atom in the fact into a fixed embedding, and the embedding is obtained from a KB embedding matrix. For example, the subject embedding e s \u2208 R d is looked up from the KB embedding matrix E f \u2208 R k,d , where k represents the size of KB vocabulary, and the size of KB embedding is equal to the number of hidden units (d) in Equation 3. Similarly, the predicate embedding e p and the object embedding e o are mapped from the KB embedding matrix E f , where E f is pre-trained using TransE (Bordes et al., 2013) to capture much more fact information in previous work<cite> (Elsahar et al., 2018)</cite> . In our model, E f can be pre-trained or randomly initiated (Details in Sec. 4.7.1). ---------------------------------- **CONTEXT-AUGMENTED FACT ENCODER** In order to combine both the context encoder information and the fact encoder information, we propose a context-augmented fact encoder which applies the gated fusion unit (Gong and Bowman, 2018) to integrate the context matrix and the fact embedding. For example, the subject context matrix C s = {c s 1 , c s 2 , ..., c s |s| } and the subject embedding vector e s are integrated by the following gated fusion: where c s is an attentive vector from e s to C s , which is similar to",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_11",
  "x": "<cite>Elsahar et al. (2018)</cite> demonstrated the effectiveness of POS copy for the context. However, such a copy mechanism heavily relies on POS tagging. Inspired by the CopyNet (Gu et al., 2016) , we directly copy words in the textual contexts C, and it does not rely on any POS tagging. Specifically, the input sequence \u03c7 for the context copy is the concatenation of all words in the textual contexts C. Unfortunately, \u03c7 is prone to contain repeated words because it consists of rich contexts for subject, predicate and object. The repeated words in the input sequence tend to cause repetition problems in output sequences (Tu et al., 2016) . We adopt the maxout pointer to address the repetition problem. Instead of summing all the probabilistic scores for repeated input words, we limit the probabilistic score of repeated words to their maximum score as Equation 13: ---------------------------------- **CONTEXT COPY** where \u03c7 m represents the m-th token in the input context sequence \u03c7, sc t,m is the probabilistic score of generating the token \u03c7 m at time step t, and sc t,m is calculated by a softmax function over \u03c7.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_12",
  "x": "We conduct experiments on the SimpleQuestion dataset (Bordes et al., 2015) , and there are 75910/10845/21687 question answering pairs (QA-pairs) for training/validation/test. In order to obtain diversified contexts, we additionally employ domain, range and topic of the predicate to improve the coverage of predicate contexts. In this way, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. For the subject and object context, we combine the most frequently mentioned entity type<cite> (Elsahar et al., 2018)</cite> with the type that best describe the entity 3 . The KB copy needs subject names as the copy source, and we map entities with their names similar to those in Mohammed et al. (2018) . The data details are in Appendix A and submitted Supplementary Data. ---------------------------------- **EVALUATION METRICS** Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_13",
  "x": "The data details are in Appendix A and submitted Supplementary Data. ---------------------------------- **EVALUATION METRICS** Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows. (1) Predicate identification: Following Mohammed et al. (2018), we employ annotators to judge whether the generated question expresses the given predicate in the fact or not. The score for predicate identification is the percentage of generated questions that express the given predicate. (2) Answer coverage: We define a novel metric called answer coverage to identify whether the generated question refers to a definitive answer.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_14",
  "x": "The data details are in Appendix A and submitted Supplementary Data. ---------------------------------- **EVALUATION METRICS** Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows. (1) Predicate identification: Following Mohammed et al. (2018), we employ annotators to judge whether the generated question expresses the given predicate in the fact or not. The score for predicate identification is the percentage of generated questions that express the given predicate. (2) Answer coverage: We define a novel metric called answer coverage to identify whether the generated question refers to a definitive answer.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_15",
  "x": "(2) Answer coverage: We define a novel metric called answer coverage to identify whether the generated question refers to a definitive answer. Specifically, answer coverage is obtained by automatically calculating the percentage of questions that contain answer type words, and answer type words are object contexts (entity types for the object are regarded as answer type words). Furthermore, it is hard to automatically evaluate the naturalness of generated questions. Following Mohammed et al. (2018) , we adopt human evaluation to measure the naturalness by a score of 0-5. ---------------------------------- **COMPARISON WITH STATE-OF-THE-ARTS** We compare our model with following methods. (1) Template: A baseline in Serban et al. (2016) , it randomly chooses a candidate fact F c in the training data to generate the question, where F c shares the same predicate with the input fact. (2) Serban et al. (2016): We compare our methods with the single placeholder model, which performs best in Serban et al. (2016) . (3) <cite>Elsahar et al. (2018)</cite> : We compare our methods with the model utilizing copy actions, the best performing model in <cite>Elsahar et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_16",
  "x": "Although this model is designed to a zero-shot setting (for unseen predicates and entity type), it has good abilities to generate better questions (on known or unknown predicates and entity types) represented in the additional context input and SPO copy mechanism. ---------------------------------- **IMPLEMENTATION DETAILS** To make our model comparable to the comparison methods, we keep most parameter values the same as <cite>Elsahar et al. (2018)</cite> . We utilize RMSProp algorithm with a decreasing learning rate (0.001), batch size (200) to optimize the model. The size of KB embeddings is 200, and KB embeddings are pre-trained by TransE (Bordes et al., 2013) . The word embeddings are initialized by the pre-trained Glove word vectors 4 with 200 dimensions. In the transformer, we set the hidden units d to 200, and we employ 4 paralleled attention head and a stack of 5 identical layers. We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics.",
  "y": "similarities"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_17",
  "x": "In the transformer, we set the hidden units d to 200, and we employ 4 paralleled attention head and a stack of 5 identical layers. We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics. It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4). ---------------------------------- **OVERALL COMPARISONS** ---------------------------------- **PERFORMANCES ON PREDICATE IDENTIFICATION** To evaluate the ability of our model on predicate identification, we sample 100 generated questions Model Pred.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_18",
  "x": "It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4). ---------------------------------- **OVERALL COMPARISONS** ---------------------------------- **PERFORMANCES ON PREDICATE IDENTIFICATION** To evaluate the ability of our model on predicate identification, we sample 100 generated questions Model Pred. Identification Serban et al. (2016) 53.5 <cite>Elsahar et al. (2018)</cite> 71.5 Our Model ans loss 75.5 from each model, and then two annotators are employed to judge whether the generated question expresses the given predicate. The Kappa for inter-annotator statistics is 0.611, and p-value for all scores is less than 0.005. As shown in Table  2 , we can see that our model has a significant improvement in the predicate identification.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_19",
  "x": "It can be seen that our model, incorporating answer-aware loss, has a significant improvement on answer coverage while there is no performance degradation on BLEU4 compared with \u03bb = 0, which indicates that answer-aware loss contributes to generating better questions. Especially, the generated questions are more precise because they refer to more definitive answers with high Ans cov . (3) It tends to correspond to alternative answers (object in the triplet fact) for some predicates such as fb:location/containedby, while other predicates (e.g. fb:person/gender) may refer to a definitive answer. To investigate our model, by incorporating answer-aware loss, does not generate an answer type word in a mandatory way, we found 20.5% predicate corresponds to the generated questions without answer type words when our model obtains the highest Ans cov (\u03bb=0.5), and it is very close to 21.7% for the one in human-annotated questions. This demonstrates that the answer-aware loss does not force all predicates to generate questions with answer type words. Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in <cite>Elsahar et al. (2018)</cite> . ---------------------------------- **ABLATION STUDY** In order to validate the effectiveness of model components, we remove some important components in our model, including context copy, KB copy, answer-aware loss and diversified contexts. The results are shown in Table 4 .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_20",
  "x": "---------------------------------- **ABLATION STUDY** In order to validate the effectiveness of model components, we remove some important components in our model, including context copy, KB copy, answer-aware loss and diversified contexts. The results are shown in Table 4 . We can see that removing any component brings performance decline on all metrics. It demonstrates that all these components are useful. Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ---------------------------------- **PERFORMANCES ON NATURALNESS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_21",
  "x": "**PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_22",
  "x": "Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ---------------------------------- **PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_23",
  "x": "Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ---------------------------------- **THE EFFECTIVENESS OF GENERATED QUESTIONS FOR ENHANCING QUESTION ANSWERING OVER KNOWLEDGE BASES** Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_24",
  "x": "Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions. To further prove the effectiveness of our model, we will see how useful the generated questions are for training a question answering system over knowledge bases. Specifically, we combine humanlabeled data with the same amount of modelgenerated data to a typical QA system (Mohammed et al., 2018) . The accuracy of QA is shown in Table 7 . We can observe that adding generative questions may weaken the performance of QA (drop from 68.97 to 68.53 in Table 7 ). Our generated questions achieve the best performance on the QA system. It indicates that our model generates more precise question and has improved QA performances greatly. In order to further explore the convergence speed, we plot the performances on valid data through epochs in Figure 3 . Our model has much more information to learn, and it may have a bad impact on the convergence speed. Nevertheless, our model can copy KB elements and textual context simultaneously, which may accelerate the convergence speed.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_25",
  "x": "---------------------------------- **RELATED WORK** Our work is inspired by a large number of successful applications using neural encoder-decoder frameworks on NLP tasks such as machine translation (Cho et al., 2014a) and dialog generation (Vinyals and Le, 2015) . Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks. Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions. To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms. Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_26",
  "x": "To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms. Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating. Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK**",
  "y": "motivation background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_27",
  "x": "Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we focus on two crucial research issues for the task of question generation over knowledge bases: generating questions that express the given predicate and refer to definitive answers rather than alternative answers. For this purpose, we present a neural encoder-decoder model which integrates diversified off-the-shelf contexts and multi-level copy mechanisms. Moreover, we design an answer-aware loss to generate questions that refer to definitive answers. Experiments show that our model achieves state-of-the-art performance on automatic and manual evaluations. For future work, we investigate error cases by analyzing the error distributions of 100 examples.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_28",
  "x": "However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms. Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating. Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we focus on two crucial research issues for the task of question generation over knowledge bases: generating questions that express the given predicate and refer to definitive answers rather than alternative answers.",
  "y": "background differences"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_0",
  "x": "While many claim to be multilingual, the MAG (Multilingual AGDIS-TIS) approach has been shown recently to outperform the state of the art in multilingual EL on 7 languages. With this demo, we extend MAG to support EL in 40 different languages, including especially low-resources languages such as Ukrainian, Greek, Hungarian, Croatian, Portuguese, Japanese and Korean. Our demo relies on online web services which allow for an easy access to our entity linking approaches and can disambiguate against DBpedia and Wikidata. During the demo, we will show how to use MAG by means of POST requests as well as using its user-friendly web interface. All data used in the demo is available at https://hobbitdata.informatik.uni-leipzig.de/agdistis/ ---------------------------------- **INTRODUCTION** A recent survey by IBM 3 suggests that more than 2.5 quintillion bytes of data are produced on the Web every day. Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data. The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K <cite>[4]</cite> .",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_1",
  "x": "For example, New York City, NY and Big Apple are all labels for the same entity. Also, multiple entities can share the same name due to homonymy and ambiguity. For example, both the state and the city of Rio de Janeiro are called Rio de Janeiro. Despite the complexity of the task, EL approaches have recently achieved increasingly better results by relying on trained machine learning models [6] . A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries. However, MAG (Multilingual AGDISTIS) <cite>[4]</cite> showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data available on more than three languages [6] . The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 . For the sake of server space, we deployed MAG-based web services for 9 languages and offer the other 31 languages for download. Additionally, we provide an English index using Wikidata to show the knowledge-base agnosticism of MAG.",
  "y": "motivation"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_2",
  "x": "Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention <cite>[4]</cite> . ---------------------------------- **DEMONSTRATION** Our demonstration will show the capabilities of MAG for different languages. We provide a graphical, web-based user interface (GUI). In addition, users can choose to use the REST interface or a Java snippet. For research purposes, MAG can be downloaded and deployed via Maven or Docker. Figure 1 illustrates an example of MAG working on Spanish. The online demo can be accessed via http://agdistis.aksw. org/mag-demo and its code can be downloaded from https://github.com/ dice-group/AGDISTIS_DEMO/tree/v2.",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_3",
  "x": "-Search by Context -This boolean parameter provides a search of candidates using a context index <cite>[4]</cite> . -Acronyms -This parameter enables a search by acronyms. In this case, MAG uses an additional index to filter the acronyms by expanding their labels and assigns them a high probability. For example, PSG equals Paris Saint-Germain. The parameter is acronym=false or acronym=true. -Common Entities -This boolean option supports finding common entities , in case, users desire to find more than ORGANIZATIONs, PLACEs and PERSONs as entity type. Knowledge-base Agnosticism. Figure 2 shows a screen capture of our demo for disambiguating mentions using Wikidata. We also provide a web service to allow further investigation. In addition, MAG is used in a domain specific problem using a music Knowledge Base (KB) [5] .",
  "y": "background uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_0",
  "x": "**SYSTEM DESCRIPTION** The system consists of multiple modules that are applied in a pipeline fashion. This architecture is a standard approach that was originally proposed in Lin et al. (2014) and was followed with slight variations by systems in the last year competition (Xue et al., 2015) . Our design most closely resembles the pipeline proposed by the top system last year<cite> (Wang and Lan, 2015)</cite> , in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice. The overall architecture of the system is shown in Figure 1 . Given the input text, the connective classifier identifies explicit discourse connectives. Next, the position classifier is invoked that determines for each explicit relation whether Arg1 is located in the same sentence as Arg2 (SS) or in a previous sentence (PS). The following three modules -SS Arg1/Arg2 Extractor, PS Arg1 Extractor, and PS Arg2 Extractor -extract text spans of the respective arguments. Finally, the explicit sense classifier is applied. Next, candidate sentence pairs for non-explicit relations are identified.",
  "y": "similarities"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_1",
  "x": "This is a binary classifier that, given a connective word or phrase (e.g. but or if . . . then) determines whether the connective functions as a discourse connective in the specific context. We use the training data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014;<cite> Wang and Lan, 2015)</cite> . Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006) . ---------------------------------- **IDENTIFYING ARG1 POSITION** For explicit relations, position of Arg2 is fixed to be the sentence where the connective itself occurs. Arg1, on the other hand, can be located in the same sentence as the connective or in a previous sentence. Given a connective and the sentence in which it occurs, the goal of the position classifier is to determine the location of Arg1.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_2",
  "x": "For explicit relations, position of Arg2 is fixed to be the sentence where the connective itself occurs. Arg1, on the other hand, can be located in the same sentence as the connective or in a previous sentence. Given a connective and the sentence in which it occurs, the goal of the position classifier is to determine the location of Arg1. This is a binary classifier with two classes: SS and PS. We employ the features proposed in Lin et al. (2014) and additional features described in last year's top system<cite> (Wang and Lan, 2015)</cite> . The position classifier is trained using the Maximum Entropy algorithm and achieves an F1 score of 99.186% on the development data. In line with prior work<cite> (Wang and Lan, 2015)</cite> , we consider PS to be the sentence that immediately precedes the connective. About 10% of explicit discourse relations have Arg1 occurring in a sentence that does not immediately precede the connective. These are missed at this point. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_3",
  "x": "**EXPLICIT RELATIONS: ARGUMENT EXTRACTION** SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_4",
  "x": "SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_5",
  "x": "We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags. We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features:",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_6",
  "x": "The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features: \u2022 nextFirstW&puncBefore -the first word token of next and the punctuation before next. \u2022 prevLastW&puncAfter -the last word token of prev and the punctuation after prev. \u2022 POS of the connective string. \u2022 The distance between the connective and the position of curr in the sentence. The argument extractors are trained with the Averaged Perceptron algorithm, implemented within Learning Based Java (LBJ) (Rizzolo and Roth, 2010) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_7",
  "x": "Conjunction, etc.) an explicit relation conveys. A 3-level sense hierarchy has been defined in PDTB, which has four top-level senses: Comparison, Contingency, Expansion, and Temporal. We use lexical and syntactic features based on previous work and also introduce new features: \u2022 C (Connective) string, C POS, prev + C, proposed in Lin et al. (2014) . \u2022 C self-category, parent-category of C, leftsibling-category of C, right-sibling-category of C, 4 C-Syn interactions, and 6 Syn-Syn interactions, introduced in Pitler et al. (2009) . \u2022 C parent-category linked context, previous connective and its POS of \"as\"(the connective and its POS of previous relation, if the connective of current relation is \"as\"), previous connective and its POS of \"when\", adopted from<cite> Wang and Lan (2015)</cite> . \u2022 Our new features: first token of C, second token of C (if exists), next word (next), C + next, prev + next, prev + C + next. Table 1 : Novel features used in the PS Arg1 and PS Arg2 extractors. Curr, prev, and next refer to the current, previous, and next constituent in the same sentence, respectively. W denotes word token, and POS denotes the part-of-speech tag of a word.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_8",
  "x": "Table 1 : Novel features used in the PS Arg1 and PS Arg2 extractors. Curr, prev, and next refer to the current, previous, and next constituent in the same sentence, respectively. W denotes word token, and POS denotes the part-of-speech tag of a word. For example, currFirstWAndCurrSecondW refers to the first two word tokens in curr, while prevLastPOS refers to the POS of the last token of prev, and nextFirstPOS refers to the POS of the first token of next. For this task, we trained two classifiers -using Maximum Entropy and Averaged Perceptron algorithms -and chose Averaged Perceptron, as its performance was found to be superior. ---------------------------------- **IDENTIFYING NON-EXPLICIT RELATIONS** The first step in identifying non-explicit relations is the generation of sentence pairs that are candidate arguments for a non-explicit relation. Following<cite> Wang and Lan (2015)</cite>, we extract sentence pairs that satisfy the following three criteria: \u2022 Sentences are adjacent \u2022 Sentences occur within the same paragraph \u2022 Neither sentence participates in an explicit relation For all pairs of sentences that meet those criteria, we take the first sentence to be the location of Arg1, and the second sentence -the location of Arg2.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_9",
  "x": "**IMPLICIT RELATIONS: ARGUMENT EXTRACTION** The argument extractors for implicit relations are implemented in a way similar to explicit relation argument extraction. Candidate sentences are split into constituents based on punctuation symbols and clause boundaries using the SBAR tag. We use features in Lin et al. (2009) and<cite> Wang and Lan (2015)</cite> and augment these with novel features. Implicit Arg1 Extractor: The Implicit Arg1 extractor employs a rich set of features. Most of these are similar to those presented for PS Arg1 and PS Arg2 extractors in that we take into account POS information, punctuation symbols that occur on the boundaries of the constituents, as well as dependency relations in the constituent itself. One key distinction of how we define the depen-dency relation features is that, in contrast to prior work that treats each dependency relation as a separate binary feature, we only consider the first two relations (r1 and r2, respectively) in curr, prev, and next, and take their conjunctions. Our intuition is that the relations in the beginning of a constituent are most important, while the other relations are not that relevant. This approach to feature generation also avoids sparseness, which was found to be a problem in earlier work. Overall, we generate seven features that use dependency relations.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_10",
  "x": "This approach to feature generation also avoids sparseness, which was found to be a problem in earlier work. Overall, we generate seven features that use dependency relations. Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and<cite> Wang and Lan (2015)</cite> to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers): \u2022 Lowercased and lemmatized verbs in curr \u2022 The first and last terms of curr \u2022 The last term of prev \u2022 The first term of next \u2022 The last term of prev + the first term of curr \u2022 The last term of curr + the first term of next \u2022 The position of curr in the sentence: start, middle, end, or whole sentence",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_12",
  "x": "Baseline denotes taking the entire sentence as argument span. ---------------------------------- **MODEL** Base features refer to features used in<cite> Wang and Lan (2015)</cite> . sense classification indicated that Averaged Perceptron should be preferred for these sub-tasks. Due to time constraints, we did not compare all three algorithms on all sub-tasks. ---------------------------------- **IMPROVING INDIVIDUAL COMPONENTS** We first evaluate the components for which we introduce new features. We use gold annotations for evaluating the individual components below.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_13",
  "x": "We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor. For PS Arg2 extractor, the classifiers are able to obtain a larger improvement compared to the baseline method.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_14",
  "x": "Explicit Sense Classifier: Table 2 evaluates the explicit sense classifier. We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor.",
  "y": "differences"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_15",
  "x": "We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor. For PS Arg2 extractor, the classifiers are able to obtain a larger improvement compared to the baseline method.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_16",
  "x": "Adding new features improves the results by three points. We note that in<cite> Wang and Lan (2015)</cite> the numbers that correspond to the entire sentence baselines are not the same as those that we obtain, so we do not report a direct comparison with their models. However, our base models implement the features they use. Implicit Arg1 Extractor: In Table 5 , we evaluate the Implicit Arg1 extractor. It achieves an improvement of 12 F1 points over the baseline method that considers the entire sentence to be the argument span. ---------------------------------- **RESULTS ON THE DEVELOPMENT SET (NO EP)** Performance of each component on the development set, as implemented in the submitted system, without EP, is shown in Table 6 . Table 8 : Official results on the test set. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_0",
  "x": "---------------------------------- **INTRODUCTION** Natural language texts are, more often than not, a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments. Text segmentation deals with automatically breaking down the structure of text into such topically contiguous segments, i.e., it aims to identify the points of topic shift (Hearst 1994; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> . Reliable segmentation results with texts that are more readable for humans, but also facilitates downstream tasks like automated text summarization (Angheluta, De Busser, and Moens 2002; Bokaei, Sameti, and Liu 2016) , passage retrieval (Huang et al. 2003; Shtekh et al. 2018) , topical classification (Zirn et al. 2016) , or dialog modeling (Manuvinakurike et al. 2016; Zhao and Kawahara 2017) . Text coherence is inherently tied to text segmentationintuitively, the text within a segment is expected to be more coherent than the text spanning different segments. Consider, e.g., the text in Figure 1 , with two topical segments. Snippets T 1 and T 2 are more coherent than T 3 and T 4 : all T 1 sentences relate to Amsterdam's history, and all T 2 sentences to Amsterdam's geography; in contrast, T 3 and T 4 contain sentences Amsterdam is younger than Dutch cities such as Nijmegen, Rotterdam, and Utrecht. Amsterdam was granted city rights in either 1300 or 1306. In the 14th century Amsterdam flourished because of trade with the Hanseatic League.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_1",
  "x": "Amsterdam is about 2 metres (6.6 feet) below sea level. T1 T2 T3 T4 Figure 1 : Snippet illustrating the relation (i.e., dependency) between text coherence and segmentation. from both topics. T 1 and T 2 being more coherent than T 3 and T 4 signals that the fourth sentence starts a new segment. Given this duality between text segmentation and coherence, it is surprising that the methods for text segmentation capture coherence only implicitly. Unsupervised segmentation models rely either on probabilistic topic modeling (Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013) or semantic similarity between sentences (Glava\u0161, Nanni, and Ponzetto 2016) , both of which only indirectly relate to text coherence. Similarly, a recently proposed state-of-the-art supervised neural segmentation model<cite> (Koshorek et al. 2018</cite> ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence. In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ). Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction.",
  "y": "differences background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_2",
  "x": "Given this duality between text segmentation and coherence, it is surprising that the methods for text segmentation capture coherence only implicitly. Unsupervised segmentation models rely either on probabilistic topic modeling (Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013) or semantic similarity between sentences (Glava\u0161, Nanni, and Ponzetto 2016) , both of which only indirectly relate to text coherence. Similarly, a recently proposed state-of-the-art supervised neural segmentation model<cite> (Koshorek et al. 2018</cite> ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence. In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ). Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction. However, CATS augments the segmentation objective with an auxiliary coherence-based objec-tive which pushes the model to predict higher coherence for original text snippets than for corrupt (i.e., fake) sentence sequences. We empirically show (1) that even without the auxiliary coherence objective, the Two-Level Transformer model for Text Segmentation (TLT-TS) yields state-of-the-art performance across multiple benchmarks, (2) that the full CATS model, with the auxiliary coherence modeling, further significantly improves the segmentation, and (3) that both TLT-TS and CATS are robust in domain transfer. Furthermore, we demonstrate models' effectiveness in zero-shot language transfer. Coupled with a cross-lingual word embedding space, 1 our models trained on English Wikipedia successfully segment texts from unseen languages, outperforming the best-performing unsupervised segmentation model (Glava\u0161, Nanni, and Ponzetto 2016) by a wide margin.",
  "y": "differences similarities"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_3",
  "x": [
   "**EXPERIMENTAL SETUP** We first describe datasets used for training and evaluation and then provide the details on the comparative evaluation setup and model optimization. ---------------------------------- **DATA** WIKI-727K Corpus. Koshorek et al. (2018) leveraged the manual structuring of Wikipedia pages into sections to automatically create a large segmentation-annotated corpus. WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora. Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow."
  ],
  "y": "background uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_4",
  "x": [
   "**EXPERIMENTAL SETUP** We first describe datasets used for training and evaluation and then provide the details on the comparative evaluation setup and model optimization. ---------------------------------- **DATA** WIKI-727K Corpus. Koshorek et al. (2018) leveraged the manual structuring of Wikipedia pages into sections to automatically create a large segmentation-annotated corpus. WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora. Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_5",
  "x": "For years, the synthetic dataset of Choi (2000) was used as a standard becnhmark for text segmentation models. CHOI dataset contains 920 documents, each of which is a concatenation of 10 paragraphs randomly sampled from the Brown corpus. CHOI dataset is divided into subsets containing only documents with specific variability of segment lengths (e.g., segments with 3-5 or with 9-11 sentences). 7 Finally, we evaluate the performance of our models on two small datasets, CITIES and ELEMENTS, created by Chen et al. (2009) from Wikipedia pages dedicated to the cities of the world and chemical elements, respectively. Other Languages. In order to test the performance of our Transformer-based models in zero-shot language transfer setup, we prepared small evaluation datasets in other languages. Analogous to the WIKI-50 dataset created by<cite> Koshorek et al. (2018)</cite> from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively. 8 ---------------------------------- **COMPARATIVE EVALUATION**",
  "y": "similarities extends"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_6",
  "x": "Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ---------------------------------- **MODEL CONFIGURATION** Model Variants.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_7",
  "x": "Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ---------------------------------- **MODEL CONFIGURATION** Model Variants. We evaluate two variants of our two-level transformer text segmentation model: with and without the auxiliary coherence modeling.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_8",
  "x": "Analogous to the WIKI-50 dataset created by<cite> Koshorek et al. (2018)</cite> from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively. 8 ---------------------------------- **COMPARATIVE EVALUATION** Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_10",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We first present and discuss the results that our models, TLT-TS and CATS, yield on the previously introduced EN evaluation datasets. We then report and analyze models' performance in the cross-lingual zero-shot transfer experiments. 9 https://tinyurl.com/y6j4gh9a 10 Given the large hyperparameter space and large training set, we only searched over a limited-size grid of hyperparameter configurations. It is thus likely that a better-performing configuration than the one reported can be found with a more extensive grid search. 11 We do not tune other transformer hyperparameters, but rather adopt the recommended values from (Vaswani et al. 2017) : filter size of 1024 and dropout probabilities of 0.1 for both attention layers and feed-forward ReLu layers. Table 1 shows models' performance on five EN evaluation datasets. Both our Transformer-based models -TLT-TS and CATS -outperform the competing supervised model of<cite> Koshorek et al. (2018)</cite> , a hierarchical encoder based on recurrent components, across the board. The improved performance that TLT-TS has with respect to the model of<cite> Koshorek et al. (2018)</cite> is consistent with improvements that Transformer-based architectures yield in comparison with models based on recurrent components in other NLP tasks (Vaswani et al. 2017; Devlin et al. 2018) .",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_12",
  "x": "**BASE EVALUATION** CATS significantly 13 and consistently outperforms TLT-TS. This empirically confirms the usefulness of explicit coherence modeling for text segmentation. Moreover,<cite> Koshorek et al. (2018)</cite> report human performance on the WIKI-50 dataset of 14.97, which is a mere one P k point better than the performance of our coherence-aware CATS model. The unsupervised GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) seems to outperform all supervised models on the synthetic CHOI dataset. We believe that this is primarily because (1) by being synthetic, the CHOI dataset can be accurately segmented based on simple lexical overlaps and word embedding similarities (and GRAPHSEG relies on similarities between averaged word embeddings) and because (2) by being trained on a much more challenging real-world WIKI-727K dataset -on which lexical overlap is insufficient for accurate segmentation -supervised models learn to segment based on deeper natural language understanding (and learn not to encode lexical overlap as reliable segmentation signal). Additionally, GRAPHSEG is evaluated separately on each subset of the CHOI dataset, for each of which it is provided the (gold) minimal segment size, which further facilitates and improves its predicted segmentations. ---------------------------------- **ZERO-SHOT CROSS-LINGUAL TRANSFER** In Table 2 we show the results of our zero-shot cross-lingual transfer experiments.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_13",
  "x": [
   "This is quite encouraging as it suggests that it is possible to, via the zero-shot language transfer, rather reliably segment texts from under-resourced languages lacking sufficiently large gold-segmented data needed to directly train language-specific segmentation models (that is, robust neural segmentation models in particular). ---------------------------------- **RELATED WORK** In this work we address the task of text segmentation -we thus provide a detailed account of existing segmentation models. Because our CATS model has an auxiliary coherencebased objective, we additionally provide a brief overview of research on modeling text coherence. ---------------------------------- **TEXT SEGMENTATION** Text segmentation tasks come in two main flavors: (1) linear (i.e., sequential) text segmentation and (2) hierarchical segmentation in which top-level segments are further broken down into sub-segments. While the hierarchical segmentation received a non-negligible research attention (Yaari 1997; Eisenstein 2009; Du, Buntine, and John-son 2013) , the vast majority of the proposed models (including this work) focus on linear segmentation (Hearst 1994; Beeferman, Berger, and Lafferty 1999; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Misra et al. 2009; Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; Koshorek et al. 2018, inter alia) . In one of the pioneering segmentation efforts, Hearst (1994) proposed an unsupervised TextTiling algorithm based on the lexical overlap between adjacent sentences and paragraphs."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_14",
  "x": "They segment lecture transcripts by first inducing a fully connected sentence graph with edge weights corresponding to cosine similarities between sparse bag-of-word sentence vectors and then running a minimum normalized multiway cut algorithm to obtain the segments. Glava\u0161, Nanni, and Ponzetto (2016) propose GRAPHSEG, a graph-based segmentation algorithm similar in nature to (Malioutov and Barzilay 2006) , which uses dense sentence vectors, obtained by aggregating word embeddings, to compute intra-sentence similarities and performs segmentation based on the cliques of the similarity graph. Finally,<cite> Koshorek et al. (2018)</cite> identify Wikipedia as a free large-scale source of manually segmented texts that can be used to train a supervised segmentation model. They train a neural model that hierarchically combines two bidirectional LSTM networks and report massive improvements over unsupervised segmentation on a range of evaluation datasets. The model we presented in this work has a similar hierarchical architecture, but uses Transfomer networks instead of recurrent encoders. Crucially, CATS additionally defines an auxiliary coherence objective, which is coupled with the (primary) segmentation objective in a multi-task learning model. ---------------------------------- **TEXT COHERENCE** Measuring text coherence amounts to predicting a score that indicates how meaningful the order of the information in the text is. The majority of the proposed text coherence models are grounded in formal theories of text coherence, among which the entity grid model (Barzilay and Lapata 2008) , based on the centering theory of Grosz, Weinstein, and Joshi (1995) , is arguably the most popular.",
  "y": "differences motivation background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_0",
  "x": "An interesting property of word vectors learned by the log-linear model is that the relations among relevant words seem linear and can be computed by simple vector addition and substraction (Mikolov et al., 2013d) . For example, the following relation approximately holds in the word vector space: ParisFrance + Rome = Italy. In<cite> (Mikolov et al., 2013b)</cite> , the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another. The authors reported a high accuracy on a bilingual word translation task. Although promising, we argue that both the word embedding and the linear transform are ill-posed, due to the inconsistence among the objective function used to learn the word vectors (maximum likelihood based on inner product), the distance measurement for word vectors (cosine distance), and the objective function used to learn the linear transform (mean square error). This inconsistence may lead to suboptimal estimation for both word vectors and the bilingual transform, as we will see shortly. This paper solves the inconsistence by normalizing the word vectors. Specifically, we enforce the word vectors to be in a unit length during the learning of the embedding. By this constraint, all the word vectors are located on a hypersphere and so the inner product falls back to the cosine distance. This hence solves the inconsistence between the embedding and the distance measurement.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_1",
  "x": "This hence solves the inconsistence between the embedding and the distance measurement. To respect the normalization constraint on word vectors, the linear transform in the bilingual projection has to be constrained as an orthogonal transform. Finally, the cosine distance is used when we train the orthogonal transform, in order to achieve full consistence. ---------------------------------- **RELATED WORK** This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches. In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages. Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA).",
  "y": "similarities uses"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_2",
  "x": "This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches. In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages. Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA). The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding. For example, (Zou et al., 2013) adds an extra term that reflects the distances of some pairs of semantically related words from different languages into the objective funtion. A similar approach is proposed in (Klementiev et al., 2012) , which casts multilingual learning as a multitask learning and encodes the multilingual information in the interaction matrix. All the above methods rely on a multilingual lexicon or a word/pharse alignment, usually from a machine translation (MT) system. (Blunsom et al., 2014) proposed a novel approach based on a joint optimization method for word alignments and the embedding.",
  "y": "similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_3",
  "x": "The bilingual word translation provided by <cite>(Mikolov et al., 2013b</cite> ) learns a linear transform from the source language to the target language by the linear regression. The objective function is as follows: 1 For efficiency, this normalization can be conducted every n mini-batches. The performance is expected to be not much impacted, given that n is not too large. where W is the projection matrix to be learned, and x i and z i are word vectors in the source and target language respectively. The bilingual pair (x i , z i ) indicates that x i and z i are identical in semantic meaning. A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation<cite> (Mikolov et al., 2013b)</cite> . However, we note that the 'closeness' of words in the projection space is measured by the cosine distance, which is fundamentally different from the Euler distance in the objective function (3) and hence causes inconsistence. We solve this problem by using the cosine distance in the transform learning, so the optimization task can be redefined as follows: Note that the word vectors in both the source and target vector spaces are normalized, so the inner product in (4) is equivalent to the cosine distance.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_4",
  "x": "1 For efficiency, this normalization can be conducted every n mini-batches. The performance is expected to be not much impacted, given that n is not too large. where W is the projection matrix to be learned, and x i and z i are word vectors in the source and target language respectively. The bilingual pair (x i , z i ) indicates that x i and z i are identical in semantic meaning. A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation<cite> (Mikolov et al., 2013b)</cite> . However, we note that the 'closeness' of words in the projection space is measured by the cosine distance, which is fundamentally different from the Euler distance in the objective function (3) and hence causes inconsistence. We solve this problem by using the cosine distance in the transform learning, so the optimization task can be redefined as follows: Note that the word vectors in both the source and target vector spaces are normalized, so the inner product in (4) is equivalent to the cosine distance. A problem of this change, however, is that the projected vector W x i has to be normalized, which is not guaranteed so far. To solve the problem, we first consider the case where the dimensions of the source and target vector spaces are the same.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_5",
  "x": "**MONOLINGUAL WORD EMBEDDING** The monolingual word embedding is conducted with the data published by the EMNLP 2011 SMT workshop (WMT11) 2 . For an easy comparison, we largely follow Mikolov's settings in<cite> (Mikolov et al., 2013b)</cite> and set English and Spanish as the source and target language, respectively. The data preparation involves the following steps. Firstly, the text was tokenized by the standard scripts provided by WMT11 3 , and then duplicated sentences were removed. The numerical expressions were tokenized as 'NUM', and special characters (such as !?,:) were removed. The word2vector toolkit 4 was used to train the word embedding model. We chose the skip-gram model and the text window was set to 5. The training resulted in embedding of 169k English tokens and 116k Spanish tokens. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_6",
  "x": "The average of the results of the 10 tests is reported as the final result. Note that not all the words translated by Google are in the vocabulary of the target language; the vocabulary coverage is 99.5% in our test. ---------------------------------- **RESULTS WITH LINEAR TRANSFORM** We first reproduce Mikolov's work with the linear transform. A number of dimension settings are experimented with and the results are reported in Table 1. The proportions that the correct translations are in the top 1 and top 5 candidate list are reported as P@1 and P@5 respectively. As can be seen, the best dimension setting is 800 for English and 200 for Spanish, and the corresponding P@1 and P@5 are 35.36% and 53.96%, respectively. These results are comparable with the results reported in <cite>(Mikolov et al., 2013b</cite> ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_0",
  "x": "Given a word f , its distributional profile is: V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i : To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f 1 and f 2 is: where V is the vocabulary. Note that in Eqn.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_1",
  "x": "A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f , its distributional profile is: V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i : To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f 1 and f 2 is: where V is the vocabulary.",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_2",
  "x": "where V is the vocabulary. Note that in Eqn. (2) w i 's are the words that appear in the context of f 1 or f 2 , otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase. ---------------------------------- **PARAPHRASES FROM BILINGUAL PIVOTING** Bilingual pivoting uses parallel corpora between the source language, F , and a pivot language T . If two phrases, f 1 and f 2 , in a same language are paraphrases, then they share a translation in other languages with p(f 1 |f 2 ) as a paraphrase score: (1 and 6) are phrases from the SMT phrase table (unfilled nodes are not).",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_3",
  "x": "It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of features. These features can be used by a log linear model to score paraphrases (Zhao et al., 2008) . We used a linear combination of these features using the equation in Sec. 3 of (Ganitkevitch and Callison-Burch, 2014) to score paraphrase pairs. PPDB version 1 is broken into different levels of coverage. The smaller sizes contain only better-scoring, high-precision paraphrases, while larger sizes aim for high coverage. ---------------------------------- **METHODOLOGY** After paraphrase extraction we have paraphrase pairs, (f 1 , f 2 ) and a score S(f 1 , f 2 ) we can induce new translation rules for OOV phrases using the steps in Algo. (1): 1) A graph of source phrases is constructed as in<cite> (Razmara et al., 2013)</cite> ; 2) translations are propagated as labels through the graph as explained in Fig. 2 ; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_4",
  "x": "We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in<cite> (Razmara et al., 2013)</cite> . The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2). ---------------------------------- **PHRASE TABLE INTEGRATION** After propagation, for each potential OOV phrase we have a list of possible translations with corresponding probabilities. A potential OOV is any phrase which does not appear in training, but could appear in unseen data. We do not look at the dev or test data to produce the augmented phrase table. The original phrase table is now augmented with new entries providing translation candidates for potential OOVs; Last column in Table 2 shows how many entries have been added to the phrase table for each experimental settings.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_5",
  "x": "They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to<cite> Razmara et al. (2013)</cite> . In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. ---------------------------------- **GRAPH RANDOM WALKS** Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights. Optimization for the Modified Adsorption (MAD) objective function in Sec. 3.2 can be viewed as a controlled random walk (Talukdar et al., 2008; Talukdar and Crammer, 2009 ). This is formalized as three actions: inject, continue and abandon with corresponding pre-defined probabilities P inj , P cont and P abnd respectively as in (Talukdar and Crammer, 2009) . A random walk through the graph will transfer labels from one node to another node, and probabilities P cont and P abnd control exploration of the graph.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_6",
  "x": "This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with<cite> Razmara et al. (2013)</cite> on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. KenLM (Heafield, 2011 ) is used to train a 5-gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework 3 . We use maximum phrase length 10. For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM). Each graph propagation iteration takes about 3 minutes. For French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results, this works very well in our data. For Arabic, AQMAR is used to exclude named-entities (Mohit et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_7",
  "x": "The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). ---------------------------------- **CASE 1: LIMITED PARALLEL DATA** In this experiment we use a setup similar to (Razmara et al., 2013 we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in<cite> (Razmara et al., 2013)</cite> . ACL/WMT 2005 4 is used for dev and test data. We re-implement their paraphrase extraction method (DP) to extract paraphrases from French side of Europarl (2M sentences). We use unigram nodes to construct graphs for both DP and PPDB. In bipartite graphs, each node is connected to at most 20 nodes. For tripartite graphs, each node is connected to 15 labeled and 5 unlabeled nodes.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_0",
  "x": "For example, Bhat introduced a method for Kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [2] . Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible<cite> [5]</cite> . Informal/irregular forms usually do not obey the conventional rules in the languages. For instance, 'khunh' (home) is a frequent form for 'khanh' in Persian conversations or 'goood ' and 'good ' are used interchangeably in English tweets. In this paper, we propose a statistical technique for finding inflectional and derivation formations of words. To this end, we introduce an unsupervised method to cluster all morphological variants of a word. The proposed algorithm learns linguistic patterns to match a word and its morphological variants based on a given large collection of documents, which is readily available on the Web. A linguistic pattern captures a transformation rule between a word and its morphological variant. The extracted rules indicate which letters in which positions of a word should be modified. Affix characters, positions of the characters, operations on the characters based on the minimum edit distance (MED) algorithm (i.e., insertion or deletion) [10] , and part-of-speech (POS) tag of the input word are the attributes of a rule.",
  "y": "motivation"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_1",
  "x": "In this section we provide a framework for the stemming algorithm to evaluate its effectiveness. We use dictionary-based cross-lingual information retrieval (CLIR) to this end. In highly inflected languages, bilingual dictionaries contain only original forms of the words. Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8, 4, 12] , or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i . c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_2",
  "x": "In highly inflected languages, bilingual dictionaries contain only original forms of the words. Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8, 4, 12] , or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i . c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part).",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_3",
  "x": "Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part). ---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** The statistics of the collection used for both rule extraction and evaluation is provided in Table 2 . We employed the statistical language modeling framework with Kullback-Leibler similarity measure of Lemur toolkit for our retrieval task.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_4",
  "x": "Queries are expanded by the top 50 terms generated by the feedback model [14, 6] . We removed Persian stop words from the queries and documents [4, <cite>5]</cite> . We used STeP1 [13] in our stemming process in Persian. We also stem the source English queries in all experiments with the Porter stemmer. We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries<cite> [5]</cite> . We have exploited 40 Persian POS tags in our experiments. 2 The retrieval results are mainly evaluated by Mean Average Precision (MAP) over top 1000 retrieved documents. Significance tests are computed using two-tailed paired t-test with 95% confidence. Precision at top 5 documents (P@5) and top 10 documents (P@10) are also reported.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_5",
  "x": "We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries<cite> [5]</cite> . We have exploited 40 Persian POS tags in our experiments. 2 The retrieval results are mainly evaluated by Mean Average Precision (MAP) over top 1000 retrieved documents. Significance tests are computed using two-tailed paired t-test with 95% confidence. Precision at top 5 documents (P@5) and top 10 documents (P@10) are also reported. ---------------------------------- **COMPARING DIFFERENT MORPHOLOGICAL PROCESSING METHODS** In this section we aim at evaluating the proposed SS4MCT method. To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11] , rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator 3 , and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system.",
  "y": "background"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_6",
  "x": "---------------------------------- **COMPARING DIFFERENT MORPHOLOGICAL PROCESSING METHODS** In this section we aim at evaluating the proposed SS4MCT method. To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11] , rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator 3 , and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system. On the other hand, we run another set of experiments without applying any morphological processing method similar to the Persian state-of-the-art CLIR methods. Iterative translation disambiguation (ITD) [11] , joint cross-lingual topical relevance model (JCLTRLM) [7] , top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in<cite> [5]</cite> (assume |c i | = 0), are the baselines without any morphological processing units. As shown in Table 3 BiCTM outperforms all the baselines when there is no morphological processing unit. Although the improvement compared to JCLTRLM is not statistically significant, for simplicity we assume this model as a base of comparisons in the next set of experiments. In other words, we study the effect of the morphological processing units on the performance of BiCTM. As shown in Table 3 the performance of the CLIR task degraded when we use the SPLIT approach.",
  "y": "uses"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_0",
  "x": "Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004 ) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German).",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_1",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004 ) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German). The ninth and tenth papers (Langone et al., 2004; Zabokrtsk\u00fd and Lopatkov\u00e1, 2004) are respectively about a corpus related to a lexicon and the reverse: a lexicon related to a corpus. This opens up the wider theme of the intergration of a number of different linguistic resources. As the natural language community produces more and more linguistic resources, especially corpora, it seems important to step back and look at the larger picture.",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_2",
  "x": "This can be a problem if one rejects some aspects of that theory. Also one may object to a particular system of annotation because some theories generalize to cover new ground (e.g., new languages) better than others. Nevertheless, advantages of accepting a corpus as standard include the following: It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_3",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German). The ninth and tenth papers (Langone et al., 2004; Zabokrtsk\u00fd and Lopatkov\u00e1, 2004) are respectively about a corpus related to a lexicon and the reverse: a lexicon related to a corpus. This opens up the wider theme of the intergration of a number of different linguistic resources. As the natural language community produces more and more linguistic resources, especially corpora, it seems important to step back and look at the larger picture.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_0",
  "x": "Traditionally, the term similar refers to semantic similarity (e.g. walking should be close to hiking, and happiness to joy), hence the model performance is usually evaluated using semantic similarity datasets. Recently, several works introduced morphology-driven models motivated by the poor performance of traditional models on morphologically complex words. Such words are often rare, and there is not enough evidence to model them correctly. The morphology-driven models allow pooling evidence from different words which have the same base form. These models work by learning per-morpheme representations rather than just per-word ones, and compose the representing vector of each word from those of its morphemes -as derived from a supervised or unsupervised morphological analysis -and (optionally) its surface form (e.g. walking = f (v walk , v ing , v walking )). The works differ in the way they acquire morphological knowledge (from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other) and in the model form (cDSMs (Lazaridou et al., 2013) , RNN (Luong et al., 2013) , LBL (Botha and Blunsom, 2014) , CBOW (Qiu et al., 2014) , SkipGram (Soricut and Och, 2015; <cite>Bojanowski et al., 2016)</cite> , GGM (Cotterell et al., 2016) ). But essentially, they all show that breaking a word into morphological components (base form, affixes and potentially also the complete surface form), learning a vector for each component, and representing a word as a composition of these vectors improves the models semantic performance, especially on rare words. In this work we argue that these models capture two distinct aspects of word similarity, semantic (e.g. sim(walking, hiking) > sim(walking, eating)) and morphological (e.g. sim(walking, hiking) > sim (walking, hiked) ), and that these two aspects are at odds with each other (should sim(walking, hiking) be lower or higher than sim(walking, walked)?). The base form component of the compositional models is mostly responsible for semantic aspects of the similarity, while the affixes are mostly responsible for morphological similarity. This analysis brings about several natural questions: is the combination of semantic and morphological components used in previous work ideal for every purpose?",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_1",
  "x": "The n-grams are used to approximate the morphemes in the target word. We generalize<cite> Bojanowski et al (2016)</cite> by replacing the set of ngrams G(w) with a set P(w) of explicit linguistic properties. Each word w t is then composed as the sum of the vectors of its linguistic properties: v wt = p\u2208P(wt) v p . The linguistic properties we consider are the surface form of the word (W), it's lemma (L) and its morphological tag (M) 1 . The lemma corre-sponds to the base-form, and the morphological tag encodes the grammatical properties of the word, from which its inflectional affixes are derived (a similar approach was taken by Cotterell and Sch\u00fctze (2015) ). Moving from a set of ngrams to a set of explicit linguistic properties, allows finer control of the kinds of information in the word representation. We train models with different subsets of {W, L, M }. ---------------------------------- **EXPERIMENTS AND RESULTS** Our implementation is based on the fastText 2 library<cite> (Bojanowski et al., 2016)</cite> , which we modify as described above.",
  "y": "extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_2",
  "x": "Most of the works on morphology-driven models evaluate the semantic performance of the models, while others perform morphological evaluation. To the best of our knowledge, this work is the first to evaluate both aspects. While our experiments focus on Modern Hebrew due to the availability of a reliable semantic similarity dataset, we believe our conclusions hold more generally. ---------------------------------- **MODELS** Our model form is a generalization of the fastText model<cite> (Bojanowski et al., 2016)</cite> , which in turn extends the skip-gram model of Mikolov et al (2013) . The skip-gram model takes a sequence of words w 1 , ..., w T and a function s assigning scores to (word, context) pairs, and maximizes where \u2113 is the log-sigmoid loss function, C t is a set of context words, and N t is a set of negative examples sampled from the vocabulary. s(w t , w c ) is defined as s(w t , w c ) = v \u22a4 wt u wc (where v wt and u wc are the embeddings of the focus and the context words). Bojanowski et al (2016) replace the word representation v wt with the set of character ngrams appearing in it: v wt = g\u2208G(wt) v g where G(w t ) is the set of n-grams appearing in w t .",
  "y": "similarities extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_3",
  "x": "We train models with different subsets of {W, L, M }. ---------------------------------- **EXPERIMENTS AND RESULTS** Our implementation is based on the fastText 2 library<cite> (Bojanowski et al., 2016)</cite> , which we modify as described above. We train the models on the Hebrew Wikipedia (\u223c4M sentences), using a window size of 2 to each side of the focus word, and dimensionality of 200. We use the morphological disambiguator of Adler (2007) to assign words with their morphological tags, and the inflection dictionary of MILA (Itai and Wintner, 2008) Semantic Evaluation Measure The common datasets for semantic similarity 4 have some notable shortcomings as noted in (Avraham and Goldberg, 2016; Faruqui et al., 2016; Batchkarov et al., 2016; Linzen, 2016) . We use the evaluation method (and corresponding Hebrew similarity dataset) that we have introduced in a previous work (Avraham and Goldberg, 2016) (AG). The AG method defines an annotation task which is more natural for human judges, resulting in datasets with improved annotator-agreement scores. Furthermore, the AG's evaluation metric takes annotator agreement into account, by putting less weight on similarities that have lower annotator agreement. An AG dataset is a collection of target-groups, where each group contains a target word (e.g. singer) and three types of candidate words: positives which are words \"similar\" to the target (e.g. musician), distractors which are words \"related but dissimilar\" to the target (e.g. microphone), and randoms which are not related to the target at all (e.g laptop).",
  "y": "extends"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_0",
  "x": "Use of multi-view feature representation demonstrated significant reduction in word error rates (WERs) compared to the use of individual features by themselves. In addition, when articulatory information was used as an additional input to a fused deep neural network (DNN) and CNN acoustic model, it was found to demonstrate further reduction in WER for the Switchboard subset and the CallHome subset (containing partly non-native accented speech) of the NIST 2000 conversational telephone speech test set, reducing the error rate by 12% relative to the baseline in both cases. This work shows that multi-view features in association with articulatory information can improve speech recognition robustness to spontaneous and non-native speech. ---------------------------------- **INTRODUCTION** Spontaneous speech typically contains a significant amount of variation, which makes it difficult to model in automatic speech recognition (ASR) systems. Such variability stems from varying speakers, pronunciation variations, speaker stylistic differences, varying recording conditions and many other factors. Recognizing words from conversational telephone speech (CTS) can be quite difficult due to the spontaneous nature of speech, its informality, speaker variations, hesitations, disfluencies etc. The Switchboard and Fisher [1] data collections are large collection of CTS datasets that have been used extensively by researchers working on conversational speech recognition [2, 3, 4, 5, 6] . Recent trends in speech recognition [7, <cite>8,</cite> 9] have demonstrated impressive performance on Switchboard and Fisher data.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_1",
  "x": "Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions. Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , <cite>VGG-nets</cite> <cite>[8]</cite> , have significantly improved recognition performance, bringing them closer to human performance [9] . Both abundance of data and sophistication of deep learning algorithms have symbiotically contributed to the advancement of speech recognition performance. The role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. This paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets: Switchboard and Fisher, when evaluated on the NIST 2000 CTS test set [2] . We investigated a traditional CNN model and explored the following: (1) Use of multiple features both in isolation and in combination. Our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel-filterbank (MFB) features. Articulatory features were found to be useful for improving recognition performance on both Switchboard and CallHome subsets of the NIST 2000 CTS test set. These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_2",
  "x": "The role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. This paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets: Switchboard and Fisher, when evaluated on the NIST 2000 CTS test set [2] . We investigated a traditional CNN model and explored the following: (1) Use of multiple features both in isolation and in combination. Our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel-filterbank (MFB) features. Articulatory features were found to be useful for improving recognition performance on both Switchboard and CallHome subsets of the NIST 2000 CTS test set. These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory. For the sake of simplicity, we used a CNN acoustic model in our experiment, where the baseline system's performance is directly comparable to the state-of-the-art CNN performance reported in <cite>[8]</cite> . We expect our results using the CNN to carry over into other neural network architectures as well. The outline of the paper is as follows. In Section 2 we present the dataset and the recognition task.",
  "y": "uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_3",
  "x": "We explored learning the fMLLR transform directly from the filterbank features (MFB_fMLLR and DOC_fMLLR) and learning the fMLLR transforms on the full dimensional cepstral versions of the features, applying the transform and then performing IDCT (MFB+fMLLR and DOC+fMLLR). Table 1 shows that the performance of fMLLR transforms learned from the cepstral version of the features are better than the ones directly from the filterbank features, which is expected, as the cepstral features are uncorrelated, which adheres to the diagonal covariance assumption of the GMM models used to learn those transforms. Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> . Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set. As a next step, we investigated the efficacy of feature combination and focused only on the CNN acoustic models. We appended the articulatory features (TVs) extracted from the SWB training set, dev04 and NIST 2000 CTS test sets, and combined them with MFB+fMLLR and DOC+fMLLR features, respectively. Finally, we combined the MFB+fMLLR and DOC+fMLLR features and added the TVs to them. Table 2 Table 2 shows that the use of articulatory features helped to lower the WER in all the cases. The DOC feature was always found to perform slightly better than the MFBs and the best results were obtained when all the features were combined together, indicating the benefit of using multiview features.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_4",
  "x": "Note that the CH subset of the NIST 2000 CTS test set was more challenging than the SWB subset, as it contains non-native speakers of English, hence introducing accented speech into the evaluation set. The use of articulatory features helped to reduce the error rates for both SWB and CH test sets, indicating their robustness to model spontaneous speech in both native (SWB) and non-native (CH) speaking styles. The FSH corpus contains speech from quite a diverse set of speakers, helping to reduce the WER of the CH subset more significantly than the SWB subset, a trend reflected in results reported in the literature <cite>[8]</cite> . Table 4 shows the system fusion results after dumping 2000-best lists from the rescored lattices from each individual system of different front-end features with fMLLR, i.e., MFB, DOC, MFB+DOC, MFB+DOC+TV, then conducting M-way combination of the subsystems using N-best ROVER [27] implemented in SRILM [28] . In this system fusion experiment, all subsystems have equal weights for N-best ROVER. As can be seen from the table, N-best ROVER based 2-way and 3-way system fusion produced a further 2% and 4% relative reduction in WER compared to the best single system (MFB+fMLLR + DOC+fMLLR + TV), for SWB and CH evaluation sets respectively. Note that the first row of Table 4 is the last row of Table 3 , i.e., the best single system. The last row 4way fusion is from combining the 4 individual systems presented in Table 3 . ---------------------------------- **CONCLUSION**",
  "y": "similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_5",
  "x": "Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set. As a next step, we investigated the efficacy of feature combination and focused only on the CNN acoustic models. We appended the articulatory features (TVs) extracted from the SWB training set, dev04 and NIST 2000 CTS test sets, and combined them with MFB+fMLLR and DOC+fMLLR features, respectively. Finally, we combined the MFB+fMLLR and DOC+fMLLR features and added the TVs to them. Table 2 Table 2 shows that the use of articulatory features helped to lower the WER in all the cases. The DOC feature was always found to perform slightly better than the MFBs and the best results were obtained when all the features were combined together, indicating the benefit of using multiview features. Note that only 100 additional neurons were used to accommodate the TV features, hence all the models were of comparable sizes. The benefit of the articulatory features stemmed from the complementary information that they contain (reflecting degree and location of articulatory constrictions in the vocal tract), as demonstrated by earlier studies [22] [23] [24] . Overall the f-CNN-DNN system trained with the combined feature set, MFB+fMLLR + DOC+fMLLR + TV, demonstrated a relative reduction in WER of 7% and 9% compared to the MFB+fMLLR CNN baseline for SWB and CH subsets of the NIST 2000 CTS test set. Table 1 and 2 also demonstrates that sequence training always gave additive performance gain over crossentropy training, supporting the in <cite>[8,</cite> 21] .",
  "y": "similarities uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_6",
  "x": [
   "**CONCLUSION** We reported the results exploring multiple features for ASR on English CTS data. We observed that the fMLLR transform helped reduce the WER of the baseline system significantly. We observed that using multiple acoustic features helped in improving the overall accuracy of the system. Use of robust features and articulatory features significantly reduced the WER for the more challenging CallHome subset of the NIST 2000 CTS evaluation set, with accented speech in that subset. We developed a fused-CNN-DNN architecture, where input convolution was only performed on the acoustic features and the articulatory features were process by a feed-forward layer. We found this architecture effective for combining acoustic features and articulatory features. The robust features and articulatory features capture complementary information, and the addition of them resulted in the best single system performance, with 12% relative reduction of WER on SWB and CH evaluation sets respectively, compared to the MFB+fMLLR CNN baseline. Note that in this study the language model has not been optimized. Future studies should investigate RNN or other neural network-based language modeling techniques that are known to perform better than word n-gram LMs."
  ],
  "y": "future_work"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_7",
  "x": "We initially validated the performance of the features (MFB, DOC and TVs) using the 360 hours SWB training dataset. The baseline DNN and CNN models had six and five hidden layers respectively, with 2048 neurons in each layer, and were trained with MFB features and its fMLLR transformed version (MFB+fMLLR). The NIST RT-04 dev04 dataset (3 hour test set from Fisher, containing 36 conversations) [2] was used as the cross-validation set during the acoustic model training step. Table 1 presents the word error rates (WER) from the baseline CNN model trained with the SWB data when evaluated on the NIST 2000 CTS test set, for both cross-entropy (CE) training and sequence training (ST) using MMI. Table 1 also shows the results obtained from the DOC features with and without a fMLLR transform. We present results from ST as they were found to be always better than the results CE training. We explored learning the fMLLR transform directly from the filterbank features (MFB_fMLLR and DOC_fMLLR) and learning the fMLLR transforms on the full dimensional cepstral versions of the features, applying the transform and then performing IDCT (MFB+fMLLR and DOC+fMLLR). Table 1 shows that the performance of fMLLR transforms learned from the cepstral version of the features are better than the ones directly from the filterbank features, which is expected, as the cepstral features are uncorrelated, which adheres to the diagonal covariance assumption of the GMM models used to learn those transforms. Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_0",
  "x": "****DEPENDENCY-BASED BILINGUAL LANGUAGE MODELS FOR REORDERING IN STATISTICAL MACHINE TRANSLATION**** **ABSTRACT** This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by <cite>Niehues et al. (2011)</cite> . Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM. An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit. ---------------------------------- **INTRODUCTION**",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_1",
  "x": "In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_2",
  "x": "This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_3",
  "x": "In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
  "y": "uses extends"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_4",
  "x": "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008) . Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008) , of a target string (Shen et al., 2008) , or both (Chiang, 2007; Chiang, 2010) . Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010) . Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during decoding by adding precomputed fragments from the source parse tree.",
  "y": "motivation background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_5",
  "x": "Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010) . Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during decoding by adding precomputed fragments from the source parse tree. The idea to combine the merits of the two SMT paradigms has been proposed before, where Huang and Mi (2010) introduce incremental decoding for a tree-based model. On a very general level, our approach is similar to theirs in that it keeps track of a sequence of source syntactic subtrees that are being translated at consecutive decoding steps. An important difference is that they keep track of whether the visited subtrees have been fully translated, while in our approach, once a syntactic structural unit has been added to the history, it is not updated anymore. In this paper, we focus on source syntactic information. During decoding we have full access to the source sentence, which allows us to obtain a better syntactic analysis (than for a partial sentence) and to precompute the units that the model operates with. We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient? Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_6",
  "x": "By including both source and target information into the representation of translation events we ob- tain a bilingual LM. The richer representation allows for a finer distinction between reorderings. For example, Arabic has a morphological marker of definiteness on both nouns and adjectives. If we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model. This kind of intuition underlies the model of <cite>Niehues et al. (2011)</cite> , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows: where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it. <cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_7",
  "x": "The richer representation allows for a finer distinction between reorderings. For example, Arabic has a morphological marker of definiteness on both nouns and adjectives. If we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model. This kind of intuition underlies the model of <cite>Niehues et al. (2011)</cite> , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows: where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it. <cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens. Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_8",
  "x": "Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity. Another disadvantage comes from the fact that we want to compare permutations of the same set of elements. For example, the two different segmentations of ba into [ba] and [b] [a] still represent the same permutation of the sequence ab. In Figure 1 one can produce a segmentation of (AsEAr Albtrwl, oil prices) into (Albtrwl, oil) and (AsEAr, prices) or leave it as is. If we allow for both segmentations, the learnt probability parameters may be different for the sum of (Albtrwl, oil) and (AsEAr, prices) and for the unsegmented phrase. Durrani et al. (2011) introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1 compares the BiLM and MTU tokenization for a specific example. Since <cite>Niehues et al. (2011)</cite> have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to <cite>Niehues et al. (2011)</cite> as the original BiLM. 4 At the same time, we do not see any specific obstacles for combining our work with MTUs. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_10",
  "x": "<cite>Niehues et al. (2011)</cite> also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part). Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence. Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of \u221210.25 for the incorrect reordering than for the correct one (\u221210.39). Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality. 5 Section 4 contains details about data and software setup. ---------------------------------- **BILM WITH DEPENDENCY INFORMATION** Dependency grammar is commonly used in NLP to formalize role-based relations between words. The intuitive notion of syntactic modification is captured by the primitive binary relation of dependence. Dependency relations do not change with the linear order of words ( Figure 2 ) and therefore can provide a characterization of a word's syntactic class that invariant under reordering.",
  "y": "motivation background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_11",
  "x": "In the next section, we explore different representations based on source dependency trees. ---------------------------------- **DEPENDENCY-BASED BILM** In this section, we introduce our model which combines the BiLM from <cite>Niehues et al. (2011)</cite> with source dependency information. We further give details on how the proposed models are trained and integrated into a phrase-based decoder. ---------------------------------- **THE GENERAL FRAMEWORK** In the previous section we outlined our framework as composed of two steps: First, a parallel sentence is tokenized according to the BiLM model<cite> (Niehues et al., 2011)</cite> . Next, words in the bilingual tokens are substituted with their contextual properties. It is thus convenient to use the following generalized definition for a token sequence t 1 ...t n in our framework:",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_13",
  "x": "**ARABIC-ENGLISH TRANSLATION EXPERIMENTS** We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by <cite>Niehues et al. (2011)</cite> : the standard one, Lex\u2022Lex, and the simplest syntactic one, Pos\u2022Pos. Results for the experiments can be found in Table 2 . In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09. Table 2 .a-b compares the performance of the baseline and original BiLM systems. Lex\u2022Lex yields strongly significant improvements over the baseline for BLEU and weakly significant improvements for TER. Therefore, for the rest of the experiments we are interested in obtaining further improvements over Lex\u2022Lex. Pos\u2192Pos\u2022Pos (Table 2 .c) demonstrates the effect of adding minimal dependency information to a BiLM.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_14",
  "x": "**CONCLUSIONS** In this paper, we have introduced a simple, yet effective way to include syntactic information into phrase-based SMT. Our method consists of enriching the representation of units of a bilingual language model (BiLM). We argued that the very limited contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units. In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models. The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems. A number of additional evaluations provided an indication for better modeling of reordering phenomena. The proposed dependency-based BiLMs resulted in an increase in 4-gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit. In this paper, we have focused on rather elementary dependency relations, which we are planning to expand on in future work. Our current approach is still strictly tied to the number of target tokens.",
  "y": "background motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_0",
  "x": "On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas nontrainable benchmarks can be confused by other types of information also encoded in the representation of a sentence. ---------------------------------- **INTRODUCTION** Neural Machine Translation (NMT) has rapidly become the new Machine Translation (MT) paradigm, significantly improving over the traditional statistical machine translation procedure . Recently, several models and variants have been proposed with increased research efforts towards multilingual machine translation (Firat et al., 2016; Lakew et al., 2018; Wang et al., 2018; Blackwood et al., 2018; Lu et al., 2018) . The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) . Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_1",
  "x": "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model. We also show that multilingual models improve trainable downstream tasks even further, demonstrating the additional abstraction that is pushed into the representations through additional translation tasks involved in training. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_2",
  "x": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model. We also show that multilingual models improve trainable downstream tasks even further, demonstrating the additional abstraction that is pushed into the representations through additional translation tasks involved in training.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_3",
  "x": "In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model. We also show that multilingual models improve trainable downstream tasks even further, demonstrating the additional abstraction that is pushed into the representations through additional translation tasks involved in training. ---------------------------------- **ARCHITECTURE** Our architecture follows the standard setup of an encoder-decoder model in machine translation with a traditional attention mechanism (Luong et al., 2015) . However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018) , plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> . Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_4",
  "x": "We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model. We also show that multilingual models improve trainable downstream tasks even further, demonstrating the additional abstraction that is pushed into the representations through additional translation tasks involved in training. ---------------------------------- **ARCHITECTURE** Our architecture follows the standard setup of an encoder-decoder model in machine translation with a traditional attention mechanism (Luong et al., 2015) . However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018) , plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_5",
  "x": "Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations. The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks. (2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks. Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (\u03c1) correlation coefficients (r/\u03c1). The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column. Results with \u2020 taken from<cite> C\u00edfka and Bojar (2018).</cite>",
  "y": "similarities"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_6",
  "x": "3 In order to obtain a sentence vector out of multiple attention heads we apply mean pooling over the attention bridge. We are also interested in the translation quality to verify the appropriateness of our models with respect to the main objective they are trained for. For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task. Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations. The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks. (2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_7",
  "x": "(2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks. Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (\u03c1) correlation coefficients (r/\u03c1). The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column. Results with \u2020 taken from<cite> C\u00edfka and Bojar (2018).</cite> of multilingual training. We can see that multilingual training objectives are generally helpful for the trainable downstream tasks. Particularly interesting is the fact that the Manyto-Many model performs best on average even though it does not add any further training examples for English (compared to the other multilingual models), which is the target language of the downstream tasks. This suggests that the model is able to improve generalizations even from other language pairs (DE-ES, FR-ES, FR-DE) that are not directly involved in training the representations of English sentences. Comparing against benchmarks, our results are in line with competitive baselines (Arora et al., 2017) . While our aim is not to beat the state of the art trained on different data, but rather to understand the impact of various sizes of attention heads in a bi-and multilingual scenario, we argue that a larger attention bridge and multilinguality constitute a preferable starting point to learn more meaningful sentence representations.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_8",
  "x": "While our aim is not to beat the state of the art trained on different data, but rather to understand the impact of various sizes of attention heads in a bi-and multilingual scenario, we argue that a larger attention bridge and multilinguality constitute a preferable starting point to learn more meaningful sentence representations. 5 SentEval: Similarity tasks Table 2 summarizes the results using Pearson's and Spearman's coefficient on the two SentEval supervised textual similarity tasks, SICK-R and STSB, and the average Pearson's measure on the remaining unsupervised similarity tasks. Two different trends become visible: i) On the unsupervised textual similarity tasks, having fewer attention heads is beneficial. Contrary to the results in the classification tasks, the best overall k=1 k=10 k=25 k=50 M-to-M Table 3 : BLEU scores for multilingual models. Baseline system in the right-most column. model is provided by a bilingual setting with only one attention head. This is in line with the findings of<cite> C\u00edfka and Bojar (2018)</cite> and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training. More surprising is the negative effect of the multilingual models. We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case. ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_0",
  "x": "**ABSTRACT** In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance. ---------------------------------- **INTRODUCTION** A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph<cite> (Das et al., 2018</cite>; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) .",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_1",
  "x": "Rather than having only two rewards, a positive and a negative reward, we introduce a ternary reward structure that also rewards agents for not answering a question. A higher reward is given to the agent for correctly answering a question compared to not answering a question. In this setup the agent learns to make a trade-off between these three possibilities to obtain the highest total reward over all questions. Additionally, because the search space of possible paths exponentially grows with the number of hops, we also investigate using Depth-First-Search (DFS) algorithm to collect paths that lead to the correct answer. We use these paths as a supervised signal for training the neural network before the reinforcement learning algorithm is applied. We show that this improves overall performance. ---------------------------------- **RELATED WORK** The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and <cite>Das et al. (2018)</cite> , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer. 1 Other approaches consider this as a link prediction problem in which multi-hop reasoning can be used to learn relational paths that link two entities.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_2",
  "x": "This is different from our setup in which there are no ground truth 'no answer' labels. ---------------------------------- **BACKGROUND: REINFORCEMENT LEARNING** We base our work on the recent reinforcement learning approaches introduced in <cite>Das et al. (2018)</cite> and Lin et al. (2018) . We denote the knowledge graph as G, the set of entities as E, the set of relations as R and the set of directed edges L between entities of the form l = (e 1 , r, e 2 ) with e 1 , e 2 \u2208 E and r \u2208 R. The goal is to find an answer entity e a given a question entity e q and the question relation r q , when (e q , r q , e a ) is not part of graph G. We formulate this problem as a Markov Decision Problem (MDP) (Sutton and Barto, 1998) with the following states, actions, transition function and rewards: States. At every timestep t, the state s t is defined by the current entity e t , the question entity e q and relation r q , for which e t , e q \u2208 E and r q \u2208 R. More formally, s t = (e t , e q , r q ). Actions. For a given entity e t , the set of possible actions is defined by the outgoing edges from e t .",
  "y": "uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_3",
  "x": "The transition function \u03b4 maps s t to a new state s t+1 based on the action taken by the agent. Consequently, s t+1 = \u03b4(s t , A t ) = \u03b4(e t , e q , r q , A t ). Rewards. The agent is rewarded based on the final state. For example, in <cite>Das et al. (2018)</cite> and Lin et al. (2018) the agent obtains a reward of 1 if the correct answer entity is reached as the final state and 0 otherwise (i.e., R(s T ) = I{e T = e a }). Figure 2a illustrates the LSTM which encodes history of the path taken. The output at timestep t is used as input to the policy network, illustrated in Figure 2b , to determine which action to take next. ---------------------------------- **TRAINING** We train a policy network \u03c0 using the REIN-FORCE algorithm of Williams (1992) which maximizes the expected reward:",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_4",
  "x": "Note that it is not guaranteed that the set of paths found using DFS are all most efficient. However as we show in our experiments, bootstrapping with these paths provide good initialization for the reinforcement learning algorithm. ---------------------------------- **TERNARY REWARD STRUCTURE** As mentioned previously, we encounter situations when the answer entity cannot be reached in the limited number of steps taken by an agent. In such cases, the system should return a special answer 'no answer' as the response. We can achieve this by adding a synthetic 'no answer' action that leads to a special entity e N OAN SW ER . This is illustrated in Figure 3 . In the framework of <cite>Das et al. (2018)</cite> a binary reward is used which rewards the learner for the answer being wrong or correct. Following a similar protocol, we could award a score of 1 to return 'no answer' when there is no answer available in the KG.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_5",
  "x": "Both the public dataset and the proprietary dataset are <cite>Das et al. (2018)</cite> , using the same train/val/test splits for FB15k-237. For Alexa69k-378 we use 10% of the full dataset for validation and test. For both datasets, we add the reverse relations of all relations in the training set in order to facilitate backward navigation following the approach of previous work. Similarly, a 'no op' relation is added for each entity between the entity and itself, which allows the agent to loop/reason multiple consecutive steps over the same entity. An overview of both datasets can be found in Table 3 . We extend the publicly available implementation of <cite>Das et al. (2018)</cite> for our experimentation. We set the size of the entity and relation representations d at 100 and the hidden state at 200. We use a single layer LSTM and train models with path length 3 (tuned using hyper-parameter search). We optimize the neural network using Adam (Kingma and Ba, 2015) with learning rate 0.001, mini-batches of size 256 with 20 rollouts per example. During the test time, we use beam search with the beam size of 100.",
  "y": "similarities uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_6",
  "x": "Unlike <cite>Das et al. (2018)</cite> , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than <cite>Das et al. (2018)</cite> . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over <cite>Das et al. (2018)</cite> and Lin et al. (2018) .",
  "y": "extends differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_0",
  "x": "Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing , POS tagging (Lample et al., 2016) , relation extraction (dos Santos et al., 2015) , translation (Bahdanau et al., 2015) , and joint tasks (Miwa and Bansal, 2016) . However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017) ), this paper -to the best of our knowledge -is the first attempt investigating regularization effects of AT in a joint setting for two related tasks. We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Sch\u00fctze (2017) ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; <cite>Bekoulis et al. (2018a)</cite> ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2). To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_1",
  "x": "These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) . Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition.",
  "y": "background"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_2",
  "x": "These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) . Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_3",
  "x": "Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017) , relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018) . AT is considered as a regularization method. Unlike other regularization methods (i.e., dropout (Srivastava et al., 2014) , word dropout (Iyyer et al., 2015) ) that introduce random noise, AT generates perturbations that are variations of examples easily misclassified by the model. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_4",
  "x": "We train on the mixture of original and adversarial examples, so the final loss is computed as: L JOINT (w;\u03b8) + L JOINT (w + \u03b7 adv ;\u03b8). ---------------------------------- **EXPERIMENTAL SETUP** We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) . We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in <cite>Bekoulis et al. (2018a)</cite> . For the Adverse Drug Events, ADE (Gurulingappa et al., 2012) , we perform 10-fold cross-validation similar to Li et al. (2017) . To obtain comparable results that are not affected by the input embeddings, we use the embeddings of the previous works.",
  "y": "uses"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_5",
  "x": "For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to Miwa and Sasaki (2014) , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of \u223c3% on both tasks compared to <cite>Bekoulis et al. (2018a)</cite> , whose quadratic scoring layer complicates NER. Table 1 and Fig. 2 show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_0",
  "x": "It is typically viewed as lying on a continuum, with expressions such as speed limit and gravy train lying towards the compositional and non-compositional ends of the spectrum, respectively, and expressions such as rush hour and fine line falling somewhere in between as semi-compositional. 1 Compositionality can also be viewed with respect to an individual component word of an MWE, where an MWE component word is compositional if its meaning is reflected in the meaning of the expression. For example, in spelling bee and grandfather clock, the first and second component words, respectively, are compositional, while the others are not. Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010) . In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., and verb-noun idioms (e.g., Salton et al., 2016) .",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_1",
  "x": "Compositionality is the degree to which the meaning of an MWE is predictable from the meanings of its component words. It is typically viewed as lying on a continuum, with expressions such as speed limit and gravy train lying towards the compositional and non-compositional ends of the spectrum, respectively, and expressions such as rush hour and fine line falling somewhere in between as semi-compositional. 1 Compositionality can also be viewed with respect to an individual component word of an MWE, where an MWE component word is compositional if its meaning is reflected in the meaning of the expression. For example, in spelling bee and grandfather clock, the first and second component words, respectively, are compositional, while the others are not. Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010) . In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_2",
  "x": "In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., and verb-noun idioms (e.g., Salton et al., 2016) . Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017) . Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007) , speech recognition (Collins et al., 2005) , and question answering (Chen et al., 2006) . Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003) , part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016) , and stock price prediction (dos Santos Pinheiro and Dras, 2017).",
  "y": "motivation background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_3",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality.",
  "y": "motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_4",
  "x": "These character sequences are fed to the neural network language model, and the hidden state of the neural network at the end of the sequence is taken as the vector representation for that sequence. 2 Once vector representations of an MWE and its component words are obtained, following <cite>Salehi et al. (2015)</cite> , the following equations are then used to compute the compositionality of an MWE: where MWE is the vector representation of the MWE, and C 1 and C 2 are vector representations for the first and second components of the MWE, respectively. 3 In both cases, we use cosine as the similarity measure. comp 1 is based on Reddy et al. (2011) . As shown in equation (1), the compositionality of an MWE is computed based on measuring the similarity of the MWE and each of its component words, and then combining these two similarities into an overall compositionality score. comp 2 is based on Mitchell and Lapata (2010) and measures compositionality by considering the similarity between the MWE and the summation of its component words' vectors. ---------------------------------- **MATERIALS AND METHODS** In this section, we describe the language model and corpus it was trained on, as well as the evaluation dataset and methodology.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_5",
  "x": "We use a publicly available TensorFlow implementation of a character-level RNN language model. 4 We use the following parameter settings as defaults: a two-layer LSTM with one-hot character embeddings and a hidden layer size of 128 dimensions. The batch size, learning rate, and dropout are set 20, 0.002, and 0, respectively. 5 We consider some alternative parameter settings to these defaults in section \u00a74. ---------------------------------- **TRAINING CORPUS** We train language models over a portion of English and German Wikipedia dumps -following <cite>Salehi et al. (2015)</cite> -from 20 January 2018. The raw dumps are preprocessed using WP2TXT 6 to remove wikimarkup, metadata, and XML and HTML tags. The text from Wikipedia contains many characters that are not typically found in MWEs, for example, non-ASCII characters. Such characters drastically increase the size of the vocabulary of the language model, which leads to very long training times.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_6",
  "x": "The proposed model is evaluated over the same three datasets as <cite>Salehi et al. (2015)</cite> , <cite>which</cite> cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions). ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) . (Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009). ---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_7",
  "x": "EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009). ---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided. In this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements. 7 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_8",
  "x": "(Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009). ---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided. In this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements. 7",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_9",
  "x": "---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided. In this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements. 7 ---------------------------------- **RESULTS** We begin by considering results using the default settings (described in section \u00a73.1) using both comp 1 and comp 2 . For comp 1 , we set \u03b1 to 0.7 for ENC and GNC following <cite>Salehi et al. (2015)</cite> ; for EVPC we set \u03b1 to 0.5.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_10",
  "x": "However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions. We now consider the compositionality of individual component words. Because of the low correlations on GNC in the previous experiments, we do not consider it further here. In this case, we compute the compositionality of a specific component word as below, where C is the vector representation of a component word. Note that this corresponds to comp 1 with \u03b1 = 1 or 0, in the case of the first and second component words, respectively. We compare these compositionality predictions with the human judgements for Table 4 : Pearson's correlation (r) for MWEs that are attested, and unattested, in each dataset, using comp 1 and comp 2 . Significant correlations (p < 0.05) are indicated with *.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_11",
  "x": "Results are shown in table 2. For ENC, and the particle component of EVPC, both comp 1 and comp 2 achieve significant correlations (i.e., p < 0.05). However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions. We now consider the compositionality of individual component words. Because of the low correlations on GNC in the previous experiments, we do not consider it further here. In this case, we compute the compositionality of a specific component word as below, where C is the vector representation of a component word. Note that this corresponds to comp 1 with \u03b1 = 1 or 0, in the case of the first and second component words, respectively.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_12",
  "x": "In the case of the compositionality of the particle component of EVPC, for both comp 1 and comp 2 , the correlations for the unattested expressions are higher than for the attested ones, although for unattested expressions the correlations are not significant. The relatively small number of unattested expressions in EVPC (13) could play a role in this finding. To further investigate this, we focused on expressions in EVPC with less than 5 usages in the training corpus. There are 71 such expressions. For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 For GNC, and the verb component of EVPC, in line with the previous results over the entire dataset, neither compositionality measure gives significant correlations, with the exception of the verb component of EVPC using comp 2 for unattested expressions, although again the number of expressions here is relatively small. In an effort to improve on the default setup we considered a range of model variations.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_13",
  "x": "The relatively small number of unattested expressions in EVPC (13) could play a role in this finding. To further investigate this, we focused on expressions in EVPC with less than 5 usages in the training corpus. There are 71 such expressions. For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 For GNC, and the verb component of EVPC, in line with the previous results over the entire dataset, neither compositionality measure gives significant correlations, with the exception of the verb component of EVPC using comp 2 for unattested expressions, although again the number of expressions here is relatively small. In an effort to improve on the default setup we considered a range of model variations. In particular we considered an RNN and GRU (instead of an LSTM), character embeddings of size 25 and 50 (instead of a one-hot representation), increasing the batch size to 100 (from 20), using dropout between 0.2-0.6, and using a bi-directional LSTM.",
  "y": "differences"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_0",
  "x": "For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_1",
  "x": "Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.",
  "y": "motivation background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_2",
  "x": "---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_3",
  "x": "In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences. The K-vec method attempts to address this question. ---------------------------------- **THE K-VEC ALGORITHM** K-vec starts by estimating the lexicon. Consider the example: fisheries --~ p~ches.",
  "y": "background motivation"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_4",
  "x": "**THE K-VEC ALGORITHM** K-vec starts by estimating the lexicon. Consider the example: fisheries --~ p~ches. The K-vec algorithm will discover this fact by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. The concordances for fisheries and p~ches are shown in Tables 1 and 2 (at the end of this paper). 1 1. These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992 show where the concordances were found in the texts. We want to know whether the distribution of numbers in Table 1 is similar to those in Table 2, and if so, we will suspect that fisheries and p~ches As can be seen in the concordances in Table 3 , for K=10, the vector is <1, 1, 0, 1, 1,0, 1, 0, 0, 0>. By almost any measure of similarity one could imagine, this vector will be found to be quite different from the one for fisheries, and therefore, we will correctly discover that fisheries is not the translation of lections. To make this argument a little more precise, it might help to compare the contingency matrices in Tables 5 and 6 . The contingency matrices show: (a) the number of pieces where both the English and French word were found, (b) the number of pieces where just the English word was found, (c) the number of pieces where just the French word was found, and (d) the number of peices where neither word was found.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_5",
  "x": "At K=100, we obtain the contingency matrix shown in Table 8 , and the t-score is significant (t=2.1). Ideally, we would like to apply the K-vec algorithm to all pairs of English and French words, but unfortunately, there are too many such pairs to consider. We therefore limited the search to pairs of words in the frequency range: 3-10. This heuristic makes the search practical, and catches many interesting pairs) ---------------------------------- **RESULTS** This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992) . The 30 significant pairs with the largest mutual information values are shown in Table 9 . As can be seen, the results provide a quick-anddirty estimate of a bilingual lexicon. When the pair is not a direct translation, it is often the translation of a collocate, as illustrated by acheteur ~ Limited and Santd -~ Welfare.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_6",
  "x": "The K-vec algorithm generates a quick-and-dirty estimate of a bilingual lexicon. This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align . In this way, we might be able to apply word_align to a broader class of language combinations including possibly English-Japanese and English-Chinese. Currently, word_align depends on charalign<cite> (Church, 1993)</cite> to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. ---------------------------------- **REFERENCES** Aho, Kernighan, Weinberger (1980) \"The AWK Programming Language,\" Addison-Wesley, Reading, Massachusetts, USA. private sector is quite weak. 1,ct us turn now to fisheries, an industry which as most important 1o The fishermen would like to see the l)epartment of Fisheries and Oceans put more effort towards the p s in particular. The budget of the Department of Fisheries and Oceans has been reduced to such ate ' habitation ' ' trom which to base his trade in fisheries and filrs.",
  "y": "background uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_0",
  "x": "Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques. ---------------------------------- **INTRODUCTION** Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016;<cite> Caliskan et al., 2017)</cite> . This finding is not particularly surprising given that (1) we are likely project our biases in the text that we produce and (2) these biases in text are bound to be encoded in word vectors due to the distributional nature (Harris, 1954) of the word embedding models (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) . For illustration, consider the famous analogy-based gender bias example from Bolukbasi et al. (2016) : \"Man is to computer programmer as woman is to homemaker\". This bias will be reflected in the text (i.e., the word man will cooccur more often with words like programmer or engineer, whereas woman will more often appear next to homemaker or nurse), and will, in turn, be captured by word embeddings built from such biased texts. While biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases (e.g., Garg et al., 2018) , they may cause ethical problems for many downstream applications and NLP models. In order to measure the extent to which various societal biases are captured by word embeddings,<cite> Caliskan et al. (2017)</cite> proposed the Word Embedding Association Test (WEAT).",
  "y": "motivation background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_1",
  "x": "In order to measure the extent to which various societal biases are captured by word embeddings,<cite> Caliskan et al. (2017)</cite> proposed the Word Embedding Association Test (WEAT). WEAT measures semantic similarity, computed through word embeddings, between two sets of target words (e.g., insects vs. flowers) and two sets of attribute words (e.g., pleasant vs. unpleasant words). While they test a number of biases, the analysis is limited in scope to English as the only language, GloVe (Pennington et al., 2014) as the embedding model, and Common Crawl as the type of text. Following the same methodology, McCurdy and Serbetci (2017) extend the analysis to three more languages (German, Dutch, Spanish) , but test only for gender bias. In this work, we present the most comprehensive study of biases captured by distributional word vector to date. We create XWEAT, a collection of multilingual and cross-lingual versions of the WEAT dataset, by translating WEAT to six other languages and offer a comparative analysis of biases over seven diverse languages. Furthermore, we measure the consistency of WEAT biases across different embedding models and types of corpora. What is more, given the recent surge of models for inducing cross-lingual embedding spaces (Mikolov et al., 2013a; Hermann and Blunsom, 2014; Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018; Hoshen and Wolf, 2018, inter alia) and their ubiquitous application in cross-lingual transfer of NLP models for downstream tasks, we investigate cross-lingual biases encoded in cross-lingual embedding spaces and compare them to bias effects present of corresponding monolingual embeddings. Our analysis yields some interesting findings: biases do depend on the embedding model and, quite surprisingly, they seem to be less pronounced in embeddings trained on social media texts. Furthermore, we find that the effects (i.e., amount) of bias in cross-lingual embedding spaces can roughly be predicted from the bias effects of the corresponding monolingual embedding spaces.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_2",
  "x": "---------------------------------- **DATA FOR MEASURING BIASES** We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli. For each bias test, WEAT specifies four stimuli sets: two sets of target words and two sets of attribute words. The sets of target words represent stimuli between which we want to measure the bias (e.g., for gender biases, one target set could contain male names and the other females names). The attribute words, on the other hand, represent stimuli towards which the bias should be measured (e.g., one list could contain pleasant stimuli like health and love and the other negative war and death).",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_3",
  "x": "Our analysis yields some interesting findings: biases do depend on the embedding model and, quite surprisingly, they seem to be less pronounced in embeddings trained on social media texts. Furthermore, we find that the effects (i.e., amount) of bias in cross-lingual embedding spaces can roughly be predicted from the bias effects of the corresponding monolingual embedding spaces. ---------------------------------- **DATA FOR MEASURING BIASES** We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli. For each bias test, WEAT specifies four stimuli sets: two sets of target words and two sets of attribute words.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_4",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> . The effect size, that is, the \"amount of bias\", is computed as the normalized measure of separation between association distributions:",
  "y": "extends uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_5",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> . The effect size, that is, the \"amount of bias\", is computed as the normalized measure of separation between association distributions:",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_6",
  "x": "**METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_7",
  "x": "---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as:",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_8",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> . The effect size, that is, the \"amount of bias\", is computed as the normalized measure of separation between association distributions:",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_9",
  "x": "We believe, however, that the additional dictionary-based training objective only propagates the distributional biases across definitionally related words. Generally, we find these results to be important as they indicate that embedding models may accentuate or diminish biases expressed in text. Corpora. In Table 4 we compare the biases of embeddings trained with the same model (GLOVE) but on different corpora: Common Crawl (i.e., noisy web content), Wikipedia (i.e., encyclopedic the definition of A and vice versa (Tissier et al., 2017) . 5 This is consistent with the original results obtained by<cite> Caliskan et al. (2017)</cite> . Corpus T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 WIKI 1.4 1.5 1.2 1.4 1.4 1.8 1.2 1.3 1.3 1.2 CC 1.5 1.6 1.5 1.6 1.4 1.9 1.1 1.3 1.4 1.3 TWEETS 1.2 1.0 1.1 1.2 1.2 1.2 \u22120.2 * 0.6 * 0.7 * 0.8 * Table 6 : XWEAT bias effects (aggregated over all six tests) for cross-lingual word embedding spaces. Rows: targets language; columns: attributes language. Asterisks indicate the inclusion of bias effects sizes in the aggregation that were insignificant at \u03b1 < 0.05. more pronounced for embeddings trained on the Common Crawl than for those obtained on encyclopedic texts (Wikipedia). Countering our intuition, the corpus of tweets seems to be consistently less biased (across all tests) than Wikipedia.",
  "y": "similarities"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_10",
  "x": "For example, effects in cross-lingual spaces increase over monolingual effects for low-bias languages (HR and TR), and decrease for high-bias languages (EN and ES). ---------------------------------- **MULTILINGUAL COMPARISON.** ---------------------------------- **CONCLUSION** In this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces. To this end, we have extended previous analyses based on the WEAT test<cite> (Caliskan et al., 2017</cite>; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text. We find that different models may produce embeddings with very different biases, which stresses the importance of embedding model selection when fair text representations are to be created. Surprisingly, we find that the user-generated texts, such as tweets, may be less biased than redacted content. Furthermore, we have investigated the bias effects in cross-lingual embedding spaces and have shown that they may be predicted from the biases of corresponding monolingual embeddings.",
  "y": "extends uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_0",
  "x": "1 The CGT transport federation have risen against \"the lack of consultation\" and consider that employees have \"nothing positive to expect from this restructuring.\" 2 While studies have shown that discourse usage of discourse connectives can be accurately identified for English [13, <cite>20]</cite> , only a few studies have focused on the disambiguation of discourse connectives in other languages. In this paper, we investigate the usefulness of features proposed in the literature for the disambiguation of English discourse connectives for French discourse connectives. 3 This paper is organized as follow: Section 2 reviews related work. Section 3 describes our approach to disambiguate French discourse connectives. Section 4 reports our results, and finally Section 5 presents our conclusions and future work. ---------------------------------- **RELATED WORK** With respect to discourse organization, discourse connectives constitute the most basic way of signaling the speaker's or writer's intentions. They provide an important clue to disambiguate discourse relations whose interpretations would be opaque without them [5, 8, [16] [17] [18] . Hence, lexicons of discourse connectives associated with the relations that they express can be very useful for discourse studies (e.g. developing discourse annotated corpora [2, 7, 21, 22] , automatic discourse analysis [13, 25] , etc.) and have been developed for English [10] , Spanish [3] , German [24] and French [6] .",
  "y": "motivation background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_1",
  "x": "It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but).",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_2",
  "x": "It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but).",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_3",
  "x": "The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand).",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_4",
  "x": "In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%. Today, due to the availability of discourse annotated corpora such as the French Discourse Treebank [6] , it is possible to analyse how the features developed for English behave when applied to French.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_5",
  "x": "In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention. Due to syntactic differences across languages and different discourse annotation methodologies, the techniques developed for one language may not be as effective in another. For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%. Today, due to the availability of discourse annotated corpora such as the French Discourse Treebank [6] , it is possible to analyse how the features developed for English behave when applied to French.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_6",
  "x": "Then, they created an English-French dictionary for discourse connectives based on the similarities and discrepancies between the discourse connectives and their most appropriate translation. Table 4 shows the entropy of French and English discourse connectives that signal the Cause relation with their most likely translations 6 as identified by Zufferey and Cartoni [27] . As Table 4 shows, there does not seem to be a direct rela- 6 Note that some translations of discourse connectives such as '\u00e9tant donn\u00e9 que' are not considered discourse connectives in the FDTB and the PDTB because they do not fit the formal definition of discourse connectives. Therefore, we do not list their entropy in Table 4 . (1) tionship between the entropy of the mapped discourse connectives. For example, while the French discourse connective 'car' has an entropy of 0.05 (i.e. 'car' is more than 99% of the time used in discourse-usage in the FDTB), its translations in English (i.e. 'because', 'since', and 'as') are very ambiguous. The disparity between the entropy of discourse connectives in the FDTB and the PDTB can be explained by the differences between the languages and the annotation methodology. Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_7",
  "x": "Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives. They proposed four syntactic features: 1. SelfCat: The highest node in the parse tree that covers the connective words but nothing more. 2. SelfCatParent: The parent of the SelfCat. To illustrate these four features, consider the parse tree of the second sentence in Example (1) shown in Figure 1 and the discourse connective 'ainsi'. The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier.",
  "y": "uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_8",
  "x": "To illustrate these four features, consider the parse tree of the second sentence in Example (1) shown in Figure 1 and the discourse connective 'ainsi'. The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ---------------------------------- **AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_9",
  "x": "The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ---------------------------------- **AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_10",
  "x": "We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ---------------------------------- **AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively. ---------------------------------- **DATA PREPARATION** Although the focus of the work is the disambiguation of French discourse connectives, we performed the same experiments with English discourse connectives as well. This allowed us to better analyse the results and make a comparison between the performance of our model for French and for English.",
  "y": "differences similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_11",
  "x": "**RESULTS** Similarly to Pitler and Nenkova<cite> [20]</cite> , we report results using a maximum entropy classifier using ten-fold cross-validation over the extracted datasets. We used the off-the-shelf implementation of the maximum entropy classifier available in WEKA [9] for our experiments. Table 5 shows the overall performance of the classifier for the disambiguation of French and English discourse connectives. The results show that the classifier can distinguish between discourse-usage and non-discourse-usage of French discourse connectives with an accuracy of 94.2% and an FMeasure of 86.2%. This is close to the results achieved for English discourse connectives over the PDTB (accuracy of 93.6% and F-score of 88.9%). ---------------------------------- **FEATURE ANALYSIS** To evaluate the contribution of each feature, we ranked the features by their information gain for both languages. As Table 6 shows, with our datasets, the syntactic features provide less information about discourse-usage or non-discourseusage for French discourse connectives than they do for English.",
  "y": "similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_12",
  "x": "For example, both 'effectivement' and 'alors' are among the top three ambiguous discourse connectives (see Table 3 ). Even though the accuracy of the classifier is higher than the baseline (except for the 'maintenant' discourse connective), the increase is small or not statistically significant. For example, the accuracy for the discourse connective 'effectivement' is 55.56% which is not statistically better than the baseline. These results show that for some connectives, the features proposed for English are sufficient (see Table 8 ), but for others, using only the connective and the syntactic features is not sufficient. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we have investigated the applicability of the syntactic and lexical features proposed by Pitler and Nenkova<cite> [20]</cite> for the disambiguation of English discourse connectives for French. Our experiments on the French Discourse Treebank (FDTB) show that even though the syntactic features are less informative for the disambiguation of French discourse connectives than for English discourse connectives, overall the features can effectively disambiguate French discourse connectives between discourse-usage and non-discourseusage as well in French as in English. The fact that the local syntactic features proposed for English can be used almost as effectively for French and Arabic [4] suggests that lexicalized discourse connectives share certain common structural features cross-linguistically and that these structures are potentially an important component in discourse processing. However, our analysis also shows that the features are not as effective for all connectives.",
  "y": "extends"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_0",
  "x": "Inspection of the predictions shows that the classifier is indeed heavily biased towards the majority class for the input features. Then we see the performance increasing for the first layer and peaking at the second layer. The performance then drops slightly for the third layer and the attention layer. ---------------------------------- **DISCUSION AND CONCLUSION** We trained an image-caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input. As improvements over previous work we used a 3-layer GRU and employed importance sampling, cyclic learning rates, ensembling and vectorial self-attention. Our results on both MBN and MFCC features are significantly higher than the previous state-of-the-art. The largest improvement comes from using the learned MBN features but our approach also improves results for MFCCs, which are the same features as were used in <cite>[15]</cite> . The learned MBN features provide better performance whereas the MFCCs are more cognitively plausible input features.",
  "y": "similarities differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_1",
  "x": "Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] . Image to spoken-caption retrieval models do not know a priori which constituents of the input are important and have no prior knowledge of lexical level semantics. We probe the resulting model to investigate whether it learns to recognise lexical units in the input without being explicitly trained to do so. We test two types of acoustic features; Mel Frequency Cepstral Coefficients (MFCCs) and Multilingual Bottleneck (MBN) features. MFCCs are features that can be computed for any speech signal without needing any other data, while the MBN features are 'learned' features that result from training a network on top of MFCCs in order to recognise phoneme states. While MBN features have been shown to be useful in several speech recognition tasks (e.g. [20, 21] ), learned audio features face the same issue as word embeddings, as humans learn to extract useful features from the audio signal as a result of learning to understand language and not as a separate process.",
  "y": "similarities"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_2",
  "x": "In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] . Image to spoken-caption retrieval models do not know a priori which constituents of the input are important and have no prior knowledge of lexical level semantics. We probe the resulting model to investigate whether it learns to recognise lexical units in the input without being explicitly trained to do so. We test two types of acoustic features; Mel Frequency Cepstral Coefficients (MFCCs) and Multilingual Bottleneck (MBN) features.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_3",
  "x": "In previous work [8] we used image-caption retrieval, where given a written caption the model must return the matching image and vice versa. We trained deep neural networks (DNNs) to create sentence embeddings without the use of prior knowledge of lexical semantics (see [7, 9, 10] for other studies on this task). The visually grounded sentence embeddings that arose capture semantic information about the sentence as measured by the Semantic Textual Similarity task (see [11] ), performing comparably to text-only methods that require word embeddings. In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_4",
  "x": "Our model consists of two parts; an image encoder and a sentence encoder as depicted in Figure 1 . The approach is based on our own text-based model described in [8] and on the speech-based models described in [13,<cite> 15]</cite> and we refer to those studies for more details. Here, we focus on the differences with previous work. For the image encoder we use a single-layer linear projection on top of the pretrained image recognition model, and nor- malise the result to have unit L2 norm. The image encoder has 2048 input units and 2048 output units. Our caption encoder consists of three main components. First we apply a 1-dimensional convolutional layer to the acoustic input features. The convolution has a stride of size 2, kernel size 6 and 64 output channels. This is the only layer where the model differs from the text-based model, which features a character embedding layer instead of a convolutional layer. The resulting features are then fed into a bi-directional Gated Recurrent Unit (GRU) followed by a self-attention layer and is lastly normalised to have unit L2 norm.",
  "y": "background uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_5",
  "x": "Following [8] , the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger (by a certain margin) than the similarity be-tween mismatching pairs. This so called hinge loss L as a function of the network parameters \u03b8 is given by: where the other caption-image pairs in the batch serve to create mismatched pairs (c, i \u2032 ) and (c \u2032 , i). We take the cosine similarity cos(x, y) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin \u03b1. We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8,<cite> 15]</cite> ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity). While [10] used only the single hardest example in the batch for text-captions, we found that this did not work for the spoken captions. Instead we found that using the hardest 25 percent worked well. The networks are trained using Adam [25] with a cyclic learning rate schedule based on [26] . The learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 \u22126 and 2 \u00d7 10 \u22124 respectively. The learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [27] .",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_6",
  "x": "Instead we found that using the hardest 25 percent worked well. The networks are trained using Adam [25] with a cyclic learning rate schedule based on [26] . The learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 \u22126 and 2 \u00d7 10 \u22124 respectively. The learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [27] . By saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost. We use a margin \u03b1 = 0.2 for the loss function. We train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch. For ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings. The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_7",
  "x": "By saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost. We use a margin \u03b1 = 0.2 for the loss function. We train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch. For ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings. The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_8",
  "x": "**WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] . Given a sentence embedding created by our model, a classifier has to decide which of the words in its vocabulary occur in the sentence. Based on the original written captions, our database contains 7,374 unique words with a combined occurrence frequency of 324,480. From these we select words that occur between 50 and a 1,000 times and are over 3 characters long so that there are enough examples in the data that the model might actually learn to recognise them, and to filter out punctuation, spelling mistakes, numerals and most function words. This leaves 460 unique words, mostly verbs and nouns, with a combined occurrence frequency of 87,020 in our data. We construct a vector for each sentence in Flickr8k indicating which of these words is present. We do not encode multiple occurrences of the same word in one sentence.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_9",
  "x": "We apply some post-processing to the acoustic features and the intermediate layer outputs to ensure that our word detection inputs are all of the same size. As the intermediate GRU layers produce 2048 features for each time step in the signal, we use average-pooling along the temporal dimension to create a single input vector and normalise the result to have unit L2 norm. The acoustic features consist of 30 (MBN) or 39 (MFCC) features for each time step, so we apply the convolutional layer followed by an untrained GRU layer to the input features, use average-pooling and normalise the result to have unit L2 norm. The word detection networks are trained for 32 epochs using Adam [25] with a constant learning rate of 0.001. We use the same data split that was used for training the multi-modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder. Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description).",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_10",
  "x": "The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_11",
  "x": "As the intermediate GRU layers produce 2048 features for each time step in the signal, we use average-pooling along the temporal dimension to create a single input vector and normalise the result to have unit L2 norm. The acoustic features consist of 30 (MBN) or 39 (MFCC) features for each time step, so we apply the convolutional layer followed by an untrained GRU layer to the input features, use average-pooling and normalise the result to have unit L2 norm. The word detection networks are trained for 32 epochs using Adam [25] with a constant learning rate of 0.001. We use the same data split that was used for training the multi-modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder. Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_12",
  "x": "The word detection networks are trained for 32 epochs using Adam [25] with a constant learning rate of 0.001. We use the same data split that was used for training the multi-modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder. Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_13",
  "x": "We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ---------------------------------- **RESULTS** The results of the word presence detection task are shown in Figure 2 and Table 2 .",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_14",
  "x": "In conclusion, we presented what are, to the best of our knowledge, the best results on spoken-caption to image retrieval. Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise. In the current study, we only investigated whether the model learns to recognise words, but the potential benefit of our model is that it might learn multi-word statements or might even learn to look at sub-lexical level information. [14, 29] have recently shown that the speech-to-image retrieval approach can be used to detect word boundaries and even discover sub-word units. Our interest is in investigating how these word and sub-word units develop over training and through the network layers. ----------------------------------",
  "y": "future_work uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_15",
  "x": "In conclusion, we presented what are, to the best of our knowledge, the best results on spoken-caption to image retrieval. Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise. In the current study, we only investigated whether the model learns to recognise words, but the potential benefit of our model is that it might learn multi-word statements or might even learn to look at sub-lexical level information. [14, 29] have recently shown that the speech-to-image retrieval approach can be used to detect word boundaries and even discover sub-word units. Our interest is in investigating how these word and sub-word units develop over training and through the network layers. ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "e803782890224294066ce447671981_0",
  "x": "****FINDING NON-LOCAL DEPENDENCIES: BEYOND PATTERN MATCHING**** **ABSTRACT** We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.",
  "y": "extends"
 },
 {
  "id": "e803782890224294066ce447671981_1",
  "x": "The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_2",
  "x": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena. This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_3",
  "x": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena. This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_4",
  "x": "**INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_5",
  "x": "Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_6",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena. This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required. On the other hand, using these simplified patterns, we may loose some structural information important for recovery of non-local dependencies. To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_7",
  "x": "In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques. A different definition of pattern allows us to significantly reduce the number of patterns extracted from the same corpus. Moreover, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena. This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required. On the other hand, using these simplified patterns, we may loose some structural information important for recovery of non-local dependencies. To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching. The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in <cite>(Johnson, 2002)</cite> . However, additional work remains to be done for our algorithm to perform well on the output of a parser.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_8",
  "x": "To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching. The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in <cite>(Johnson, 2002)</cite> . However, additional work remains to be done for our algorithm to perform well on the output of a parser. ---------------------------------- **FROM THE PENN TREEBANK TO A DEPENDENCY TREEBANK** This section describes the corpus of dependency structures that we used to evaluate our algorithm. The corpus was automatically derived from the Penn Treebank II corpus (Marcus et al., 1993) , by means of the script chunklink.pl (Buchholz, 2002) that we modified to fit our purposes. The script uses a sort of head percolation table to identify heads of constituents, and then converts the result to a dependency format. We refer to (Buchholz, 2002) for a thorough description of the conversion algorithm, and will only emphasize the two most important modifications that we made. One modification of the conversion algorithm concerns participles and reduced relative clauses modifying NPs.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_9",
  "x": "This simple heuristics does not allow us to handle all reduced relative clauses, because some of them correspond to PPs or NPs rather than VPs, but the latter are quite rare in the Treebank. The second important change to Buchholz' script concerns the structure of VPs. For every verb cluster, we choose the main verb as the head of the cluster, and leave modal and auxiliary verbs as dependents of the main verb. A similar modification was used by Eisner (1996) for the study of dependency parsing models. As will be described below, this allows us to \"factor out\" tense and modality of finite clauses from our patterns, making the patterns more general. ---------------------------------- **PATTERN EXTRACTION AND MATCHING** After converting the Penn Treebank to a dependency treebank, we first extracted non-local dependency patterns. As in <cite>(Johnson, 2002)</cite> , our patterns are minimal connected fragments containing both nodes involved in a non-local dependency. However, in our case these fragments are not connected sets of local trees, but shortest paths in local dependency graphs, leading from heads to non-local dependents.",
  "y": "similarities uses"
 },
 {
  "id": "e803782890224294066ce447671981_10",
  "x": "Patterns do not include POS tags of the involved words, but only labels of the dependencies. Thus, a pattern is a directed graph with labeled edges, and two distinguished nodes: the head and the dependent of a corresponding non-local dependency. When several patterns intersect, as may be the case, for example, when a word participates in more than one nonlocal dependency, these patterns are handled independently. Figure 2 shows examples of dependency graphs (above) and extracted patterns (below, with filled bullets corresponding to the nodes of a nonlocal dependency). As before, dotted lines denote non-local dependencies. The definition of a structure matching a pattern, and the algorithms for pattern matching and pattern extraction from a corpus are straightforward and similar to those described in <cite>(Johnson, 2002)</cite> . The total number of non-local dependencies found in the Penn WSJ is 57325. The number of different extracted patterns is 987. The 80 most frequent patterns (those that we used for the evaluation of our algorithm) cover 53700 out of all 57325 nonlocal dependencies (93,7%). These patterns were further cleaned up manually, e.g., most Penn functional tags (-TMP, -CLR etc., but not -OBJ, -SBJ, -PRD) were removed.",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_12",
  "x": "We performed experiments with two statistical classifiers: the decision tree induction system C4.5 (Quinlan, 1993) and the Tilburg Memory-Based Learner (TiMBL) (Daelemans et al., 2002) . In most cases TiBML performed slightly better. The results described in this section were obtained using TiMBL. For each of the 16 structural patterns, a separate classifier was trained on the set of (feature-vector, label) pairs extracted from the training corpus, and then evaluated on the pairs from the test corpus. Table 1 shows the results for some of the most frequent patterns, using conventional metrics: precision (the fraction of the correctly labeled dependencies among all the dependencies found), recall (the fraction of the correctly found dependencies among all the dependencies with a given label) and f-score (harmonic mean of precision and recall). The table also shows the number of times a pattern (together with a specific non-local dependency label) actually occurs in the whole Penn Treebank corpus (the column Dependency count). In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_13",
  "x": "In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> . Our results are presented in Table 3 . The first three columns show the results for those non-local dependencies that are actually covered by our 16 patterns (i.e., for 93.7% of all non-local dependencies). The last three columns present the evaluation with respect to all non-local dependencies, thus the precision is the same, but recall drops accordingly. These last columns give the results that can be compared to <cite>Johnson's results</cite> for section 23 (Table 4) Table 3 : Overall performance of our algorithm. ---------------------------------- **ON SECTION 23**",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_17",
  "x": "The last three columns present the evaluation with respect to all non-local dependencies, thus the precision is the same, but recall drops accordingly. These last columns give the results that can be compared to <cite>Johnson's results</cite> for section 23 (Table 4) Table 3 : Overall performance of our algorithm. ---------------------------------- **ON SECTION 23** On parser output P R f P R f Overall 0.80 0.70 0.75 0.73 0.63 0.68 Table 4 : Results from <cite>(Johnson, 2002)</cite> . It is difficult to make a strict comparison of our results and those in <cite>(Johnson, 2002)</cite> . The two algorithms are designed for slightly different purposes: while <cite>Johnson's approach</cite> allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes). ---------------------------------- **DISCUSSION** The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_18",
  "x": "The two algorithms are designed for slightly different purposes: while <cite>Johnson's approach</cite> allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes). ---------------------------------- **DISCUSSION** The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns. One of the reasons for this improvement is that our approach allows us to discriminate between different syntactic phenomena involving non-local dependencies. In most cases our patterns correspond to linguistic phenomena. That helps to understand why a particular construction is easy or difficult for our approach, and in many cases to make the necessary modifications to the algorithm (e.g., adding other features to instances of patterns). For example, for patterns 11 and 12 (see Tables 1 and 2 ) our classifier distinguishes subject and object reasonably well, apparently, because the feature has a local object is explicitly present for all instances (for the examples 11 and 12 in Table 2 , expand has a local object, but do doesn't). Another reason is that the patterns are general enough to factor out minor syntactic differences in linguistic phenomena (e.g., see example 4 in Table 2). Indeed, the most frequent 16 patterns cover 93.7% of all non-local dependencies in the corpus.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_19",
  "x": "Obviously, because of parsing errors the performance drops significantly: e.g., in the experiments reported in <cite>(Johnson, 2002 )</cite> the overall fscore decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4 ). While experimenting with Collins' parser (Collins, 1999) , we found that for our algorithm the accuracy drops even more dramatically, when we train the classifier on Penn Treebank data and test it on parser output. One of the reasons is that, since we run our algorithm not on the parser's output itself but on the output automatically converted to dependency structures, conversion errors also contribute to the performance drop. Moreover, the conversion script is highly tailored to the Penn Treebank annotation (with functional tags and empty nodes) and, when run on the parser's output, produces structures with somewhat different dependency labels. Since our algorithm is sensitive to the exact labels of the dependencies, it suffers from these systematic errors. One possible solution to that problem could be to extract patterns and train the classification algorithm not on the training part of the Penn Treebank, but on the parser output for it. This would allow us to train and test our algorithm on data of the same nature. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** We have presented an algorithm for recovering longdistance dependencies in local dependency structures.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_20",
  "x": "We have presented an algorithm for recovering longdistance dependencies in local dependency structures. We extend the pattern matching approach of <cite>Johnson (2002)</cite> with machine learning techniques, and use dependency structures instead of constituency trees. Evaluation on the Penn Treebank shows an increase in accuracy. However, we do not have yet satisfactory results when working on a parser output. The conversion algorithm and the dependency labels we use are largely based on the Penn Treebank annotation, and it seems difficult to use them with the output of a parser. A parsing accuracy evaluation scheme based on grammatical relations (GR), presented in (Briscoe et al., 2002) , provides a set of dependency labels (grammatical relations) and a manually annotated dependency corpus. Non-local dependencies are also annotated there, although no explicit difference is made between local and non-local dependencies. Since our classification algorithm does not depend on a particular set of dependency labels, we can also use the set of labels described by Briscoe et al, if we convert Penn Treebank to a GR-based dependency treebank and use it as the training corpus. This will allow us to make the patterns independent of the Penn Treebank annotation details and simplify testing the algorithm with a parser'u output. We will also be able to use the flexible and parameterizable scoring schemes discussed in (Briscoe et al., 2002) .",
  "y": "differences extends"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_0",
  "x": "The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms. ---------------------------------- **INTRODUCTION** Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_1",
  "x": "The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms. ---------------------------------- **INTRODUCTION** Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_2",
  "x": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_3",
  "x": "TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion. For example, the sequence of triples (J.K. Rowling, CreatedRole, Harry Potter), (Harry Potter, Describedin, Harry Potter and the Philosopher's Stone) can be used to infer the new fact (J.K. Rowling, WroteBook, Harry Potter and the Philosopher's Stone), which does not appear in the original KBs. Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> . For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_4",
  "x": "These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion. For example, the sequence of triples (J.K. Rowling, CreatedRole, Harry Potter), (Harry Potter, Describedin, Harry Potter and the Philosopher's Stone) can be used to infer the new fact (J.K. Rowling, WroteBook, Harry Potter and the Philosopher's Stone), which does not appear in the original KBs. Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> . For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> .",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_5",
  "x": "but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path. As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA. Figure 1 illustrates the basic idea for relation-specific and path-specific projections. Each entity is projected by M r and M p into the corresponding relation and path spaces. These different embedding spaces hold the following hypothesis: in the relation-specific space, relation r is regarded as a translation from head h r to tail t r ; likewise, p * , the path representation by the composition of relation embeddings, is regarded as a translation from head h p to tail t p in the path-specific space. We design two types of compositions to dynamically construct the path-specific projection M p without extra parameters. Moreover, with slight changes on negative sampling, we also propose that relationspecific and path-specific type constraints can be seamlessly incorporated into our model. Our main contributions are as follows: 1) To reinforce the reasoning ability of knowledge embedding models, the consistent semantics and the path spaces are introduced.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_6",
  "x": "The projected entity vectors are h r =M r h and t r =M r t; thus, the new score function is defined as S(h,r,t)= h r + r \u2212 t r . Another research direction focuses on improving the prediction performance by using prior knowledge in the form of relation-specific type constraints [Krompass et al., 2015; Chang et al., 2014; Wang et al., 2015] . Note that each relation should possess Domain and Range fields to indicate the subject and object types, respectively. For example, the relation haschildren's Domain and Range types both belong to a person. By exploiting these limited rules, the harmful influence of a merely data-driven pattern can be avoided. Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space. A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> . PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> . PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_7",
  "x": "PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> . PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations. In large-scale KBs, relation paths have great significance for enhancing the reasoning ability for more complicated situations. However, none of the aforementioned models take full advantage of the consistent semantics of relation paths. ---------------------------------- **OUR MODEL** The consistent semantics expressed by reliable relation paths has a significant impact on learning meaningful embeddings. Here, we propose a compositional learning model of relation path embedding (RPE), which includes novel path-specific projection and type constraints. All entities constitute the entity set \u03b6, and all relations constitute the relation set R. RPE uses PRA to select reliable relation paths.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_8",
  "x": "As for FB13 and WN11, we do not depend on the auxiliary data, and the domain and range of each relation are approximated by the triples from the original datasets. ---------------------------------- **LINK PREDICTION** The link prediction task consists of predicting the possible h or t for test triples when h or t is missed. FB15K is employed for this task. ---------------------------------- **EVALUATION PROTOCOL** We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014;<cite> Lin et al., 2015b]</cite> . First, for each test triple (h,r,t), we replace h or t with every entity in \u03b6. Second, each corrupted triple is calculated by the corresponding score function S(h,r,t).",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_9",
  "x": "The final step is to rank the original correct entity with these scores in descending order. Two evaluation metrics are reported: the average rank of correct entities (Mean Rank) and the proportion of correct entities ranked in the top 10 (Hits@10). Note that if a corrupted triple already exists in the knowledge base, then it should not be considered to be incorrect. We prefer to remove these corrupted triples from our dataset, and call this setting \"Filter\". If these corrupted triples are reserved, then we call this setting \"Raw\". In both settings, if the latent representations of entity and relation are better, then a lower mean rank and a higher Hits@10 should be achieved. Because we use the same dataset, the baseline results reported in<cite> [Lin et al., 2015b</cite>;<cite> Lin et al., 2015a</cite>; Ji et al., 2016] are directly used for comparison. ---------------------------------- **IMPLEMENTATION** We set the dimension of entity embedding m and relation embedding n among {20, 50, 100, 120}, the margin \u03b3 1 among {1, 2, 3, 4, 5}, the margin \u03b3 2 among {3, 4, 5, 6, 7, 8}, the learning rate \u03b1 for SGD among {0.01, 0.005, 0.0025, 0.001, 0.0001}, the batch size B among {20, 120, 480, 960, 1440, 4800}, and the balance factor \u03bb among {0.5, 0.8, 1,1.5, 2}. The threshold \u03b7 was set in the range of {0.01, 0.02, 0.04, 0.05} to reduce the calculation of meaningless paths.",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_10",
  "x": "RPE with path-specific type constraints and projection (RPE (PC + ACOM) and RPE (PC + MCOM)) is a compromise between RPE (PC) and RPE (ACOM). Table 3 presents the separated evaluation results by mapping properties of relations on FB15K. Mapping properties of relations follows the same rules in [Bordes et al., 2013] , and the metrics are Hits@10 on head and tail entities. From Table 3 , we can conclude that 1) RPE (ACOM) outperforms all baselines in all mapping properties of relations. In particular, for the 1-to-N, N-to-1, and N-to-N types of relations [Bordes et al., 2013] 75.9 70.9 77.8 TransE (bern) [Bordes et al., 2013] 75.9 81.5 85.3 TransH (unif) [Wang et al., 2014] 77.7 76.5 78.4 TransH (bern) [Wang et al., 2014] 78.8 83.3 85.8 TransR (unif)<cite> [Lin et al., 2015b]</cite> 85.5 74.7 79.2 TransR (bern)<cite> [Lin et al., 2015b]</cite> 85.9 82.5 87.0 PTransE (ADD, 2-hop)<cite> [Lin et al., 2015a]</cite> 80.9 73.5 83.4 PTransE (MUL, 2-hop)<cite> [Lin et al., 2015a]</cite> 79.4 73.6 79.3 PTransE (ADD, 3-hop)<cite> [Lin et al., 2015a]</cite> 80 that plague knowledge embedding models, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity's prediction and 6.9%, 7.0%, and 5.1% on tail entity's prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop). 2) RPE (MCOM) does not perform as well as RPE (ACOM), and we believe that this result is because RPE's path representation is not consistent with RPE (MCOM)'s composition of projections. Although RPE (PC) improves little compared with PTransE, we will indicate the effectiveness of relation-specific and pathspecific type constraints in triple classification. 3) We use the relation-specific projection to construct path-specific ones dynamically; then, entities are encoded into relation-specific and path-specific spaces simultaneously. The experiments are similar to link prediction, and the results of experiments results further demonstrate the better expressibility of our model. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_11",
  "x": "---------------------------------- **EVALUATION PROTOCOL** We set different relation-specific thresholds {\u03b4 r } to perform this task. For a test triple (h,r,t), if its score S(h,r,t) is below \u03b4 r , then we predict it as a positive one; otherwise, it is negative. {\u03b4 r } is obtained by maximizing the classification accuracies on the valid set. ---------------------------------- **IMPLEMENTATION** We directly compare our model with prior work using the results about knowledge embedding models reported in<cite> [Lin et al., 2015b]</cite> n=50, m=50, \u03b3 1 =5, \u03b3 2 =6, \u03b1=0.0001, B=1440, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on WN11; n=100, m=100, \u03b3 1 =3, \u03b3 2 =6, \u03b1=0.0001, B=960, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on FB13; and n=100, m=100, \u03b3 1 =4, \u03b3 2 =5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7 =0.05, taking the L 1 norm on FB15K. We exploit RPE (initial) for initiation, and we set the path length as 2 and the maximum epoch as 500. Table 4 lists the results for triple classification on different datasets, and the evaluation metric is classification accuracy.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_0",
  "x": "These results motivate the integration of the FullNetwork embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach. ---------------------------------- **INTRODUCTION** Image annotation (also known as caption retrieval) is the task of automatically associating an input image with a describing text. Image annotation methods are an emerging technology, enabling semantic image indexing and search applications. The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications. State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_1",
  "x": "While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_2",
  "x": "**INTRODUCTION** Image annotation (also known as caption retrieval) is the task of automatically associating an input image with a describing text. Image annotation methods are an emerging technology, enabling semantic image indexing and search applications. The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications. State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_3",
  "x": "Image annotation methods are an emerging technology, enabling semantic image indexing and search applications. The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications. State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] .",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_4",
  "x": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead.",
  "y": "motivation uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_5",
  "x": "We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_6",
  "x": "Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_7",
  "x": "In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation. Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation. The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task. FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM).",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_8",
  "x": "In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation. Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation. The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task. FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM).",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_9",
  "x": "---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation. Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_10",
  "x": "---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair. Assuming that a correct pair of elements should be closer in the multimodal space than a random pair.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_11",
  "x": "In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair. Assuming that a correct pair of elements should be closer in the multimodal space than a random pair. The loss L can be formally defined as follows: Where i is an image vector, c is its correct caption vector, and i k and c k are sets of random images and captions respectively.",
  "y": "differences extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_12",
  "x": "The mapping of standardized values into these three categories is done through the definition of two constant thresholds. The optimal values of these thresholds can be found empirically for a labeled dataset [21] . Instead, we use threshold values shown to perform consistently across several domains [9] . ---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs.",
  "y": "differences background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_13",
  "x": "---------------------------------- **MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair. Assuming that a correct pair of elements should be closer in the multimodal space than a random pair.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_14",
  "x": "This formulation includes a margin term \u03b1 to avoid pulling the image and caption closer once their distance is smaller than the margin. This makes the optimization focus on distant pairs instead of improving the ones that are already close. ---------------------------------- **EXPERIMENTS** In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks. To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ---------------------------------- **DATASETS**",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_15",
  "x": "To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ---------------------------------- **DATASETS** In our experiments we use three different publicly available datasets: The Flickr8K dataset [11] contains 8,000 hand-selected images from Flickr, depicting actions and events. Five correct captions are provided for each image. Following the provided splits, 6,000 images are used for train, 1,000 are used in validation and 1,000 more are kept for testing. The Flickr30K dataset [12] is an extension of Flickr8K.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_16",
  "x": "These splits are the same ones used by <cite>Kiros et al. [1]</cite> and by Karpathy and Fei-Fei [22] . The MSCOCO dataset [13] includes images of everyday scenes containing common objects in their natural context. For captioning, 82,783 images and 413,915 captions are available for training, while 40,504 images and 202,520 captions are available for validation. Captions from the test set are not publicly available. Previous contributions consider using a subset of the validation set for validation and a different subset for test. In most cases, such subsets are composed by either 1,000 or 5,000 images for each set, with their corresponding 5 captions per image. In our experiments we consider both settings. ---------------------------------- **IMPLEMENTATION AND EVALUATION DETAILS** The caption sentences are word-tokenized using the Natural Language Toolkit (NLTK) for Python [23] .",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_17",
  "x": "Using all the words present in the dataset is likely to produce overfitting problems when training on examples containing words that only occur a few times. This overfitting problem may not have a huge impact on performance, but it may add undesired noise in the multimodal representation. The original setup [<cite>1]</cite> limited the word embedding to the 300 most frequent words, while using 300 GRUs. The Bi-LSTM model [25] in contrast defines the vocabulary size to include words appearing more than 5 time in the dataset, leading to dictionaries of size 2,018 for Flickr8k, 7,400 for Flickr30k and 8,801 for MSCOCO. Our own preliminary experiments on the validation set showed that increasing multimodal space dimensionality and dictionary length slightly improved the performance of image retrieval, in detriment of image annotation. However, the combined performance difference remains rather small when using non-extreme parameter values (e.g., a model with 10,000 words vocabulary on MSCOCO dataset show a 0.4% average recall reduction when compared with a 2,000 words model). Since we are building a model for solving both tasks, we kept the parameters obtaining the highest combined score in the validation set. For the Flickr datasets, the word embedding is limited to the 1,000 most frequent words. For the MSCOCO dataset, we use a larger dictionary, considering the 2,000 most frequent words. In both cases we use 2,048 GRUs, which is also the dimensionality of the resultant multimodal embedding space.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_18",
  "x": "The models are trained up to 25 epochs, and the best performing model on the validation set is chosen (i.e., early stopping). We use gradient clipping for the GRUs with a threshold of 2. We use ADAM [24] as optimization algorithm, with a learning rate of 0.0002 for the Flickr datasets, and 0.00025 for MSCOCO. To evaluate both image annotation and image retrieval we use the following metrics: \u2022 Recall@K (R@K) is the fraction of images for which a correct caption is ranked within the top-K retrieved results (and vice-versa for sentences). Results are provided for R@1, R@5 and R@10. \u2022 Median rank (Med r) of the highest ranked ground truth result. ---------------------------------- **RESULTS** For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_19",
  "x": "For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA). Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used. In this particular case, the FN-MME outperforms the CNN-MME* by 3 percentual points on average for the Flickr datasets, and roughly by 4 points for the MSCOCO dataset. To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_20",
  "x": "We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used. In this particular case, the FN-MME outperforms the CNN-MME* by 3 percentual points on average for the Flickr datasets, and roughly by 4 points for the MSCOCO dataset. To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval. For the Flickr datasets, particularly for image annotation tasks, the performance of the FN-MME is significantly closer to the state of the art than the other variants of the same model (CNN-MME, CNN-MME \u2020, CNN-MME*). Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset. However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_21",
  "x": "First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used. In this particular case, the FN-MME outperforms the CNN-MME* by 3 percentual points on average for the Flickr datasets, and roughly by 4 points for the MSCOCO dataset. To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval. For the Flickr datasets, particularly for image annotation tasks, the performance of the FN-MME is significantly closer to the state of the art than the other variants of the same model (CNN-MME, CNN-MME \u2020, CNN-MME*). Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset. However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_22",
  "x": "To measure the relevance of the improvement provided by using the FNE, we compare the FN-MME model with the current state-of-the-art for image annotation and image retrieval. For the Flickr datasets, particularly for image annotation tasks, the performance of the FN-MME is significantly closer to the state of the art than the other variants of the same model (CNN-MME, CNN-MME \u2020, CNN-MME*). Remarkably, the FN-MME provides the best reported results on image annotation for the MSCOCO dataset. However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_23",
  "x": "However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_24",
  "x": "However, let us remark that the competitive W2VV method [18] has no reported results for MSCOCO. The results of the FN-MME for image retrieval tasks are significantly further from the stateof-the-art. Overall, the competitiveness of FN-MME increases with dataset size. ---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "future_work"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_0",
  "x": "The alternative approach, exemplified by Collins (1997) and Charniak (2000) , is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search. The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.'s approach.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_1",
  "x": "The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000) , is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search.",
  "y": "motivation"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_2",
  "x": "For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search. The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.'s approach. We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation. The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data. ---------------------------------- **THE SHIFT-REDUCE PARSING PROCESS**",
  "y": "differences uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_3",
  "x": "The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.'s approach. We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation. The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data. ---------------------------------- **THE SHIFT-REDUCE PARSING PROCESS** The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> . The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue. A stack holds the partial parse trees that are built during the parsing process.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_4",
  "x": "Parser actions, including SHIFT and various kinds of REDUCE, define functions from states to states by shifting word-POS pairs onto the stack and building partial parse trees. The actions used by the parser are: \u2022 SHIFT, which pushes the next word-POS pair in the queue onto the stack; \u2022 REDUCE-unary-X, which makes a new unary-branching node with label X; the stack is popped and the popped node becomes the child of the new node; the new node is pushed onto the stack; \u2022 REDUCE-binary-{L/R}-X, which makes a new binary-branching node with label X; the stack is popped twice, with the first popped node becoming the right child of the new node and the second popped node becoming the left child; the new node is pushed onto the stack; \u2022 TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_5",
  "x": "Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008) . The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of Figure 2 : the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 .",
  "y": "extends"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_6",
  "x": "The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008) . The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of Figure 2 : the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 . ---------------------------------- **THE BINARIZATION PROCESS** The algorithm in Figure 2 is used to map CTB trees into binarized trees, which are required by the shift-reduce parsing process. For any tree node with more than two child nodes, the algorithm works by first finding the head node, and then processing its right-hand-side and left-hand-side, respectively.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_7",
  "x": "For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008) . The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of Figure 2 : the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 . ---------------------------------- **THE BINARIZATION PROCESS**",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_8",
  "x": "**RESTRICTIONS ON THE SEQUENCE OF ACTIONS** Not all sequences of actions produce valid binarized trees. In the deterministic parser of <cite>Wang et al. (2006)</cite> , the highest scoring action predicted by the classifier may prevent a valid binary tree from being built. In this case, Wang et al. simply return a partial parse consisting of all the subtrees on the stack. In our parser a set of restrictions is applied which guarantees a valid parse tree. For example, two simple restrictions are that a SHIFT action can only be applied if the queue of incoming words Variables: state item item = (S, Q), where S is stack and Q is incoming queue; the agenda agenda; list of state items next; Algorithm: for item \u2208 agenda: if item.score = agenda.bestScore and item.isFinished: next.push(item. TakeAction(move)) agenda = next.getBBest() Outputs: rval Figure 3 : the beam-search decoding algorithm is non-empty, and the binary reduce actions can only be performed if the stack contains at least two nodes. Some of the restrictions are more complex than this; the full set is listed in the Appendix.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_9",
  "x": "Individual features are generated from Description Feature templates Unigrams S0tc, S0wc, S1tc, S1wc, S2tc, S2wc, S3tc, S3wc, N0wt, N1wt, N2wt, N3wt, S0lwc, S0rwc, S0uwc, S1lwc, S1rwc, S1uwc, Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c, S0wN0w, S0wN0t, S0cN0w, S0cN0t, N0wN1w, N0wN1t, N0tN1w, N0tN1t S1wN0w, S1wN0t, S1cN0w, S1cN0t, ---------------------------------- **FEATURE SET** Separator S0wp, S0wcp, S0wq, S0wcq, S1wp, S1wcp, S1wq, S1wcq S0cS1cp, S0cS1cq Table 1 : Feature templates these templates by first instantiating a template with particular labels, words and tags, and then pairing the instantiated template with a particular action. In the table, the symbols S 0 , S 1 , S 2 , and S 3 represent the top four nodes on the stack, and the symbols N 0 , N 1 , N 2 and N 3 represent the first four words in the incoming queue. S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_10",
  "x": "S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack. It takes three different values: 1 (no matching right bracket has been pushed onto stack), 2 (a matching right bracket has been pushed onto stack) and 3 (a matching right bracket has been pushed onto stack, but then popped off). The \"Separator\" row shows features that include one of the separator punctuations (i.e. \"\uff0c\", \"\u3002\", \"\u3001\" and \"\uff1b\") between the head words of S 0 and S 1 .",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_11",
  "x": "The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack. It takes three different values: 1 (no matching right bracket has been pushed onto stack), 2 (a matching right bracket has been pushed onto stack) and 3 (a matching right bracket has been pushed onto stack, but then popped off). The \"Separator\" row shows features that include one of the separator punctuations (i.e. \"\uff0c\", \"\u3002\", \"\u3001\" and \"\uff1b\") between the head words of S 0 and S 1 . These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol. Each unique separator punctuation between S 0 and S 1 is only counted once when generating the global feature vector.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_12",
  "x": "When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack. It takes three different values: 1 (no matching right bracket has been pushed onto stack), 2 (a matching right bracket has been pushed onto stack) and 3 (a matching right bracket has been pushed onto stack, but then popped off). The \"Separator\" row shows features that include one of the separator punctuations (i.e. \"\uff0c\", \"\u3002\", \"\u3001\" and \"\uff1b\") between the head words of S 0 and S 1 . These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_13",
  "x": "Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. ---------------------------------- **EXPERIMENTS** The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data. Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X \u2192 X nodes resulting from the previous steps were collapsed into one X node. For all experiments, we used the EVALB tool 1 for evaluation, and used labeled recall (LR), labeled precision (LP ) and F 1 score (which is the harmonic mean of LR and LP ) to measure parsing accuracy.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_14",
  "x": "Each unique separator punctuation between S 0 and S 1 is only counted once when generating the global feature vector. q represents the count of any separator punctuation between S 0 and S 1 . Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. ---------------------------------- **EXPERIMENTS** The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_15",
  "x": "The tests were performed using the development test data and gold-standard POStags. The figure shows the benefit of using a beam size greater than 1, with comparatively little accuracy gain being obtained beyond a beam size of 8. Hence we set the beam size to 16 for the rest of the experiments. ---------------------------------- **THE INFLUENCE OF BEAM-SIZE** ---------------------------------- **TEST RESULTS ON CTB2** The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_16",
  "x": "**THE INFLUENCE OF BEAM-SIZE** ---------------------------------- **TEST RESULTS ON CTB2** The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_17",
  "x": "**THE INFLUENCE OF BEAM-SIZE** ---------------------------------- **TEST RESULTS ON CTB2** The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_18",
  "x": "The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, POS tagging accuracy is not very high, with the perceptron-based tagger achieving an accuracy of only 93%. The beam-search decoding framework we use could accommodate joint parsing and tagging, although the use of features based on the tags of incoming words complicates matters somewhat, since these features rely on tags having been assigned to all words in a pre-processing step. We leave this problem for future work. In a recent paper, Petrov and Klein (2007) Table 4 . However, we did not include their scores in the table because they used a different training set from CTB5, which is much larger than the CTB2 training set used by all parsers in the table.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_19",
  "x": "We compare the dependencies read off our constituent parser using CTB5 data with the dependency parser from Zhang and Clark (2008) . The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 8 . Our constituent parser gave higher accuracy than the dependency parser. It is interesting that, though the constituent parser uses many fewer feature templates than the dependency parser, the features do include constituent information, which is unavailable to dependency parsers. ---------------------------------- **RELATED WORK** Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_20",
  "x": "**RELATED WORK** Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) . The main difference between our parser and Ratnaparkhi's is that we use a global discriminative model, whereas Ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is the use of different incremental parsing processes. To achieve better performance for Chinese parsing, our parser is based on the shiftreduce parsing process. In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004) .",
  "y": "differences"
 },
 {
  "id": "e9b2f32ed29589b4a6d49d3b30fc3a_0",
  "x": "The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents. ---------------------------------- **OVERVIEW** Morphologically rich languages like Arabic (Beesley, K. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation <cite>(Brown et al. 1993</cite> ) and information retrieval (Franz, M. and McCarley, S. 2002) . In this paper, we present a cross-lingual English-Arabic search engine combined with an on demand ArabicEnglish statistical machine translation system that relies on source language analysis for both improved search and translation. We developed novel statistical learning algorithms for performing Arabic word segmentation (Lee, Y. et al 2003) into morphemes and morphological source language (Arabic) analysis (Lee, Y. et al 2003b) . These components improve both monolingual (Arabic) search and cross-lingual (English-Arabic) search and machine translation. In addition, the system supports either document translation or convolutional models for cross-lingual search (Franz, M. and McCarley, S. 2002) .",
  "y": "motivation extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_0",
  "x": "---------------------------------- **INTRODUCTION** Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (<cite>Rush et al., 2015</cite>) . This paper also shares a similar goal and motivation to <cite>previous work</cite>: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_1",
  "x": "**INTRODUCTION** Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (<cite>Rush et al., 2015</cite>) . This paper also shares a similar goal and motivation to <cite>previous work</cite>: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>.",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_2",
  "x": "Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) .",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_5",
  "x": "Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_6",
  "x": "The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_7",
  "x": "<cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. x i is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which <s> canadian prime \u2026 year <s> canada \u2026 nato input sentence headline is represented as a sequence of indicator vectors, whose length is M . That is, x i \u2208 {0, 1} |V | , and whose length is L. Here, we assume L < M .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_8",
  "x": "x i is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which <s> canadian prime \u2026 year <s> canada \u2026 nato input sentence headline is represented as a sequence of indicator vectors, whose length is M . That is, x i \u2208 {0, 1} |V | , and whose length is L. Here, we assume L < M . Y C,i is a short notation of the list of vectors, which consists of the sub-sequence in Y from y i\u2212C+1 to y i . We assume a one-hot vector for a special start symbol, such as \"\u27e8S\u27e9\", when i < 1. Then, <cite>ABS</cite> outputs a summary\u0176 given an input sentence X as follows: where nnlm(Y C,i ) is a feed-forward neural network language model proposed in (Bengio et al., 2003) , and enc(X, Y C,i ) is an input sentence encoder with attention mechanism.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_9",
  "x": "Let S \u2208 R D\u00d7(CD) be a weight matrix for mapping the context embedding of C output words onto embeddings obtained from nodes. Then, we define the attention-based AMR encoder 'encAMR(A, Y C,i )' as follows: Finally, we combine our attention-based AMR encoder shown in Equation 14 as an additional term of Equation 3 to build our headline generation system. ---------------------------------- **EXPERIMENTS** To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in <cite>Rush et al. (2015)</cite> . ---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_11",
  "x": "Then, we define the attention-based AMR encoder 'encAMR(A, Y C,i )' as follows: Finally, we combine our attention-based AMR encoder shown in Equation 14 as an additional term of Equation 3 to build our headline generation system. ---------------------------------- **EXPERIMENTS** To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in <cite>Rush et al. (2015)</cite> . ---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_12",
  "x": "**DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_13",
  "x": "---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'.",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_14",
  "x": "All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters. We tried several values for the initial learning rate, and selected the value that achieved the best performance for each method. We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_15",
  "x": "<cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_17",
  "x": "We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_18",
  "x": "The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the <unk> was auctioned off for $ #.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_19",
  "x": "For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_20",
  "x": "We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_21",
  "x": "We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_22",
  "x": "<cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the <unk> was auctioned off for $ #. # million -lrbeuro# . # million -rrb-. G: massive ##th century us mental hospital fetches $ #.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_23",
  "x": "The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the <unk> was auctioned off for $ #. # million -lrbeuro# .",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_26",
  "x": "# million -lrbeuro# . # million -rrb-. G: massive ##th century us mental hospital fetches $ #. # million at auction A: west african art sells for $ #. # million in P: west african art auctioned off for $ #. # million I(3): brooklyn , the new bastion of cool for many new yorkers , is poised to go mainstream chic . G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic Figure 3 : Examples of generated headlines on <cite>Gigaword</cite>. I: input, G: true headline, A: <cite>ABS</cite> (re-run), and P: <cite>ABS</cite>+AMR. <cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_27",
  "x": "G: massive ##th century us mental hospital fetches $ #. # million at auction A: west african art sells for $ #. # million in P: west african art auctioned off for $ #. # million I(3): brooklyn , the new bastion of cool for many new yorkers , is poised to go mainstream chic . G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic Figure 3 : Examples of generated headlines on <cite>Gigaword</cite>. I: input, G: true headline, A: <cite>ABS</cite> (re-run), and P: <cite>ABS</cite>+AMR. <cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_29",
  "x": "# million at auction A: west african art sells for $ #. # million in P: west african art auctioned off for $ #. # million I(3): brooklyn , the new bastion of cool for many new yorkers , is poised to go mainstream chic . G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic Figure 3 : Examples of generated headlines on <cite>Gigaword</cite>. I: input, G: true headline, A: <cite>ABS</cite> (re-run), and P: <cite>ABS</cite>+AMR. <cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_35",
  "x": "Moreover, <cite>ABS</cite>+AMR generated a consistent subject in the third example. The comparison between <cite>ABS</cite>+AMR(w/o attn) and <cite>ABS</cite>+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. ---------------------------------- **RELATED WORK** Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of <cite>Rush et al. (2015)</cite> : the combination of the feed-forward neural network language model and attention-based sentence encoder. also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_0",
  "x": "****UTILIZING MONOLINGUAL DATA IN NMT FOR SIMILAR LANGUAGES: SUBMISSION TO SIMILAR LANGUAGE TRANSLATION TASK.**** **ABSTRACT** This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi \u2192 Nepali direction in which we have examined the performance of a Recursive Neural Network (RNN) based Neural Machine Translation (NMT) system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by <cite>(Artetxe et al., 2017)</cite> and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data. ---------------------------------- **INTRODUCTION** In this paper, we present the submission for Similar Language Translation Task in WMT 2019. The task focuses on improving machine translation results for three language pairs Czech-Polish (Slavic languages), Hindi-Nepali (Indo-Aryan languages) and Spanish-Portuguese (Romance languages). The main focus of the task is to utilize monolingual data in addition to parallel data because the provided parallel data is very small in amount. The detail of task is provided in (Barrault et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_1",
  "x": "Positional encoding provides the system with information of order of words. NMT needs lots of parallel data to train a system. This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a) , utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016) , complete unsupervised architectures <cite>(Artetxe et al., 2017)</cite> (Lample et al., 2018 ) and many others have been proposed. ---------------------------------- **UTILIZING MONOLINGUAL DATA IN NMT** There has been good amount of work done on how we can utilize monolingual data to improve performance of an NMT system. Back-Translation was introduced by (Sennrich et al., 2016b) , to utilize monolingual data of target language. This requires a translation system in opposite direction.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_2",
  "x": "In iterative Back-Translation, systems in both directions improve each other (Hoang et al., 2018) , it is done in an incremental fashion. To generate backtranslated data, current system in opposite direction is utilized. In (Currey et al., 2017) , target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data. In (Zhang and Zong, 2016) , source side monolingual data is utilized to iteratively generate synthetic sentences from the same model. In (Domhan and Hieber, 2017) , there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word. In (Burlot and Yvon, 2018) , it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in <cite>(Artetxe et al., 2017)</cite> follows an architecture where encoder is shared and decoder is separate for each language. Encoder tries to map sentences from both languages in the same space, which is supported by cross-lingual word embeddings. They fix cross-lingual word embeddings in the encoder while training, which helps in generating cross-lingual sentence representations in the same space.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_3",
  "x": "\u2022 Semi-supervised RNN based NMT system \u2022 Utilization of copied data in RNN based NMT First system is pure RNN based NMT system. To train this we have utilized only parallel corpora. Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized. We have utilized architecture proposed in <cite>(Artetxe et al., 2017)</cite> where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation. This architecture can also be utilized for completely unsupervised setting. Third system is also a pure RNN based NMT system where additional parallel data (synthetic data) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized. In this way the amount of available data becomes three times of the original data. All the data is combined together, shuffled and then provided to the NMT system, there is no identification provided to distinguish between parallel data and copy data. To train all three systems we have utilized the implementation of <cite>(Artetxe et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_4",
  "x": "This architecture can also be utilized for completely unsupervised setting. Third system is also a pure RNN based NMT system where additional parallel data (synthetic data) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized. In this way the amount of available data becomes three times of the original data. All the data is combined together, shuffled and then provided to the NMT system, there is no identification provided to distinguish between parallel data and copy data. To train all three systems we have utilized the implementation of <cite>(Artetxe et al., 2017)</cite> . ---------------------------------- **EXPERIMENTAL DETAILS** ---------------------------------- **DATASET** We have utilized monolingual corpora of both languages in our primary system.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_0",
  "x": "**ABSTRACT** Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings. ---------------------------------- **INTRODUCTION** Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; <cite>Conneau et al., 2018</cite>; , the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see<cite> Conneau et al. (2018)</cite> ). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words .",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_2",
  "x": "Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see<cite> Conneau et al. (2018)</cite> ). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words . Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) . The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_3",
  "x": "Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) . The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in<cite> Conneau et al. (2018)</cite> . Contributions We show that Generalized Procrustes Analysis (GPA) (Gower, 1975) , a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset (Dinu et al., 2015) from a P@1 score of 66.2% to 67.6%.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_4",
  "x": "Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) . The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in<cite> Conneau et al. (2018)</cite> . Contributions We show that Generalized Procrustes Analysis (GPA) (Gower, 1975) , a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset (Dinu et al., 2015) from a P@1 score of 66.2% to 67.6%.",
  "y": "differences"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_5",
  "x": "In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ---------------------------------- **COMPARISON OF PA AND GPA** High resource setting We first present a direct comparison of PA and GPA on BDI from English to five fairly high-resource languages: Arabic, Finnish, German, Russian, and Spanish. The Wikipedia corpus sizes for these languages are reported in Table 1 .",
  "y": "similarities uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_6",
  "x": "To understand the consequences of this difference, consider a single step of the GPA algorithm where after updating G according to Eq.4 we are recomputing T 1 using SVD. Due to the fact that G is partly based on E 1 , these two spaces are bound to be more similar to each other than E 1 and E 2 are. 4 Finding a good mapping between E 1 and G, i.e. a good setting of T 1 , should therefore be easier than finding a good mapping from E 1 to E 2 directly. In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_7",
  "x": "To understand the consequences of this difference, consider a single step of the GPA algorithm where after updating G according to Eq.4 we are recomputing T 1 using SVD. Due to the fact that G is partly based on E 1 , these two spaces are bound to be more similar to each other than E 1 and E 2 are. 4 Finding a good mapping between E 1 and G, i.e. a good setting of T 1 , should therefore be easier than finding a good mapping from E 1 to E 2 directly. In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_8",
  "x": "In this sense, by mapping E 1 onto G, rather than onto E 2 (as PA would do), we are solving an easier problem and reducing the chance of a poor solution. ---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_9",
  "x": "Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ---------------------------------- **COMPARISON OF PA AND GPA** High resource setting We first present a direct comparison of PA and GPA on BDI from English to five fairly high-resource languages: Arabic, Finnish, German, Russian, and Spanish. The Wikipedia corpus sizes for these languages are reported in Table 1 . Results are listed in Table 2 . GPA improves over PA consistently for all five languages. Most notably, for Finnish it scores 2.5% higher than PA. Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_10",
  "x": "High resource setting We first present a direct comparison of PA and GPA on BDI from English to five fairly high-resource languages: Arabic, Finnish, German, Russian, and Spanish. The Wikipedia corpus sizes for these languages are reported in Table 1 . Results are listed in Table 2 . GPA improves over PA consistently for all five languages. Most notably, for Finnish it scores 2.5% higher than PA. Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals. We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings. Results are reported in Table 3 . We observe that GPA outperforms PA consistently on Italian and German with the WaCky embeddings, and on all languages with the Wikipedia embeddings.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_11",
  "x": "We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings. Results are reported in Table 3 . We observe that GPA outperforms PA consistently on Italian and German with the WaCky embeddings, and on all languages with the Wikipedia embeddings. Notice that once more, Finnish benefits the most from a switch to GPA in the Wikipedia embeddings setting, but it is also the only language to suffer from that switch in the WaCky setup. Interestingly, PA fails to learn a good alignment for Italian and Finnish when supervised with numerals, while GPA performs comparably with numerals as with other forms of supervision. Conneau et al. (2018) point out that improvement from subsequent iterations of PA is generally negligible, which we also found to be the case. We also found that while PA learned a slightly poorer alignment than GPA, it did so faster. With our criterion for early stopping, PA converged in 5 to 10 epochs, while GPA did so within 10 to 15 epochs 7 . In the case of Italian and Finnish alignment supervised by numerals, PA converged in 8 and 5 epochs, respectively, but clearly got stuck in local minima. GPA took considerably longer to converge: 27 and 74 epochs, respectively, but also managed to find a reasonable alignment between the language spaces.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_13",
  "x": "Another common mistake we observed in mismatches between PA and GPA predictions, was the wrong choice between two antonyms, e.g. stable v. unstable and visible v. unnoticeable. Distributional word representations are known to suffer from limitations with respect to capturing opposition of meaning (Mohammad et al., 2013) , so it is not surprising that both PA-and GPA-learned alignments can fail in making this distinction. While it is not the case that GPA always outperforms PA on a queryto-query basis in these rather challenging cases, on average GPA appears to learn an alignment more robust to subtle morphological and semantic differences between neighboring words. Still, there are cases where PA and GPA both choose the wrong morphological variant of an otherwise correctly identified target word, e.g. transformation v. transformations. Notice that many of the queries for which both algorithms fail, do result in a nearly synonymous word being predicted, e.g. participant for attendee, earns for gets, footage for video, etc. This serves to show that the learned alignments are generally good, but they are not sufficiently precise. This issue can have two sources: a suboptimal method for learning the alignment and/or a ceiling effect on how good of an alignment can be obtained, within the space of orthogonal linear transformations. ---------------------------------- **PROCRUSTES FIT** To explore the latter issue and to further compare the capabilities of PA and GPA, we perform a Procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of<cite> Conneau et al. (2018)</cite> 9 for both training and evaluation 10 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_14",
  "x": "---------------------------------- **MULTILINGUALITY** Finally, we note that there may be specific advantages to including support languages for which large monolingual corpora exist, as those should, theoretically, be easier to align with English (also a high-resource language): variance in vector directionality, as studied in Mimno and Thompson (2017) , increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. ---------------------------------- **RELATED WORK** Bilingual embeddings Many diverse crosslingual word embedding models have been proposed . The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013) . In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017 Artetxe et al., , 2018 <cite>Conneau et al., 2018</cite>; Lu et al., 2015) . The approach most similar to ours, Faruqui and Dyer (2014) , uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t \u2212 1.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_0",
  "x": "To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010) . The main contributions of this paper are as follows: \u2022 We address the data sparseness problem in unknown named entity classification using factorization machines. \u2022 We demonstrate that factorization machines achieve state-of-the-art performance in sparse named entity classification task using a reduced feature set and a compact model. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_1",
  "x": "Named entity classification is the task of classifying text-based elements into various categories , including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models. ---------------------------------- **INTRODUCTION** To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.",
  "y": "motivation"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_2",
  "x": "The main contributions of this paper are as follows: \u2022 We address the data sparseness problem in unknown named entity classification using factorization machines. \u2022 We demonstrate that factorization machines achieve state-of-the-art performance in sparse named entity classification task using a reduced feature set and a compact model. ---------------------------------- **RELATED WORK** A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004) . These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, <cite>Primadhanty et al. (2015)</cite> explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix. Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_3",
  "x": "**EXPERIMENTS** As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization<cite> (Primadhanty et al., 2015)</cite> . ---------------------------------- **SETTINGS** Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not. all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not. all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_4",
  "x": "**SETTINGS** Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not. all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not. all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not. num-tokens=1, num-tokens=2, num-tokens>2: Whether the candidate consists of 1, 2, or more tokens. dummy: Dummy feature to capture context features. based on the CoNLL-2003 English dataset, which omits named entity candidates that appear in the training data from the development and test data. Table 1 shows the number of tokens and types in the given dataset.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_5",
  "x": "dummy: Dummy feature to capture context features. based on the CoNLL-2003 English dataset, which omits named entity candidates that appear in the training data from the development and test data. Table 1 shows the number of tokens and types in the given dataset. This dataset contains five tags: person (PER), location (LOC), organization (ORG), miscellaneous (MISC), and non-entities (O). Features. We used a subset of features from experiments performed by <cite>Primadhanty et al. (2015)</cite> . Table 3 summarizes the features used in our experiment, including context and entity features. Tools. In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM. Further, we employed libFM 1.4.2 1 (Rendle, 2012) to build a named entity classifier using factorization machines.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_6",
  "x": "Evaluation metrics. For our evaluation, we used precision, recall, and F1-score. The scores were calculated on all tags except for non-entities (O). ---------------------------------- **RESULTS** Table 2 presents results of our experiments. Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features. Overall, the microaveraged F1 score improved by 1.4 points.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_7",
  "x": "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features. Overall, the microaveraged F1 score improved by 1.4 points. From these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines. ---------------------------------- **DISCUSSION** Experimental results show that performance on ORG was improved. For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features. The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags.",
  "y": "similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_8",
  "x": "From these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines. ---------------------------------- **DISCUSSION** Experimental results show that performance on ORG was improved. For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features. The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_9",
  "x": "The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_10",
  "x": "Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model.",
  "y": "differences similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_11",
  "x": "Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_12",
  "x": "Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model.",
  "y": "differences"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_0",
  "x": "We investigate how well they handle morphological case, which is important for parsing. Our experiments on Czech, German and Russian suggest that adding explicit morphological case-either oracle or predicted-improves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling. ---------------------------------- **INTRODUCTION** Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj\u00f6rkelund et al., 2017) . Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by <cite>Tsarfaty et al. (2010</cite> Tsarfaty et al. ( , 2013 : \u2022 Can we represent words abstractly so as to reflect shared morphological aspects between them? \u2022 Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem.",
  "y": "motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_1",
  "x": "It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task. These results suggest that neural dependency parsers do not adequately infer this crucial linguistic feature directly from the input text. ---------------------------------- **DEPENDENCY PARSING MODEL** We use a neural graph-based dependency parser similar to that of Kiperwasser and Goldberg (2016) and Zhang et al. (2017) for all our experiments. We treat our parser as a black box and experiment only with the input representations of the parser. Let w = w 1 , . . . , w |w| be an input sentence of length |w| and let w 0 denote an artificial ROOT token.",
  "y": "motivation background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_2",
  "x": "Using the same method, we also supply the gold full analysis, to tease out the importance of case specifically. Finally, we experiment with multitask learning (MTL; S\u00f8gaard and Goldberg, 2016; Coavoux and Crabb\u00e9, 2017) , using the bi-LSTM states of the lower layer of the bi-LSTM encoder to predict case feature. We then look at the performance when we replace gold case with predicted case. We train a morphological tagger to predict case information. The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. However, we found that using predicted case in a pipeline model gives more improvements than MTL.",
  "y": "background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_3",
  "x": "We then look at the performance when we replace gold case with predicted case. We train a morphological tagger to predict case information. The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. However, we found that using predicted case in a pipeline model gives more improvements than MTL. We also observe an interesting pattern in which MTL achieves better tagging accuracy than the pipeline model but lower performance in parsing (Table 2 ). This is surprising since it suggests that the MTL model must learn to effectively encode case in the model's representation, but must not effectively use it for parsing.",
  "y": "differences"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_0",
  "x": "****CONSTITUENCY AND DEPENDENCY RELATIONSHIP FROM A TREE ADJOINING GRAMMAR AND ABSTRACT CATEGORIAL GRAMMARS PERSPECTIVE**** **ABSTRACT** This paper gives an Abstract Categorial Grammar (ACG) account of<cite> (Kallmeyer and Kuhlmann, 2012)</cite>'s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG derivation trees, we have a unified picture where derivation trees and dependency trees are related but independent equivalent ways to account for the same surface-meaning relation. ---------------------------------- **INTRODUCTION** Tree Adjoining Grammars (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997 ) is a tree grammar formalism relying on two operations between trees: substitution and adjunction. In addition to the tree generated by a sequence of such operations, there is a derivation tree which records this sequence. Derivation trees soon appeared as good candidates to encode semantic-like relations between the elementary trees they glue together.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_1",
  "x": "Solving these problems often leads to modifications of derivation tree structures (Schabes and Shieber, 1994; Kallmeyer, 2002; Joshi et al., 2003; Rambow et al., 2001; Chen-Main and Joshi, To appear) . While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently<cite> (Kallmeyer and Kuhlmann, 2012)</cite> has proposed a transformation from standard derivation trees to dependency trees. This paper provides an ACG perspective on this transformation. The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define. ---------------------------------- **ABSTRACT CATEGORIAL GRAMMARS** ACGs provide a framework in which several grammatical formalisms may be encoded (de Groote and Pogodalla, 2004) . They generate languages of linear \u03bb-terms, which generalize both string and tree languages.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_2",
  "x": "While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently<cite> (Kallmeyer and Kuhlmann, 2012)</cite> has proposed a transformation from standard derivation trees to dependency trees. This paper provides an ACG perspective on this transformation. The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define. ---------------------------------- **ABSTRACT CATEGORIAL GRAMMARS** ACGs provide a framework in which several grammatical formalisms may be encoded (de Groote and Pogodalla, 2004) . They generate languages of linear \u03bb-terms, which generalize both string and tree languages. A key feature is to provide the user direct control over the parse structures of the grammar, the abstract language, which allows several grammatical formalisms to be defined in terms of ACG, in particular TAG (de Groote, 2002) .",
  "y": "background motivation"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_3",
  "x": "---------------------------------- **TAG AS ACG** As Fig. 1 shows, the encoding of TAG into ACG uses two ACGs We exemplify the encoding 2 of a TAG analyzing (1) 3 1 In addition to defining L on the atomic types and on the constants of \u03a3, we have: ) with the proviso that for any constant c : 2 We refer the reader to (Pogodalla, 2009 ) for the details. 3 The TAG literature typically uses this example, and<cite> (Kallmeyer and Kuhlmann, 2012)</cite> as well, to show the mismatch between the derivation trees and the expected se- This sentence is usually analyzed in TAG with a derivation tree where the to love component scopes over all the other arguments, and where claims and seems are unrelated, as Fig. 2(a) shows. The three higher-order signatures are: \u03a3 der\u03b8 : Its atomic types include s, vp, np, s A , vp A . . . where the X types stand for the categories X of the nodes where a substitution can occur while the X A types stand for the categories X of the nodes where an adjunction can occur. For each elementary tree \u03b3 lex. entry it contains a constant C lex.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_4",
  "x": "bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's process to translate derivation trees into dependency trees is a two-step process. The first one does the actual transformation, using macro-tree transduction, while the second one modifies the way to get the yield from the dependency trees rather than from the derivation ones. ---------------------------------- **FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree. In the case of to love, the s node is farther from the head than the vp node. Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_5",
  "x": "bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's process to translate derivation trees into dependency trees is a two-step process. The first one does the actual transformation, using macro-tree transduction, while the second one modifies the way to get the yield from the dependency trees rather than from the derivation ones. ---------------------------------- **FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree. In the case of to love, the s node is farther from the head than the vp node. Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_6",
  "x": "This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree. In the case of to love, the s node is farther from the head than the vp node. Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree. We represent the dependency tree for (1) as In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_7",
  "x": "Therefore any adjunction on the s node (e.g. claims) should be higher than the one on the vp node (e.g. seems) in the dependency tree. We represent the dependency tree for (1) as In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once. (Yoshinaka, 2006) has shown how to encode such MTTs as the composition G \u2022 G \u22121 of two ACGs, and we will use a very similar construct. (Kallmeyer and Kuhlmann, 2012) adds to the transformation from derivation trees to dependency trees the additional constraint that the string associated with a dependency structure is computed directly from the latter, without any reference to the derivation tree. To achieve this, they use two distinct yield functions: yield TAG from derivation trees to strings, and yield dep from dependency trees to strings. ---------------------------------- **THE YIELD FUNCTIONS**",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_8",
  "x": "(Yoshinaka, 2006) has shown how to encode such MTTs as the composition G \u2022 G \u22121 of two ACGs, and we will use a very similar construct. (Kallmeyer and Kuhlmann, 2012) adds to the transformation from derivation trees to dependency trees the additional constraint that the string associated with a dependency structure is computed directly from the latter, without any reference to the derivation tree. To achieve this, they use two distinct yield functions: yield TAG from derivation trees to strings, and yield dep from dependency trees to strings. ---------------------------------- **THE YIELD FUNCTIONS** Let us imagine an initial tree \u03b3 i and an auxiliary tree \u03b3 a with no substitution nodes. The yield of the derived tree resulting from the operations of the derivation tree \u03b3 of Fig. 3 defined in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , w 2 where x, y denotes a tuple of strings. Because of the adjunction, the corresponding dependency structure has a reverse order (\u03b3 = \u03b3 a (\u03b3 i )), the requirement on yield dep imposes that In the interpretation of derivation trees as strings, initial trees (with no substitution nodes) Abstract Indeed, an initial tree can have several adjunction sites.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_9",
  "x": "Were we not interested in the yields but only in the dependency structures, we wouldn't have to manage this ambiguity. This is true both for<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's approach and ours. But as we have here a unified framework for the two-step process they propose, this lexical blow up will result in a multiplicity of types as Section 5 shows. ---------------------------------- **DISAMBIGUATED DERIVATION TREES** In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love . C 10 to love and C 01 to love are used for sentences where only one adjunction at the s or at the vp node occurs respectively. C 00 to love : np np s is used when no adjunction occurs. 6 This really mimics (Yoshinaka, 2006) 's encoding of<cite> (Kallmeyer and Kuhlmann, 2012)</cite> MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it). The superscripts (2(n + 1))(2(n \u2212 1)) express that an adjunction that has n adjunctions above it is translated as a function that takes a 2(n + 1)-tuple as argument and returns a 2(n \u2212 1)-tuple.",
  "y": "motivation background similarities"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_10",
  "x": "This is true both for<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's approach and ours. But as we have here a unified framework for the two-step process they propose, this lexical blow up will result in a multiplicity of types as Section 5 shows. ---------------------------------- **DISAMBIGUATED DERIVATION TREES** In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love . C 10 to love and C 01 to love are used for sentences where only one adjunction at the s or at the vp node occurs respectively. C 00 to love : np np s is used when no adjunction occurs. 6 This really mimics (Yoshinaka, 2006) 's encoding of<cite> (Kallmeyer and Kuhlmann, 2012)</cite> MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it). The superscripts (2(n + 1))(2(n \u2212 1)) express that an adjunction that has n adjunctions above it is translated as a function that takes a 2(n + 1)-tuple as argument and returns a 2(n \u2212 1)-tuple. To model auxiliary trees which are CTAs we need a different strategy.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_11",
  "x": "log (t 0 ) = claim bill (seem (love john mary) ---------------------------------- **CONCLUSION** In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings. It was known from (Yoshinaka, 2006) how to encode a macrotree transducer into a G dep \u2022G \u22121 der ACG composition. Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_12",
  "x": "**CONCLUSION** In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings. It was known from (Yoshinaka, 2006) how to encode a macrotree transducer into a G dep \u2022G \u22121 der ACG composition. Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_13",
  "x": "**CONCLUSION** In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings. It was known from (Yoshinaka, 2006) how to encode a macrotree transducer into a G dep \u2022G \u22121 der ACG composition. Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_0",
  "x": "When you're away from the phone and someone takes a message for you, at the very least you'd expect to be told who called and whether they left a number for you to call back. If the same call is picked up by a voicemail system, even such basic information like the name of the caller and their phone number may not be directly available, forcing one to listen to the entire message 1 in the worst case. By contrast, information about the sender of an email message has always been explicitly represented in the message headers, starting with early standardization attempts (Bhushan et al., 1973) and including the two decade old current standard (Crocker, 1982) . Applications that aim to present voicemail messages through an email-like interface -take as an example the idea of a \"uniform inbox\" presentation of email, voicemail, and other kinds of messages 2 -must deal with the problem of how to obtain information analogous to what would be contained in email headers. Here we will discuss one way of addressing this problem, treating it exclusively as the task of extracting relevant information from voicemail transcripts. In practice, e.g. in the context of a sophisticated voicemail front-end ) that is tightly integrated with an organization-wide voicemail system and private branch exchange (PBX), additional sources of information may be available: the voicemail system or the PBX might provide information about the originating station of a call, and speaker identification can be used to match a caller's voice against models of known callers ). Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of <cite>Huang et al. (2001)</cite> , but the features and techniques we use are very different. While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names. <cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_1",
  "x": "Here we will discuss one way of addressing this problem, treating it exclusively as the task of extracting relevant information from voicemail transcripts. In practice, e.g. in the context of a sophisticated voicemail front-end ) that is tightly integrated with an organization-wide voicemail system and private branch exchange (PBX), additional sources of information may be available: the voicemail system or the PBX might provide information about the originating station of a call, and speaker identification can be used to match a caller's voice against models of known callers ). Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of <cite>Huang et al. (2001)</cite> , but the features and techniques we use are very different. While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names. <cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner . We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_2",
  "x": "While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names. <cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner . We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features. The success of the method proposed here is also due to the use of a rich set of features for candidate classification. For example, the majority of phone numbers in voicemail messages has either four, seven, or ten digits, whereas nine digits would indicate a social security number. In our two-phase approach it is straightforward for the second-phase classifier to take the length of a candidate phone number into account.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_3",
  "x": "Most caller phrases tend to start one or two words into the message. This is because they are typically preceded by a one-word ('hi') or two-word ('hi Jane') greeting. Figure 1 shows the empirical distribution of the beginning of the caller phrase across the 7065 applicable transcripts in the development data. As can be seen, more than 97% of all caller phrases start somewhere between one and seven words from the beginning of the message, though in one extreme case the start of the caller phrase occurred 135 words into the message. These observations strongly suggest that when extracting caller phrases, positional cues should be taken into account. This is good news, especially since intrinsic features of the caller phrase may not be as reliable: a caller phrase is likely to contain names that are problematic for an automatic speech recognizer. While this is less of a problem when evaluating on manual transcriptions, the experience reported in<cite> (Huang et al., 2001)</cite> suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name extraction on automatically generated transcripts. We therefore avoid using anything but a small number of greetings and commonly occurring words like 'hi', 'this', 'is' etc. and a small number of common first names for extracting caller phrases and use positional information in addition to word-based features. We locate caller phrases by first identifying their start position in the message and then predicting the length of the phrase. The empirical distribution of caller phrase lengths in the development data is shown in Figure 2 .",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_4",
  "x": "Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task. A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand. We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_6",
  "x": "Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task. A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand. We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_7",
  "x": "While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data. This becomes clearer when one evaluates on unseen transcripts produced by an automatic speech recognizer (ASR), 3 as summarized in Table 2 . Rows HZP strict and HZP containment repeat the figures for the best model from<cite> (Huang et al., 2001</cite> ) when evaluated on automatic transcriptions. The difference is that HZP strict uses the strict evaluation criterion described above, whereas HZP containment uses the weaker criterion of containment, i.e., an extracted phrase counts as correct if it contains exactly one whole actual phrase. Row JA containment summarizes the performance of our approach when evaluated on 101 unseen automatically transcribed messages. Since we did not have any labeled automatic transcriptions available to compare with the predicted caller phrase labels using the strict criterion, we only report results based on the weaker criterion of containment. In fact, we count caller phrases as correct as long as they contain the full name of the caller, since this is the common denominator in the otherwise somewhat heterogeneous labeling of our training corpus; more on this issue in the next section.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_8",
  "x": "While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data. This becomes clearer when one evaluates on unseen transcripts produced by an automatic speech recognizer (ASR), 3 as summarized in Table 2 . Rows HZP strict and HZP containment repeat the figures for the best model from<cite> (Huang et al., 2001</cite> ) when evaluated on automatic transcriptions. The difference is that HZP strict uses the strict evaluation criterion described above, whereas HZP containment uses the weaker criterion of containment, i.e., an extracted phrase counts as correct if it contains exactly one whole actual phrase. Row JA containment summarizes the performance of our approach when evaluated on 101 unseen automatically transcribed messages. Since we did not have any labeled automatic transcriptions available to compare with the predicted caller phrase labels using the strict criterion, we only report results based on the weaker criterion of containment.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_10",
  "x": "The difference between the approach in<cite> (Huang et al., 2001</cite> ) and ours may be partly due to the performance of the ASR components: <cite>Huang et al. (2001)</cite> report a word error rate of 'about 35%', whereas we used a recognizer (Bacchiani, 2001 ) with a word error rate of only 23%. Still, the reduced performance of the HZP model on ASR transcripts compared with manual transcripts is points toward overfitting, or reliance on features that do not generalize to ASR transcripts. Our main approach, on the other hand, uses classifiers that are extremely knowledgepoor in comparison with the many features of the log-linear models for the various named entity taggers, employing no more than a few dozen categorical features. ---------------------------------- **CALLER NAMES** Extracting an entire caller phrase like 'this is Pat Caller' may not be all that relevant in practice: the prefix 'this is' does not provide much useful information, so simply extracting the name of the caller should suffice. This is more or less a problem with the annotation standard used for marking up voicemail transcripts. We decided to test the effects of changing that standard post hoc. This was relatively easy to do, since proper names are capitalized in the message transcripts. We heuristically identify caller names as the leftmost longest contiguous subsequence of capitalized words inside a marked-up caller phrase.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_11",
  "x": "Rows HZP rules and HZP log-linear refer to the rule-based baseline and the best log-linear model of<cite> (Huang et al., 2001</cite> ) and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths. Our main results appear in the remaining rows. The performance of our hand-crafted extraction grammar (in row JA extract) was about what we had seen on the development data before, with recall being as high as one could reasonably expect. As mentioned above, using a simple pruning step in the second phase (see JA extract + prune) results in a doubling of precision and leaves recall essentially unaffected (a single fragmentary phone number was wrongly excluded). Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall. Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_12",
  "x": "The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers. So the observed performance difference is most likely due to the difference in extraction methods. ---------------------------------- **CONCLUSION AND OUTLOOK** The novel contributions of this paper can be summarized as follows: \u2022 We demonstrated empirically that positional cues can be an important source of information for locating caller names and phrases. \u2022 We showed that good performance on the task of extracting caller information can be achieved using a very small inventory of lexical and positional features. \u2022 We argued that for extracting telephone numbers it is extremely useful to take the length of their numeric representation into account.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_13",
  "x": "Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall. Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers. So the observed performance difference is most likely due to the difference in extraction methods. ---------------------------------- **CONCLUSION AND OUTLOOK** The novel contributions of this paper can be summarized as follows:",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_14",
  "x": "\u2022 We argued that for extracting telephone numbers it is extremely useful to take the length of their numeric representation into account. Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account. The self-identification of a caller and the phone numbers mentioned in the same message are not uncorrelated, since there is usually only a small number of ways to reach any particular caller.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_15",
  "x": "\u2022 We argued that for extracting telephone numbers it is extremely useful to take the length of their numeric representation into account. Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account. The self-identification of a caller and the phone numbers mentioned in the same message are not uncorrelated, since there is usually only a small number of ways to reach any particular caller.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_16",
  "x": "\u2022 We showed that good performance on the task of extracting caller information can be achieved using a very small inventory of lexical and positional features. \u2022 We argued that for extracting telephone numbers it is extremely useful to take the length of their numeric representation into account. Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_0",
  "x": "****THE UTILITY OF PARSE-DERIVED FEATURES FOR AUTOMATIC DISCOURSE SEGMENTATION**** **ABSTRACT** We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively. ---------------------------------- **INTRODUCTION** Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005) , sentence compression<cite> (Sporleder and Lapata, 2005)</cite> , natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006) . These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string \"Prices have dropped but remain quite high, according to CEO Smith,\" which has three discourse segments, each labeled with either \"Nucleus\" or \"Satellite\" depending on how central the segment is to the coherence of the text.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_1",
  "x": "Using the RST Treebank as training and evaluation data, Soricut and Marcu (2003) demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. <cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments. They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_2",
  "x": "Using the RST Treebank as training and evaluation data, Soricut and Marcu (2003) demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees. Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. <cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments. They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task.",
  "y": "motivation"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_3",
  "x": "Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features. As we shall show, its performance is not the highest that can be achieved via context-free parser derived features. In this paper, we train a classifier using a general machine learning approach and a range of finitestate and context-free derived features. We investigate the impact on discourse segmentation performance when one feature set is used versus another, in such a way establishing the utility of features derived from context-free parses. In the course of so doing, we achieve the best reported performance on this task, an absolute F-score improvement of 5.0% over SPADE, which represents a more than 34% relative error rate reduction. By focusing on segmentation, we provide an approach that is generally applicable to all of the various annotation approaches, given the similarities between the various sentence-level segmentation guidelines.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_4",
  "x": "---------------------------------- **EVALUATION** Previous research into RST-DT segmentation and parsing has focused on subsets of the 991 sentence test set during evaluation. Soricut and Marcu (2003) omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing. By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_5",
  "x": "**EVALUATION** Previous research into RST-DT segmentation and parsing has focused on subsets of the 991 sentence test set during evaluation. Soricut and Marcu (2003) omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing. By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set.",
  "y": "differences"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_6",
  "x": "By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set. Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries. Given a word string w 1 . . . w k , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_7",
  "x": "First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used. All other trials use the full 991 sentence test set. Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries. Given a word string w 1 . . . w k , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries. Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0<j<k for a sentence of length k. Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output. Then In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_8",
  "x": "Given a word string w 1 . . . w k , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i. For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries. Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0<j<k for a sentence of length k. Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output. Then In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type. In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. ---------------------------------- **BASELINE SPADE SETUP**",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_9",
  "x": "Secondly, Charniak and Johnson (2005) showed how reranking of the 50-best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the <cite>Sporleder and Lapata (2005)</cite> 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence initial boundaries in the scoring, as discussed in the previous section. As can be seen from these results, our improved configuration of SPADE gives us large improvements over the previously reported SPADE performance on this subset. As a result, we feel that we can use SPADE 490 as a very strong baseline for evaluation on the entire test set. Additionally, we modified the SPADE script to allow us to provide our segmentations to the full discourse parsing that it performs, in order to evaluate the improvements to discourse parsing yielded by any improvements to segmentation. ---------------------------------- **SEGMENTATION CLASSIFIER** For this paper, we trained a binary classifier, which was applied independently at each word w i in the string w 1 . . .",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_10",
  "x": "Each base constituent X begins with a word labeled with B X , which signifies that this word begins the constituent. All other words within a constituent X are labeled and I(nside) tags I X , and words outside of any base constituent are labeled O. In such a way, each word is labeled with both a POS-tag and a B/I/O tag. For our three sequences (lexical, POS-tag and shallow tag), we define n-gram features surrounding the potential discourse boundary. If the current word is w i , the hypothesized boundary will occur between w i and w i+1 . For this boundary position, the 6-gram including the three words before and the three words after the boundary is included as a feature; additionally, all n-grams for n < 6 such that either w i or w i+1 (or both) is in the n-gram are included as features. In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither w i nor w i+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features. This feature set is very close to that used in <cite>Sporleder and Lapata (2005)</cite> , but not identical. Their n-gram feature definitions were different (though similar), and they made use of cue phrases from Knott (1996) . In addition, they used a rulebased clauser that we did not.",
  "y": "similarities"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_11",
  "x": "To achieve this, we modified the SPADE script to accept our segmentations when building the fully hierarchical discourse tree. The results for three systems are presented in Table 3 : SPADE, our \"Full finite-state\" system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence<cite> (Sporleder and Lapata, 2005</cite>; Marcu, 1998; Marcu, 1999; Cristea et al., 2005) . Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p < 0.001. ---------------------------------- **DISCUSSION AND FUTURE DIRECTIONS** Our results show that context-free parse derived features are critical for achieving the highest level of accuracy in sentence-level discourse segmentation. Given that edus are by definition clause-like units, it is not surprising that accurate full syntactic parse trees provide highly relevant information unavailable from finite-state approaches.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_0",
  "x": "**INTRODUCTION** A major challenge in natural language parsing is the unsupervised induction of syntactic structure. While most parsing methods are currently supervised or semi-supervised (McClosky et al. 2006; Henderson 2004; Steedman et al. 2003) , they depend on hand-annotated data which are difficult to come by and which exist only for a few languages. Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available. There has been a resurgence of interest in unsupervised parsing during the last few years. Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings \u2264 10 words (WSJ10) using a constituentcontext model called CCM. Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10. While Klein and Manning's approach may be described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP <cite>(Bod 2006)</cite> . U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as \"UML-DOP\"). The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_1",
  "x": "U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as \"UML-DOP\"). The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings. Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest \u2026 to\u2026 or take \u2026 by surprise. Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Manning (2002, 2004) . Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included. In this paper we will also test an alternative model for unsupervised all-subtrees 400 parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima'an (2005) , and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus. While we do not achieve as high an f-score as the UML-DOP model in<cite> Bod (2006)</cite> , we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in<cite> Bod (2006)</cite> . We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995) , showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing. Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition. We show that U-DOP* outperforms the supervised DOP model if tested on the German-English Europarl corpus in a syntax-based MT system.",
  "y": "differences uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_2",
  "x": "Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows: (1) Divide a corpus into an EC and HC (2) Assign all unlabeled binary trees to the sentences in EC, and store them in a shared parse forest (3) Convert the subtrees from the parse forests into a compact PCFG reduction (see next section) (4) Compute the shortest derivations for the sentences in HC (as in DOP*) (5) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG (6) Use the STSG to compute the most probable parse trees for new test data by means of Viterbi n-best (see next section) We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted? ---------------------------------- **CONVERTING SHARED PARSE FORESTS INTO PCFG REDUCTIONS** The main computational problem is how to deal with the immense number of subtrees in U-DOP*. There exists already an efficient supervised algorithm that parses a sentence by means of all subtrees from a treebank. This algorithm was extensively described in Goodman (2003) and converts a DOP-based STSG into a compact PCFG reduction that generates eight rules for each node in the treebank. The reduction is based on the following idea: every node in every treebank tree is assigned a unique number which is called its address.",
  "y": "extends"
 },
 {
  "id": "f54235664f013f0fec918222be9198_3",
  "x": "That is, even with unlimited training data, DOP's estimator is not guaranteed to converge to the correct distribution. Zollmann and Sima'an (2005) developed a statistically consistent estimator for DOP which is based on the assumption that maximizing the joint probability of the parses in a treebank can be approximated by maximizing the joint probability of their shortest derivations (i.e. the derivations consisting of the fewest subtrees). This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006) , it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. On the basis of this shortest-derivation assumption, Zollmann and Sima'an come up with a model that uses held-out estimation: the training corpus is randomly split into two parts proportional to a fixed ratio: an extraction corpus EC and a held-out corpus HC. Applied to DOP, held-out estimation would mean to extract fragments from the trees in EC and to assign their weights such that the likelihood of HC is maximized. If we combine their estimation method with Goodman's reduction of DOP, Zollman and Sima'an's procedure operates as follows: (1) Divide a treebank into an EC and HC (2) Convert the subtrees from EC into a PCFG reduction (3) Compute the shortest derivations for the sentences in HC (by simply assigning each subtree equal weight and applying Viterbi 1-best) (4) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG Zollmann and Sima'an show that the resulting estimator is consistent. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003) . Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in<cite> Bod (2006)</cite> is particularly time consuming and can only operate by randomly sampling trees. Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_4",
  "x": "However, the same node may be part of more than one tree. A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context. The total number of nodes is cubic in sentence length n. This means that there are O(n 3 ) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied. This is a huge reduction compared to<cite> Bod (2006)</cite> where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2 , where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003) . Figure 2. PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG. The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima'an 2005, theorem 5.2) . This means that U-DOP* results in an STSG which is much more succinct than previous DOP-based STSGs. Moreover, as in Bod (1998 Bod ( , 2000 , we use an extension of Good-Turing to smooth the subtrees and to deal with 'unknown' subtrees.",
  "y": "differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_5",
  "x": "Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> . This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters. Both U-DOP*'s and UML-DOP's estimators are known to be statistically consistent. But while U-DOP*'s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters. Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard -see Sima 'an 1996) . We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing. ---------------------------------- **EVALUATION ON HAND-ANNOTATED CORPORA** To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and<cite> Bod (2006)</cite> : Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation. As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings. The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Sch\u00fctze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5).",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_6",
  "x": "This means that U-DOP* results in an STSG which is much more succinct than previous DOP-based STSGs. Moreover, as in Bod (1998 Bod ( , 2000 , we use an extension of Good-Turing to smooth the subtrees and to deal with 'unknown' subtrees. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> . This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters. Both U-DOP*'s and UML-DOP's estimators are known to be statistically consistent. But while U-DOP*'s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters. Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard -see Sima 'an 1996) . We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing. ---------------------------------- **EVALUATION ON HAND-ANNOTATED CORPORA** To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and<cite> Bod (2006)</cite> : Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_7",
  "x": "The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> . For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing. As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC. Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp. Penn WSJ10, the entire Penn WSJ and the first 2 million sentences from the NANC (North American News Text) corpus which contains a total of approximately 24 million sentences from different news sources.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_8",
  "x": "We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Manning (2002, 2004) . The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> . For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing. As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC. Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_9",
  "x": "It is well known that such a binarization has a negative effect on the f-score. Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences \u2264 40 words, while the binarized version achieves only 64.6% f-score. To compare U-DOP*'s results against some supervised parsers, we additionally evaluated a PCFG treebank grammar and the supervised DOP* parser using the same test set. For these supervised parsers, we employed the standard training set, i.e. Penn's WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model. Table 5 . Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penn's WSJ As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set. While a similar result was obtained in<cite> Bod (2006)</cite> , the absolute difference between unsupervised parsing and the treebank grammar was extremely small in<cite> Bod (2006)</cite>: 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*. ---------------------------------- **EVALUATION ON UNLABELED CORPORA IN A PRACTICAL APPLICATION**",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_0",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering. First, most previous work on distant supervision focuses on generating positive examples, but we show that using existing datasets to identify negative training examples is beneficial as well. Second, we propose an approach to fine-tuning BERT with disparate datasets that works well in practice: our heuristic is to proceed in a stage-wise manner, beginning with the dataset that is \"furthest\" from the test data and ending with the \"closest\". ---------------------------------- **BACKGROUND AND RELATED WORK**",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_1",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering. First, most previous work on distant supervision focuses on generating positive examples, but we show that using existing datasets to identify negative training examples is beneficial as well. Second, we propose an approach to fine-tuning BERT with disparate datasets that works well in practice: our heuristic is to proceed in a stage-wise manner, beginning with the dataset that is \"furthest\" from the test data and ending with the \"closest\". ---------------------------------- **BACKGROUND AND RELATED WORK**",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_2",
  "x": "---------------------------------- **INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering.",
  "y": "extends"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_3",
  "x": "In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering. First, most previous work on distant supervision focuses on generating positive examples, but we show that using existing datasets to identify negative training examples is beneficial as well. Second, we propose an approach to fine-tuning BERT with disparate datasets that works well in practice: our heuristic is to proceed in a stage-wise manner, beginning with the dataset that is \"furthest\" from the test data and ending with the \"closest\". ---------------------------------- **BACKGROUND AND RELATED WORK** In this paper, we tackle the \"end-to-end\" variant of the question answering problem, where the system is only provided a large corpus of articles. This stands in contrast to reading comprehension datasets such as SQuAD (Rajpurkar et al., 2016) , where the system works with a single pre-determined document, or most QA benchmarks today such as TrecQA (Yao et al., 2013) , WikiQA (Yang et al., 2015) , and MS-MARCO (Bajaj et al., 2016) , where the system is provided a list of candidate passages to choose from.",
  "y": "differences uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_4",
  "x": "This stands in contrast to reading comprehension datasets such as SQuAD (Rajpurkar et al., 2016) , where the system works with a single pre-determined document, or most QA benchmarks today such as TrecQA (Yao et al., 2013) , WikiQA (Yang et al., 2015) , and MS-MARCO (Bajaj et al., 2016) , where the system is provided a list of candidate passages to choose from. This task definition, which combines a strong element of information retrieval, traces back to the Text Retrieval Conferences (TRECs) in the late 1990s (Voorhees and Tice, 1999) , but there is a recent resurgence of interest in this formulation (Chen et al., 2017) . The roots of the distant supervision techniques we use trace back to at least the 1990s (Yarowsky, 1995; Riloff, 1996) , although the term had not yet been coined. Such techniques have recently become commonplace, especially as a way to gather large amounts of labeled examples for data-hungry neural networks and other machine learning algorithms. Specific recent applications in question answering include Bordes et al. (2015) , Chen et al. (2017) , Lin et al. (2018) , as well as Joshi et al. (2017) for building benchmark test collections. ---------------------------------- **APPROACH** In this work, we fix the underlying model and focus on data augmentation techniques to explore how to best fine-tune BERT. We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_5",
  "x": "We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking. Each paragraph is then fed into the BERT reader along with the original natural language question for inference. Our reader is built using Google's reference implementation, but with a small tweak: to allow comparison and aggregation of results from different segments, we remove the final softmax layer over different answer spans; cf. . For each candidate paragraph, we apply inference over the entire paragraph, and the reader selects the best text span and provides a score. We then combine the reader score with the retriever score via linear interpolation: S = (1 \u2212 \u00b5) \u00b7 S Anserini + \u00b5 \u00b7 S BERT , where \u00b5 \u2208 [0, 1] is a hyperparameter (tuned on a training sample). One major shortcoming with BERTserini is that <cite>Yang et al. (2019)</cite> only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents. This contrasts with the diversity of paragraphs that the model will likely encounter at inference time, since they are selected from potentially millions of articles. The solution to this problem, of course, is to fine tune BERT with the types of paragraphs it is likely to see at inference time. Unfortunately, such data does not exist for modern QA test collections.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_6",
  "x": "For these, we use the 2018-12-01 dump of Chinese Wikipedia, tokenized with Lucene's CJKAnalyzer into overlapping bigrams. We apply hanziconv 1 to transform the corpus into simplified characters for CMRC and traditional characters for DRCD. Following <cite>Yang et al. (2019)</cite> , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans. As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score. Statistics for the datasets are shown in Table 4. 2 For data augmentation, based on preliminary experiments, we find that examining n = 10 candidates from passage retrieval works well, and we further discover that effectiveness is insensitive to the amount of negative samples. Thus, we eliminate the need to tune d by simply using all passages that do not contain the answer as negative examples. The second block of Table 4 shows the sizes of the augmented datasets constructed using our distant supervision techniques: DS(+) contains positive examples only, while DS(\u00b1) includes both positive and negative examples. There are two additional characteristics to note about our data augmentation techniques: The most salient characteristic is that SQuAD, CMRC, and DRCD all have source answers drawn from Wikipedia (English or Chinese), while TriviaQA includes web pages as well as Wikipedia.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_7",
  "x": "In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score. Statistics for the datasets are shown in Table 4. 2 For data augmentation, based on preliminary experiments, we find that examining n = 10 candidates from passage retrieval works well, and we further discover that effectiveness is insensitive to the amount of negative samples. Thus, we eliminate the need to tune d by simply using all passages that do not contain the answer as negative examples. The second block of Table 4 shows the sizes of the augmented datasets constructed using our distant supervision techniques: DS(+) contains positive examples only, while DS(\u00b1) includes both positive and negative examples. There are two additional characteristics to note about our data augmentation techniques: The most salient characteristic is that SQuAD, CMRC, and DRCD all have source answers drawn from Wikipedia (English or Chinese), while TriviaQA includes web pages as well as Wikipedia. Therefore, for the first three collections, the source and augmented datasets share the same document genre-the primary difference is that data augmentation increases the amount and diversity of answer passages seen by the model during training. For TriviaQA, however, we consider the source and augmented datasets as coming from different genres (noisy web text vs. higher quality Wikipedia articles). Furthermore, the TriviaQA augmented dataset is also much largersuggesting that those questions are qualitatively different (e.g., in the manner they were gathered). These differences appear to have a substantial impact, as experiment results show that TriviaQA behaves differently than the other three collections.",
  "y": "similarities"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_8",
  "x": "---------------------------------- **RESULTS** Our main results on SQuAD are shown in Table 2 . The row marked \"SRC\" indicates fine tuning with SQuAD data only and matches the BERTserini condition of <cite>Yang et al. (2019)</cite> ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade). As expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using DS(+) definitely (Wang et al., 2017) 29.1 37.5 - Kratzwald and Feuerriegel (2018) 29.8 --Par. R. 28.5 -83.1 Par. R. + Answer Agg. 28.9 --Par. R. + Full Agg. 30.2 --MINIMAL (Min et al., 2018) 34.7 42.5 64.0",
  "y": "differences"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_0",
  "x": "Additional prospects for future practical research lines will also be discussed in this comment. The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_1",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space.",
  "y": "motivation background future_work"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_2",
  "x": "The topological analysis of complex textual networks has been widely studied in the recent years. As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_3",
  "x": "As for cooccurrence networks of characters, it was possible to verify that they follow the scale-free and small-world features [4] . Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools.",
  "y": "motivation future_work"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_4",
  "x": "In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_5",
  "x": "In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language.",
  "y": "uses"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_6",
  "x": "For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_7",
  "x": "For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ---------------------------------- **** the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "uses"
 },
 {
  "id": "f856c4fb5e6e00729d33b15b24aff6_0",
  "x": "They must count on their interlocutors to recognize the background knowledge they presuppose by general inference from the logic of their behavior as a cooperative contribution to the task (Thomason et al., 2006) . Such reasoning becomes particularly important in problematic cases, such as when systems must finetune the form and meaning of a clarification request so that the response is more likely to resolve a pending task ambiguity (DeVault and Stone, 2007) . I expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of NLG. Modeling efforts will remain crucial to the exploration of these new capabilities. When we build and assemble models of actions and interpretations, we get systems that can plan their own behavior simply by exploiting what they know about communication. These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress <cite>(Cassell et al., 2000</cite>; Koller and Stone, 2007) shows that there's still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon. Modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings.",
  "y": "future_work motivation"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_0",
  "x": "Neural Machine Translation (NMT) systems, introduced only in 2013, have achieved state of the art results in many MT tasks. MetaMind's submissions to WMT '16 seek to push the state of the art in one such task, English\u2192German newsdomain translation. We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture. ---------------------------------- **INTRODUCTION** The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_1",
  "x": "**ABSTRACT** Neural Machine Translation (NMT) systems, introduced only in 2013, have achieved state of the art results in many MT tasks. MetaMind's submissions to WMT '16 seek to push the state of the art in one such task, English\u2192German newsdomain translation. We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture. ---------------------------------- **INTRODUCTION** The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_2",
  "x": "MetaMind's submissions to WMT '16 seek to push the state of the art in one such task, English\u2192German newsdomain translation. We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture. ---------------------------------- **INTRODUCTION** The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_3",
  "x": "This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs. Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) . The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et al., 2013) . Another focus of recent research has been ways of using monolingual corpus data, available in much larger quantities, to augment the limited parallel corpora used to train translation models.",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_4",
  "x": "There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) . The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et al., 2013) . Another focus of recent research has been ways of using monolingual corpus data, available in much larger quantities, to augment the limited parallel corpora used to train translation models. One way to accomplish this is to train a separate monolingual language model on a large corpus of the target language, then use this language model as an additional input to the decoder or for re-ranking output translations (G\u00fcl\u00e7ehre et al., 2015) . More recently, Sennrich (2015b) introduced the concept of augmentation through back-translation, where an entirely separate translation model is trained on a parallel corpus from the target language to the source language. This backwards translation model is then used to machine-translate a monolingual corpus from the target language into the source language, producing a pseudo-parallel corpus to augment the original parallel training corpus. We extend this back-translation method by translating a very large monolingual German corpus into English, then concatenating a unique subset of this augmentation corpus to the original parallel corpus for each training epoch. ---------------------------------- **MODEL DESCRIPTION**",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_5",
  "x": "We extend this back-translation method by translating a very large monolingual German corpus into English, then concatenating a unique subset of this augmentation corpus to the original parallel corpus for each training epoch. ---------------------------------- **MODEL DESCRIPTION** The model identified as metamind-single is based on the attention-based encoder-decoder framework described in <cite>Luong (2015)</cite> , using the attention mechanism referred to as \"Global attention (dot). \" The encoder is a four-layer stacked LSTM recurrent neural network whose inputs (at the bottom layer) are vectors w in t corresponding to the subword units in the input sentence and which saves the topmost output state at each timestep e t as the variable-length encoding matrix E. The decoder also contains a four-layer stacked LSTM whose states (c 0 and h 0 for each layer) are initialized to the last states for each layer of the encoder. At the first timestep, the decoder LSTM receives as input an initialization word vector w out 0 ; its topmost output state h t is concatenated with an encoder context vector \u03ba t computed as: score(h t , e s ) = h t e s \u03b1 st = softmax all s (score(h t , e s )) This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h output probabilities = softmax(W outh ) For subsequent timesteps, the decoder LSTM receives as input the previous word vector w out t\u22121 concatenated with the previous output vectorh. Decoding is performed using beam search, with beam width 16.",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_6",
  "x": "At the first timestep, the decoder LSTM receives as input an initialization word vector w out 0 ; its topmost output state h t is concatenated with an encoder context vector \u03ba t computed as: score(h t , e s ) = h t e s \u03b1 st = softmax all s (score(h t , e s )) This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h output probabilities = softmax(W outh ) For subsequent timesteps, the decoder LSTM receives as input the previous word vector w out t\u22121 concatenated with the previous output vectorh. Decoding is performed using beam search, with beam width 16. The beam search decoder differs slightly from <cite>Luong (2015)</cite> in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences. ---------------------------------- **Y-LSTM MODEL** The model identified as metamind-ylstm uses a novel attentional framework we call the Y-LSTM. The encoder is a five-layer stacked LSTM recurrent neural network language model (RNN-LM) with subword-vector inputs w in t , whose topmost output state h top t is used as input to a softmax layer which predicts the next input token.",
  "y": "differences"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_7",
  "x": "For metamind-ylstm, we trained a Y-LSTM model using the same corpora, with data-parallel synchronous SGD across four GPUs enabling a batch size of 320 and training speed of about 1,500 subword units per second. The run submitted as metamind-ylstm uses a single snapshot of this model after 9 total training epochs. The run submitted as metamind-ensemble uses an equally-weighted ensemble of three snapshots of the metamind-single model (after 10, 11, and 12 epochs) and a single snapshot of the metamind-ylstm model after 9 total training epochs. ---------------------------------- **RESULTS** Results for all three runs described above are presented in Table 1 . Only the ensemble was submitted to the human evaluation process, with a final ranking of second place (behind U. Edinburgh's ensemble of four independently initialized models). Our best single model matches the performance of the best model from U. Edinburgh, which applies a similar attentional framework, subword splitting, and back-translated augmentation. The Y-LSTM model underperformed relative to the model based on <cite>Luong (2015)</cite> , but provided a small additional boost to the ensemble. The primary contribution of this model is to demonstrate that purely attentional NMT is possible: the only inputs to the decoder are through the attention mechanism.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_0",
  "x": "However, compared to the field of computer vision, less work has been done for speech processing. In this paper, we provide a review of two representative efforts on this topic and propose the novel concept of fine-grained disentangled speech representation learning. ---------------------------------- **INTRODUCTION** Representation learning is a fundamental challenge in machine learning and artificial intelligence. While there are multiple criteria for an ideal representation, disentangled representation (illustrated in Figure 1 ), which explicitly separates the underlying causal factors of the observed data, has been of particular interest, because it can be useful for a large variety of tasks and domains [1, 2, 3, 4, 5] . For example, in [5] , the authors show that learning disentangling latent factors corresponding to pose and identity in photos of human faces can improve the performance of both pose estimation and face verification. Learning disentangled representation from high-dimensional data is not a trivial task and multiple techniques, such as \u03b2-VAE [1] , Info-GAN [2] , and DC-IGN [3] , have been developed to address this problem. While disentangling natural image representation has been studied extensively, much less work has focused on natural speech, leaving a rather large void in the understanding of this problem. In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different.",
  "y": "background uses"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_1",
  "x": "In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>. The key idea is that assuming that the speech data is generated from two separate latent variable sets z1 and z2, where z1 contains segment-level (short-term) variables and z2 contains sequencelevel (long-term) variables (z2 that are further conditioned on an s-vector \u00b52). Leveraging the multi-scale nature that different factors affect speech at different time scales (e.g., speaker identity affects the fundamental frequency and volume of speech signal at the sequence level while the phonetic content affects the speech signal at the segment level), by training an autoencoder in a sequence-to-sequence manner, z1 can be forced to encode segment-level information (e.g., speech content), while z2 and \u00b52 can be forced to encode sequence-level information (e.g., speaker identity). In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss).",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_2",
  "x": "The key idea is that assuming that the speech data is generated from two separate latent variable sets z1 and z2, where z1 contains segment-level (short-term) variables and z2 contains sequencelevel (long-term) variables (z2 that are further conditioned on an s-vector \u00b52). Leveraging the multi-scale nature that different factors affect speech at different time scales (e.g., speaker identity affects the fundamental frequency and volume of speech signal at the sequence level while the phonetic content affects the speech signal at the segment level), by training an autoencoder in a sequence-to-sequence manner, z1 can be forced to encode segment-level information (e.g., speech content), while z2 and \u00b52 can be forced to encode sequence-level information (e.g., speaker identity). In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss). By alternatively training the auto-encoder and C, the z is learned to be an encoding of speech content information.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_3",
  "x": "Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss). By alternatively training the auto-encoder and C, the z is learned to be an encoding of speech content information. Further, the residual information of the speech is reconstructed through another GAN and auxiliary classifier. The experiment shows that such a scheme can be successfully applied to a voice conversion task. The main insight of this work is how to use supervision to conduct representation disentanglement.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_4",
  "x": "This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss). By alternatively training the auto-encoder and C, the z is learned to be an encoding of speech content information. Further, the residual information of the speech is reconstructed through another GAN and auxiliary classifier. The experiment shows that such a scheme can be successfully applied to a voice conversion task. The main insight of this work is how to use supervision to conduct representation disentanglement. In summary, although the implementation is very different, in order to learn disentangled representations, both works add constraints to the latent variables. Such a constraint can be a prior assumption (in the unsupervised case) or a regularization term (in the supervised case). While both efforts show good empirical performance in real tasks and lay the groundwork for future efforts, the learned disentangled representation is relatively coarse-grained. That is, in [<cite>6</cite>] , z1 and z2 are in fact corresponding to general fast-changing and slow-changing information, i.e., z1 may contain other fast-changing information such as emotion, while z2 may contain slow-changing factors such as background and channel noise. In [7] , the authors actually separate out speaker information and general non-speaker information, which may contain a lot of detailed information.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_5",
  "x": "Natural speech signals can be viewed as produced by multiple finegrained causal factors and disentangling these factors leads to the following benefits: Synthesis: Learning fine-grained disentangled representation can help more flexible speech synthesis. Assume that disentangled latent variables corresponding to age, personality, friendliness, emotion, and content are learned; we may then be able to synthesize speech signals corresponding to arbitrary combinations of these factors, according to the requirement of the application scenario. Further, this may support novel AI applications, such as speech style transfer and predicting the future voice of a given subject (similar technology has been adopted in computer vision, e.g., image style transfer [16] and face aging [17] ). In contrast, a coarse-grained disentangled representation [<cite>6</cite>, 7] may only support a simple voice speaker conversion task. Inference: Learning fine-grained disentangled representation can also help with more accurate inference and reasoning. When we attempt to predict one target variable, we usually want to eliminate the interference of other factors. For example, a speech recognition system is expected to be emotionindependent, while a speech emotion recognition system is expected to be text-independent. Historically, some manually designed algorithms are used to eliminate the effects of unrelated factors, e.g., speaker normalization [18, 19] and speaker adaptation [20, 21] are commonly used to eliminate the impacts of speaker variability. However, it is difficult to manually design algorithms for all underlying factors. Previous work in representation learning has shown that by disentangling different (independent) factors, all corresponding inference tasks [5] can gain performance improvements.",
  "y": "differences"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_0",
  "x": "Despite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of Ma and Hovy (2016) , but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016) , and on POS tagging when reproducing Ma and Hovy (2016) . Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Ma and Hovy, 2016) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_1",
  "x": "Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_2",
  "x": "\u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_3",
  "x": "A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_4",
  "x": "\u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_5",
  "x": "Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly. \u2022 Hardware environment can also affect system accuracy.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_6",
  "x": "Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly. \u2022 Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_7",
  "x": "Our framework is based on PyTorch with batched implementation, which is highly efficient, facilitating quick configurations for new tasks. ---------------------------------- **RELATED WORK** Collobert et al. (2011) proposed a seminal neural architecture for sequence labeling. It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models.",
  "y": "extends"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_8",
  "x": "dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. <cite>These models</cite> achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_9",
  "x": "<cite>These models</cite> achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with <cite>most previous work</cite> on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only. ---------------------------------- **NEURAL SEQUENCE LABELING MODELS**",
  "y": "similarities"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_10",
  "x": "3) Our findings are more consistent with <cite>most previous work</cite> on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only. ---------------------------------- **NEURAL SEQUENCE LABELING MODELS** Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively. Subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_11",
  "x": "---------------------------------- **CHARACTER SEQUENCE REPRESENTATIONS** Character features such as prefix, suffix and capitalization can be represented with embeddings through a feature-based lookup table (Collobert et al., 2011; Strubell et al., 2017) , or neural networks without human-defined features (Lample et al., 2016; Ma and Hovy, 2016) . In this work, we focus on neural character sequence representations without hand-engineered features. Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; <cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) . In our experiments, we take the same structure as Ma and Hovy (2016) , using one layer CNN structure with max-pooling to capture character-level representations. Figure 2 (a) shows the CNN structure on representing word \"Mexico\". Character LSTM. Shown as Figure 2 (b), in order to model the global character sequence information of a word \"Mexico\", we utilize a bidirectional LSTM on the character sequence of each word and concatenate the left-to-right final state F LST M and the right-to-left final state B LST M as character sequence representations.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_12",
  "x": "**WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; <cite>Chiu and Nichols, 2016;</cite> Liu et al., 2018) . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011; dos Santos et al., 2015; Strubell et al., 2017) . Word CNN. Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings. If a character sequence representation layer is used, then word embeddings and character sequence representations are concatenated for word representations. For each CNN layer, a window of size 3 slides along the sequence, extracting local features on the word inputs and a ReLU function (Glorot et al., 2011 ) is followed. We follow Strubell et al. (2017) by using 4 CNN layers. Batch normalization (Ioffe and Szegedy, 2015) and dropout (Srivastava et al., 2014) are applied following each CNN layer.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_13",
  "x": "As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of Lample et al. (2016) can be reproduced by our CLSTM+WLSTM+CRF. In most cases, our \"Nochar\" based models underperform their corresponding prototypes Strubell et al., 2017) , which utilize the hand-crafted features. Table 5 shows the results of the chunking task. Peters et al. (2017) give the best reported single model results (95.00\u00b10.08), and our CLSTM+WLSTM+CRF gives a comparable performance (94.93\u00b10.05). We re-implement Zhai et al. (2017) 's model in our Nochar+WLSTM but cannot reproduce their results, this may because that they use grid search for hyperparameter selection. Our Nochar+WCNN+CRF can give comparable results with Collobert et al. (2011) , even ours does not include character information. The results of the POS tagging task is shown in Table 6 .",
  "y": "uses"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_14",
  "x": "Models run on CPU give a lower mean F1-score than models run on GPU, while the difference is insignificant (p > 0.2). Optimizer. We compare different optimizers including SGD, Adagrad (Duchi et al., 2011 ), Adadelta (Zeiler, 2012 , RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014) . The results are shown in Figure 5 5 . In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training. Our observation is consistent with most literature (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) . ---------------------------------- **ANALYSIS** Decoding speed. We test the decoding speeds of the twelve models on the NER dataset using a Nvidia GTX 1080 GPU.",
  "y": "similarities"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_0",
  "x": "****HYBRID METHODS FOR POS GUESSING OF CHINESE UNKNOWN WORDS**** **ABSTRACT** This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words. The rule-based model is sensitive to the type, length, and internal structure of unknown words, and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. ---------------------------------- **INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_1",
  "x": "By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. ---------------------------------- **INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information. The rule-based model is sensitive to the type, length, and internal structure of unknown words, with overgeneration controlled by additional constraints.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_2",
  "x": "**ABSTRACT** This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words. The rule-based model is sensitive to the type, length, and internal structure of unknown words, and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. ---------------------------------- **INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_3",
  "x": "Wu and Jiang (2000) calculated P (Cat,Pos,Len) for each character, where Cat is the POS of a word containing the character, Pos is the position of the character in that word, and Len is the length of that word. They then calculated the POS probabilities for each unknown word as the joint probabilities of the P(Cat,Pos,Len) of its component characters. This approach was applied to unknown nouns, verbs, and adjectives that are two to four characters long 2 . They did not report results on unknown word tagging, but reported that the new word identification and tagging mechanism increased parser coverage. We will show that this approach suffers reduced recall for multisyllabic words if the training corpus is small. Goh (2003) reported a precision of 59.58% on all unknown words using Support Vector Machines. Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48).",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_4",
  "x": "Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH** We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data. ---------------------------------- **THE RULE-BASED MODEL** The motivations for developing a set of rules for this task are twofold.",
  "y": "differences background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_5",
  "x": "We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH** We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data. ---------------------------------- **THE RULE-BASED MODEL** The motivations for developing a set of rules for this task are twofold. First, the rule-based approach was dismissed without testing in previous studies.",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_0",
  "x": "For the headline text darker color corresponds to higher importance. In this paper, we outline a fundamentally different approach to online video popularity analysis that allows social media creators both to predict video popularity as well as to understand the impact of its headline or video frames on the future popularity. To that end, we propose to use an attention-based model and gradient-weighted class activation maps [9] , inspired by the recent successes of the attention mechanism in other domains [15, <cite>16]</cite> . Although some works focused on understanding the influence of image parts on its popularity [6, 1] , our method addresses videos, not images, and exploits the temporal characteristics of video clips through the attention mechanism. By extending the baseline popularity prediction method with the attention mechanism, we enable more intuitive visualization of the impact visual and textual features of the video have on its final popularity, while achieving state-of-the-art results on the popularity prediction task. ---------------------------------- **ARXIV:1804.09949V1 [CS.CV] 26 APR 2018** ---------------------------------- **POPULARITY PREDICTION WITH ATTENTION** We cast the problem of social media video popularity prediction as a binary classification task, as in [13, 11] .",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_1",
  "x": "For each frame feature vector we apply a learnable linear transformation followed by ReLU, obtaining a sequence of frame embeddings (q j ) N j=1 . The final video embedding is a weighted average of these embed- Weights \u03b1 i are computed with attention mechanism implemented as a two-layer neural network<cite> [16]</cite> : the first layer produces a hidden representation u i = tanh(W u q i + b u ) and the second layer outputs unnormalized importance a i = W a u i + b a . W a can be interpreted as a trainable high level representation of the most informative vector in u i space. Final weights are normalized with softmax: Headline. We represent a headline as a sequence of pre-trained GloVe [7] word vectors (w t ) N t=1 . We handle sequences of variable length using a bidirectional LSTM. Similarly to video frames, we use a two-layer attention mechanism on hidden state vectors h t to let the network learn the importance coefficients \u03b2 t for each word. The final text representation d is a weighted average of hidden state vectors d = N t=1 \u03b2 t h t .",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_2",
  "x": "More precisely, we generate heatmaps pointing to regions contributing to popularity in each frame. To this end, we compute gradients of the popular class score\u015d with respect to the output of the last convolutional layer of ResNet50 A \u2208 R K\u00d7K\u00d7F . Gradients are then used to compute weights \u2202\u015d \u2202A f i,j that applied to the convolutional output create class activation map H = max(0, We then normalize the heatmap values to [0, 1] and use attention weights to 1 We use the first seconds of a video as this is how Facebook counts views, but we can extend our method to longer videos through sampling. scale the heatmap by \u03b1 i / max(\u03b1). This way we obtain a sequence-wide normalized heatmap of frame regions influencing the final popularity score. For visualizations in the text domain, we use attention weights \u03b2 t used to compute text representation d. These weights capture relative importance of words in their context to headline popularity, as shown in<cite> [16]</cite> in the context of sentiment analysis. ---------------------------------- **EXPERIMENTS**",
  "y": "background uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_0",
  "x": "**INTRODUCTION** Keyphrases are employed to capture the most salient topics of a long document and are indexed in databases for convenient retrieval. Researchers annotate their scientific publications with high quality keyphrases to ensure discoverability in large scientific repositories. Keyphrases could either be extractive (part of the document) or abstractive. Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. This process is similar to abstractive summarization but instead of a summary the models generate keyphrases. Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_1",
  "x": "Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases. ---------------------------------- **PROPOSED ADVERSARIAL MODEL** As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> . Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i . We employ catSeq model<cite> (Yuan et al. 2018)</cite> for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU.",
  "y": "uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_2",
  "x": "All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i . We employ catSeq model<cite> (Yuan et al. 2018)</cite> for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU. To incorporate the out-of-vocabulary words, we use a copying mechanism (Gu et al. 2016). We also make use of attention mechanism to help the generator identify the relevant components of the source text. Discriminator -We propose a new hierarchical-attention model as the discriminator, which is trained to distinguish between human-curated and machine-generated keyphrases. The first layer of this model consists of m + 1 bi-GRUs. The first bi-GRU encodes the input document d as a sequence of vectors: h = {h 1 , h 2 , ..., h n }. The other m bi-GRUs, which have the same weight parameters, encode each keyphrase as a vector: {k 1 , k 2 , ..., k m }. We then use an attention-based approach (Luong, Pham, and Manning 2015) to build context vectors c j for each keyphrase, where c j is a weighted average over h. By concatenating c j and k j , we get a contextualized representation e j = [c j ; k j ] of keyphrase y j .",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_0",
  "x": "Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. ---------------------------------- **RELATED WORK** There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet 1 for both OH and opinion target extraction. More recently, <cite>Wiegand and Klakow (2010)</cite> explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010) , a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_1",
  "x": "In (Johansson and Moschitti, 2010) , a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007) , for example, use a list of manually compiled communication verbs. ---------------------------------- **DATA** As a large unlabeled (training) corpus, we chose the North American News Text Corpus. As a labeled (test) corpus, we use the MPQA corpus. 2 We use the definition of OHs as described in<cite> (Wiegand and Klakow, 2010)</cite> . The instance space are all noun phrases (NP) in that corpus. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_2",
  "x": "This heuristic would exclude Sentence 3 as some civilian experts should be considered the patient of make available rather than its agent. We use grammatical dependencies from a syntactic parser rather than the output of a semantic parser for the detection of agents as in our initial experiments with semantic parsers the detection of agents of predicate adjectives and nouns was deemed less reliable. The grammatical dependency relations that we consider implying an agent are illustrated in the left half of Table 2 . We consider two different methods for extracting an OH from the contexts of protoOHs: supervised learning and rule-based classification. ---------------------------------- **SUPERVISED LEARNING** The simplest way of using the contexts of agentive protoOHs is by using supervised learning. This means that on our unlabeled training corpus we consider each NP with the head being an agentive protoOH as a positive data instance and all the remaining NPs occurring in those sentences as negative instances. With this definition we train a supervised classifier based on convolution kernels (Collins and Duffy, 2001 ) as this method has been shown to be quite effective for OH extraction<cite> (Wiegand and Klakow, 2010)</cite> . Convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part-of-speech sequences, that are directly provided to the learner.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_3",
  "x": "Convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part-of-speech sequences, that are directly provided to the learner. Thus a classifier can be built without the taking the burden of implementing an explicit feature extraction. We chose the best performing set of tree kernels (Collins and Duffy, 2001; Moschitti, 2006) from that work. It comprises two tree kernels based on constituency parse trees and a tree kernel based on semantic role trees. Apart from a set of sequence kernels (Taylor and Christianini, 2004) , this method also largely outperforms a traditional vector kernel using a set of features that were found predictive in previous work. We exclude sequence and vector kernels in this work not only for reasons of simplicity but also since their addition to tree kernels only results in a marginal improvement. Moreover, the features in the vector kernel heavily rely on taskspecific resources, e.g. a sentiment lexicon, which are deliberately avoided in our low-resource classifier as our method should be applicable to any language (and for many languages sentiment resources are either sparse or do not exist at all). In addition to <cite>Wiegand and Klakow (2010)</cite> , we have to discard the content of candidate NPs (e.g. the candidate opinion holder NP [N P Cand [N N S advocates] ] is reduced to [N P Cand ]), the reason for this being that in our automatically generated training set, OHs will always be protoOHs. Retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns (our resulting classifier should detect any OH and not only protoOHs). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_4",
  "x": "Again, we use the supervised learner based on tree kernels ( \u00a74.1). We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree . Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier. All kernels are combined by plain summation. ---------------------------------- **EXPERIMENTS** The documents were parsed using the Stanford Parser.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_5",
  "x": "We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree . Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier. All kernels are combined by plain summation. ---------------------------------- **EXPERIMENTS** The documents were parsed using the Stanford Parser. 4 Semantic roles were obtained by using the parser by Zhang et al. (2008) .",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_6",
  "x": "Table 4 lists the most highly ranked verbs that are extracted. 6 As an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task-specific resources, i.e. communication verbs from the Appraisal Lexicon (AL) (Bloom et al., 2007) and opinion words from the Subjectivity Lexicon (SL) (Wilson et al., 2005) . Both resources have been found predictive for OH extraction (Bloom et al., 2007;<cite> Wiegand and Klakow, 2010)</cite> . Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_7",
  "x": "\u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns. As far as the classifier built with the help of protoOHs is concerned, adding highly ranked adjectives and nouns consistently improves the performance (mostly recall) when added to the set of 7 We use the Stanford tagger: nlp.stanford.edu/software/CRF-NER.shtml Table 4 : List of verbs most highly correlating with protoOHs; \u2020 : included in AL; * : included in SL. highly ranked verbs. The heuristics further improve the rule-based classifier which is achieved by notably increasing precision. None of the baselines is as robust as the best rule-based classifier using protoOHs (i.e. V250+A100+N100). Considering our discussion in \u00a74.2, it comes as no surprise that the best (pseudo-)supervised classifier does not perform as well as our best rule-based classifier (induced by protoOHs). The fact that, in addition to that, our proposed method also largely outperforms the rule-based classifier relying on both AL and SL when no heuristics are used and is still slightly better when they are incorporated supports the effectiveness of our method. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_8",
  "x": "Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns. As far as the classifier built with the help of protoOHs is concerned, adding highly ranked adjectives and nouns consistently improves the performance (mostly recall) when added to the set of 7 We use the Stanford tagger: nlp.stanford.edu/software/CRF-NER.shtml Table 4 : List of verbs most highly correlating with protoOHs; \u2020 : included in AL; * : included in SL. highly ranked verbs.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_9",
  "x": "The impact of generalization is less advantageous since by increasing recall precision drops more dramatically. Only Clus in conjunction with the heuristics manages to preserve sufficient precision. ---------------------------------- **SELF-TRAINING AND GENERALIZATION** ---------------------------------- **INCORPORATING KNOWLEDGE FROM PROTOOHS INTO SUPERVISED LEARNING** As a maximum amount of labeled training data we chose 60000 instances (i.e. NPs) which is even a bit more than used in<cite> (Wiegand and Klakow, 2010)</cite> . In addition, we also test 1%, 5%, 10%, 25% and 50% of the training set. From the remaining data instances, we use 25000 instances as test data. In order to deliver generalizing results, we randomly sample the training and test partitions five times and report the averaged results.",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_10",
  "x": "This is why the different classifiers presented should primarily be compared to our own baseline (TKPlain) and not the numbers presented in previous work as they always use the maximal size of labeled training data and additionally task-specific resources (e.g. sentiment lexicons). The results show that using the information extracted from the unlabeled data can be usefully combined with the labeled training data. Tree augmentation causes both precision and recall to rise. This observation is consistent with<cite> (Wiegand and Klakow, 2010)</cite> where, however, AL and SL are considered for augmentation. When the vector kernel with the prediction of the rule-based classifier is also included, precision drops slightly but recall is notably boosted resulting in an even more increased F-Score. The results also show that for the setting that we have in focus, i.e. using only few labeled training data, our proposed method is particularly useful. For example, when TKPlain is as good as the best classifier exclusively built from unlabeled data (50.88% in Table 5 ), i.e. at 10%, there is a very notable increase in F-Score when the additional knowledge is added, i.e. the F-Score of TKAug+VK[heur] is increased by approx. 4% points. The degree of improvement towards TKPlain decreases the more labeled training data are used. However, when 100% of the labeled data are used, all of the other classifiers using additional information still outperform TKPlain.",
  "y": "similarities"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_0",
  "x": "These are especially useful when dealing with ambiguous kNCs. The need for annotated data is a drawback of supervised approaches. Manual annotations are costly and time-consuming. To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996) ; Girju (2007) ; Sinha (2009); Tsvetkov and Wintner (2010) ; Ziering et al. (2013) . <cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure. Although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_1",
  "x": "Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996) ; Girju (2007) ; Sinha (2009); Tsvetkov and Wintner (2010) ; Ziering et al. (2013) . <cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure. Although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. Moreover, the system needs part of speech (PoS) tags and splitting information for determining 2NCs and is therefore rather language-dependent. In this paper, we present a precise, high-coverage and knowledge-lean method for bracketing kNCs (for k \u2265 3) occurring in parallel data. Our method uses the distances of words that are aligned to kNC components in parallel languages. For example, the 3NC human rights violations can be bracketed using the positions of aligned words in the Italian fragment . . .",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_2",
  "x": "The fact, that the alignment of the third noun, violations (violazioni), is separated from the rest, points us in the direction of LEFT-branching. Using less restricted forms of cross-lingual supervision, we achieve a much higher coverage than <cite>Ziering and Van der Plas (2014)</cite> . Furthermore, our results are more accurate. In contrast to previous unsupervised methods, our system is applicable in both token-and type-based modes. Token-based bracketing is context-dependent and allows for a better treatment of structural ambiguity (as in luxury cattle truck). We generate large amounts of high-quality bracketed kNCs in a multilingual context that can be used to train supervised learners. ---------------------------------- **ALIGNED WORD DISTANCE BRACKETING** The aligned word distance bracketing (AWDB) is inspired by Behaghel's First Law saying that elements which belong close together intellectually will also be placed close together (Behaghel, 1909) . For each language l, we apply the AWDB algorithm on a kNC as shown in Figure 1 : we start bottomup with one constituent per noun.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_3",
  "x": "For practical purposes, an additional back-off model is put in place. In order to mitigate word alignment problems and data sparseness, we additionaly bracket kNCs in a type-based fashion, i.e., we collect all kNC structures of a kNC type from various contexts. ---------------------------------- **EXPERIMENTS** Tools and data. While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_4",
  "x": "While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish. We use the initial version (basic dataset), that contains English word sequences that conform PoS chunks and their alignments. We select English word sequences whose PoS pattern conforms three nouns.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_5",
  "x": "---------------------------------- **EXPERIMENTS** Tools and data. While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_6",
  "x": "It is trained on the English Wikipedia 5 tagged by TreeTagger (Schmid, 1995) . We inspect all 3NCs in the context of one token to the left and right, w 0 {N 1 N 2 N 3 }w 4 . If P noun (N i ) < \u03b8 or P noun (w j ) \u2265 \u03b8, we remove the 3NC from our dataset. We inspected a subset of all 3NCs in the database and estimated the best filter quality to be with \u03b8 = 0.04. This threshold discards increasing land abandonment but keeps human rights abuse. Our final dataset contains 14,941 tokens and 8824 types. Systems in comparison. We compare AWDB with the bracketing approach of <cite>Ziering and Van der Plas (2014)</cite>. For both systems, we use the majority vote across all nine aligned languages, in a token-and type-based version. We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_7",
  "x": "We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus. 6 As scorer, we use the Chi squared (\u03c7 2 ) measure, which worked best in previous work (Nakov and Hearst, 2005) . We consider both the adjacency (i.e., (N 1 , N 2 ) vs. (N 2 , N 3 ) , (Marcus, 1980) ) and the dependency (i.e., (N 1 , N 2 ) vs. (N 1 , N 3 ) , (Lauer, 1995) ) model. We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT. Human annotation. We observed that there is only a very small overlap between test sets of previous work on NP bracketing and the Europarl database. The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained. Thus, we decided to create our own test set. 2 statmt.org/europarl 3 ims.uni-stuttgart.de/data/NCDatabase.html 4 opus.lingfil.uu.se 5 en.wikipedia.org 6 For a fair comparison, we leave systems that have access to external knowledge, such as web search engines, aside.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_8",
  "x": "We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus. 6 As scorer, we use the Chi squared (\u03c7 2 ) measure, which worked best in previous work (Nakov and Hearst, 2005) . We consider both the adjacency (i.e., (N 1 , N 2 ) vs. (N 2 , N 3 ) , (Marcus, 1980) ) and the dependency (i.e., (N 1 , N 2 ) vs. (N 1 , N 3 ) , (Lauer, 1995) ) model. We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT. Human annotation. We observed that there is only a very small overlap between test sets of previous work on NP bracketing and the Europarl database. The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained. Thus, we decided to create our own test set. 2 statmt.org/europarl 3 ims.uni-stuttgart.de/data/NCDatabase.html 4 opus.lingfil.uu.se 5 en.wikipedia.org 6 For a fair comparison, we leave systems that have access to external knowledge, such as web search engines, aside.",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_9",
  "x": "Table 1 presents the coverage of each system, based on the full dataset. Our first result is that type-based cross-lingual bracketing outperforms token-based and achieves up to 91.2% in coverage. As expected, the system of <cite>Ziering and Van der Plas (2014)</cite> does not cover more than 48.1%. The \u03c7 2 method and the back-off models cover all 3NCs in our dataset. The fact that AWDB type misses 8.8% of the dataset is mainly due to equal distances between aligned words (e.g., crisis resolution mechanism is only aligned to closed compounds, such as the Swedish krisl\u00f6sningsmekanism or to nouns separated by one preposition, such as the Spanish mecanismo de resoluci\u00f3n de crisis). In future work, we will add more languages in the hope to find more variation and thus get an even higher coverage. ---------------------------------- **RESULTS AND DISCUSSION** ---------------------------------- **SYSTEM**",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_10",
  "x": "---------------------------------- **SYSTEM** Acc Table 2 : Direct comparison on common test sets; \u2020 = significantly better than the systems in comparison Table 2 directly compares the systems on common subsets (com), i.e., on 3NCs for which all systems in the set provide a result. The main reason why cross-lingual systems make bracketing errors is the quality of automatic word alignment. AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 . This can be explained with the flexible structure of AWDB, which can exploit more data and is thus more robust to word alignment errors. AWDB significantly outperforms \u03c7 2 in accuracy but is inferior in harmonic(com). The last four lines of Table 2 show all systems with full coverage. AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_11",
  "x": "The main reason why cross-lingual systems make bracketing errors is the quality of automatic word alignment. AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 . This can be explained with the flexible structure of AWDB, which can exploit more data and is thus more robust to word alignment errors. AWDB significantly outperforms \u03c7 2 in accuracy but is inferior in harmonic(com). The last four lines of Table 2 show all systems with full coverage. AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy. The harmonic mean numbers for the system of <cite>Ziering and Van der Plas (2014)</cite> illustrate that coverage gain of types outweighs a higher accuracy of tokens. Our intuition that token-based approaches are superior in accuracy is hardly reflected in the present results. We believe that this is due to the domain-specificity of Europarl.",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_0",
  "x": "In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation <cite>[3]</cite> . However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission. We present here, MIRIAM, (Multimodal Intelligent inteRactIon for Autonomous systeMs), as seen in Figure 1 . MIRIAM allows for these 'on-demand' queries for status and explanations of behaviour. MIRIAM interfaces with the Neptune autonomy software provided by SeeByte Ltd and runs alongside their SeeTrack interface. In this paper, we focus on explanations of behaviours and describe a method that is agnostic to the type of autonomy method. With respect to providing communication for monitoring, please refer to [5] for further details and an overview of the system. ---------------------------------- **EXPLANATIONS FOR REMOTE AUTONOMY** Types of explanations include why to provide a trace or reasoning and why not to elaborate on the system's control method or strategy [4] .",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_1",
  "x": "In this paper, we focus on explanations of behaviours and describe a method that is agnostic to the type of autonomy method. With respect to providing communication for monitoring, please refer to [5] for further details and an overview of the system. ---------------------------------- **EXPLANATIONS FOR REMOTE AUTONOMY** Types of explanations include why to provide a trace or reasoning and why not to elaborate on the system's control method or strategy [4] . Lim et al. (2009) [10] show that both why and why not explanations increase understanding but only why increases trust. We adopt here the 'speak-aloud' method whereby an expert provides rationalisation of the AxV behaviours while watching videos of missions on the SeeTrack software. This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models. Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play <cite>[3]</cite> . An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 .",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_0",
  "x": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing -93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%. ---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g.,<cite> Dyer et al. (2016)</cite> with 92.4 F 1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5). There is a one-to-one mapping between a tree and its sequential form.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_1",
  "x": "**PREVIOUS WORK** We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; <cite>Dyer et al., 2016)</cite> are parsing models that have the current best results in NN parsing. ---------------------------------- **LSTM-LM** The LSTM-LM of Zaremba et al. (2014) turns (x 1 , \u00b7 \u00b7 \u00b7 , x t\u22121 ) into h t , a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013) , and uses h t to guess x t : where W is a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016) . In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014) . Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a conditional probability:",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_2",
  "x": "**RNNG** Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree<cite> (Dyer et al., 2016)</cite> : where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2). RNNG and our model differ in how they compute the conditioning event (z 1 , \u00b7 \u00b7 \u00b7 , z t\u22121 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section. ---------------------------------- **MODEL** Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees: where h t is a hidden state of an LSTM. Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y (x), whose size is polynomial, and use LSTM-LM to find y that satisfies ----------------------------------",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_3",
  "x": "2 We use the reimplementation by Huang et al. (2010) . (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as<cite> Dyer et al. (2016)</cite> do with their discriminative and generative models. 3 ---------------------------------- **TRAINING AND DEVELOPMENT** ---------------------------------- **SUPERVISION** We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data.",
  "y": "similarities"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_5",
  "x": "**SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG<cite> (Dyer et al., 2016)</cite> , both of which are trained on the WSJ only. ---------------------------------- **SEMI-SUPERVISION** We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) (Vinyals et al., 2015) ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in Table 3 . LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 . ---------------------------------- **IMPROVED SEMI-SUPERVISION**",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_6",
  "x": "Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014) . ---------------------------------- **CONCLUSION** The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models<cite> (Dyer et al., 2016)</cite> . We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of Vinyals et al. (2015) and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).",
  "y": "differences"
 }
]