[
 {
  "id": "008d5261ee7385a2b7e39772938f51_0",
  "x": "To compare our work with <cite>Athar (2011)</cite> , we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984) . As is evident from Table 1 , including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. In light of the overall sparsity of detectable citation sentiment in a paper, and of the envisaged applica-tions, this is a very positive result.",
  "y": "uses"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_1",
  "x": "This setup has been shown to produce good results earlier as well (Pang et al., 2002; <cite>Athar, 2011</cite>) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2 .",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_2",
  "x": "Table 2 : Results for joint context and sentiment detection. Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation. The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by <cite>Athar (2011)</cite> . However, we can observe that the F scores decrease as more context is introduced. This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries.",
  "y": "similarities"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_3",
  "x": "The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data <cite>(Athar, 2011)</cite> . ---------------------------------- **RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> .",
  "y": "background extends"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_4",
  "x": "**RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) .",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_5",
  "x": "**RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) .",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_0",
  "x": "Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by <cite>Yang et al. (2018)</cite> . The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslabnlp/doc lm. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_1",
  "x": "See the implementation of Zaremba et al. (2014) 1 , for an example. RNN language models obtain conditional probability p(w t+1 |w 1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w 1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem.",
  "y": "motivation"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_2",
  "x": "However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings. In addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because DOC provides a shortcut connection to the output. We conduct experiments on standard benchmark datasets for language modeling: the Penn Treebank and WikiText-2.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_3",
  "x": "RNN language models obtain conditional probability p(w t+1 |w 1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w 1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_4",
  "x": "To compute the probability distribution, RNN language models encode sequence w 1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_5",
  "x": "3 Language Modeling as Matrix Factorization <cite>Yang et al. (2018)</cite> indicated that the training of language models can be interpreted as a matrix 2 Actually, we apply a bias term in addition to the weight matrix but we omit it to simplify the following discussion. factorization problem. In this section, we briefly introduce their description. Let word sequence w 1:t be context c t . Then we can regard a natural language as a finite set of the pairs of a context and its conditional probability distribution: L = {(c 1 , P * (X|c 1 )), ..., (c U , P * (X|c U ))}, where U is the number of possible contexts and X \u2208 {0, 1} V is a variable representing a onehot vector of a word.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_6",
  "x": "<cite>Yang et al. (2018)</cite> also argued that rank(A ) is as high as vocabulary size V based on the following two assumptions: 1. Natural language is highly context-dependent. In addition, since we can imagine many kinds of contexts, it is difficult to assume a basis that represents a conditional probability distribution for any contexts. 2. Since we also have many kinds of semantic meanings, it is difficult to assume basic meanings that can create all other semantic meanings by such simple operations as addition and subtraction; compressing V is difficult. In summary, <cite>Yang et al. (2018)</cite> indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 .",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_7",
  "x": "To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution. In this study, we propose Direct Output Connection (DOC), which is a generalization method of MoS. DOC computes probability distributions from the middle layers in addition to the final layer. In other words, DOC directly connects the middle layers to the output. Figure 1 shows an overview of DOC, that uses the middle layers (including word embeddings) to compute the probability distributions.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_8",
  "x": "To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution. In this study, we propose Direct Output Connection (DOC), which is a generalization method of MoS. DOC computes probability distributions from the middle layers in addition to the final layer. In other words, DOC directly connects the middle layers to the output. Figure 1 shows an overview of DOC, that uses the middle layers (including word embeddings) to compute the probability distributions.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_9",
  "x": "We varied the number of probability distributions from each layer in situation J = 20 except for the top row. The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval. Since we found that the dropout rate for vector k j,ct greatly influences \u03b2 in Equation 13, we varied it from 0.3 to 0.6 with 0.1 intervals.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_10",
  "x": "We varied the number of probability distributions from each layer in situation J = 20 except for the top row. The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval. Since we found that the dropout rate for vector k j,ct greatly influences \u03b2 in Equation 13, we varied it from 0.3 to 0.6 with 0.1 intervals.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_11",
  "x": "For the nonmonotone interval, we adopted the same value as Zolna et al. (2018) . Table 2 summarizes the hyperparameters of our experiments. represents the number of probability distributions from hidden state h n t . To find the best combination, we varied the number of probability distributions from each layer by fixing their total to 20: J = 20. Moreover, the top row of Table 3 shows the perplexity of AWD-LSTM with MoS reported in <cite>Yang et al. (2018)</cite> for comparison.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_12",
  "x": "Table 5 shows the ranks of matrices containing log probability distributions from each method. In other words, Table 5 describes\u00c3 in Equation 9 for each method. As shown by this table, the output of AWD-LSTM is restricted to D 3 7 . In contrast, AWD-LSTM-MoS<cite> (Yang et al., 2018)</cite> and AWD-LSTM-DOC outputted matrices whose ranks equal the vocabulary size. This fact indicates that DOC (including MoS) can output the same matrix as the true distributions in view of a rank.",
  "y": "similarities"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_13",
  "x": "We compare AWD-LSTM-DOC with AWD-LSTM (Merity et al., 2018) and AWD-LSTMMoS<cite> (Yang et al., 2018)</cite> . We trained each model with the same hyperparameters from our language modeling experiments (Section 5). We selected the model that achieved the best perplexity on the validation set during the training. State-of-the-art results Dyer et al. (2016) 91.7 93.3 Fried et al. (2017) (ensemble) 92.72 94.25 Suzuki et al. (2018) (ensemble) 92.74 94.32 Kitaev and Klein (2018) 95.13 - Moreover, AWD-LSTM-DOC outperformed AWD-LSTM and AWD-LSTM-MoS. These results correspond to the performance on the language modeling task (Section 5.3). The middle part shows that AWD-LSTM-DOC also outperformed AWD-LSTM and AWD-LSTMMoS in the ensemble setting.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_14",
  "x": "Zilly et al. (2017) proposed recurrent highway networks that use highway layers (Srivastava et al., 2015) to deepen recurrent connections. Zoph and Le (2017) adopted reinforcement learning to construct the best RNN structure. However, as mentioned, Melis et al. (2018) established that the standard LSTM is superior to these architectures. Apart from RNN architecture, proposed the input-tooutput gate (IOG), which boosts the performance of trained language models. As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_15",
  "x": "Apart from RNN architecture, proposed the input-tooutput gate (IOG), which boosts the performance of trained language models. As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions. In this study, we generalized their approach to use the middle layers of RNNs. Finally, our proposed method, DOC, achieved the state-of-the-art score on the standard benchmark datasets. Some studies provided methods that boost performance by using statistics obtained from test data.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_16",
  "x": "Although these methods might also improve the performance of DOC, we omitted such investigation to focus on comparisons among methods trained only on the training set. ---------------------------------- **CONCLUSION** We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> . DOC raises the expressive power of RNN language models and improves quality of the model.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_17",
  "x": "**CONCLUSION** We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> . DOC raises the expressive power of RNN language models and improves quality of the model. DOC outperformed MoS and achieved the best perplexities on the standard benchmark datasets of language modeling: PTB and WikiText-2. Moreover, we investigated its effectiveness on machine translation and headline generation.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_0",
  "x": "---------------------------------- **INTRODUCTION** Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_1",
  "x": "**INTRODUCTION** Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_2",
  "x": "Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string \"Indra\" that ultimately causes a misclassification of \"Indra\" as an organization (ORG) instead of person (PER) in a downstream NER task. proach they refer to as contextual string embeddings.",
  "y": "motivation"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_3",
  "x": "Our proposed approach dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see <cite>Akbik et al. (2018)</cite> ). It also requires a memory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors. This is illustrated in Algorithm 1: to embed a word (in a sentential context), we first call the embed() function (line 2) and add the resulting 1 https://github.com/zalandoresearch/flair embedding to the memory for this word (line 3). We then call the pooling operation over all contextualized embeddings for this word in the memory (line 4) to compute the pooled contextualized embedding.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_4",
  "x": "We contribute our approach and all pre-trained models to the open source FLAIR 1 framework (Akbik et al., 2019) , to ensure reproducibility of these results. Our proposed approach (see Figure 2 ) dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a 1 https://github.com/zalandoresearch/flair sentence context (see <cite>Akbik et al. (2018)</cite> ). It also requires a memory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors. This is illustrated in Algorithm 1: to embed a word (in a sentential context), we first call the embed() function (line 2) and add the resulting embedding to the memory for this word (line 3).",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_5",
  "x": "It implements the standard BiLSTM-CRF sequence labeling architecture (Huang et al., 2015) and includes pre-trained contextual string embeddings for many languages. To FLAIR, we add an implementation of our proposed pooled contextualized embeddings. Hyperparameters. For our experiments, we follow the training and evaluation procedure outlined in <cite>Akbik et al. (2018)</cite> and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017) . That is, we use an LSTM with 256 hidden states and one layer (Hochreiter and Schmidhuber, 1997) , a locked dropout value of 0.5, a word dropout of 0.05, and train using SGD with an annealing rate of 0.5 and a patience of 3.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_6",
  "x": "Following Peters et al. (2017) , we then repeat the experiment 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance. Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_7",
  "x": "Following Peters et al. (2017) , we then repeat the experiment 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance. Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines.",
  "y": "similarities uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_8",
  "x": "Baselines. Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in <cite>Akbik et al. (2018)</cite> 2 . By comparing against this baseline, we isolate the impact of our proposed pooled contextualized embeddings. Table 2 : Ablation experiment using contextual string embeddings without word embeddings. We find a more significant impact on evaluation numbers across all datasets, illustrating the need for capturing global next to contextualized semantics.",
  "y": "uses"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_0",
  "x": "Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ). While useful, citation texts might lack the appropriate context from the reference article<cite> [4,</cite> 5, 18] . For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17] .",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_1",
  "x": "This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by <cite>4</cite> experts. Baselines. To our knowledge, the only published results on TAC 201<cite>4</cite> is<cite> [4]</cite> , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to<cite> [4]</cite> , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in<cite> [4]</cite> ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and <cite>4</cite>) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] . All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_2",
  "x": "In addition to<cite> [4]</cite> , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in<cite> [4]</cite> ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and <cite>4</cite>) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] . All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods. Our methods. We rst report results based on training the embeddings on Wikipedia (WE Wiki ). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either outof-vocabulary or not captured in the correct context using general embeddings, therefore we also train biomedical embeddings (WE Bio ) <cite>4</cite> .",
  "y": "extends differences"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_3",
  "x": "The best baseline performance is the query reformulation (QR) method by<cite> [4]</cite> which improves over other baselines. We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) . However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word \"expression\" gives better intuition why is that the case.",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_4",
  "x": "Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is<cite> [4]</cite> where the authors approached the problem using a vector space model similarity ranking and query reformulations. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_0",
  "x": "Open RE combines the above tasks by predicting new facts for an open set of relations. The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts. Existing models often focus on either targeted IE or open RE.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_1",
  "x": "Open RE combines the above tasks by predicting new facts for an open set of relations. The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts. Existing models often focus on either targeted IE or open RE.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_2",
  "x": "CORE is inspired by the combined factorization and entity model (FE) of<cite> Riedel et al. (2013)</cite> . As FE, CORE associates latent semantic representations with entities, relations, and arguments. In contrast to FE, CORE uses factorization machines (Rendle, 2012) as its underlying framework, which allows us to incorporate context in a flexible way. CORE is able to leverage and integrate arbitrary contextual information associated with the input facts into its open RE factorization model. To support reasoning under the openworld assumption, we propose an efficient method for parameter estimation in factorization machines based on Bayesian personalized ranking (Rendle et al., 2009) .",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_3",
  "x": "Targeted IE. Targeted IE methods aim to extract from natural-language text new instances of a set of predefined relations, usually taken from a KB. Most existing methods make use of distant supervision, i.e., they start with a set of seed instances (pairs of entities) for the relations of interest, search for these seed instances in text, learn a relation extractor from the so-obtained training data, and optionally iterate (Mintz et al., 2009; Surdeanu et al., 2012; Min et al., 2013) . Open RE models are more general then targeted IE methods in that they additionally reason about surface relations that do not correspond to KB relations. For this reason,<cite> Riedel et al. (2013)</cite> argued and experimentally validated that open RE models can outperform targeted IE methods.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_4",
  "x": "The general problem with relation clustering is its \"black and white\" approach to relations: either two relations are the same or they are different. This assumption generally does not hold for the surface relations extracted by open IE systems<cite> (Riedel et al., 2013)</cite> ; examples of other types of relationships between relations include implication or mutual exclusion. Tensor factorization. Matrix or tensor factorization approaches try to address the above problem: instead of clustering relations, they directly predict facts. Both matrix and tensor models learn and make use of semantic representations of relations and their arguments.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_5",
  "x": "In the open RE context, however, input relations are semantically related so that many subject-object pairs belong to multiple relations. The key advantage of matrix methods is (1) that this restriction allows them to use additional features-such as features for each subjectobject pair-and (2) that they scale much better with the number of relations. Examples of such matrix factorization models include (Tresp et al., 2009; Jiang et al., 2012; Fan et al., 2014; Huang et al., 2014) . have also shown that a combination of matrix and tensor factorization models can be fruitful. Closest to our work is the \"universal schema\" matrix factorization approach of<cite> Riedel et al. (2013)</cite> , which combines a latent features model, a neighborhood model and an entity model but does not incorporate context.",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_6",
  "x": "This closed-world approach essentially assumes that all unobserved facts are false, which may not be a suitable assumption for the sparsely observed relations of open RE. Following<cite> Riedel et al. (2013)</cite> , we adopt the open-world assumption instead, i.e., we treat each unobserved facts as unknown. Since factorization machines originally require explicit target values (e.g., feedback in recommender systems), we need to adapt parameter estimation to the open-world setting. In more detail, we employ a variant of the Bayesian personalized ranking (BPR) optimization criterion (Rendle et al., 2009 ). We associate with each training point x a set of negative samples X \u2212 x .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_7",
  "x": "To overcome this problem, the negative sample set needs to be related to x in some way. Since we ultimately use our model to rank tuples for each relation individually, we consider as negative evidence for x only unobserved facts from the same relation<cite> (Riedel et al., 2013)</cite> . In more detail, we (conceptually) build a negative sample set X \u2212 r for each relation r \u2208 R. We include into X \u2212 r all facts r(t)-again, along with their context-such that t \u2208 T is an observed tuple but r(t) is an unobserved fact. Thus the subject-object pair t of entities is not observed with relation r in the input data (but with some other relation). The set of negative samples associated with each training point x is defined by the relation r of the fact contained in x, that is X \u2212 x = X \u2212 r .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_8",
  "x": "---------------------------------- **EXPERIMENTS** We conducted an experimental study on realworld data to compare our CORE model with other state-of-the-art approaches. 2 Our experimental study closely follows the one of<cite> Riedel et al. (2013)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_9",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Dataset. We made use of the dataset of<cite> Riedel et al. (2013)</cite> , but extended it with contextual information. The dataset consisted of 2.5M surface facts extracted from the New York Times corpus (Sandhaus, 2008) , as well as 16k facts from Freebase.",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_10",
  "x": "Using the metadata 3 of each New York Times article, we enriched each surface fact by the following contextual information: news desk (e.g., sports desk, foreign desk), descriptors (e.g., finances, elections), online section (e.g., sports, business), section (e.g., a, d), publication year, and bag-of-words of the sentence from which the surface fact has been extracted. Training data. From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in<cite> Riedel et al. (2013)</cite> . Tab. 1 summarizes statistics of the resulting dataset.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_11",
  "x": "From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in<cite> Riedel et al. (2013)</cite> . Tab. 1 summarizes statistics of the resulting dataset. Here we considered a fact or tuple as linked if both of its entities were linked to Freebase, as partiallylinked if only one of its entities was linked, and as non-linked otherwise. In contrast to previous work<cite> (Riedel et al., 2013</cite>; , we retain partially-linked and non-linked facts in our dataset.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_12",
  "x": "Open RE models produce predictions for all relations and all tuples. To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the subsample of 10k tuples (\u2248 6% of all tuples) of<cite> Riedel et al. (2013)</cite> . The subsample consisted of 20% linked, 40% partially-linked and 40% non-linked tuples. For each (surface) relation and method, we predicted the top-100 new facts (not in training) for the tuples in the subsample. Considered methods.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_13",
  "x": "Considered methods. We compared various forms of our CORE model with PITF and the matrix factorization model NFE. Our study focused on these two factorization models because they outperformed other models (including nonfactorization models) in previous studies<cite> (Riedel et al., 2013</cite>; . All models were trained with the full training data described above. PITF (Drumond et al., 2012) .",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_15",
  "x": "NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> . It uses a linear combination of three component models: a neighborhood # (in parentheses) in the top-100 evaluation-set tuples for surface relations. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best value per relation in bold (unique winner) or italic (multiple winners). Average weighs are # column values.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_16",
  "x": "The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training. CORE. We include multiple variants of our model in the experimental study, each differing by the amount of context being used.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_17",
  "x": "We use the original source code of<cite> Riedel et al. (2013)</cite> for training. CORE. We include multiple variants of our model in the experimental study, each differing by the amount of context being used. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Each tuple type is a pair of subject-object types of (e.g. (person, location)).",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_18",
  "x": "NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> . The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_19",
  "x": "4 Our version adds support for BPR and parallelizes the training algorithm. Methodology. To evaluate the prediction performance of each method, we followed<cite> Riedel et al. (2013)</cite> . We considered a collection of 19 Freebase relations (Tab. 2) and 10 surface relations (Tab. 3) and restrict predictions to tuples in the evaluation set. Evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_20",
  "x": "where indicator I k takes value 1 if the k-th prediction is true and 0 otherwise, and # denotes the number of true tuples for the relation in the top-100 predictions of all models. The denominator is included to account for the fact that the evaluation set may include less than 100 true facts. MAP 100 # reflects how many true facts are found by each method as well as their ranking. If all # facts are found and ranked top, then MAP 100 # = 1. Note that our definition of MAP 100 # differs slightly from<cite> Riedel et al. (2013)</cite> ; our metric is more robust because it is based on completely labeled evaluation data. To compare the prediction performance of each system across multiple relations, we averaged MAP 100 # values, in both an unweighted and a weighted (by #) fashion.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_21",
  "x": "Parameters. For all systems, we used d = 100 latent factors, \u03bb = 0.01 for all variables, a constant learning rate of \u03b7 = 0.05, and ran 1000 epochs of stochastic gradient ascent. These choices correspond to the ones of<cite> Riedel et al. (2013)</cite> ; no further tuning was performed. ---------------------------------- **RESULTS.**",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_0",
  "x": "External knowledge-bases such as FrameNet (Baker et al., 1998) , Wikipedia, Yago (Suchanek et al., 2007) , and Freebase (Bollacker et al., 2008) , can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_1",
  "x": "Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in <cite>Ratinov and Roth (2012)</cite> ).",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_2",
  "x": "When mentions are compared during inference, we use the features computed from the top ranked entity candidate of the antecedent mention. As mentions are merged, the ranked lists of candidate entities are also merged and reranked, often changing the top-ranked entity candidate used in subsequent comparisons. The large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions. Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012) , (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B 3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B 3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in <cite>Ratinov and Roth (2012)</cite> , and documents that contain a large number of mentions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_3",
  "x": "---------------------------------- **LINKING TO WIKIPEDIA** To create the initial entity candidate lists for proper noun mentions, we query a knowledge base searcher (Dalton and Dietz, 2013) with the text of these mentions. These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty. Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) .",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_4",
  "x": "These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty. Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) . Additionally, since linking is often performed in pre-processing, two mentions that are determined coreferent during inference could still be linked to different KB entities. To avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_5",
  "x": "After linking to Wikipedia, we have a list of candidate KB entities for each mention. Each entity has access to external information keyed on the Wikipedia article, but this information could more generally come from any knowledge base. Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity. For example, it would be useful to know that Massachusetts is also referred to as \"The 6th State\", however this information is not readily available from Wikipedia.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_6",
  "x": "Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity. For example, it would be useful to know that Massachusetts is also referred to as \"The 6th State\", however this information is not readily available from Wikipedia. 1 We instead use the corpus described in Spitkovsky and Chang (2012) that consists of anchor texts of links to Wikipedia that appear on web pages. This collection of anchor texts is sufficiently extensive to cover many common misspellings of entity names, as well as many name variations missing from Wikipedia.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_7",
  "x": "Static linking: Identical to dynamic linking except that entity candidate lists are not merged during inference (i.e., Algorithm 1 without line 17). This approach is comparable to the fixed alignment model, as in the approaches of Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> . ---------------------------------- **RESULTS** As in Bengston and Roth (2008) , we evaluate our system primarily using the B 3 metric (Bagga and Baldwin, 1998) , but also include pairwise, MUC and CEAF(m) metrics.",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_8",
  "x": "**PERFORMANCE ON TRANSCRIPTS** The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document. In particular, Table 2 : Evaluation on the ACE test data, with the system trained on the train and development sets. ACE contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies. Since these transcripts provide an additional challenge for alignment and coreference, <cite>Ratinov and Roth (2012)</cite> only use the set of non-transcripts for their evaluation.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_9",
  "x": "ACE contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies. Since these transcripts provide an additional challenge for alignment and coreference, <cite>Ratinov and Roth (2012)</cite> only use the set of non-transcripts for their evaluation. Using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts. To identify the transcripts in the test set, we use the approximation from <cite>Ratinov and Roth (2012)</cite> that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter. The performance is shown in Table 3 , while the improvement over our baseline is shown in Figure 3 .",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_10",
  "x": "Our static linking matches the performance of <cite>Ratinov and Roth (2012)</cite> on the non-transcripts. Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment. Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non-transcripts. This indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_11",
  "x": "<cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> .",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_12",
  "x": "<cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_13",
  "x": "There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> . Dalton and Dietz (2013) introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012) . Alignment to an external knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009) , cross-document coreference (Finin et al., 2009; Singh et al., 2010) , and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_0",
  "x": "Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang's model with average gains of 1.91 points absolute in BLEU. ---------------------------------- **INTRODUCTION** Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; <cite>Chiang, 2007)</cite> and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion.",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_1",
  "x": "Our approach maintains the advantages of Chiang's HPB model while at the same time incorporating head information and flex- ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang's HPB as well as a SAMT-style refined version of HPB. ---------------------------------- **HEAD-DRIVEN HPB TRANSLATION MODEL** Like Chiang (2005) and<cite> Chiang (2007)</cite> , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar.",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_2",
  "x": "Instead of collapsing all non-terminals in the source language into a single symbol X as in<cite> Chiang (2007)</cite> , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: is regarded as a head if it is dominated by a word outside of this sequence. Note that this definition (i) allows for a word sequence to have one or more heads (largely due to the fact that a word sequence is not necessarily linguistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective. For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN-AD.",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_3",
  "x": "For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN-AD. It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004 ) and Chiang's HPB model (Chiang, 2005; <cite>Chiang, 2007)</cite> . We extract HD-HRs and NRRs based on initial phrase pairs, respectively.",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_4",
  "x": "---------------------------------- **HD-HRS: HEAD-DRIVEN HIERARCHICAL RULES** As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang's HPB model<cite> (Chiang, 2007)</cite> , except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads.",
  "y": "differences similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_5",
  "x": "**HD-HRS: HEAD-DRIVEN HIERARCHICAL RULES** As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang's HPB model<cite> (Chiang, 2007)</cite> , except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here.",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_6",
  "x": "We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here. Similar to Chiang's HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same constraints as described in<cite> Chiang (2007)</cite> . Moreover, we discard rules that have non-terminals with more than four heads.",
  "y": "similarities uses"
 },
 {
  "id": "04461d946dadc759e4be1207655159_8",
  "x": "During training, we extract four types of NRRs and calculate probabilities for each type. To speed up decoding, we currently (i) only use monotone and swap NRRs and (ii) limit the number of non-terminals in a NRR to 2. ---------------------------------- **FEATURES AND DECODING** Given e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of<cite> Chiang (2007)</cite> , including:",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_0",
  "x": "Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a) , summarization (Zhou et al., 2006 ), text coherence (Lapata and Barzilay, 2005) , tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011) , etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012) , resulting in inadequate evidence to generalize to robust sentential semantics. <cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_1",
  "x": "<cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc. This is a result of simply using document IDs as features to represent a word. Modeling quality lexical semantics in latent variable models does not draw enough attention in the community, since people usually apply dimension reduction techniques for documents, which have abundant words for extracting the document level semantics.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_2",
  "x": "In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the <cite>WTMF</cite> model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997) , a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: In <cite>WTMF</cite>/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics. Moreover, the problem worsens for adjectives, adverbs and verbs, which have a much narrower semantic scope than the whole sentence.",
  "y": "motivation uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_3",
  "x": "We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the <cite>WTMF</cite> model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997) , a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: In <cite>WTMF</cite>/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_4",
  "x": "We believe modeling selectional preference capturing local evidence completes the semantic picture for words, hence further rendering better sentential semantics. To our best knowledge, this is the first work to model selectional preference for sentence/document semantics. We also integrate knowledge-based semantics in the <cite>WTMF</cite> framework. Knowledge-based semantics, a human-annotated clean resource, is an important complement to corpus-based noisy cooccurrence information. We extract similar word pairs from Wordnet (Fellbaum, 1998) .",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_5",
  "x": "---------------------------------- **WEIGHTED TEXTUAL MATRIX FACTORIZATION** <cite>Our</cite> previous work (<cite>Guo and Diab, 2012b</cite> ) models the sentences in the weighted matrix factorization framework ( Figure 1 ). The corpus is stored in an M \u00d7 N matrix X, with each cell containing the TF-IDF values of words. The rows of X are M distinct words and columns are N sentences.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_6",
  "x": "Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. ---------------------------------- **WEIGHTED TEXTUAL MATRIX FACTORIZATION**",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_7",
  "x": "We extract similar word pairs from Wordnet (Fellbaum, 1998) . Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK.",
  "y": "uses motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_8",
  "x": "P and Q is optimized by minimize the objective function: where \u03bb is a regularization term. Missing tokens are modeled by assigning a different weight w m for each 0 cell in the matrix X. We can see the inner product of a word vector P \u00b7,i and a sentence vector Q \u00b7,j is used to approximate the cell X ij . The graphical model of <cite>WTMF</cite> is illustrated in Figure 2a . A w i /s j node is a latent vector P \u00b7,i /Q \u00b7,j , corresponding to a word/sentence, respectively.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_9",
  "x": "**INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details).",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_10",
  "x": "The Figure 2c shows the final WTMF+PK model. ---------------------------------- **INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_11",
  "x": "Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details). ---------------------------------- **EXPERIMENTAL SETTING** We build the model WTMF+PK on the same corpora as used in <cite>our</cite> previous work (<cite>Guo and Diab, 2012b</cite>) , comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples).",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_12",
  "x": "**INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details).",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_13",
  "x": "Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details). ---------------------------------- **EXPERIMENTAL SETTING** We build the model WTMF+PK on the same corpora as used in <cite>our</cite> previous work (<cite>Guo and Diab, 2012b</cite>) , comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples).",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_14",
  "x": "Pearson correlation between the system's answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) . Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors).",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_15",
  "x": "We use cosine similarity to measure the similarity scores between two sentences. Pearson correlation between the system's answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) .",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_16",
  "x": "We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin.",
  "y": "background uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_17",
  "x": "Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other. Table 1 also presents the performance on each individual dataset.",
  "y": "differences uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_18",
  "x": "The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other.",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_19",
  "x": "Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%.",
  "y": "differences extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_20",
  "x": "---------------------------------- **EVALUATION ON THE STS12 DATASETS** Observing the performance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results. The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75, 100, 125, 150} on <cite>WTMF</cite> and WTMF+PK.",
  "y": "uses differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_21",
  "x": "Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics \u03b3 and for the knowledge-based semantics \u03b4. Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ----------------------------------",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_22",
  "x": "Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ---------------------------------- **RELATED WORK**",
  "y": "background differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_23",
  "x": "We will refer to our proposed novel model as WTMF+PK. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ).",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_24",
  "x": "Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ---------------------------------- **RELATED WORK**",
  "y": "background differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_25",
  "x": "We will refer to our proposed novel model as WTMF+PK. It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d .",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_26",
  "x": "Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011) , function words (Griffiths et al., 2005) , selectional preference (Ritter et al., 2010) , synonyms and antonyms (Yih et al., 2012) , etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005 ) the LDA performance degrades in the text categorization task including the modeling of function words.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_27",
  "x": "In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011) , function words (Griffiths et al., 2005) , selectional preference (Ritter et al., 2010) , synonyms and antonyms (Yih et al., 2012) , etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005 ) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics.",
  "y": "differences extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_28",
  "x": "SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010) , where co-occurrence information was not efficiently exploited. Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets.",
  "y": "differences extends"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_0",
  "x": "In the next three sections we present detailed statistics for the current LEXSYS grammar that give an indication of what the grammar contains, its current size, and why it has grown to this size. In order to ease the process of engineering such a large grammar, we have made use of the lexical knowledge representation language DATR (Evans & Gazdar, 1996) to compactly encode the elementary trees <cite>(Evans et al., 1995</cite>; Smets & Evans, 1998) . In Section 5 we present some figures that show how the size of the encoding of the grammar has increased during the grammar development process as the number and complexity of elementary trees has grown. We have addressed problems that result from trying to parse with such a large grammar by using a technique proposed by (Evans & Weir, 1997) and (Evans & Weir, 1998) in which all the trees that each word can anchor are compactly represented using a collection of finite state automata. In Section 6 we give some data that shows the extent to which this technique is successful in compacting the grammar.",
  "y": "uses"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_1",
  "x": "For example, get can be followed by one of 12 different prepositions which means that there are at least 12 33 trees for the single subcategorization # trees # sets # merged # minimized ratio merged / in set states (mean) states (mean) minimized 1-10 112 17.9 6.9 2.6 11-20 83 53.9 13. ---------------------------------- **ENCODING FOR GRAMMAR DEVELOPMENT** Following<cite> (Evans et al., 1995)</cite> and (Smets & Evans, 1998 ) the LEXSYS grammar is encoded using DATR, a non-monotonic knowledge representation language. In 1998, the grammar contained 620 trees organized into 44 tree families and produced using 35 rules.",
  "y": "similarities uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_0",
  "x": "In this paper, we describe the DeepNNNER entry to The 2nd Workshop on Noisy User-generated Text (WNUT) Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission adopts the bidirectional LSTM-CNN model of<cite> Chiu and Nichols (2016)</cite>, as it has been shown to perform well on both newswire and Web texts. It uses word embeddings trained on large-scale Web text collections together with text normalization to cope with the diversity in Web texts, and lexicons for target named entity classes constructed from publicly-available sources. Extended evaluation comparing the effectiveness of various word embeddings, text normalization, and lexicon settings shows that our system achieves a maximum F1-score of 47.24, performance surpassing that of the shared task's second-ranked system. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_1",
  "x": "Traditional approaches to NER on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources (Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014) . Recently, neural network models -especially those that use recursive models -have shown that state of the art performance can be achieved with little feature engineering (Collobert et al., 2011; Santos et al., 2015; <cite>Chiu and Nichols, 2016)</cite> . However, despite their popularity for NER on newswire texts, neural networks have not been widely adopted for NER on Web texts, with the exception of the feed-forward neural network (FFNN) model of Godin et al. (2015) . In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission is based on the model of<cite> Chiu and Nichols (2016)</cite> , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0).",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_2",
  "x": "In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission is based on the model of<cite> Chiu and Nichols (2016)</cite> , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0). In contrast to CRFs, FFNNs, and other windowed models, the BLSTM gives our model effectively infinite context on both sides of a word during sequential labeling. The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace. Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_3",
  "x": "Our shared task submission is based on the model of<cite> Chiu and Nichols (2016)</cite> , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0). In contrast to CRFs, FFNNs, and other windowed models, the BLSTM gives our model effectively infinite context on both sides of a word during sequential labeling. The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace. Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources.",
  "y": "extends"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_4",
  "x": "Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources. The rest of our paper is organized as follows. In Section 2, we describe the adaptations made to<cite> Chiu and Nichols (2016)</cite> 's model. In Section 3, we describe the evaluation methodology.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_5",
  "x": "An overview is given Figure 1 . Our system is based on the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> , and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details. ---------------------------------- **FEATURES** Feature embeddings for words are constructed by concatenating together the features listed here.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_6",
  "x": "Word embeddings are critical for high-performance neural networks in NLP tasks (Turian et al., 2010) . In this paper, we compare six publicly available pre-trained word embeddings. The embeddings are described in detail in Table 3 . The neural embeddings of Collobert et al. (2011) were chosen because<cite> Chiu and Nichols (2016)</cite> reported them to be the highest performing on both CoNLL-2003 and OntoNotes 5.0 datasets. To evaluate embeddings trained on data closer to the WNUT dataset, we also selected the GloVe embeddings of Pennington et al. (2014) , trained on both Web text and tweets, and word2vec embeddings trained on Google News data (Mikolov et al., 2013 ) and on tweets (Godin et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_7",
  "x": "**CNN-EXTRACTED CHARACTER FEATURES** Following<cite> Chiu and Nichols (2016)</cite> , we use a CNN to extract features from 25 dim. character embeddings randomly-initialized from a uniform distribution between -0.5 and 0.5. To accommodate text normalization, we added embeddings for the normalization symbols described in Section 2.2, namely <url>, <user>, <smile>, <lolface>, <sadface>, <neutralface>, <heart>, <number> and <hashtag>. All experiments were conducted with the same character embeddings. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_8",
  "x": "Finally, in order to maximize coverage, the Product lexicon is a combination of the subtype Device from the DBpedia ontology and the lexicon product distributed with WNUT dataset. Every other category is as described in Table 1 . To generate lexicon features, we apply the partial matching algorithm of<cite> Chiu and Nichols (2016)</cite> to the input text, as shown in Figure 2 . Each lexicon and match type (BIOES) is associated with a randomly-initialized 5 dim. embedding.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_9",
  "x": "To facilitate matching, all entries were stripped of parentheses and tokenized with the Penn Treebank tokenization script. ---------------------------------- **CAPITALIZATION FEATURE** Following<cite> Chiu and Nichols (2016)</cite>, we used different symbols for word-level capitalization feature each assigned a randomly initialized embedding: allCaps, upperInitial, lowercase, mixedCaps and noinfo. Similar symbols were used for character-level (upper case, lower case, punctuation, other).",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_10",
  "x": "**TRAINING AND INFERENCE** We follow the training and inference methodology of<cite> Chiu and Nichols (2016)</cite> , training our neural network to maximize the sentence-level log-likelihood from Collobert et al. (2011) . Training is done by mini-batch SGD with a fixed learning rate, and we apply dropout (Pham et al., 2014) to the output nodes. All feature representations are \"unfrozen\" and allowed to be updated by the training algorithm. We used the IOB tag scheme to annotate named entities.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_11",
  "x": "For a more detailed survey, see<cite> (Chiu and Nichols, 2016)</cite> . Most recent approaches to NER have been characterized by the use of CRF, SVM, and perceptron models, where performance is heavily dependent on feature engineering. Ratinov and Roth (2009) used non-local features, a gazetteer extracted from Wikipedia, and Brown-cluster-like word representations. Lin and Wu (2009) used phrase features obtained by performing k-means clustering over a private database of search engine query logs in place of a lexicon. Passos et al. (2014) proposed a model that infused word embeddings with lexical knowledge.",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_12",
  "x": "The WNUT 2015 Shared Task included text normalization and named entity tokenization and detection tasks (Baldwin et al., 2015) , with most systems using machine learning methods like CRF together with a variety of features including lexicons, orthographic features, and distributional information. In contrast with conventional NER, there was only one neural network entry (Godin et al., 2015) , and most systems tended to prefer Brown clusters to word embeddings. The state of the art at WNUT 2015 used a cascaded model of entity tokenization, followed by linking to knowledge bases, and, finally, classification with random forests (Yamada et al., 2015) . Our system adopts the architecture of<cite> Chiu and Nichols (2016)</cite> , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets. Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_13",
  "x": "Our system adopts the architecture of<cite> Chiu and Nichols (2016)</cite> , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets. Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data. ---------------------------------- **CONCLUSION** In this paper, we described the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter, which adopted the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_0",
  "x": "P W captures the intuition that the connection between two sentences is stronger the more entities they share by means of weighted edges, where the weights equal the number of entities shared by sentences (Newman, 2004) . The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid <cite>(Barzilay and Lapata, 2008)</cite> uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information. Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences. Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity. This brings the entity graph closer to Stoddard's (1991, p.30 ) notion of cohesion: \"The relative cohesiveness of a text depends on the number of cohesive ties [...] and on the distance between the nodes and their associated cohesive elements.",
  "y": "background motivation"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_1",
  "x": "For discrimination we use 20 permutations of each text. Table 1 shows the results. Results for Guinaudeau and Strube (2013) , G&S, are reproduced, results for<cite> Barzilay and Lapata (2008)</cite> , B&L, and Elsner and Charniak (2011) , E&C, were reproduced by Guinaudeau and Strube (2013) . The unweighted graph, P U , does not need normalization. Hence the results for the entity graph and the normalized entity graph are identical.",
  "y": "background"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_2",
  "x": "**SUMMARY COHERENCE RATING** We follow<cite> Barzilay and Lapata (2008)</cite> for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc .",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_3",
  "x": "Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc . P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant. We believe that better weighting schemes based on linguistic insights eventually will outperform P U and B&L (left for future work).",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_4",
  "x": "In experiments,<cite> Barzilay and Lapata (2008)</cite> assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children. We follow them with regard to data (107 article pairs), experimental setup and evaluation. Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica. The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities. Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model.",
  "y": "background motivation uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_0",
  "x": "---------------------------------- **INTRODUCTION** Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008) , statistical discourse parsers were developed (Lin et al., 2012;<cite> Ghosh et al., 2011</cite>; Xu et al., 2012) .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_1",
  "x": "Argument position classification involves detection of the location of Arg1 with respect to Arg2: usually either the same sentence (SS) or previous ones (PS). 1 Argument span extraction, on the other hand, is extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from PDTB. Since arguments of explicit discourse relations can appear in the same sentence or in different ones (i.e. relations can be intra-or inter-sentential); there are two approaches to argument span extraction. In the first approach the parser decision is not conditioned on whether the relation is intra-or inter-sentential (e.g.<cite> (Ghosh et al., 2011)</cite> ).",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_2",
  "x": "In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) . In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of<cite> (Ghosh et al., 2011)</cite> with the separate models for arguments of intra-sentential and intersentential explicit discourse relations. To compare to the other approaches (i.e. (Lin et al., 2012) and (Xu et al., 2012) ) we adopt the immediately previous sentence heuristic to select a candidate Arg1 sentence for the inter-sentential relations. Additionally to the heuristic, we train and test CRF argument span extraction models to extract exact argument spans. The paper is structured as follows.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_3",
  "x": "In the former approach argument span extraction is applied right after discourse connective detection, while the latter approach also requires argument position classification. The decision on argument span can be made on different levels: from token-level to sentence-level. In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) . In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of<cite> (Ghosh et al., 2011)</cite> with the separate models for arguments of intra-sentential and intersentential explicit discourse relations. To compare to the other approaches (i.e. (Lin et al., 2012) and (Xu et al., 2012) ) we adopt the immediately previous sentence heuristic to select a candidate Arg1 sentence for the inter-sentential relations.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_4",
  "x": "In this paper we follows the approach of<cite> (Ghosh et al., 2011</cite> (Prasad et al., 2008) ); and distribution of Arg2 with respect to extent in inter-sentential explicit discourse relations. SS = same sentence as the connective; IPS = immediately previous sentence; NAPS = non-adjacent previous sentence; FS = some sentence following the sentence containing the connective; SingFull = Single Full sentence; SingPart = Part of single sentence; MultFull = Multiple full sentences; MultPart = Parts of multiple sentences. ---------------------------------- **IMMEDIATELY PREVIOUS SENTENCE HEURISTIC** According to Prasad et al. (2008) 's analysis of explicit discourse relations annotated in PDTB, out of 18,459 relations, 11,236 (60.9%) have both of the arguments in the same sentence (SS case), 7,215 (39.1%) have Arg1 in the sentences preceding the Arg2 (PS case), and only 8 instances have Arg1 in the sentences following Arg2 (FS case).",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_5",
  "x": "CRF-based discourse parser of<cite> Ghosh et al. (2011)</cite> , which processes SS and PS cases with the same model, uses \u00b12 sentence window as a hypothesis space (5 sentences: 1 sentence containing the connective, 2 preceding and 2 following sentences). The window size is motivated by the observation that it entirely covers arguments of 94% of all explicit relations. The authors also report that the performance of the parser on inter-sentential relations (i.e. mainly PS case) has F-measure of 36.0. However, since in 44.2% of inter-sentential explicit discourse relations Arg1 fully covers the sentence immediately preceding Arg2 (see Table 1 partially copied from (Prasad et al., 2008) ), the heuristic that selects the immediately previous sentence and tags all of its tokens as Arg1 already yields F-measure of 44.2 over all PDTB (the performance on the test set may vary). The same heuristic is mentioned in (Lin et al., 2012 ) and (Xu et al., 2012) as a majority classifier for the relations with Arg1 in previous sentences.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_6",
  "x": "Arg1 appears in SS and PS cases evenly. Consequently, assigning the position of the Arg1 considering the discourse connective, together with its syntactic category and its position in the sentence, for PDTB will be correct in more than 95% of instances. In the literature, the task of argument position classification was addressed by several researchers (e.g. (Prasad et al., 2010) , (Lin et al., 2012) ). Lin et al. (2012) , for instance, report F 1 of 97.94% for a classifier trained on PDTB sections 02-21, and tested on section 23. The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in<cite> (Ghosh et al., 2011)</cite> which is an additional motivation to process intra-and inter-sentential relations separately.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_7",
  "x": "The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in<cite> (Ghosh et al., 2011)</cite> which is an additional motivation to process intra-and inter-sentential relations separately. ---------------------------------- **PARSING MODELS** We replicate and evaluate the discourse parser of<cite> (Ghosh et al., 2011)</cite> , then modify it to process intraand inter-sentential explicit relations separately. This is achieved by integrating Argument Position Classification and Immediately Previous Sentence heuristic into the parsing pipe-line.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_8",
  "x": "For instance, a discourse connective when might have the CONN feature 'Temporal' or 'Contingency' depending on the discourse relation it appears in, or 'NULL' in case of non-discourse usage. The value of the feature is 'NULL' for all tokens except the discourse connective. Boolean Main Verb (BMV) is a feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003) . For instance in the sentence Prices collapsed when the news flashed, the main verb is collapsed; thus, its BMV feature is '1', whereas for the rest of tokens it is '0'. Previous Sentence Feature (PREV) signals if a sentence immediately precedes the sentence starting with a connective, and its value is the first token of the connective<cite> (Ghosh et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_9",
  "x": "The feature is similar to a heuristic to select the sentence immediately preceding a sentence starting with a connective as a candidate for Arg1. Arg2 Label (ARG2) is an output of Arg2 span extraction model, and it is used as a feature for Arg1 span extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation, the possible values of ARG2 are IOBE-tagged labels, i.e. 'ARG2-B' -if a word is the first word of Arg2, 'ARG2-I' -if a word is inside the argument span, 'ARG2-E' -if a word is in the last word of Arg2, and 'O' otherwise. CRF++ 2 -conditional random field implementation we use -allows definition of feature templates. Via templates these features are enriched with ngrams: tokens with 2-grams in the window of \u00b11 to- Figure 1: Single model discourse parser architecture of<cite> (Ghosh et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_10",
  "x": "**SINGLE MODEL DISCOURSE PARSER** The discourse parser of<cite> (Ghosh et al., 2011</cite> ) is a cascade of CRF models to sequentially label Arg2 and Arg1 spans (since Arg2 label is a feature for Arg1 model) (see Figure 1 ). There is no distinction between intra-and inter-sentential relations, rather the single model jointly decides on the position and the span of an argument (either Arg1 or Arg2, not both together) in the window of \u00b12 sentences (the parser will be further abbreviated as W5P -Window 5 Parser). The single model parser achieves F-measure of 81.7 for Arg2 and 60.3 for Arg1 using CONNL evaluation script. The performance is higher than<cite> (Ghosh et al., 2011</cite> ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_11",
  "x": "The performance is higher than<cite> (Ghosh et al., 2011</cite> ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives. These models are the baseline for comparison with separate models architecture. However, we change the evaluation method (see Section 6). Figure 2 depicts the architecture of the discourse parser processing intra-and inter-sentential relations separately. It is a combination of argument position classification with specific CRF models for each of the arguments of SS and PS cases, i.e. there are 4 CRF models -SS Arg1 and Arg2, and PS Arg1 and Arg2 (following sentence case (FS) is ignored).",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_12",
  "x": "Finally, we compare the results of the separate model parser to (Lin et al., 2012) and (Xu et al., 2012) . ---------------------------------- **EVALUATION** There are two important aspects regarding the evaluation. First, in this paper it is different from<cite> (Ghosh et al., 2011)</cite> ; thus, we first describe it and evaluate the difference.",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_13",
  "x": "Following<cite> (Ghosh et al., 2011)</cite> PDTB is split as Sections 02-22 for training, 00-01 for development, and 23-24 for testing. ---------------------------------- **CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script. However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_14",
  "x": "**CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script. However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary. However, in PDTB arguments can (1) span over several sentences, (2) be non-contiguous in the same sentence. Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_15",
  "x": "Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections. In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3. In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly (even one token difference is counted as an error); and References in Gold is the total number of arguments in the reference. String-based evaluation of the single model discourse parser with gold features reduces F 1 for Arg2 from 81.7 to 77.8 and for Arg1 from 60.33 to 55.33.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_16",
  "x": "However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary. However, in PDTB arguments can (1) span over several sentences, (2) be non-contiguous in the same sentence. Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections. In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_17",
  "x": "As expected, for the harder case of Arg1, performances are lower. ---------------------------------- **CONCLUSION** In this paper we compare two strategies for the argument span extraction: to process intra-and intersentential explicit relations by a single model, or separate ones. We extend the approach of<cite> (Ghosh et al., 2011)</cite> to argument span extraction cast as token-level sequence labeling using CRFs and integrate argument position classification and immediately previous sentence heuristic.",
  "y": "extends"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_0",
  "x": "---------------------------------- **INTRODUCTION** Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015) . Seq2seq models have also been applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016) , are an intuitively more promising approach.",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_1",
  "x": "However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016) , are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG research area. Our aim is to update the Seq2seq approach proposed in<cite> Vinyals et al. (2015)</cite> as a stronger baseline of constituency parsing.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_2",
  "x": "More specifically, it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the outputs of strong Seq2seq models and task-specific models, e.g., RNNG. The contributions of this paper are summarized as follows: (1) a strong baseline for constituency parsing based on general purpose Seq2seq models 1 , (2) an empirical investigation of several generic techniques that can (or cannot) contribute to improve the parser performance, (3) empirical evidence that Seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information. ---------------------------------- **CONSTITUENCY PARSING BY SEQ2SEQ** Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_4",
  "x": "As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016) . First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: If we set a large negative value at the m-th element in b, namely b m \u2248 \u2212\u221e, then the m-th element in p j becomes approximately 0, namely p j,m \u2248 0, regardless of the value of the k-th element in o j . We refer to this operation to set value \u2212\u221e in b as a mask.",
  "y": "background motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_5",
  "x": "As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016) . First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: If we set a large negative value at the m-th element in b, namely b m \u2248 \u2212\u221e, then the m-th element in p j becomes approximately 0, namely p j,m \u2248 0, regardless of the value of the k-th element in o j . We refer to this operation to set value \u2212\u221e in b as a mask.",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_6",
  "x": "(2) if the number of predicted XX-tags (or POS-tags) is equivalent to that of the words in a given input sentence, then we mask the XX-tags (or all the POS-tags) and all the open brackets. If both conditions (1) and (2) are satisfied, then the decoding process is finished. The additional cost for controlling the mask is to count the number of XX-tags and the open and closed brackets so far generated in the decoding process. The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) .",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_7",
  "x": "The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) . To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d 7 , as initial values of the encoder embedding layer. ---------------------------------- **MODEL ENSEMBLE**",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_8",
  "x": "We only evaluated the development data (PTB Sec. 22) to prevent over-tuning to the test data. ---------------------------------- **EXPERIMENTS** Our experiments used the English Penn Treebank data (Marcus et al., 1994) , which are the most widely used benchmark data in the literature. We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in<cite> Vinyals et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_9",
  "x": "We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in<cite> Vinyals et al. (2015)</cite> . For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature. Table 7 : List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch systems: scores with bold font represent our scores. ments unless otherwise specified.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_10",
  "x": "The baseline Seq2seq models, (a) and (f), produced the malformed parse trees. We postprocessed such malformed parse trees by simple rules introduced in <cite>(Vinyals et al., 2015)</cite> . On the other hand, we confirmed that all the results applying the technique explained in Sec. 3.4 produced no malformed parse trees. Ensembling and Reranking: Table 5 shows the results of our models with model ensembling and LM-reranking. For ensemble, we randomly selected eight of the ten Seq2seq models reported in Table 4 .",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_0",
  "x": "SITGs constitute a restricted subset of syntax directed stochastic grammars for translation, and are very related to context-free grammars. These can be used to analyse two strings simultaneously, which makes them specially useful for extracting bilingual segments from a parallel corpus in a syntax-oriented manner. In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach.",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_1",
  "x": "In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach. Next, in section 3, we will sum up the grounds of SITGs and the modifications proposed in (S\u00e1nchez and Bened\u00ed, 2006a) . In section 4, we present the translation results on the Europarl corpus, obtained when applying one learning iteration on SITGs with several number of nonterminals.",
  "y": "extends"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_2",
  "x": "**SITGS FOR PHRASE EXTRACTION** First, we built an initial SITG by following the method described in <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ). Then, both source and target languages in the training corpus were bracketed by using FreeLing (Asterias et al., 2006) , which is an opensource suite of language analysers. This being done, we then used the bracketed corpus to perform one estimation iteration on the initial SITG and obtain improved SITGs. Finally, the SITG obtained after the estimation iteration was used to parse the bracketed training corpus and extract segment pairs to setup a phrase-based translation model.",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_3",
  "x": "The purpose of building SITGs with several non-terminal symbols was to analyse whether augmenting the number of non-terminals would improve word reorderings between both input and output languages. Adding non-terminal symbols may provide more complexity to the grammar built, and hence increases its expressive power. <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ) Translation results of this setup can be seen in Table 1 . Here, all the weights of the log-linear model were adjusted my MERT training, and the language model used was a 5-gram interpolated with Knesser-Ney discount. It is interresting to point out that one estimation iteration for any number of non-terminal symbols has deffinitely an improving effect.",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_4",
  "x": "---------------------------------- **DISCUSSION** Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus. We have widely exceeded this baseline. On the other hand, the Moses toolkit (Philipp Koehn, 2007) , which is a state of the art statistical machine translation system, obtains in this task a score of 31.0 BLEU.",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_5",
  "x": "**DISCUSSION** Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus. We have widely exceeded this baseline. On the other hand, the Moses toolkit (Philipp Koehn, 2007) , which is a state of the art statistical machine translation system, obtains in this task a score of 31.0 BLEU. However, when constrained to use only the inverse and direct translation models as we did, the score drops to 29.6 BLEU, which is only 1.7 points away from our best score, with only the direct and the inverse translation models, and 0.7 points away from our overall best score.",
  "y": "differences"
 },
 {
  "id": "0860b08831b01e7e98c66ced63b256_0",
  "x": "The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings <cite>[12]</cite> . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees. There are three main steps in our rescoring approach. The first step is to have the parser to produce n-best parse trees with their structural scores. For each parsed tree including words, part-of-speech (PoS) and semantic role labels.",
  "y": "motivation"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_0",
  "x": "Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_1",
  "x": "****A DETERMINISTIC ALGORITHM FOR BRIDGING ANAPHORA RESOLUTION**** **ABSTRACT** Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns.",
  "y": "background motivation"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_2",
  "x": "Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_3",
  "x": "In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with<cite> Hou et al. (2013b)</cite>'s best system MLN II.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_4",
  "x": "In addition, Delmed is exploring distribution arrangements with Fresenius USA, Delmed said. Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns. However, such patterns only consider head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification. In Example 1, in order to find the antecedent (dialysis products) for the bridging anaphor \"distribution arrangements\", we have to understand the semantics of the modification \"distribution\".",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_5",
  "x": "(1) While the discussions between Delmed and National Medical Care have been discontinued, Delmed will continue to supply dialysis products through National Medical after their exclusive agreement ends in March 1990, Delmed said. In addition, Delmed is exploring distribution arrangements with Fresenius USA, Delmed said. Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns. However, such patterns only consider head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_6",
  "x": "Our approach combines the semantics of an NP's head with the semantics of its modifications by vector average using embeddings bridging. We show that this simple, efficient method achieves the competitive results on ISNotes for the task of bridging anaphora resolution compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. The main contributions of our work are: (1) a general word representation resource 2 for bridging; and (2) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an NP based on its head noun and modifications. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_7",
  "x": "Pre-vious work on bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b</cite> ) explored word co-occurrence counts in certain syntactic preposition patterns to calculate word relatedness. For instance, the big hit counts of the query \"the door of the house\" in large corpora could indicate that door and house stand in a part-of relation. These patterns encode associative relations between nouns which cover a variety of bridging relations. Unlike previous work which only consider a small number of prepositions per anaphor, the PP context model (Hou, 2018) uses all prepositions for all nouns in big corpora. It also includes the possessive structure of NPs.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_8",
  "x": "Lassalle and Denis (2011) evaluated their system on mereological bridging anaphors annotated in the DEDE corpus and reported an accuracy of 23%. Markert et al. (2012) released a corpus called ISNotes which contains unrestricted bridging annotations. Based on this corpus,<cite> Hou et al. (2013b)</cite> proposed a joint inference framework for bridging anaphora resolution using Markov logic networks (Domingos and Lowd, 2009 framework resolves all bridging anaphors in one document together by modeling that semantically related anaphors are likely to share the same antecedent. ISNotes is a challenging corpus for bridging. First, bridging anaphors are not limited to definite NPs as in previous work (Poesio et al., 1997 (Poesio et al., , 2004 Lassalle and Denis, 2011) .",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_9",
  "x": "Also in ISNotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. We therefore choose ISNotes to evaluate our algorithm for bridging anaphora resolution. Our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning-based approach <cite>(Hou et al., 2013b)</cite> . We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_10",
  "x": "We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes. We apply our algorithm with small adaptations to select antecedents for bridging anaphors on these corpora. The moderate results demonstrate that embeddings bridging is a general word representation resource for bridging. (Parker et al., 2011; Napoles et al., 2012) .",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_11",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Following<cite> Hou et al. (2013b)</cite> 's experimental setup, we resolve bridging anaphors to entity antecedents. Entity information is based on the OntoNotes coreference annotation. We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\".",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_12",
  "x": "We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_13",
  "x": "We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_14",
  "x": "Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_15",
  "x": "For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_16",
  "x": "In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_17",
  "x": "**USING NP HEAD + MODIFIERS** We carried out experiments using the deterministic algorithm described in Section 4 together with different word embeddings. Again we do not add the suffix \" PP\" to the bridging anaphors for GloVe GigaWiki14 and GloVe Giga. Table 6 lists the best results of the two models for bridging anaphora resolution from<cite> Hou et al. (2013b)</cite> . pairwise model III is a pairwise mentionentity model based on various semantic, syntactic and lexical features.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_18",
  "x": "It models that semantically or syntactically related anaphors are likely to share the same antecedent and achieves an accuracy of 41.32% on the ISNotes corpus. The results for GloVe GigaWiki14 and GloVe Giga are similar on two settings (using NP head vs. using NP head + modifiers). For embeddings PP, the result on using NP head + modifiers (31.67%) is worse than the result on using NP head (33.03%). However, if we apply embeddings PP to a bridging anaphor's head and modifiers, and only apply embeddings PP to the head noun of an antecedent candidate, we get an due to the improved antecedent candidate selection strategy described in Section 4. acc models from<cite> Hou et al. (2013b)</cite> Table 6 : Results of using NP head plus modifications in different word representations for bridging anaphora resolution compared to the best results of two models from<cite> Hou et al. (2013b)</cite> .",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_19",
  "x": "This corresponds to our observations in the previous section that the representations for words without the suffix \" PP\" in embeddings PP are not as good as in embeddings bridging due to less training instances. Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in<cite> Hou et al. (2013b)</cite> . There is no significant difference between NP head + modifiers based on embeddings bridging and MLN model II (randomization test with p < 0.01). To gain an insight into the contribution of embeddings bridging on different relation types, we analyze the results of our method using embeddings bridging on three relation types: set-of, part-of, and other. The accuracies on these three relation types are 17.78%, 50.0%, and 39.16%, respectively.",
  "y": "similarities"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_20",
  "x": "**COMBINING NP HEAD + MODIFIERS WITH MLN II** For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> . Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems 8 for bridging anaphora resolution in ISNotes. It shows that combining our deterministic approach (NP Head + modifiers) with MLN II slightly improves the result compared to Hou (2018) .",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_21",
  "x": "**COMBINING NP HEAD + MODIFIERS WITH MLN II** For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> . Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems 8 for bridging anaphora resolution in ISNotes. It shows that combining our deterministic approach (NP Head + modifiers) with MLN II slightly improves the result compared to Hou (2018) .",
  "y": "background uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_22",
  "x": "Based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors. We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features. We also demonstrate that using embeddings bridging yields better results than using embeddings PP for bridging anaphora resolution. For the task of bridging anaphora resolution,<cite> Hou et al. (2013b)</cite> pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations. In this work we explore the context within NPs-that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_23",
  "x": "Based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors. We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features. We also demonstrate that using embeddings bridging yields better results than using embeddings PP for bridging anaphora resolution. For the task of bridging anaphora resolution,<cite> Hou et al. (2013b)</cite> pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations. In this work we explore the context within NPs-that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging.",
  "y": "future_work"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_0",
  "x": "**ABSTRACT** We study the problem of jointly embedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. <cite>Wang et al. (2014a)</cite> rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.",
  "y": "motivation"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_1",
  "x": "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of <cite>Wang et al. (2014a)</cite> , which is encouraging as we do not use any anchor information. ---------------------------------- **INTRODUCTION** Knowledge base embedding has attracted surging interest recently.",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_2",
  "x": "(2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of <cite>Wang et al. (2014a)</cite> solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_3",
  "x": "Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach <cite>(Wang et al., 2014a)</cite> . Results show that our approach consistently achieves better or comparable performance. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_4",
  "x": "Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; <cite>Wang et al., 2014a</cite>; Lin et al., 2015) . Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' \u2212 'Queen' \u2248 'Man' \u2212 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. <cite>Wang et al. (2014a)</cite> combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.",
  "y": "background uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_5",
  "x": "Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' \u2212 'Queen' \u2248 'Man' \u2212 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. <cite>Wang et al. (2014a)</cite> combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings convenience to tasks requiring computation between knowledge bases and text.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_6",
  "x": "We follow the jointly embedding framework of <cite>(Wang et al., 2014a)</cite> , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> . However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_7",
  "x": "The vocabulary of words is V. The union vocabulary of entities and words together is I = E \u222a V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\". We follow the jointly embedding framework of <cite>(Wang et al., 2014a)</cite> , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> . However, to make the content self-contained, we still need to briefly explain L K and L T .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_8",
  "x": "However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b \u2212 0.5 \u00b7 h + r \u2212 t 2 2 , b = 7 as suggested by <cite>Wang et al. (2014a)</cite> . Pr(r|h, t) and Pr(t|h, r) are defined in the same way. The loss function of knowledge model is then defined as",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_9",
  "x": "Then the loss function of text model is Alignment Model This part is different from <cite>Wang et al. (2014a)</cite> . For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e: Pr(w|e) = exp{z(e, w)} w\u2208V exp{z(e,w)} , where z(e, w) = b \u2212 0.5 \u00b7 e \u2212 w 2 2 .",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_10",
  "x": "We conduct experiments on the following tasks: link prediction , triplet classification (Socher et al., 2013) , relational fact extraction , and analogical reasoning (Mikolov et al., 2013b) . The last one evaluates quality of word embeddings. We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in <cite>(Wang et al., 2014a)</cite> respectively. \"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_11",
  "x": "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from ) is used as the knowledge base. For triplet classification, a large dataset provided by <cite>(Wang et al., 2014a )</cite> is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text corpus.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_12",
  "x": "For all tasks, Wikipedia articles are used as the text corpus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in <cite>(Wang et al., 2014a)</cite> , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases. Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r \u2212 t .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_13",
  "x": "Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; <cite>Wang et al., 2014a)</cite> . We follow the same protocol in <cite>(Wang et al., 2014a)</cite> . We train their models via our own implementation on our dataset. The results are in Table 2 . \"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\".",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_14",
  "x": "It is used in (Socher et al., 2013; Wang et al., 2014b; <cite>Wang et al., 2014a)</cite> . We follow the same protocol in <cite>(Wang et al., 2014a)</cite> . We train their models via our own implementation on our dataset. The results are in Table 2 . \"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\". The best configurations of the models are: k = 150, \u03b1 = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_15",
  "x": "<cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores.",
  "y": "uses motivation"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_16",
  "x": "<cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_17",
  "x": "We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores. The precision-recall curves are plot in Fig. (1) .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_18",
  "x": "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b) . We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of <cite>(Wang et al., 2014a)</cite> . For a true analogical pair like (\"France\", \"Paris\") and (\"China\", \"Beijing\"), we hide \"Beijing\" and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of \"China\" + \"Paris\" -\"France\". We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for \"Skip-gram\", \"Jointly(anchor)\", \"Jointly(name)\" and \"Jointly(desp)\".",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_19",
  "x": "From the table we observe that: (1) Both \"Jointly(anchor)\" and \"Jointly(desp)\" outperform \"Skip-gram\". (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_20",
  "x": "From the table we observe that: (1) Both \"Jointly(anchor)\" and \"Jointly(desp)\" outperform \"Skip-gram\". (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "differences"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_0",
  "x": "Some transition-based dependency parsers have deduction systems that use dynamic programming to enable exact inference in polynomial time and space (Huang and Sagae, 2010; Kuhlmann et al., 2011) . For non-projective parsing, though, the only tabularization of a transition-based parser is, to our knowledge, that of Cohen et al. (2011) . They define a deduction system for (an isomorphic variant of)<cite> Attardi's (2006)</cite> transition system, which covers a subset of non-projective trees. The exact inference algorithm runs in Opn 7 q time, where n denotes sentence length. In this paper, we show how Cohen et al.'s (2011) system can be modified to generate a new family of deduction systems with corresponding transition systems.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_4",
  "x": "---------------------------------- **ATTARDI'S (2006) SYSTEM** We now introduce the widely-used<cite> Attardi (2006)</cite> system, which includes transitions that create arcs between non-consecutive subtrees, thus allowing it to produce some non-projective trees. To simplify exposition, here we present Cohen et al.'s (2011) isomorphic version. The set of transitions consists of a shift transition (sh) and four reduce transitions (re).",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_5",
  "x": "A reduce transition re h,m creates a dependency arc between h (head) and m (modifier) and reduces m. For example, re s 0 ,s 1 rp\u03c3|s 1 |s 0 , \u03b2, Aqs \" p\u03c3|s 0 , \u03b2, A Y tps 0 , s 1 quq . Row 1 of Fig. 1 depicts the four Attardi reduces. The distance between h and m in a re h,m transition is called its degree. A system limited to degree-1 transitions can only parse projective sentences. As shown in Fig. 1 ,<cite> Attardi's (2006)</cite> system has two degree-2 transitions (re s 0 ,s 2 and re s 2 ,s 0 ) that allow it to cover 87.24% of the nonprojective trees in UD 2.1.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_6",
  "x": "More generally, an Attardi system of degree D adds re s 0 ,s D and re s D ,s 0 to the system of degree D\u00b41. ---------------------------------- **IMPROVING COVERAGE** A key observation is that a degree-D Attardi system does not contain all possible transitions of degree within D. Since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non-projective treebank trees <cite>(Attardi, 2006</cite>; G\u00f3mez-Rodr\u00edguez, 2016) , we hypothesize that adding some of these \"missing\" reduce transitions into the system's inventory should increase coverage. The challenge is to simultaneously maintain run-time guarantees, as there exists a known trade-off between coverage and complexity (G\u00f3mez-Rodr\u00edguez, 2016 Cohen et al. (2011) , rather than Opn 3\u00a83`1 q; and (ii) another has degree 2 but better runtime than Cohen et al.'s (2011) system.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_7",
  "x": "With adaptation of the \"hook trick\" described in Eisner and Satta (1999) , we can reduce the running time to Opn 7 q. ---------------------------------- **OUR NEW VARIANTS** In this section, we modify Cohen et al.'s (2011) set of reduce deduction rules to improve coverage or time complexity. Since each such deduction rule corresponds to a reduce transition, each revision to the deduction system yields a variant of<cite> Attardi's (2006)</cite> parser.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_8",
  "x": "7 We report the global coverage over the 76,084 non-projective sentences from all the training treebanks. One might assume that adding more degree-1 transitions wouldn't improve coverage of trees with non-crossing edges. On the other hand, since their addition doesn't affect the asymptotic run-time, we define ALLDEG1 to include all five degree-1 transitions from R into the<cite> Attardi (2006)</cite> system. Surprisingly, using ALLDEG1 improves non-projective coverage from 87.24% to 93.32%. Furthermore, recall that we argued above that,",
  "y": "uses"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_0",
  "x": "However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015;<cite> Limsopatham and Collier, 2016)</cite> . Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem. <cite>Limsopatham and Collier (2016)</cite> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT (Devlin et al., 2018) and Gated Recurrent Units (GRU) (Cho et al., 2014) with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings (Peters et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_1",
  "x": "However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015;<cite> Limsopatham and Collier, 2016)</cite> . Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem. <cite>Limsopatham and Collier (2016)</cite> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT (Devlin et al., 2018) and Gated Recurrent Units (GRU) (Cho et al., 2014) with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings (Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_2",
  "x": "Huang and Lu (2015) survey the work done in the organization of biomedical NLP (BioNLP) challenge evaluations up to 2014. These tasks are devoted to the normalization of (1)<cite> (Limsopatham and Collier, 2016)</cite> 73.39 ----CNN<cite> (Limsopatham and Collier, 2016)</cite> 81.41 ----RNN<cite> (Limsopatham and Collier, 2016)</cite> 79.98 ----Attentional Char-CNN (Niu et al., 2018) 84.65 ----Hierarchical Char-CNN (Han et al., 2017) - Table 2 : The performance of the proposed models and the state-of-the-art methods in terms of accuracy. (4) diseases from clinical reports (ShARe/CLEF eHealth 2013; SemEval 2014 task 7). Similarly, the CLEF Health 2016 and 2017 labs addressed the problem of ICD coding of freeform death certificates (without specified entity mentions). Traditionally, linguistic approaches based on dictionaries, association measures, and syntactic properties have been used to map texts to a concept from a controlled vocabulary (Aronson, 2001; Van Mulligen et al., 2016; Mottin et al., 2016; Ghiasvand and Kate, 2014; Tang et al., 2014) .",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_0",
  "x": "We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. ---------------------------------- **INTRODUCTION** Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016) . This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013) , Flickr30K <cite>(Young et al., 2014)</cite> , and MS COCO (Chen et al., 2015) .",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_1",
  "x": "However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternative-description text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data. Multi30K is an extension of the Flickr30K dataset <cite>(Young et al., 2014)</cite> with 31,014 German translations of English descriptions and 155,070 independently collected German descriptions. The translations were collected from professionally contracted translators, whereas the descriptions were collected from untrained crowdworkers. The key difference between these corpora is the relationship between the sentences in different languages.",
  "y": "extends"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_2",
  "x": "The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> . Each image is paired with five English descriptions, which were collected from Amazon Mechanical Turk 1 . The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_3",
  "x": "The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> . Each image is paired with five English descriptions, which were collected from Amazon Mechanical Turk 1 . The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences. ----------------------------------",
  "y": "background extends"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_4",
  "x": "We introduced Multi30K: a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. Our dataset is an extension of the popular Flickr30K dataset with descriptions and professional translations in German. The descriptions were collected from a crowdsourcing platform, while the translations were collected from professionally contracted translators. These differences are deliberate and part of the larger scope of studying multilingual multimodal data in different contexts. The descriptions were collected as similarly as possible to the original Flickr30K dataset by translating the instructions used by<cite> Young et al. (2014)</cite> into German.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_0",
  "x": "Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis. ---------------------------------- **INTRODUCTION** Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_1",
  "x": "**INTRODUCTION** Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> . In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs. The search logs consist of user queries and relevant search results retrieved by a search engine.",
  "y": "motivation"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_2",
  "x": "We refer to this evaluation set as MS-251 in our experiments. We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags. We then examined all the instances where the annotators disagreed, and corrected the discrepancy.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_3",
  "x": "We refer to this evaluation set as MS-251 in our experiments. We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags. We then examined all the instances where the annotators disagreed, and corrected the discrepancy.",
  "y": "differences"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_4",
  "x": "Some other trends become appar- ent in Table 2 . Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_5",
  "x": "Our best system achieves a 21.2% relative reduction in error on their annotations. Some other trends become appar- ent in Table 2 . Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_6",
  "x": "Table 3 : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline. 5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. Unfortunately their data is not available so we cannot use it to compare to their results. R\u00fcd et al. (2011) create features based on search engine results, that they use in an NER system applied to queries. They report report significant improvements when incorporating features from the snippets.",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_7",
  "x": "Table 3 : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline. 5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. Unfortunately their data is not available so we cannot use it to compare to their results. R\u00fcd et al. (2011) create features based on search engine results, that they use in an NER system applied to queries. They report report significant improvements when incorporating features from the snippets.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_0",
  "x": [
   "Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. ---------------------------------- **INTRODUCTION** Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001) , syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002) ."
  ],
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_1",
  "x": [
   "Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001) , syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002) . Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target languages, the projected parses are not always fully connected and can have edges missing (Hwa et al., 2005; Ganchev et al., 2009 )."
  ],
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_2",
  "x": [
   "This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003) , the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006) . Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009) . But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009 ). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees."
  ],
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_3",
  "x": "Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009) . But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009 ). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In Spanish and Bulgarian projected data extracted by<cite> Ganchev et al. (2009)</cite> In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_4",
  "x": [
   "**RELATED WORK** Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin's parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation."
  ],
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_5",
  "x": "The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their parser,<cite> Ganchev et al. (2009)</cite> and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in<cite> (Ganchev et al., 2009</cite> ) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse.",
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_6",
  "x": "The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in<cite> (Ganchev et al., 2009</cite> ) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010 ) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_7",
  "x": "These three cases are identified in the source tree and appropriate transformations are made to the source parse itself before projecting the relations using word alignments. ---------------------------------- **EXPERIMENTS** We carried out all our experiments on parallel corpora belonging to English-Hindi, EnglishBulgarian and English-Spanish language pairs. While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_8",
  "x": "**EXPERIMENTS** We carried out all our experiments on parallel corpora belonging to English-Hindi, EnglishBulgarian and English-Spanish language pairs. While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> . The datasets of Bulgarian and Spanish that contributed to the best accuracies for<cite> Ganchev et al. (2009)</cite> were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish). The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively.",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_9",
  "x": "For Bulgarian and Spanish, we used the same test data that was used in the work of<cite> Ganchev et al. (2009)</cite> . These test datasets had sentences from the training section of the CoNLL Shared Task (Nivre et al., 2007) that had lengths less than or equal to 10. All the test datasets have gold POS tags. A baseline parser was built to compare learning from partial parses with learning from fully connected parses. Full parses are constructed from partial parses in the projected data by randomly assigning parents to unconnected parents, similar to the work in (Hwa et al., 2005) .",
  "y": "background uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_0",
  "x": "Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection <cite>(Felbo et al., 2017)</cite> . The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_1",
  "x": "Our contribution in this paper is twofold. First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages. Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier. We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji <cite>(Felbo et al., 2017)</cite> , which is most noticeable in the case of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes.",
  "y": "differences"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_2",
  "x": "**METHODOLOGY** Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_3",
  "x": "In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_4",
  "x": "In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_5",
  "x": "We also show results from additional experiments in which the label space ranged from 20 to 200 emojis. These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between October 2015 and May 2018. Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 <cite>(Felbo et al., 2017)</cite> . Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture.",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_0",
  "x": "Relatively to dictionary-based segmentation (we call it \"DS\" for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test. Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> . Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> .",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_1",
  "x": "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> . Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> . The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail). We provide a more detailed evaluation metric to analyze these two methods, including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_2",
  "x": "The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail). We provide a more detailed evaluation metric to analyze these two methods, including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test. The precision and F measure are existing metrics and the definitions of them are clear. Here we just employ them to evaluate segmentation results.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_3",
  "x": "The precision and F measure are existing metrics and the definitions of them are clear. Here we just employ them to evaluate segmentation results. Furthermore, our error analysis on the results of combination reveals that confidence measure in <cite>(Zhang et al., 2006a)</cite> has a representation flaw and we propose an EIV tag method to revise it. Finally, we give an empirical comparison between existing pure CT method and combination, which shows that pure CT method can produce state-of-the-art results on both IV word and overall segmentation. <cite>(Zhang et al., 2006a)</cite> The rest of this paper is organized as follows.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_6",
  "x": "However, the exponential time and space of the length of the input sentence are needed for such a search and it is always intractable in practice. Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice. Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result. Here, the term \"dictionary-based\" is exactly the method implemented in <cite>(Zhang et al., 2006a)</cite> , it does not mean the generative language model in general. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_7",
  "x": "Under CT scheme, each character in one sentence is labeled as \"B\" if it is the beginning of a word, \"O\" tag means the current character is a single-character word, other character is labeled as \"I\". For example, \"\u5168\u4e2d\u56fd (whole China)\" is labeled as \" In <cite>(Zhang et al., 2006a)</cite> , the above CT method is developed as subword-based tagging. First, the most frequent multi-character words and all single characters in training corpus are collected as subwords. During the subwordbased tagging, a subword is viewed as an unit instead of several separate characters and given only one tag.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_10",
  "x": "We will see later in this section, by this evaluation, some facts covered by the bakeoff evaluation can be illustrated by our new evaluation metric. Here, we repeat two experiments described in <cite>(Zhang et al., 2006a)</cite> , namely dictionary-based approach and subword-based tagging. For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 . We present all the segmentation results in Table  6 to see the strength and weakness of each method conveniently. Based on IV and OOV recall as we show in Table 1 , <cite>Zhang argues</cite> that the DS performs better on IV word identification while CT performs better on OOV words. But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV.",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_11",
  "x": "Here, we repeat two experiments described in <cite>(Zhang et al., 2006a)</cite> , namely dictionary-based approach and subword-based tagging. For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 . We present all the segmentation results in Table  6 to see the strength and weakness of each method conveniently. Based on IV and OOV recall as we show in Table 1 , <cite>Zhang argues</cite> that the DS performs better on IV word identification while CT performs better on OOV words. But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV. The reason for low IV precision of DS is that many OOV words are segmented into two IV words by DS.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_12",
  "x": "In this way, the dictionary-based approach and the statistical model are combined. We choose the confidence measure to study because it is straight-forward. We show in this section that there is a representation flaw in the formula of confidence measure in <cite>(Zhang et al., 2006a )</cite>. And we propose an \"EIV\" tag method to solve this problem. Our experiments show that confidence measure with EIV tag outperforms CT and DS alone.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_13",
  "x": "Confidence Measure (CM) means to seek an optimal tradeoff between performance on IV and OOV words. The basic idea of CM comes from the belief that CT performs better on OOV words while DS performs better on IV words. When both results of CT and DS are available, the CM can be calculated according to the following formula in <cite>(Zhang et al., 2006a)</cite> : Here, w is a subword, iob t is \"IOB\" tag given by CT and w t is \"IOB\" tag generated by DS. In the first term of the right hand side of the formula,",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_14",
  "x": "In the first term of the right hand side of the formula, is the marginal probability of iob t (we call this marginal probability \"MP\" for short will be kept, otherwise it will be replaced with w t . Thus, the CM ultimately is the marginal probability of the \"IOB\" tag (MP). In the experiment of this paper, MP is used as CM because it is equivalent to <cite>Zhang\"s CM</cite> but more convenient to express. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_15",
  "x": "---------------------------------- **EXPERIMENTS AND ERROR ANALYSIS ABOUT COMBINATION** We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula. Furthermore, we propose an EIV tag method to make CM yield a better result. In this paper, \uf061 = 0.8 and t = 0.7 (Parameters in two papers, <cite>Zhang et al. 2006a</cite> and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold.",
  "y": "motivation uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_16",
  "x": "**EXPERIMENTS AND ERROR ANALYSIS ABOUT COMBINATION** We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula. Furthermore, we propose an EIV tag method to make CM yield a better result. In this paper, \uf061 = 0.8 and t = 0.7 (Parameters in two papers, <cite>Zhang et al. 2006a</cite> and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold. Here, in Table 4 , we provide some statistics on the results of CT when MP is less than 0.875.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_17",
  "x": "Therefore, the CM formula seems somewhat unreasonable. The error analysis about how many original errors are eliminated and how many new errors are introduced by CM is provided in Table 5 (the columns about CM). Table 5 illustrates that, after combining the two results, most original errors on IV words are corrected because DS can achieve higher IV recall as described in <cite>Zhang\"s paper</cite>. But on OOV part, more new errors are introduced by CM and these new errors decrease the precision of the IV words. For example, the OOV words \"\u8b66\u536b\u961f\u5458 (guard member)\" and \" \u8bbe\u8ba1\u8d39 (design fee)\" is recognized correctly by CT but with low CM. In the combining procedure, these words are wrongly split as IV errors: \"\u8b66\u536b (guard) \u961f\u5458 (member)\" and \"\u8bbe\u8ba1 (design) \u8d39 (fee)\".",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_18",
  "x": "We can see from the Table 5 columns about EIV, there are more errors eliminated than the new errors introduced after EIV condition added into CM and most CT tags of subwords contained in OOV words maintained unchanged as we supposed. And then, our results (in Table  6 lines about EIV) are comparable with that in <cite>Zhang\"s paper</cite>. Thus, there may be some similar strategies in <cite>Zhang\"s CM</cite> too but not presented in <cite>Zhang\"s paper</cite>. ---------------------------------- **DISCUSSION AND RELATED WORKS** Although the method such as confidence measure can be helpful at some circumstance, our experiment shows that pure character-based tagging (pure CT) can work well with reasonable features and tag set.",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_20",
  "x": "In Table 6 , the lines about \"pure CT\" provide the results generated by pure CT with 6-tag set. We can see from the Table 6 this pure CT approach achieves the state-of-the-art results on all the corpora. On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result. Even on IV word, this pure CT approach outperforms <cite>Zhang\"s CT method</cite> and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too. Moreover, this character-based tagging approach is more clear and simple than the confidence measure method.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_21",
  "x": "Table 6 Results of different approach used in our experiments (White background lines are the results we repeat <cite>Zhang\"s methods</cite> and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this paper, we first provided a detailed evaluation metric, which provides the necessary information to judge the performance of each method on IV and OOV word identification. Second, by this evaluation metric, we show that characterbased tagging outperforms dictionary-based segmentation not only on OOV words but also on IV words within Bakeoff closed tests.",
  "y": "uses differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_22",
  "x": "---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this paper, we first provided a detailed evaluation metric, which provides the necessary information to judge the performance of each method on IV and OOV word identification. Second, by this evaluation metric, we show that characterbased tagging outperforms dictionary-based segmentation not only on OOV words but also on IV words within Bakeoff closed tests. Furthermore, our experiments show that confidence measure in <cite>Zhang\"s paper</cite> has a representation flaw and we propose an EIV tag method to revise the combination.",
  "y": "motivation"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_0",
  "x": "---------------------------------- **** later works proposed partially-or purely-convolutional CTC models [8] [9] [10] [11] and convolution-heavy encoder-decoder models [16] for ASR. However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_1",
  "x": "Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC).",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_2",
  "x": "Wide contexts also enable incorporation of noise/speaker contexts, as [27] suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",
  "y": "similarities uses"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_3",
  "x": "Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors.",
  "y": "similarities uses"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_4",
  "x": "**FORMULATION** Let H \u2208 R T \u00d7d h denote a sublayer's input. The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_5",
  "x": "For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give where \u03c3 is row-wise softmax. Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_6",
  "x": "Note that CTC will still require U \u2264 T /k, however. ---------------------------------- **POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_7",
  "x": "1 We downsample by a factor of k = 3 (this also gave an ideal T /k \u2248 dh for our data; see Table 1 ). We perform Nesterov-accelerated gradient descent on batches of 20 utterances. As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> . Let n denote the global step number of the batch (across epochs); the learning rate is given by 1 Rescaling so that these differences also have var.",
  "y": "extends differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_0",
  "x": "Recently, deep neural network (DNN) has shown to provide good results for modeling acoustic and textual information for emotion identification. In [5] , textual and acoustic information of the utterance are used by a DNN to obtain hidden feature representations for both the modality. These features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker. Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_1",
  "x": "Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework. The proposed attention mechanism is trained to exploit both textual and acoustic information in tandem. We refer to this attention method as the multi-hop.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_2",
  "x": "Recently,<cite> [7,</cite> 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy. However, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording. ---------------------------------- **MODEL** This section describes the methodologies that are applied to the speech emotion recognition task.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_3",
  "x": "**BIDIRECTIONAL RECURRENT ENCODER** Motivated by the architecture used in<cite> [7,</cite> 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal. To model the sequential nature of the speech signal, we use a bidirectional recurrent encoder (BRE) as shown in the Figure 1 (a). We also added a residual connection to the model for promoting convergence during training [20] . A sequence of feature vectors is fed as input to the BRE, which leads to the formation of hidden states of the model as given by the following equation:",
  "y": "motivation"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_4",
  "x": "A sequence of feature vectors is fed as input to the BRE, which leads to the formation of hidden states of the model as given by the following equation: where f \u03b8 , f \u03b8 are the forward and backward long short-term memory (LSTM) with weight parameter \u03b8, ht represents the hidden state at t-th time step, and xt represents the t-th MFCC features in audio signal. The hidden representations ( \u2212 \u2192 h t, \u2190 \u2212 h t) from forward/backward LSTMs are concatenated for produce the feature, ot. To follow previous research <cite>[7]</cite> , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t . Finally, an emotion class is predicted from the acoustic signal by applying a softmax function to the final hidden representation at the last time step, o A last .",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_5",
  "x": "**PROPOSED MULTI-HOP ATTENTION** We propose a novel multi-hop attention method to predict the importance of audio and text, referred to multi-hop attention (MHA). Figure 1 shows the architecture of the proposed MHA model. Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa).",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_6",
  "x": "Figure 1 shows the architecture of the proposed MHA model. Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa). First, the acoustic and textual data are encoded with the audio-BRE and text-BRE, respectively, using equation (1). We then consider the final hidden representation of audio-BRE, o A last , as a context vector and apply attention method to the textual sequence, o T t .",
  "y": "differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_7",
  "x": "**DATASET AND EXPERIMENTAL SETUP** To train and evaluate our model, we use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) [8] dataset, which includes five sessions of utterances between two speakers (one male and one female). Total 10 unique speakers participated in this work. For consistent comparison with previous works<cite> [7,</cite> 18] , all utterances labeled \"excitement\" are merged with those labeled \"happiness\". We assign single categorical emotion to the utterance with majority of annotators agreed on the emotion labels.",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_8",
  "x": "In the training process, we perform 10-fold cross-validation where each 8, 1, 1 folds are used for the train set, development set, and test set, respectively. ---------------------------------- **FEATURE EXTRACTION AND IMPLEMENTATION DETAILS** As this research is extended work from previous research <cite>[7]</cite> , we use the same feature extraction method as done in our previous work. After extracting 40-dimensional Mel-frequency cepstral coefficients (MFCC) feature (frame size is set to 25 ms at a rate of 10 ms with the Hamming window) using Kaldi [22] , we concatenate it with its first, second order derivates, making the feature dimension to 120.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_9",
  "x": "---------------------------------- **PERFORMANCE EVALUATION** To measure the performance of systems, we report the weighted accuracy (WA) and unweighted accuracy (UA) averaging over the 10-fold cross-validation experiments. We use the same dataset and features as other researchers<cite> [7,</cite> 18] . Table 1 presents performances of proposed approaches for recognizing speech emotion in comparison with various models.",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_10",
  "x": "In audio-BRE ( Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of<cite> [7,</cite> 25] . The text-BRE shows improvement in classifying most of the labels in Fig. 2(b) . In particular, angry and happy classes are correctly classified by 32% (57.14 to 75.41) and 63% (40.21 to 65.56) relative in accuracy with respect to audio-BRE, receptively. However, it incorrectly predicted instances of the happy class as sad class in 10% of the time, even though these emotional states are opposites of one another. The MHA-2 (our best system, Fig. 2(c) ) compensates for the weaknesses of the single modality models and benefits from their strengths.",
  "y": "similarities"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_0",
  "x": "Many researches have been conducted to involve AI into poem generation [Zhang and Lapata, 2014; Cheng et al., 2018] , creation of classical or pop music<cite> [Manzelli et al., 2018</cite>; Hadjeres et al., 2017] and automatic images generation [van den Oord et al., 2016; Yan et al., 2016; Xu et al., 2018] . Whereas there are few researches exploring the possibility of artificial imagination for artwork creation. To generate an imaginative and creative Mind Map, the key problem is automatic topic expansion. There are several challenges: first of all, language is an informative system. Thus many linguistic features should be considered in words and relation extension.",
  "y": "background"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_1",
  "x": "We also performed lexicon and language model adaption to enhance the recognition of words in art domain. After that, we developed a keyword extraction function to extract meaningful words from the converted text for further expansion. We adopted open-source software jieba 2 for Chinese and Stanford parser<cite> [Toutanova and Manning, 2000]</cite> for English POS tagging. Only informative words such as noun, verb and adjective words are kept and TFIDF weights are calculated for further filtering. The output of this module is the keywords.",
  "y": "uses"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_2",
  "x": "**TOPIC EXPANSION** As imagination is the soul for artistic Mind Map, Mappa Mundi employs several features to increase information variety<cite> [Liu et al., 2019]</cite> during topic expansion. It firstly uses word embeddings to find candidates based on semantic similarity <cite>[Mikolov et al., 2013</cite>;<cite> Pennington et al., 2014</cite>;<cite> Peters et al., 2018]</cite> . To enrich linguistic information of expansions, it also takes the morphological and phonological features into account. Words sharing similar characters or morphemes or phonetic syllables are selected as candidates.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_0",
  "x": "Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b) 's skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch * The first two authors contributed equally. 1 The code used to run the experiments is available at https://github.com/sebastianruder/ latent-variable-vecmap. (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008) . However, like more recent approaches <cite>(Artetxe et al., 2017)</cite> , our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_1",
  "x": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison.",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_2",
  "x": "It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_3",
  "x": "It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_4",
  "x": "4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent. Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in \u00a76. In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_5",
  "x": "We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in \u00a76. In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in \u00a76, giving them practical utility. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_6",
  "x": "Next, we will describe the M-step. Given an optimal matching m computed in \u00a74.1, we search for a matrix \u2126 \u2208 R d\u00d7d . We additionally enforce the constraint that \u2126 is a real orthogonal matrix, i.e., \u2126 \u2126 = I. Previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> found that the orthogonality constraint leads to noticeable improvements. Our M-step optimizes two objectives independently. First, making use of the result in equation (6), we optimize the following:",
  "y": "background uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_7",
  "x": "Optimizing equation (9) can also been done in closed form; the point which minimizes distance to the data points (thereby maximizing the log-probability) is the centroid: \u00b5 = 1 /|utrg| \u00b7 i\u2208utrg t i . ---------------------------------- **REINTERPRETATION OF ARTETXE ET AL. (2017) AS A LATENT-VARIABLE MODEL** The self-training method of<cite> Artetxe et al. (2017)</cite> , our strongest baseline in \u00a76, may also be interpreted as a latent-variable model in the spirit of our exposition in \u00a73. Indeed, we only need to change the edge-set prior p(m) to allow for edge sets other than those that are matchings.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_8",
  "x": "Artetxe et al. (2017) , on the other hand, allow for one-to-many alignments. 2011; Mena et al., 2018 ) may have been a computationally more effective manner to deal with the latent matchings. We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM. To formalize<cite> Artetxe et al. (2017)</cite> 's contribution as a latent-variable model, we lay down some more notation. Let A = {1, . . . , n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_9",
  "x": "2011; Mena et al., 2018 ) may have been a computationally more effective manner to deal with the latent matchings. We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM. To formalize<cite> Artetxe et al. (2017)</cite> 's contribution as a latent-variable model, we lay down some more notation. Let A = {1, . . . , n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment. The set A is to be interpreted as the set of all one-to-many alignments a on the bipartite vertex set V = V trg \u222a V src such that a i = j means the i th vertex in V trg is aligned to the j th vertex in V src .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_10",
  "x": "thus, we can simply find a component-wise as follows: Artetxe et al. (2017) 's M-step The M-step remains unchanged from the exposition in \u00a73 with the exception that we fit \u2126 given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_11",
  "x": "Artetxe et al. (2017) 's M-step The M-step remains unchanged from the exposition in \u00a73 with the exception that we fit \u2126 given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ---------------------------------- **EXPERIMENTS**",
  "y": "extends differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_12",
  "x": "Datasets For bilingual dictionary induction, we use the English-Italian dataset by and the English-German and English-Finnish datasets by<cite> Artetxe et al. (2017)</cite> . For cross-lingual word similarity, we use the RG-65 and WordSim-353 cross-lingual datasets for English-German and the WordSim-353 cross-lingual dataset for EnglishItalian by Camacho-Collados et al. (2015) . ---------------------------------- **MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl).",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_13",
  "x": "**MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 10 In line with , we additionally use a dictionary of identically spelled strings in both vocabularies. Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_14",
  "x": "**MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 10 In line with , we additionally use a dictionary of identically spelled strings in both vocabularies. Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_15",
  "x": "Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations. Unless stated otherwise, we induce a dictionary of 200,000 source and 200,000 target words as in previous work (Mikolov et al., 2013c; Artetxe et al., 2016) . For optimal 1:1 alignment, we have observed the best results by keeping the top k = 3 most similar target words. If using a rank constraint, we restrict the matching in the Estep to the top 40,000 words in both languages. 11 Finding an optimal alignment on the 200,000 \u00d7 200,000 graph takes about 25 minutes on CPU; 12 with a rank constraint, matching takes around three minutes.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_16",
  "x": "If using a rank constraint, we restrict the matching in the Estep to the top 40,000 words in both languages. 11 Finding an optimal alignment on the 200,000 \u00d7 200,000 graph takes about 25 minutes on CPU; 12 with a rank constraint, matching takes around three minutes. Baselines We compare our approach with and without the rank constraint to the original bilingual mapping approach by Mikolov et al. (2013c) . In addition, we compare with Zhang et al. (2016) and Xing et al. (2015) who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively. Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_17",
  "x": "Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> . Both Mikolov et al. (2013c) and<cite> Artetxe et al. (2017)</cite> are special cases of our famework and comparisons to these approaches thus act as an ablation study. Specifically, Mikolov et al. (2013c) does not employ orthogonal Procrustes, but rather allows the learned matrix \u2126 to range freely. Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets. 13",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_18",
  "x": "Specifically, Mikolov et al. (2013c) does not employ orthogonal Procrustes, but rather allows the learned matrix \u2126 to range freely. Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets. 13 ---------------------------------- **RESULTS**",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_19",
  "x": "The rank constraint gen- 11 We validated both values with identical strings using the 5,000 word lexicon as validation set on English-Italian. 12 Training takes a similar amount of time as <cite>(Artetxe et al., 2017)</cite> due to faster convergence. 13 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 14 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia). erally performs similarly or boosts performance, while being about 8 times faster.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_20",
  "x": "All approaches do better with identical strings compared to numerals, indicating that the former may be generally suitable as a default weakly-supervised seed lexicon. ---------------------------------- **ENGLISH-ITALIAN** On cross-lingual word similarity, our approach yields the best performance on WordSim-353 and RG-65 for English-German and is only outperformed by<cite> Artetxe et al. (2017)</cite> on English-Italian Wordsim-353. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_21",
  "x": "Following , we define the hubness N k (y) at k of a target word y as follows: where Q is a set of query source language words and NN k (x, G) denotes the k nearest neighbors of x in the graph G. 16 In accordance with Lazaridou et al. (2015), we set k = 20 and use the words in the evaluation dictionary as query terms. We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness. Interestingly, compared to , hubs seem to occur less often and are more meaningful in current cross-lingual word embedding models.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_22",
  "x": "Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following , we define the hubness N k (y) at k of a target word y as follows: where Q is a set of query source language words and NN k (x, G) denotes the k nearest neighbors of x in the graph G. 16 In accordance with Lazaridou et al. (2015), we set k = 20 and use the words in the evaluation dictionary as query terms. We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness.",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_23",
  "x": "Interestingly, compared to , hubs seem to occur less often and are more meaningful in current cross-lingual word embedding models. 18 For instance, the neighbors of 'gleichg\u00fcltigkeit' all relate to indifference and words appearing close to 'luis' or 'jorge' are Spanish names. This suggests that the prior might also be beneficial in other ways, e.g. by enforcing more reliable translation pairs for subsequent iterations. Low-resource languages Cross-lingual embeddings are particularly promising for low-resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks (besides the English-Finnish language pair). We perform experiments with our method with and without a rank constraint and<cite> Artetxe et al. (2017)</cite> for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_24",
  "x": "We perform experiments with our method with and without a rank constraint and<cite> Artetxe et al. (2017)</cite> for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to . For all languages, we use fastText embeddings (Bojanowski et al., 2017) trained on Wikipedia, the evaluation dictionaries provided by Conneau et al. (2018) , and a seed lexicon based on identical strings to reflect a realistic use case. We note that English does not share scripts with Bengali and Hindi, making this even more challenging. We show results in Table 4 . Surprisingly, the method by<cite> Artetxe et al. (2017)</cite> a similar self-learning method that uses word embeddings, with an implicit one-to-many alignment based on nearest neighbor queries.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_25",
  "x": "proposed a max-marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax (Smi, 2017) or scaling the similarity values (Conneau et al., 2018) . ---------------------------------- **CONCLUSION** We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of<cite> Artetxe et al. (2017)</cite> . Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rather than generative, approach inspired by Irvine and CallisonBurch (2013) .",
  "y": "extends"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_0",
  "x": "To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic \"take what you can get\" viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_1",
  "x": "Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic \"take what you can get\" viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers.",
  "y": "motivation"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_2",
  "x": "We show that this simple approach to instance selection offers substantial improvements: across all languages, we learn better taggers with significantly fewer training instances. Dictionaries. Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training. We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., e src is a lexicon src embedded into an l-dimensional space.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_3",
  "x": "Dictionaries. Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training. We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., e src is a lexicon src embedded into an l-dimensional space. We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_4",
  "x": "We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from <cite>Li et al. (2012)</cite> and . The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph).",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_5",
  "x": "We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from <cite>Li et al. (2012)</cite> and . The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1 , first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_6",
  "x": "Baselines. We compare to the following weaklysupervised POS taggers: -AGIC: Multi-source annotation projection with Bible parallel data by Agi\u0107 et al. (2015) . -DAS: The label propagation approach by Das and Petrov (2011) over Europarl data. -GARRETTE: The approach by that works with projections, dictionaries, and unlabeled target text. -LI: Wiktionary supervision<cite> (Li et al., 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_7",
  "x": "On the test sets (Table 4 , right) DSDS reaches 87.2 over 8 test languages intersecting <cite>Li et al. (2012)</cite> and Agi\u0107 et al. (2016) . It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011) , compared to their 83.4. This shows that our novel \"soft\" inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps. We did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_8",
  "x": "In Table 1 we directly report the accuracies from the original contributions by DAS, LI, GARRETTE, and AGIC over the same test data. We additionally attempted to reach the scores of LI by running their tagger over the Table 1 data setup. The results are depicted in Figure 4 as mean accuracies over EM iterations until convergence. We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest. This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy.",
  "y": "differences"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_9",
  "x": "This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores of <cite>Li et al. (2012)</cite> , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries. Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC. Similar applies to T\u00e4ckstr\u00f6m et al. (2013) , as they use 1-5M near-perfect parallel sentences. Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011) , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_10",
  "x": "---------------------------------- **RELATED WORK** Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) . Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_11",
  "x": "Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) . Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case.",
  "y": "differences extends"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_0",
  "x": "****RETOUCHDOWN: ADDING TOUCHDOWN TO STREETLEARN AS A SHAREABLE RESOURCE FOR LANGUAGE GROUNDING TASKS IN STREET VIEW**** **ABSTRACT** The Touchdown dataset <cite>(Chen et al., 2019)</cite> provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_2",
  "x": "Until recently, the most commonly studied domains were map-based (Thompson et al., 1993) or game-like (Macmahon et al., 2006; Misra et al., 2017 Misra et al., , 2018 Hermann et al., 2017; Hill et al., 2017) . These environments enabled substantial progress, but the complexity and diversity of the visual input they provide is limited. This greatly simplifies both the language and vision challenges. To address this, recent tasks based on simulated environments include photo-realistic visual input, such as Room-to-Room (R2R; Anderson et al., 2018) , Talk-the-Walk (de Vries et al., 2018) and Touchdown <cite>(Chen et al., 2019)</cite> , all of which rely on panorama photos. A major challenge of creating simulations that use realworld photographs is they at times capture bystanders and their property.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_3",
  "x": "We re-implement the best-reported models on the navigation and spatial description resolution tasks from<cite> Chen et al. (2019)</cite> to compare performance with our data release to the original Touchdown paper. The key difference between the two settings is that our released panoramas contain additional blurred patches (Section 2). Another minor difference is that we use a word-piece tokenizer (Devlin et al., 2019) instead of a full-word tokenizer. Spatial Description Resolution. SDR results are given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_4",
  "x": "Spatial Description Resolution. SDR results are given in Table 1 . Following<cite> Chen et al. (2019)</cite> , we report mean distance error and accuracy with different thresholds (40px, 80px, and 120px), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance. Our Retouchdown reimplementation of LINGUNET obtains better performance on the accuracy measures, but worse performance on mean distance error. To check whether this is a consequence of the blur-ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in Table 1.",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_6",
  "x": "\u2022 Success weighted by Edit Distance (SED): normalized graph edit distance between the agent path and true path, with points only awarded for successful paths. \u2022 Normalized Dynamic Time Warping (nDTW): a minimized cumulative distance between the agent path and true path, normalized by path length. \u2022 Success weighted Dynamic Time Warping (SDTW): nDTW, with points awarded only for successful paths. TC, SPD, and SED are defined in<cite> Chen et al. (2019)</cite> and nDTW and SDTW are defined in Ilharco et al. (2019) . VLN results are given in Table 2 .",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_0",
  "x": "Only Hamilton et al. (2016) used SVD PPMI in some of their very recent experiments and showed it to be adequate for exploring historical semantics. The Google Books Ngram corpus (GBN; Michel et al. (2011 ), Lin et al. (2012 ) is used in most of the studies we already mentioned, including our current study and its predecessor <cite>(Hellrich and Hahn, 2016a)</cite> . It contains about 6% of all books published between 1500 and 2009 in the form of n-grams (up to pentagrams), together with their frequency for each year. This corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection (Pechenick et al., 2015) . GBN is multilingual, with its English part being subdivided into regional segments (British, US) and topic categories (general language and fiction texts).",
  "y": "background uses"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_1",
  "x": "Neural word embeddings (Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1). Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only <cite>(Hellrich and Hahn, 2016a)</cite> , and also influenced by the design decisions of Kim et al. (2014) and Kulkarni et al. (2015) which were the first to use word embeddings in diachronic studies. Our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning (and, consequently, meaning shifts).",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_2",
  "x": "There are two strategies for managing the huge number of potential contexts a word can appear in. Skip-gram hierarchical softmax (SGHS) uses a binary tree to more efficiently represent the vocabulary, whereas skip-gram negative sampling (SGNS) updates only a limited number of word vectors during each training step. SGNS is preferred in general, yet SGHS showed slight benefits in some reliability scenarios in our prior investigations <cite>(Hellrich and Hahn, 2016a)</cite> . There are two sources of randomness involved in the training of neural word embeddings: First, the random initialization of all word vectors before any examples are processed. Second, the order in which these examples are processed.",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_3",
  "x": "We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples <cite>(Hellrich and Hahn, 2016a)</cite> . Word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics (Kenter et al., 2015) or compare neighborhoods in embedding space for preselected words (Jo, 2016) . Besides temporal variations, word embeddings can also used to analyze geographic ones, e.g., the distinction between US American and British English variants (Kulkarni et al., 2016) . Most of these studies were performed with algorithms from the word2vec family, respectively GloVe in Jo (2016), and are thus likely to be affected by the same systematic reliability problems on which we focus here. Only Hamilton et al. (2016) used SVD PPMI in some of their very recent experiments and showed it to be adequate for exploring historical semantics.",
  "y": "background motivation"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_5",
  "x": "We processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations <cite>(Hellrich and Hahn, 2016a)</cite> . We tested both SGNS with 5 noise words and SGHS training strategies and trained for 10 iterations, saving the resulting embeddings after each epoch. During each epoch the learning rate was decreased from 0.025 to 0.0001. The averaged cosine values between word embeddings before and after an epoch are used as a convergence measure c (Kim et al., 2014; Kulkarni et al., 2015) . It is defined for a vocabulary with n words and a matrix W containing word embedding vectors (normalized to length 1) for words i from training epochs e and e-1:",
  "y": "motivation"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_6",
  "x": "Influence of Neighborhood Size. Reliability at different top-n cut-offs is very similar for all languages and time spans under scrutiny, confirming previous observations in<cite> Hellrich and Hahn (2016a)</cite> and strengthening the suggestion to use only top-1 reliability for evaluation. Figure 2 illustrates this phenomenon with an SGNS trained on 1900-1904 English Fiction data. We assume this to be connected with the general decrease in word2vec embedding utility for high values of n already observed by Schnabel et al. (2015) . Influence of Word Frequency.",
  "y": "similarities"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_0",
  "x": "However, the generated stories are still lacking in common sense reasoning, e.g., they often contain sentences deprived of world knowledge. We propose a simple multi-task learning scheme to achieve quantitatively better common sense reasoning in language models by leveraging auxiliary training signals from datasets designed to provide common sense grounding. When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> story generation dataset. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_1",
  "x": "Evaluating common sense qualitatively in a model's samples is difficult, as it is subject to human bias and dependent on sampling procedure. We propose evaluating the common sense of a model automatically by ranking the model's perplexity on spurious (plausible, but nonsense) text completions from SWAG (Zellers et al., 2018) and Story Cloze (Mostafazadeh et al., 2016) datasets, which are designed for common sense grounding. Our contributions are as follows: We propose a simple way to define better CSR in generative models, which leads to an auxiliary multi-task objective to directly bias our model to generate text with better common sense. When fine-tuning is combined with multi-task learning in a two-stage pipeline, we improve the model's CSR and outperform state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> Our primary task is to perform language modeling (Elman, 1990; Bengio et al., 2003; Dai and Le, 2015) on the WritingPrompts dataset. A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization:",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_2",
  "x": "When fine-tuning is combined with multi-task learning in a two-stage pipeline, we improve the model's CSR and outperform state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> Our primary task is to perform language modeling (Elman, 1990; Bengio et al., 2003; Dai and Le, 2015) on the WritingPrompts dataset. A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization: We train our model using a standard cross-entropy loss between next-step true tokens and predicted probabilities given current tokens. WritingPrompts<cite> (Fan et al., 2018</cite> ) is a dataset of prompts and short stories crawled from Reddit. The aim of the dataset is to produce a story given a free-text prompt.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_3",
  "x": "We perform three types of evaluation on the model to assess its readability, reliance on the prompt (prompt ranking) and CSR. Readability is measured in terms of model perplexity on the test set of WritingPrompts. Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in<cite> Fan et al. (2018)</cite> . We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word. Both sub-word perplexity and word-level perplexities are reported in our experiments.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_4",
  "x": "Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in<cite> Fan et al. (2018)</cite> . We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word. Both sub-word perplexity and word-level perplexities are reported in our experiments. Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_5",
  "x": "We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word. Both sub-word perplexity and word-level perplexities are reported in our experiments. Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt. Following<cite> Fan et al. (2018)</cite> , we count a random story sample as correct when it ranks the true prompt with the lowest perplexity.",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_7",
  "x": "Story Generation: Recent work in neural story generation (Kiros et al., 2015; Roemmele, 2016) has shown success in using hierarchical methods (Yao et al., 2018; <cite>Fan et al., 2018)</cite> to generate stories. In these schemes, a neural architecture is engineered to first generate an outline or a prompt, then to expand the prompt into a full-length story. Our work performs hierarchical generation, but our main focus is on achieving better common sense in the generated text rather than engineering task-specific architectures. Common Sense Reasoning: Common sense reasoning (CSR) has been studied through many benchmarks such as SWAG (Zellers et al., 2018) , Story Cloze (Mostafazadeh et al., 2016) , the Winograd Schema Challenge (Levesque et al., 2012) , and CommonsenseQA (Talmor et al., 2018) . Recent methods (Peters et al., 2018; Radford et al., 2018) on these benchmarks focus on large-scale pre-training of language models.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_0",
  "x": "Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a) 91.32 - Table 3 : Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_1",
  "x": "Comparing the L, NN and This models, the observations are consistent with the web domain. Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a) 91.32 - Table 3 : Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline.",
  "y": "similarities"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_2",
  "x": "Recently, <cite>Chen and Manning (2014)</cite> use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically. Discrete manual features and continuous features complement each other. A natural question that arises from the contrast is whether traditional discrete features and continuous neural features can be integrated for better accuracies.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_3",
  "x": "**PARSER** We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) . Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. <cite>Chen and Manning (2014)</cite> can be viewed as a neutral alternative of MaltParser (Nivre, 2008) . Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second).",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_4",
  "x": "---------------------------------- **PARSER** We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) . Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. <cite>Chen and Manning (2014)</cite> can be viewed as a neutral alternative of MaltParser (Nivre, 2008) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_5",
  "x": "---------------------------------- **MODELS** Following <cite>Chen and Manning (2014)</cite> , training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011) . We unify below the five deterministic parsing models in Figure 1 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_6",
  "x": "The score of an action a is defined by where \u03c3 represents the sigmoid activation function, \u2212 \u2192 \u03b8 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). ---------------------------------- **BASELINE NEURAL (NN)** We take the Neural model of <cite>Chen and Manning (2014)</cite> as another baseline (Figure 1(b) ).",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_7",
  "x": "where \u2212 \u2192 \u03b8 h is the set of parameters between the input and hidden layers. The score of an action a is defined as where \u2212 \u2192 \u03b8 c,a is the set of parameters between the hidden and output layers. We use the arc-standard features \u03a6 e as <cite>Chen and Manning (2014)</cite> , which is also based on the arc-eager templates of Zhang and Nivre (2011) , similar to those of the baseline model L. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_8",
  "x": "We clean the web domain texts following the method of Ma et al. (2014b) . Automatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Following <cite>Chen and Manning (2014)</cite> , we use the pre-trained word embedding released by Collobert et al. (2011) , and set h = 200 for the hidden layer size, \u03bb = 10 \u22128 for L2 regularization, and \u03b1 = 0.01 for the initial learning rate of Adagrad. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_9",
  "x": "<cite>Chen and Manning (2014)</cite> fine-tune word embeddings in supervised training, consistent with Socher et al. (2013) . Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) Table 2 : Main results on SANCL.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_10",
  "x": "For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of <cite>Chen and Manning (2014)</cite> are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> . In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_11",
  "x": "Note also tat for all experiments, the POS and label embedding features of <cite>Chen and Manning (2014)</cite> are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> . In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_12",
  "x": "In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. ---------------------------------- **FINAL RESULTS** The final results across web domains are shown in Table 2 . Our logistic regression linear parser and re-implementation of <cite>Chen and Manning (2014)</cite> give comparable accuracies to the perceptron ZPar 2 and Stanford NN Parser 3 , respectively.",
  "y": "similarities uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_13",
  "x": "Note that the accuracies of our parsers are lower than the best systems in the SANCL shared task, which use ensemble models. Our parser enjoys the fast speed of deterministic parsers, and in particular the baseline NN parser<cite> (Chen and Manning, 2014)</cite> . ---------------------------------- **WSJ EXPERIMENTS** For comparison with related work, we conduct experiments on Penn Treebank corpus also.",
  "y": "similarities"
 },
 {
  "id": "1781b27c13dca15752cb6aa8a9fc38_0",
  "x": "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; Seo et al., 2016) , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network <cite>Miao et al., 2016)</cite> , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.",
  "y": "background uses"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_0",
  "x": "Large-scale lexical semantic resources that provide relational information about words have recently received much focus in the field of Natural Language Processing (NLP). In particular, data-driven models for lexical semantics require the creation of broad-coverage, hand-annotated corpora with predicateargument information, i.e. rich information about words expressing a semantic relation having argument slots filled by the interpretations of their grammatical complements. Corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents<cite> (Gildea and Jurafsky, 2002)</cite> . That is, given an input sentence and a target predicator the system labels constituents with general roles like Agent, Patient, Theme, etc., or more specific roles, as in (1) . (1) 1 The task of automatic semantic role labelling (or shallow semantic parsing) is a first step towards text understanding and has found use in a variety of NLP applications including information extraction (Surdeanu et al., 2003), machine translation (Boas, 2002) , question answering (Narayanan and Harabagiu, 2004) , summarisation (Melli et al., 2005) , recognition of textual entailment relations (Burchardt and Frank, 2006) , etc.",
  "y": "background"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_1",
  "x": "For instance, test (8) intuitively lexicalises a dyadic relation between a Conceiver (tester) and a Conceived (tested) entity. A sought entity denoted by a for-PP is represented as part of a secondary Notion relation situated at the background of the primary (testing) relation. Conceived entities that are peripheral to the essential relation lexicalised by the predicate are associated with a more specific property termed Conceived background state of affairs (Conceived bsoa). These arguments receive less focus in the meaning of the predicate, in a sense that they are not absolutely necessary to understand the predicate's meaning. The representation of test (8), stereotype (<cite>10</cite>), and find out (11) in terms of two Notion relations, one of which is treated as more salient, reifies the concept of relative significance of Proto-Role properties in the verbal semantics.",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_0",
  "x": "**INTRODUCTION** Recently, pre-trained language representation models such as GPT (Radford et al., 2018 (Radford et al., , 2019 , ELMo (Peters et al., 2018) , BERT<cite> (Devlin et al., 2019)</cite> and XLNet have achieved promising results in NLP tasks, including reading comprehension (Rajpurkar et al., 2016) , natural language inference (Bowman et al., 2015; Williams et al., 2018) and sentiment classification (Socher et al., 2013) . These models capture contextual information from large-scale unlabelled corpora via well-designed pre-training * Equal contribution \u2020 Corresponding author: Minlie Huang tasks. The literature has commonly reported that pre-trained models can be used as effective feature extractors and achieve state-of-the-art performance on various downstream tasks . Although pre-trained language representation models have achieved transformative performance, the pre-training tasks like masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> neglect to consider the linguistic knowledge.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_1",
  "x": "Recently, pre-trained language representation models such as GPT (Radford et al., 2018 (Radford et al., , 2019 , ELMo (Peters et al., 2018) , BERT<cite> (Devlin et al., 2019)</cite> and XLNet have achieved promising results in NLP tasks, including reading comprehension (Rajpurkar et al., 2016) , natural language inference (Bowman et al., 2015; Williams et al., 2018) and sentiment classification (Socher et al., 2013) . These models capture contextual information from large-scale unlabelled corpora via well-designed pre-training * Equal contribution \u2020 Corresponding author: Minlie Huang tasks. The literature has commonly reported that pre-trained models can be used as effective feature extractors and achieve state-of-the-art performance on various downstream tasks . Although pre-trained language representation models have achieved transformative performance, the pre-training tasks like masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> neglect to consider the linguistic knowledge. We argue that such knowledge is important for some NLP tasks, particularly for sentiment analysis.",
  "y": "motivation"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_2",
  "x": "**PRE-TRAINED LANGUAGE REPRESENTATION MODEL** Early work on pre-trained language representation models mainly focuses on distributed word representations, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) . Since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts. Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT<cite> (Devlin et al., 2019)</cite> becomes prevalent recently. These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_3",
  "x": "Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed. Due to the important role of entities in language understanding, two heuristic ways have been studied to make the pre-trained model aware of entities, i.e. introducing knowledge graph (Zhang et al., 2019) / knowledge base (Peters et al., 2019) explicitly and designing entity-specific masking strategies during pretraining (Sun et al., 2019a,b) . Considering the implicit relationship among different NLP tasks, post-training approaches Li et al., 2019) conduct supervised training on the pretrained BERT with transfer tasks which are related to target tasks, in order to get a better initialization for target tasks. The model structure and the pre-training tasks of BERT are also worth exploring.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_4",
  "x": "Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed. Due to the important role of entities in language understanding, two heuristic ways have been studied to make the pre-trained model aware of entities, i.e. introducing knowledge graph (Zhang et al., 2019) / knowledge base (Peters et al., 2019) explicitly and designing entity-specific masking strategies during pretraining (Sun et al., 2019a,b) . Considering the implicit relationship among different NLP tasks, post-training approaches Li et al., 2019) conduct supervised training on the pretrained BERT with transfer tasks which are related to target tasks, in order to get a better initialization for target tasks. The model structure and the pre-training tasks of BERT are also worth exploring.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_5",
  "x": "Our task is formulated as follows: given a text sequence X = (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n ) of length n , our goal is to acquire the representation of the whole sequence H = (h 1 , h 2 , \u00b7 \u00b7 \u00b7 , h n ) \u2208 R n\u00d7d that captures the contextual information and the linguistic knowledge simultaneously. In this formulation, d indicates the dimension of the representation vector. Figure 1 shows the overview of our model pipeline which contains three stages: 1) Acquiring the prior sentiment polarity for each word with its corresponding part-of-speech tag; 2) Conducting pre-training via two tasks i.e. label-aware masked language modeling and next sentence prediction; 3) Fine-tuning on sentiment analysis tasks with different settings. Compared with the vanilla pretrained models like BERT<cite> (Devlin et al., 2019)</cite> , our model enriches the input sequence with its linguistic knowledge including part-of-speech tags and sentiment polarity labels, and utilizes a modified masked language model to capture the relationship between sentence-level sentiment labels and word-level knowledge in addition to context dependency. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_6",
  "x": "( 2) where the reciprocal of the SN of each sense weights the respective score since in the Senti-WordNet smaller sense number indicates more frequent use of this sense in natural language. Note that if we can't find any sense for (x i , pos i ) in Sen-tiWordNet, the label of N eutral will be assigned. ---------------------------------- **PRE-TRAINING TASKS** During pre-training, Label-aware masked language model (LA-MLM) and next sentence prediction (NSP) are adopted as the pre-training tasks where the setting of NSP is identical to the one proposed by<cite> Devlin et al. (2019)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_7",
  "x": "**FINE-TUNING SETTING** Equipped with the ability to utilize linguistic knowledge via pre-training, our model can be finetuned to different sentiment analysis tasks, including sentence-level / aspect-level sentiment classification. We follow the fine-tuning setting of the existing work <cite>(Devlin et al., 2019</cite>; : Sentence-level Sentiment Classification: The input of this task is a text sequence ([CLS], x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n , [SEP]). The sentiment label is obtained based on the hidden state of [CLS]. Aspect-level Sentiment Classification: In addition to the text sequence, the input additionally contains an aspect term / aspect category sequence (a 1 , \u00b7 \u00b7 \u00b7 , a l ).",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_8",
  "x": "**PRE-TRAINING DATASET AND IMPLEMENTATION** We adopted the Yelp Dataset Challenge 2019 2 as our pre-training dataset. This dataset contains 6,685,900 reviews with 5-class review-level sentiment labels. Each review consists of 8.1 sentences and 127.8 words on average. Since our method can adapt to all the BERTstyle pre-training models, we used vanilla BERT<cite> (Devlin et al., 2019)</cite> as the base framework to construct Transformer blocks in this paper and leave the exploration of other models like RoBERTa as future work.",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_10",
  "x": "We can see that Sen-tiLR outperforms the baselines in both accuracy and Macro-F1 on these datasets, indicating that our model can successfully grasp the sentiment of the given aspects. Since the improvement of Macro-F1 is more notable than that of accuracy, it is convinced that our model actually does better in all the three sentiment classes. Due to the sparsity of aspect terms compared with aspect categories, our model improved a larger margin on the task of aspect category sentiment classification than the<cite> (Devlin et al., 2019)</cite> . To explore whether the performance of Sen-tiLR on common NLP tasks will improve or degrade, we evaluated our model on General Language Understanding Evaluation (GLUE) benchmark , which collects diverse language understanding tasks. We fine-tuned Sen-tiLR on each task of GLUE respectively, and compared its performance with vanilla BERT.",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_11",
  "x": "We fine-tuned Sen-tiLR on each task of GLUE respectively, and compared its performance with vanilla BERT. Since the test sets of GLUE are not publicly available, we reported the results on development sets in Table 6 . Note that we directly used the results of BERT on SST-2, MNLI, QNLI and MRPC which are reported by<cite> Devlin et al. (2019)</cite> and reimplemented the BERT model fine-tuned on the rest of the tasks by ourselves. From Table 6 , SentiLR surely gets better results on the tasks in sentiment analysis like SST-2. We also observe that our model outperforms BERT on CoLA, MRPC, QNLI tasks, and gets comparative results on the other datasets.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_0",
  "x": "---------------------------------- **MODELS** First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of Luong et al. (2015) , and presented by<cite> Chopra et al. (2016)</cite> . Next, we introduce our multi-task learning approach of sharing the parameters between abstractive summarization and entailment generation models. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_1",
  "x": "**BASELINE MODEL** Our baseline model is a strong, multi-layered encoder-attention-decoder model with bilinear attention, similar to Luong et al. (2015) and following the details in<cite> Chopra et al. (2016)</cite> . Here, we encode the source document with a two-layered LSTM-RNN and generate the summary using another two-layered LSTM-RNN decoder. The word probability distribution at time step t of the decoder is defined as follows: where g is a non-linear function and c t and s t are the context vector and LSTM-RNN decoder hidden state at time step t, respectively.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_2",
  "x": "Following previous work (Nallapati et al., 2016; <cite>Chopra et al., 2016</cite>; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) . ---------------------------------- **TRAINING DETAILS** We use the following simple settings for all the models, unless otherwise specified.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_4",
  "x": "---------------------------------- **SUMMARIZATION RESULTS: GIGAWORD** Baseline Results and Previous Work Our baseline is a strong encoder-attention-decoder model based on Luong et al. (2015) and presented by<cite> Chopra et al. (2016)</cite> . As shown in Table 1 , it is reasonably close to some of the state-of-theart (comparable) results in previous work, though making this baseline further strong (e.g., based on pointer-copy mechanism) is our next step. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_5",
  "x": "**SUMMARIZATION RESULTS: DUC** Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81<cite> Chopra et al. (2016)</cite> 28.97 8.26 24.06 Nallapati et al. (2016) our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_0",
  "x": "****FASTFUSIONNET: NEW STATE-OF-THE-ART FOR DAWNBENCH SQUAD**** **ABSTRACT** In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet <cite>[12]</cite> . FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19] .",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_1",
  "x": "---------------------------------- **** In this technical report, we analyze the inference bottlenecks of FusionNet <cite>[12]</cite> and introduce FastFusionNet that tackles them. In our experiments, we show that FastFusionNet achieves new state-of-the-art training and inference time on SQuAD based on the metrics of DAWNBench. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_2",
  "x": "**ANALYSIS OF FUSIONNET** FusionNet <cite>[12]</cite> is reading comprehension model built on top of DrQA by introducing Fully-aware attention layers (context-question attention and context self-attention), contextual embeddings [21] , and more RNN layers. Their proposed fully-aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as DenseNet [11] . FusionNet was the state-of-the-art reading comprehension model at the time of writing (Oct. 4th 2017). Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_3",
  "x": "Their proposed fully-aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as DenseNet [11] . FusionNet was the state-of-the-art reading comprehension model at the time of writing (Oct. 4th 2017). Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. <cite>[12]</cite> . Additionally, the LSTM layers contribute to 58.8% [19] , GRU [3] , LSTM [10] , QANet Encoding block (with 2 conv layers and a 8-head attention) [39] , 5 Convolution layers with gated linear unit (GLU) [6, 35] . All input and hidden sizes are 128.",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_4",
  "x": "We time a 5-layer CNN since it matches the performance of one layer SRU. ---------------------------------- **FASTFUSIONNET** Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet <cite>[12]</cite> . There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_5",
  "x": "Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet <cite>[12]</cite> . There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers. We closely follow the implementation of Huang et al. <cite>[12]</cite> described in their paper except for the changes above. Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size. In the following explanation, we use [A; B] to represent concatenation in the feature dimension.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_6",
  "x": "There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers. We closely follow the implementation of Huang et al. <cite>[12]</cite> described in their paper except for the changes above. Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size. In the following explanation, we use [A; B] to represent concatenation in the feature dimension. Attn(Q, K, V) represents the attention mechanism taking the query Q, the key K, and the value V as inputs.",
  "y": "similarities uses"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_7",
  "x": "Like others <cite>[12]</cite> we use a randomly initialized the trainable embedding layer with 12 dimensions for POS tags and 8 dimensions for NER. We use question matching features proposed by Chen et al. [2] as well, which contains a hard version and a soft version. The hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form appears in the question, respectively. The soft version uses a trainable attention module that learns to represent each context word as a mixture of question words. Overall, the i-th context token is represented as C i which has 624 dimensions, and the j-th question token is represented as a 300-d Q j glove vector.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_8",
  "x": "where The Question Understanding Layer is another 2-layer BiSRU combining Q and Q h into Q u , i.e. where The Question-Context Attention Layer is a fully-aware attention module <cite>[12]</cite> which takes the history (concatenation of GloVe, low-level, and high-level features) of each context word and question words as query and key for three attention modules, and represents each context word as three different vectors: ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_9",
  "x": "Our FastFusionNet reaches F1 75% in 4 epochs and achieves at F1 82.5% at the end which matches the reported F1 82.5% of FusionNet without CoVe on SQuAD development set <cite>[12]</cite> . The training time track aims to minimize the time to train a model up to at least 75% F1 score on SQuAD development set. Table 1 shows that our FastFusionNet reaches F1 75.0% within 20 minutes (after 4 epochs), which gives a 45% speedup compared to the winner DrQA(ParlAI) on the leaderboard. Notably, we use an Nvidia GTX-1080 GPU which is about 22% slower than their Nvidia RTX-2080 GPU. When controlling the generation of GPUs and comparing our model with a DrQA (ParlAI) trained on an Nvidia V100, our model achieves a 3.1\u00d7 speedup.",
  "y": "differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_10",
  "x": "When controlling the generation of GPUs and comparing our model with a DrQA (ParlAI) trained on an Nvidia V100, our model achieves a 3.1\u00d7 speedup. Compared to FusionNet, FastFusionNet is 23% faster to reach 75% F1 score; however, in terms of the training time per epoch, it is in fact 2.6\u00d7 as fast as FusionNet. <cite>[12]</cite> here since our reimplementation is about 0.5% F1 score worse. The inference time track evaluates the average 1-example inference latency of a model with an F1 score at least 75%. Our FastFusionNet reduces the 1-example latency down to 7.9 ms, which is 2.8\u00d7 as fast as a BERT-base and 12.7\u00d7 over BiDAF.",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_0",
  "x": "Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem. Our work is similar in spirit to e.g. (Roy, 2002; Skocaj et al., 2011) but advances it in several aspects <cite>(Yu et al., 2016)</cite> . In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent). Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time. Unlike a lot of past work (Silberer and Lapata, 2014; Thomason et al., 2016; Matuszek et al., 2014) , here we assume that the agent is in the position of a child, who does not have any prior knowledge of perceptual categories.",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_1",
  "x": "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans. In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor. What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail). \u2022 VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft",
  "y": "background"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_2",
  "x": "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans. In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor. What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail). \u2022 VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_3",
  "x": "We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of<cite> Yu et al. (2016)</cite> . The framework consists of two core modules: Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class. It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (Zhang, 2004) -to incrementally learn attribute predictions. The visual classifiers ground visual attribute words such as 'red', 'circle' etc.",
  "y": "similarities uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_4",
  "x": "---------------------------------- **WHEN TO LEARN: ADAPTIVE CONFIDENCE THRESHOLD** The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_5",
  "x": "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans). ---------------------------------- **HOW TO LEARN: NATURAL INTERACTION WITH HUMANS**",
  "y": "motivation"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_6",
  "x": "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans).",
  "y": "uses extends differences"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_0",
  "x": "This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007) , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing <cite>(Collins and Roark</cite>, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010) , joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013) , and joint segmentation, POS-tagging and parsing . In addition to the aforementioned tasks, the framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process.",
  "y": "uses"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_1",
  "x": "**TUTORIAL OVERVIEW** In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007) , as well as beam-search and the early-update strategy <cite>(Collins and Roark, 2004)</cite> . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing. In each case, we illustrate how the task is turned into an incremental left-to-right output-building process, and how rich features are defined to give competitive accuracies.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_0",
  "x": "The latter scenario is known as zero-shot RE (Rockt\u00e4schel et al., 2015) . <cite>Levy et al. (2017)</cite> present a reformulation of RE, where the task is framed as reading comprehension. In this formulation, each relation type (e.g. author, occupation) is mapped to at least one natural language question template (e.g. \"Who is the author of x?\"), where x is filled with an entity (e.g. \"Inferno\"). The model is then tasked with finding an answer (\"Dante Alighieri\") to this question with respect to a given context. They show that this formulation of the problem both outperforms off-the-shelf RE systems in the typical RE setting and, in addition, enables generalization to unspecified and unseen types of relations.",
  "y": "background motivation"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_1",
  "x": "Open RE methods (Yates et al., 2007; Fader et al., 2011) do not require relationspecific data, but treat different phrasings of the same relation as different relations and rely on a combination of syntactic features (e.g. dependency parses) and normalisation rules, and so have limited generalization capacity. Zero-shot relation extraction <cite>Levy et al. (2017)</cite> propose a novel approach towards achieving this generalization by transforming relations into natural language question templates. For instance, the relation born in(x, y) can be expressed as \"Where was x born?\" or \"In which place was x born?\". Then, a reading comprehension model (Seo et al., 2016; Chen et al., 2017) can be trained on question, answer, and context examples where the x slot is filled with an entity and the y slot is either an answer if the answer is present in the context, or NIL. The model is then able to extract relation instances (given expressions of the relations as questions) from raw text.",
  "y": "background motivation"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_2",
  "x": "2 Slot-filling data To extract the contexts for each triple in our dataset we use the distant supervision method described by <cite>Levy et al. (2017)</cite> . For each Wikidata document belonging to a given entity 1 we take all the denormalized tuples (property, entity 2 ) and extract the first sentence in the text containing both entity 1 and entity 2 . Negatives (contexts without answers) are constructed by finding pairs of triples with common entity 2 type (to ensure they contain good distractors), swapping their context if entity 2 is not present in the context of the other triple. Querification <cite>Levy et al. (2017)</cite> created 1192 question templates for 120 Wikidata properties. A template contains a placeholder for an entity x (e.g. for property \"author\", some templates are \"Who wrote the novel x?\" and \"Who is the author of x?\"), which can be automatically filled in to create questions so that question \u2248 template(property, x)).",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_3",
  "x": "Querification <cite>Levy et al. (2017)</cite> created 1192 question templates for 120 Wikidata properties. A template contains a placeholder for an entity x (e.g. for property \"author\", some templates are \"Who wrote the novel x?\" and \"Who is the author of x?\"), which can be automatically filled in to create questions so that question \u2248 template(property, x)). For our multilingual dataset, we had these templates translated by human translators. The translators attempted to translate each of the original 1192 templates. If a template was difficult to translate, they were in- structed to discard it. They were also instructed to create their own templates, paraphrasing the original ones when possible. This resulted in a varying number of templates for each of the properties across languages. In addition to the entity placeholder, some languages with richer morphology (Spanish, Italian, and German) required extra placeholders in the templates because of agreement phenomena (gender). We added a placeholder for definite articles, as well as one for gender-dependent filler words. The gender is automatically inferred from the Wikipedia page statistics and a few heuristics.",
  "y": "extends"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_5",
  "x": "Following <cite>Levy et al. (2017)</cite> , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2). Our goal is to answer these three questions: A) how well can RE models be transferred across languages? B) in the difficult UnREL setting, can the variance between languages in the number of instances of relations (see Figure 2 ) be exploited to enable more robust RE ? C) can one jointly-trained multilingual model which performs RE in multiple languages perform comparably to or outperform its individual monolingual counterparts? For all experiments, we take the multiple templates approach where a model sees different paraphrases of the same question during training. This approach was shown by <cite>Levy et al. (2017)</cite> to have significantly better paraphrasing abilities than when only one question template or simpler relation descriptions are employed. Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> .",
  "y": "similarities motivation"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_6",
  "x": "**EXPERIMENTS** Following <cite>Levy et al. (2017)</cite> , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2). Our goal is to answer these three questions: A) how well can RE models be transferred across languages? B) in the difficult UnREL setting, can the variance between languages in the number of instances of relations (see Figure 2 ) be exploited to enable more robust RE ? C) can one jointly-trained multilingual model which performs RE in multiple languages perform comparably to or outperform its individual monolingual counterparts? For all experiments, we take the multiple templates approach where a model sees different paraphrases of the same question during training. This approach was shown by <cite>Levy et al. (2017)</cite> to have significantly better paraphrasing abilities than when only one question template or simpler relation descriptions are employed.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_7",
  "x": "Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> . We compute precision, recall and F1 by comparing spans predicted by the 3 https://github.com/google-research/ bert/blob/master/multilingual.md models with gold answers. Precision is equal to the true positives divided by total number of nonnil answers predicted by a system. Recall is equal to the true positives divided by the total number of instances that are non-nil in the ground truth answers. Word order and punctuation are not considered.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_11",
  "x": "All monolingual models' word embeddings were initialised using fastText embeddings trained on each language's Wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub-section 5.1 where GloVe (Pennington et al., 2014) was used for comparability with <cite>Levy et al. (2017)</cite> . ---------------------------------- **RELATED WORK** Multilingual NLU Advances in natural language understanding tasks have been as impressive as they have been fast-paced. Until recently, however, the multilingual aspect of such tasks has not received as much attention.",
  "y": "background"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_12",
  "x": "This is pri-Lang. UnENT UnREL <cite>Levy et al. (2017)</cite> Faruqui and Kumar (2015) employed a pipeline of machine translation systems to translate to English, then Open RE systems to perform RE on the translated text, followed by crosslingual projection back to source language. Verga et al. (2016) apply the universal schema framework (Riedel et al., 2013) on top of multilingual embeddings to extract relations from Spanish text without using Spanish training data. This approach, however, only enables generalization to unseen entities and does not have the flexibility to predict unseen relations. Furthermore, both of these works faced a fundamental difficulty with evaluation.",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_0",
  "x": "These representations may refer to whole sentences, word contexts or any other construct. For concreteness, let u and v be two representations we want to match. In order to facilitate the matching, it is often beneficial to explicitly create new features like element-wise absolute difference (|u\u2212v|) and element-wise product (u \u00b7 v) that augment u and v. The combined feature vector is then processed by further layers in the task specific neural network. For example, Tai et al. (2015) use these heuristics to improve semantic representations. Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_1",
  "x": "Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> . This is also used in the more sophisticated models of Chen et al. (2017) , where u and v represent word contexts. Several of these approaches are explored in the Compare-and-Aggregate framework by Wang & Jiang (2017) . In this paper we focus on polynomial features like u \u00b7 v for the natural language inference task, where it is trying to capture similarity between u and v. It is also a monomial of degree 2. We investigate two aspects of such terms -the use of scaling and the use of higher degree polynomials.",
  "y": "background uses"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_2",
  "x": "The standard matching feature of <cite>Mou et al. (2016)</cite> uses a concatenation of u, v, |u\u2212v| and u \u00b7 v. We define the following new matching feature vector that scales the multiplicative term by a constant factor \u03b7 > 0. (1) To incorporate polynomial multiplicative features between u and v of degree 3 and 4, we define the following define the following matching feature vectors. and In w poly3 , the additional term is the sum of the two possible monomials of degree 3 involving both u and v. In w poly4 , the fourth degree term is the sum of the 3 possible monomials of degree 4 involving both u and v. Note that we scale the 3rd degree terms by \u03b7 2 and the 4th degree terms by \u03b7 3 .",
  "y": "background uses"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_3",
  "x": "In each case, the feature vector is fed into a fully connected layer(s), before computing the 3-way softmax in the classification layer. It is possible to use each of the degree 3 and 4 terms separately as a feature, but this did not make our models substantially more accurate. Choosing \u03b7 = 1 in w poly2 reduces it to the matching feature vector proposed by <cite>Mou et al. (2016)</cite> . The same procedure is repeated for the other baseline model, namely ESIM Chen et al. (2017) . In this case, u represents one of the intermediate states of a bidirectional LSTM encoding of the premise (hypothesis) and v represents the hypothesis (premise) states weighted by relevance to the premise (hypothesis) state.",
  "y": "uses"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_0",
  "x": "Recent advances in deep learning have shown exceptional results in language-related tasks such as machine translation, question answering, or sentiment analysis. However, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination. It is thus crucial to learn to communicate by interaction, i.e., communication must emerge out of necessity. Such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language. Several recent works<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Havrylov and Titov 2017; Lazaridou et al. 2018; <cite>Mordatch and Abbeel 2018)</cite> , have shown that in multi-agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.",
  "y": "background motivation"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_1",
  "x": "In<cite> (Lazaridou, Peysakhovich, and Baroni 2016)</cite> , the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in (Havrylov and Titov 2017) , the message is considered to be a sequence of symbols. (Lazaridou et al. 2018) demonstrates that successful communication can also emerge in environments which present raw pixel input. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (<cite>Mordatch and Abbeel 2018)</cite> further extends the scope of mode of communication by also studying the emergence of non-verbal communication.",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_2",
  "x": "Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (<cite>Mordatch and Abbeel 2018)</cite> further extends the scope of mode of communication by also studying the emergence of non-verbal communication. While these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication. Historically, written language systems have shown complex patterns in evolution over time.",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_3",
  "x": "**REFERENTIAL GAME FRAMEWORK** In our work, we have used two referential game setups that are slight modifications to the ones used in<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Lazaridou et al. 2018 ). There are two players, a sender and a receiver. From a given set of images where the sender only has access to the target image t; Distractor Aware (D-Aware): where the sender has access to the candidate set C = t \u222a D. In both these variations, the sender has to come up with a message M l = {m j } l j=1 , which is a sequence of l brushstrokes.",
  "y": "extends differences"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_4",
  "x": "The payoff is 1 for both the agents iff R \u03c6 (f s (S \u03b8 (R(M i ), h i , V )), U ) = t , where i is the last timestep of the episode. In all other cases and intermediate timesteps, the payoff is 0. Because of the high dimensional search space introduced due to brushstrokes, we use Proximal Policy Optimization (PPO)<cite> (Schulman et al. 2017)</cite> for optimizing the weights of sender and receiver agents. ---------------------------------- **IMAGES** We have used CIFAR-10 dataset (Krizhevsky, Hinton, and others 2009) , as a source of images.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_0",
  "x": "****ARE YOU A RACIST OR AM I SEEING THINGS? ANNOTATOR INFLUENCE ON HATE SPEECH DETECTION ON TWITTER.**** **ABSTRACT** Hate speech in the form of racism and sexism is commonplace on the internet <cite>(Waseem and Hovy, 2016)</cite> . For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_1",
  "x": "For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on <cite>the data set</cite> released by <cite>Waseem and Hovy (2016)</cite>. We find that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_2",
  "x": "In addition, it is important to understand how different manners of obtaining labeling can influence the classification models and how it is possible to obtain good annotations, while ensuring that annotators are not likely to experience adverse effects of annotating hate speech. Our contribution We provide annotations of 6, 909 tweets for hate speech by annotators from CrowdFlower and annotators that have a theoretical and applied knowledge of hate speech, henceforth amateur and expert annotators 1 . Our data set extends the <cite>Waseem and Hovy (2016)</cite> data set by 4, 033 tweets. We also illustrate, how amateur and expert annotations influence classification efforts. Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators.",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_3",
  "x": "We also illustrate, how amateur and expert annotations influence classification efforts. Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators. ---------------------------------- **DATA** Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_4",
  "x": "Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators. ---------------------------------- **DATA** Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_5",
  "x": "Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> . In addition to \"racism\", \"sexism\", and \"neither\", we add the label \"both\" for tweets that contain both racism and sexism. We add this label, as the intersection of multiple oppressions can differ from the forms of oppression it consists of (Crenshaw, 1989) , and as such becomes a unique form of oppression.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_6",
  "x": "The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_7",
  "x": "The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_8",
  "x": "We recruit feminist and antiracism activists to annotate the data set. We present the annotators with <cite>the tests</cite> from <cite>Waseem and Hovy (2016)</cite> . If a tweet fails any of <cite>the tests</cite>, the annotators are instructed to label it as the relevant form of hate speech. Expert annotators are given the choice of skipping tweets, if they are not confident in which label to assign, and a \"Noise\" label in case the annotators are presented with non-English tweets. Due to privacy concerns, all expert annotators are treated as a single entity.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_9",
  "x": "The amateur annotators are not provided with the option to skip tweets, as they are not presented tweets the experts had skipped or labeled as \"Noise\". Annotator agreement Considering annotator agreement, we find that the inter-annotator agreement among the amateur annotators is \u03ba = 0.57 (\u03c3 = 0.08). Majority Vote Full Agreement Expert 0.34 0.70 The low agreement in Table 2 provides further evidence to the claim by Ross et al. (2016) that annotation of hate speech is a hard task. Table 2 suggests that if only cases of full agreement are considered, it is possible to obtain good annotations using crowdsourcing. Overlap Considering the overlap with the <cite>Waseem and Hovy (2016)</cite>, we see that the agreement is extremely low (mean pairwise \u03ba = 0.14 between all annotator groups and <cite>Waseem and Hovy (2016)</cite> ).",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_10",
  "x": "Interestingly, we see that the vast majority of disagreements between our annotators and <cite>Waseem and Hovy (2016)</cite> , are disagreements where our annotators do not find hate speech but <cite>Waseem and Hovy (2016)</cite> the influence of the features listed in Table 4 for each annotator group. Model Selection We perform a grid search over all possible feature combinations to find the best performing features. We find that the features with the highest performance are not necessarily the features with the best performance. For instance, token unigrams obtains the highest F1-score, precision, and the second highest recall on the amateur annotations, yet this feature fails to classify the minority classes. Features We use a range of features focusing on both the textual information given in the tweets as well as extra-linguistic information including POS tags obtained using Gimpel et al. (2011) and Spacy 2 .",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_11",
  "x": "We obtain up to 3200 tweets for each user in our data set, calculate the TF-IDF scores, and identify the top 100 terms. We then add a binary feature signifying the occurrence of each of these 100 terms. Interestingly, this feature performs worse than any other feature. Particularly when trained on expert annotations, suggesting that hate speech may be more situational or that users engaging in hate speech, do not only, or even primarily engage in hate speech. Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_12",
  "x": "Particularly when trained on expert annotations, suggesting that hate speech may be more situational or that users engaging in hate speech, do not only, or even primarily engage in hate speech. Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set. To counteract the low coverage in <cite>Waseem and Hovy (2016)</cite> , we use a lexicon trained on Twitter (Sap et al., 2014) to calculate the probability of gender. Using these probabilities we assign binary gender. Both the probability of a gender for a user and the binary gender are used as individual features.",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_13",
  "x": "Minority Class Misclassification We find that some features trained on expert and amateur annotations result in misclassification on the minority classes, including identifying no instances of the mi- nority classes (see Table 4 ). These misclassifications of the minority classes are largely due to the small number of instances in those classes. In spite of this, we do not believe that only boosting the size of the minority classes is a good approach, as we should seek to mimic reality in our data sets for hate speech detection. Results Running our system on the <cite>Waseem and Hovy (2016)</cite> data set, we find that our best performing system does not substantially outperform on the binary classification task <cite>Waseem and Hovy (2016</cite> Interestingly, the main cause of error is false positives. This holds true using both amateur and expert annotations.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_14",
  "x": "We mitigate personal bias in our annotations, as multiple people have participated in the annotation process. <cite>Waseem and Hovy (2016)</cite> may suffer from personal bias, as the only the authors annotated, and only the annotations positive for hate speech were reviewed by one other person. It is our contention that hate speech corpora should reflect real life, in that hate speech is a rare occurrence comparatively. Given that some of our features obtain high F1-scores, in spite of not classifying for the minority classes, we suggest that the unweighted F1-score may not be an appropriate metric to evaluate classification on hate speech corpora. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_15",
  "x": "<cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features. The primary challenge this paper presents, is the need for good annotation guidelines, if one wishes to detect specific subsets of abusive language.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_16",
  "x": "While Sood et al. (2012) incorporate edit distances to find variants of slurs, they are not able to find terms that do not occur in these lists. Nobata et al. (2016) address this, by using comprehensive lists of slurs obtained from Hatebase 4 . <cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_17",
  "x": "Nobata et al. (2016) address this, by using comprehensive lists of slurs obtained from Hatebase 4 . <cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_18",
  "x": "---------------------------------- **CONCLUSION** We find that using expert annotations can produce models that perform comparably to previous classification efforts. Our best model is on par with previous work on the <cite>Waseem and Hovy (2016)</cite> data set for the binary classification task but under-performs for the multi-class classification task. We suggest that a weighted F1-score be applied in evaluation of classification efforts on hate speech corpora, such that misclassification on minority classes is penalized.",
  "y": "differences similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_19",
  "x": [
   "Our annotation and classification results expand on the claim of Ross et al. (2016) that hate speech is hard to annotate without intimate knowledge of hate speech. Furthermore, we find that considering only cases of full agreement among amateur annota-tors can produce relatively good annotations as compared to expert annotators. This can allow for a significant decrease in the annotations burden of expert annotators by asking them to primarily consider the cases in which amateur annotators have disagreed. Future Work We will seek to further investigate the socio-linguistic features such as gender and location. Furthermore, we will expand to more forms of hate speech."
  ],
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_0",
  "x": "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively. ---------------------------------- **RELATED WORK 2.1 FINITE STATE TRANSDUCER**",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_1",
  "x": "For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence. Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (Sproat, 1996; Roark et al., 2012) such as integer grammars (e.g., \"26\" \u2192 \"twenty six\") or time grammars (e.g., \"5:26\" \u2192 \"five twenty six\"). Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_2",
  "x": "We have observed accuracy improvements by using an n-gram LM to re-rank hypotheses generated by WFSTs. However, an n-gram LM's context awareness is limited. ---------------------------------- **DATA-DRIVEN APPROACHES** Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) .",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_3",
  "x": "**DATA-DRIVEN APPROACHES** Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) . To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context<cite> (Sproat and Jaitly, 2016</cite>; Yolchuyeva et al., 2018) .",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_4",
  "x": "---------------------------------- **MODEL** ---------------------------------- **BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_5",
  "x": "**BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data. Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<cite> (Sproat and Jaitly, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_6",
  "x": "---------------------------------- **DATASET** The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from <cite>Sproat and Jaitly (2016)</cite> . The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text. Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_7",
  "x": "Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place. Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame. As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances. Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the <cite>Sproat and Jaitly (2016)</cite> data release and split into training, validation, and test data. However, the training data for window-based and sentencebased models are not identical due to differences in input configurations.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_8",
  "x": "**MODEL** Train ELECTRONIC text is not suitable for our system as it primarily reads out URLs letter by letter, e.g., \"Forbes.com\" \u2192 \"f o r b e s dot c o m\" (as opposed to \"forbes dot com\"). Therefore, we exclude ELECTRONIC data in our experiments. There are large numbers of <self> tokens present in the dataset. We follow <cite>Sproat and Jaitly (2016)</cite> in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_9",
  "x": "For the subword model, both the source and target sentences are segmented into subword sequences. Subword units are concatenated to words for evaluation. ---------------------------------- **BASELINE MODEL SETUP** Our first approach replicates the window-based seq2seq model of <cite>Sproat and Jaitly (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_10",
  "x": "Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_11",
  "x": "Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown.",
  "y": "similarities"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_12",
  "x": "Categories are sorted by frequency. * TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_13",
  "x": "**LINGUISTIC FEATURES** We use the following linguistic features: 1) capitalization: upper, lower, mixed, nonalphanumerical, foreign characters; 2) position: (Bird et al., 2009) . Edit labels are the most expensive to obtain in real life. Our labels are generated directly from the Google FST<cite> (Sproat and Jaitly, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_0",
  "x": "Systems, such as treebank-based parsers<cite> (Charniak, 2001</cite>; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008) , are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Haji\u010d et al., 2009 ), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences.",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_1",
  "x": "For example, users of the Charniak parser<cite> (Charniak, 2001)</cite> should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't. Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks. Thus if a system uses multiple parsers, such differences must be accounted for. Differences that are not important for a particular application should be ignored (e.g., by merging alternative analyses). For example, in the case of spurious attachment ambiguity, a system may need to either accept both as right answers or derive a common representation for both.",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_2",
  "x": "We use Charniak, UMD and KNP parsers<cite> (Charniak, 2001</cite>; Huang and Harper, 2009; Kurohashi and Nagao, 1998) , JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions. Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; recognizing dates and number expressions.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_0",
  "x": "****RECOGNIZING HUMOUR USING WORD ASSOCIATIONS AND HUMOUR ANCHOR EXTRACTION**** **ABSTRACT** This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors<cite> (Yang et al., 2015)</cite> , for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_1",
  "x": "As such, it is no surprise that computational humour, and humour recognition in particular, has received increased attention from the NLP community with SemEval-2017 devoting two tasks to it: ranking humorous tweets (Potash et al., 2017) and interpreting English puns (Miller et al., 2017) . This recent attention has lead to advancements such as sequence-based neural humour models capable of implicitly learning a joke structure and semantic features (Bertero and Fung, 2016a; Donahue et al., 2017) . While these approaches offer good performance, their reliance on complicated neural architectures over explicitly engineered features present a problem for interpretability which may make it difficult to diagnose problems if results go wrong. Works which take a more interpretable statistical machine learning approach have their own drawbacks. For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_2",
  "x": "Works which take a more interpretable statistical machine learning approach have their own drawbacks. For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour. Additionally, many works fail to take advantage of joke structure, treating texts as unordered bags-of-words (Bertero and Fung, 2016b; Mihalcea and Strapparava, 2005; Yan and Pedersen, 2017) . This paper aims to marry the interpretability of statistical machine learning approaches with the more nuanced models of joke structure and joke semantics of neural approaches. Specifically, we explore the effectiveness modelling joke semantics using semantic relatedness features based on word associations, rather than the more common semantic similarity features based on word embeddings.",
  "y": "background uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_3",
  "x": "Specifically, we experiment with integrating the extraction of humour anchors, the \"meaningful, complete, minimal set of word spans\"<cite> (Yang et al., 2015)</cite> that allow humour to occur, into the humour classification process itself, the first work to do so. We are also making the code used to run these experiments publicly available 1 . ---------------------------------- **RELATED WORK** Informally, jokes are generally divided into setups and punchlines, with the setup establishing a context and the punchline delivering the humour.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_4",
  "x": "**JOKE SEMANTICS** Following SSTH, we expect that punchlines that do not sufficiently overlap with their setup are unfunny as they do not flow logically from the context. Similarly, punchlines that are not sufficiently incongruent with their setup are unfunny as there is no re-evaluation. As overlap and incongruity are difficult to measure directly, one common approach is instead to use word embeddings, such as Word2Vec (Mikolov et al., 2013) , to calculate the cosine similarities between pairs of vectors representing words in a document <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) . However, measuring incongruity and overlap in terms of similarity is a rather odd choice. Just because two scripts overlap does not imply they are similar.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_5",
  "x": "As mentioned in Section 1, joke structure and joke semantics are not entirely independent. This means poor models of joke structure can affect the performance of features designed to capture joke semantics. Despite the issues mentioned in Section 2.1,<cite> Yang et al. (2015)</cite> 's \"incongruity\" feature set, maximum and minimum word embedding similarities between pairs of words in a document, perform fairly well. Cattle and Ma (2017b) takes a similar approach with their word association features. The problems comes from the fact that both works compute these values across all possible pairs of words in a document.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_6",
  "x": "While this can be somewhat alleviated by judicial filtering of stopwords, this does not guarantee meaningful word pairs either. Yang et al. (2015) , in addition to their humour classifier, also introduces a method for identifying jokes' humour anchors (HAs), the \"meaningful, complete, minimal set of word spans\" that allow humour to occur. While this is slightly different from identifying a joke's setup and punchline, focusing only on pairs of HAs would help reducing noise by increasing the precision of meaningful word pairs selection without sacrificing recall. However,<cite> Yang et al. (2015)</cite> does not use their extracted HAs to improve their humour classification performance. Likely this is due to the fact that their proposed HA extraction method requires a separate, fully trained humour prediction model which is robust to word order.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_7",
  "x": "However, these models are much more complex than<cite> Yang et al. (2015)</cite> 's approach, require more training data, and suffer from a lack of interpretability. ---------------------------------- **METHODOLOGY** ---------------------------------- **DATASETS**",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_8",
  "x": "---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_9",
  "x": "**METHODOLOGY** ---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_10",
  "x": "---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs.",
  "y": "background uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_11",
  "x": "---------------------------------- **BASELINE** For our baseline we implemented our own version of<cite> Yang et al. (2015)</cite> 's highest performing classifier. This model was chosen due to its high performance and its use of statistical machine learning techniques which make it a fair point of comparison. Features include, for each document, minimum and maximum Word2Vec similarities between all word pairs, the total number of word sense combinations in each document according to WordNet, minimum and maximum WordNet path similarities between all word pairs, number of words with negative/positive polarity as well as weak/strong subjectivities according to the Wilson et al. (2005) sentiment lexicon, number of and length of longest alliteration and rhyme chains according to the CMU Pronouncing Dictionary 3 , labels of the five nearest neighbours in the training set according to word frequencies, and an averaged Word2Vec embedding across all words for a total of 318 feature dimensions.",
  "y": "extends"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_12",
  "x": "Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength. Since, as described in Section 2.1, word associations are directional, we also compute the minimum, maximum, and average associations strengths between the reverse ordered word pairs, which we refer to as the backward strength. Following Cattle and Ma (2016) , we also compute the difference between these two values on both a micro (per word) and macro (per document) level.",
  "y": "similarities"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_13",
  "x": "All Word2Vec features, including those described below, use Google's pre-trained 300 dimension Word2Vec embeddings 5 . ---------------------------------- **SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_14",
  "x": "**SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength. Since, as described in Section 2.1, word associations are directional, we also compute the minimum, maximum, and average associations strengths between the reverse ordered word pairs, which we refer to as the backward strength.",
  "y": "background similarities"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_15",
  "x": "However, we also experiment computing these features only across pairs of humour anchor (HA) words. HAs are extracted using the method described in<cite> Yang et al. (2015)</cite> using the same baseline humour model described in Section 3.2 for anchor candidate evaluation. HA extraction's requirement of a fully trained anchor candidate evaluator raises the problem of what data that evaluator should be trained on. Given that, as described in Section 3.1, we experiment on two separate humour datasets, we train the anchor candidate evaluator on the opposite dataset from the overall humour classifier. This is to avoid overfitting or biasing the anchor candidate evaluator by training on the test data.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_16",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** The results of our experiments are reported in Table 1 . In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline. One interesting aspect to note is that our model uses only 28 feature dimensions compared to<cite> Yang et al. (2015)</cite> 's 318.",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_17",
  "x": "In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline. One interesting aspect to note is that our model uses only 28 feature dimensions compared to<cite> Yang et al. (2015)</cite> 's 318. While this is not exactly a fair comparison in the case of our ML-based word association strengths (our ML strength predictor takes 415 feature dimensions as input), graph-based associations perform similarly and do truly use only 28 dimensions. Overall performance is similar across both datasets with the only notable exception being graph-based USF performing better on OL than PotD. This is likely due to OL being better suited than PotD to USF's relatively smaller set of associations (72,176 pairs and 10,617 unique words versus EAT's 325,588 and 23,218). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_18",
  "x": "The wonderfully simple extraction method described in<cite> Yang et al. (2015)</cite> only makes HAs more intriguing. Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_19",
  "x": "Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_20",
  "x": "Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance.",
  "y": "background similarities"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_21",
  "x": "We chose our baseline<cite> Yang et al. (2015)</cite> humour classifier as our anchor candidate scorer for simplicity but their HA extraction algorithm is able to work with any humour recognition model so long as it is robust to word order and capable of generating a humour score (in our case, we used humour probability). Therefore, using a more accurate humour model may have led to better performance. Another reason HAs may have hurt humour recognition performance is that it may be make nonhumorous documents more humorous. Yang et al. (2015) 's method finds the combination of humour anchor candidates that cause the largest drop humour score, i.e. by design it selects a subset of words which positively effect humour scores. As such, HAs may be more likely to be judged as humorous even if the documents they are extracted from are not.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_0",
  "x": "Natural language generation (NLG) is an important component in spoken dialog systems (SDSs). A model for NLG involves sequence to sequence learning. State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . Convolutional sequence to sequence based models have been used in the domain of machine translation but their application as natural language generators in dialogue systems is still unexplored. In this work, we propose a novel approach to NLG using convolutional neural network (CNN) based sequence to sequence learning.",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_1",
  "x": "Entrainment to users way of speaking is essential for generating more natural and high quality natural language responses. Most of the approaches for incorporating entrainment are rule-based models. Recent advances have been in the direction of developing a fully trainable context aware NLG model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . However, all these approaches are based on recurrent sequence to sequence architecture. Convolutional neural networks are largely unexplored in the domain of NLG for SDS inspite of having several advantages (Waibel et al., 1989; LeCun and Bengio, 1995) .",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_2",
  "x": "CNN reranker implements one dimensional convolution on beam search responses and generates binary vectors. These binary vectors are used to penalize the responses having missing and/or irrelevant information. We evaluate our model on the Alex Context natural language generation (NLG) dataset of <cite>Du\u0161ek and Jurcicek (2016a)</cite> and demonstrate that our model outperforms the RNNbased model of <cite>Du\u0161ek and Jurcicek (2016a)</cite> (TGen model) in automatic metrics. Training time of proposed model is observed to be significantly lower than TGen model. The main contributions of this work are (i) ConvSeq2Seq generator for NLG and (ii) CNN-based reranker for ranking n-best beam search responses for obtaining semantically appropriate responses with respect to input DA.",
  "y": "differences uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_3",
  "x": "Model proposed by <cite>Du\u0161ek and Jurcicek (2016a)</cite> serves as a baseline sequence to sequence generation model (TGen model) for SDS which takes into account the context. The model takes into account the preceding user utterance while generating natural language output. The model implemented three modifications to the model proposed by Du\u0161ek and Jurcicek (2016b) . The first modification was prepending context to the input DAs. The second modification was implementing a separate encoder for user utterances/contexts.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_4",
  "x": "The model implemented three modifications to the model proposed by Du\u0161ek and Jurcicek (2016b) . The third modification was implementing a N-gram match reranker. This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ).",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_5",
  "x": "The third modification was implementing a N-gram match reranker. This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ). In the next section, we present the proposed CNN-based sequence to sequence generator for NLG. ---------------------------------- **PROPOSED APPROACH**",
  "y": "background uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_6",
  "x": "Here, \u03c9 and W are constants. We implement the N-gram match reranker as given by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . We describe the proposed convolutional sequence to sequence generator in Section 3.1 and convolutional reranker in Section 3.2. ---------------------------------- **CONVSEQ2SEQ GENERATOR**",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_7",
  "x": "Responses having missing information and/or irrelevant information are heavily penalized. Convolutional networks are excellent feature extractors and have achieved state-of-the-art results in many text classification and sentence-level classification tasks such as sentiment analysis, question classification, etc (Kim, 2014; Kalchbrenner et al., 2014) . This classifier takes as input a natural language response and outputs a binary vector. Each element of binary vector is a binary decision on the presence of DA type or slot-value combinations. For the dataset which we have used<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> , there are 19 such classes of DA types and slot-value combinations.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_8",
  "x": "These 19 classes are shown in Figure 3 . Input DAs are converted to similar binary vector. Hamming distance between the classifier output and binary vector representation of input DA is considered as reranking penalty. The weighted reranking penalties of all the n-best responses are subtracted from their log-probabilities similar to <cite>Du\u0161ek and Jurcicek (2016a)</cite> . The architecture and working of the CNN reranker on an input instance from training dataset is shown in Figure 4 .",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_9",
  "x": "The studies in this work are performed on Alex Context natural language generation (NLG) dataset<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ). This dataset is intended for fully trainable NLG systems in task-oriented spoken dialogue systems (SDS). It is in the domain of public transport information and has four dialogue act (DA) types namely request, inform, iconfirm and inform no match. It contains 1859 data instances each having 3 target responses. Each data instance consists of a preceding context (user utterance), source meaning representation and target natural language responses/sentences.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_10",
  "x": "Each data instance consists of a preceding context (user utterance), source meaning representation and target natural language responses/sentences. Data is delexicalized and split into training, validation and test sets as done by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . For training and validation, the three paraphrases are used as separate instances. For evaluation they are used as three target references. Input to our ConvSeq2Seq generator is a DA prepended with user utterance.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_11",
  "x": "The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . For comparison, we have considered NIST (Doddington, 2002) , BLEU (Papineni et al., 2002) , METEOR (Denkowski and Lavie, 2014) , ROUGE L (Lin, 2004) and CIDEr metrics (Vedantam et al., 2015) . For this study, we have considered script \"mtevalv13a-sig.pl\" (version 13a) that implements these metrics. This script was used for E2E NLG challenge (Novikova et al., 2017) . We focus on the evaluations using this version.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_12",
  "x": "Our model has also been evaluated using the metric script \"mtevalv11b.pl\" (version 11b) to compare our results with those stated in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . The 13a version takes into account the closest reference length with respect to candidate length for calculation of brevity penalty. This is in accordance with IBM BLEU. On the contrary, 11b version takes shortest reference length for measuring brevity penalty. This is the reason behind higher BLEU scores in the 11b version when compared to 13a version.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_13",
  "x": "A slight improvement in the scores of our ConvSeq2Seq generator after using CNN reranker is seen in Table 2 except for BLEU score. We see an improvement of 6.7 BLEU points when using N-gram match reranker with \u03c9 set to 5. A decrease in scores of other metrics is seen. These inconsistencies are due to the way brevity penalty is calculated for computing BLEU scores in 11b version of metric implementation. BLEU and NIST scores of the TGen model given in Table 2 match with that represented in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> .",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_0",
  "x": "Transformer (Vaswani et al., 2017 ) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017) , language understanding (Devlin et al., 2018) , sequence prediction<cite> (Dai et al., 2019)</cite> , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) . Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence. At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full Transformer model as exemplified with neural machine translation application (Vaswani et al., 2017) : 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence. 2) Decoder self-attention considers the target sentence (e.g., predicted target sequence for translation) as input, generating a sequence of decoded representations 1 , where each decoded token depends on previous decoded tokens.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_1",
  "x": "3) Decoder-encoder attention considers both encoded and decoded sequences, generating a sequence with the same length as the decoded sequence. It should be noted that some applications has only the decoder self-attention such as sequence prediction<cite> (Dai et al., 2019)</cite> . In all cases, the Transformer's attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ).",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_2",
  "x": "At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_3",
  "x": "We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are. The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) .",
  "y": "background motivation"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_4",
  "x": "Furthermore, our proposed formulation highlights naturally the main components of Transformer's attention, enabling a better understanding of this mechanism: recent variants of Transformers (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components. Among all the components, we argue that the most important one is the construction of the kernel function. We empirically study multiple kernel forms and the ways to integrate positional embedding in neural machine translation (NMT) using IWSLT'14 GermanEnglish (De-En) dataset (Edunov et al., 2017) and sequence prediction (SP) using WikiText-103 dataset (Merity et al., 2016) . ---------------------------------- **ATTENTION**",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_5",
  "x": "The inspiration for connecting the kernel (Scholkopf and Smola, 2001 ) and attention instantiates from the observation: both operations concurrently processes all inputs and calculate the similarity between the inputs. We first introduce the background (i.e., the original formulation) of attention and then provide a new reformulation within the class of kernel smoothers (Wasserman, 2006) . Next, we show that this new formulation allows us to explore new family of attention while at the same time offering a framework to categorize previous attention variants (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) . Last, we present a new form of attention, which requires fewer parameters and empirically reaches competitive performance as the state-of-the-art models. For notation, we use lowercase representing a vector (e.g., x), bold lowercase representing a matrix (e.g., x), calligraphy letter denoting a space (e.g., X ), and S denoting a set.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_6",
  "x": "As a result, Transformer (Vaswani et al., 2017) introduced positional embedding to indicate the positional relation for the inputs. Formally, a sequence x = [x 1 , x 2 , \u22ef, x T ] defines each element as x i = (f i , t i ) with f i \u2208 F being the nontemporal feature at time i and t i \u2208 T as an temporal feature (or we called it positional embedding). Note that f i can be the word representation (in neural machine translation (Vaswani et al., 2017) ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ). t i can be a mixture of sine and cosine functions (Vaswani et al., 2017) or parameters that can be learned during back-propagation<cite> (Dai et al., 2019</cite>; Ott et al., 2019) . The feature vector are defined over a joint space X \u2236= (F \u00d7 T ).",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_7",
  "x": "Decoder self-attention further introduces a mask to block the visibility of elements in S x k to x q . Particularly, decoder self-attention considers the decoded sequence as inputs (x k = x q ), where the decoded token at time t is not allowed to access the future decoded tokens (i.e., tokens decoded at time greater than t). On the contrary, encoder selfattention and decoder-encoder attention consider no additional mask to Eq. (1). Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)).",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_8",
  "x": "On the contrary, encoder selfattention and decoder-encoder attention consider no additional mask to Eq. (1). Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)). In our paper, we aim at providing an unified view via the lens of kernel. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_9",
  "x": "where v(x k ) outputs the \"values\" and is a probability function depends on k and N when k(\u22c5, \u22c5) is always positive. In the prior work (Vaswani et al., 2017) Note that the kernel form k(x q , x k ) in the original Transformer (Vaswani et al., 2017 ) is a asymmetric exponential kernel with additional mapping W q and W k (Wilson et al., 2016; Li et al., 2017) 2 . The new formulation defines a larger space for composing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_10",
  "x": "In the following, we study these components by dissecting Eq. (2) into: 1) kernel feature space X , 2) kernel construction k(\u22c5, \u22c5), 3) value function v(\u22c5), and 4) set filtering function M (\u22c5, \u22c5). 2.2.1 Kernel Feature Space X In Eq. (2), to construct a kernel on X , the first thing is to identify the kernel feature space X . In addition to modeling sequences like word sentences (Vaswani et al., 2017) or music signals (Huang et al., 2018b) , the Transformer can also be applied to images (Parmar et al., 2018) , sets , and multimodal sequences (Tsai et al., 2019a) . Due to distinct data types, these applications admit various kernel feature space: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> : with F being non-positional feature space and T being the positional embedding space of the position in the sequence.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_11",
  "x": "For the rest of the paper, we will focus on the setting for sequence Transformer X = (F \u00d7 T ) and discuss the kernel construction on it. ---------------------------------- **KERNEL CONSTRUCTION AND THE ROLE OF** Positional Embedding k(\u22c5, \u22c5) The kernel construction on X = (F \u00d7 T ) has distinct design in variants of Transformers (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Shaw et al., 2018; Child et al., 2019) . Since now the kernel feature space considers a joint space, we will first discuss the kernel construction on F (the non-positional feature space) and then discuss how different variants integrate the positional embedding (with the positional feature space T ) into the kernel.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_12",
  "x": "Kernel construction on X = (F \u00d7 T ). The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T .",
  "y": "background uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_13",
  "x": "(i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions:",
  "y": "background uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_14",
  "x": "(i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions:",
  "y": "background uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_16",
  "x": "In contrast, Eq. (5) represents the kernel as a product of two kernels (one for f i and another for t i ), which is able to capture the similarities for both temporal and non-temporal components. (ii) Transformer-XL<cite> (Dai et al., 2019)</cite> , Music Transformer (Huang et al., 2018b) , Self-Attention with Relative Positional Embedding (Shaw et al., 2018) :",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_17",
  "x": "(iv) Decoder Self-Attention in Transformer-XL<cite> (Dai et al., 2019)</cite> : For each query x q in the decoded sequence, M (x q , S x k ) returns a set containing S 1 and additional memories (M (x q , S x k ) = S 1 + S mem , M (x q , S x k ) \u2283 S 1 ). S mem refers to additional memories. (v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance.",
  "y": "background uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_18",
  "x": "(v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_19",
  "x": "(v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_20",
  "x": "In our experiment, we find it reaching competitive performance as comparing to the current state-of-the-art designs (Eq. (5) by<cite> Dai et al. (2019)</cite> ). We fix the size of the weight matrices W \u22c5 in Eq. (9) and Eq. (5) which means we save 33% of the parameters in attention from Eq. (9) ---------------------------------- **EXPERIMENTS** By viewing the attention mechanism with Eq. (2), we aims at answering the following questions regarding the Transformer's designs:",
  "y": "similarities uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_21",
  "x": "Q4. Is positional embedding required in value function? We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> .",
  "y": "background uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_22",
  "x": "Q4. Is positional embedding required in value function? We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_23",
  "x": "For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> . For fairness of comparisons, we train five random initializations and report test accuracy with the highest validation score. We fix the position-wise operations in Transformer 3 and only change the attention mechanism. Similar to prior work (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> , we report BLEU score for NMT and perplexity for SP. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_24",
  "x": "We present the results in Table 1 . First, we see that by having PE as a look-up (Edunov et al., 2017) and SP stands for sequence prediction on WikiText-103 dataset (Merity et al., 2016) . \u2191 means the upper the better and \u2193 means the lower the better. Table 2 : Kernel Types. Other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (2017) for NMT and the configuration by<cite> Dai et al. (2019)</cite> for SP.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_25",
  "x": "34.14 24.13 24.21 table, it outperforms the case with having PE as direct-sum in feature space, especially for SP task. Note that the look-up table is indexed by the relative position (i.e., t q \u2212 t k ) instead of absolute position. Second, we see that PE in the product kernel proposed by<cite> Dai et al. (Dai et al., 2019)</cite> may not constantly outperform the other integration types (it has lower BLEU score for NMT). Our proposed product kernel reaches the best result in NMT and is competitive to the best result in SP. ----------------------------------",
  "y": "differences uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_26",
  "x": "Note that, for fairness, other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (Vaswani et al., 2017) for NMT and the configuration by<cite> Dai et al. (Dai et al., 2019)</cite> for SP. We first observe that the linear kernel does not converge for both NMT and SP. We argue the reason is that the linear kernel may have negative value and thus it violates the assumption in kernel smoother that the kernel score must be positive (Wasserman, 2006) . Next, we observe the kernel with infinite feature space (i.e., exponential and rbf kernel) outperforms the kernel with finite feature space (i.e., polynomial kernel). And we see rbf kernel performs the best for NMT and exponential kernel performs the best for SP. We conclude that the choice of kernel matters for the design of attention in Transformer.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_27",
  "x": "**ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) .",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_28",
  "x": "In the experiment, we fix the size of W \u22c5 in the kernel, and thus adopting the symmetric kernel benefits us from saving parameters. ---------------------------------- **ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic.",
  "y": "background differences"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_29",
  "x": "---------------------------------- **ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation).",
  "y": "differences"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_30",
  "x": "Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> ) is permutation equivariant. Here, we present the non-permutation-equivariant problem on the decoder self-attention: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> is not permutation equivariant. To proceed the proof, we need the following definition and propositions. ---------------------------------- **PROPOSITION 2. ATTENTION WITH THE SET FILTERING FUNC-**",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_31",
  "x": "Here, we present the non-permutation-equivariant problem on the decoder self-attention: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> is not permutation equivariant. To proceed the proof, we need the following definition and propositions. ---------------------------------- **PROPOSITION 2. ATTENTION WITH THE SET FILTERING FUNC-** Proof.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_0",
  "x": "Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009) , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically<cite> (Plank and Moschitti, 2013)</cite> . This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch\u00fctze, 2014) , named entity recognition (Daum\u00e9 III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum\u00e9 III, 2007; Daum\u00e9 III et al., 2010; Blitzer et al., 2011) , etc.",
  "y": "motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_1",
  "x": "The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch\u00fctze, 2014) , named entity recognition (Daum\u00e9 III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum\u00e9 III, 2007; Daum\u00e9 III et al., 2010; Blitzer et al., 2011) , etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_2",
  "x": "The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008) ) effectively.",
  "y": "background motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_3",
  "x": "In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu-rally and effectively. The application of word representations such as word clusters in domain adaptation of RE <cite>(Plank and Moschitti, 2013</cite> ) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains. We extend this motivation by further evaluating word embeddings (Bengio et al., 2001; Bengio et al., 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Turian et al., 2010) on feature-based methods to adapt RE systems to new domains.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_4",
  "x": "Exploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 so that it could generalize well on new domains. Eventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems. Following<cite> Plank and Moschitti (2013)</cite> , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the singlesystem DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_5",
  "x": "5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments. We apply the same feature set as Sun et al. (2011) but remove the entity and mention type information 2 .",
  "y": "uses motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_6",
  "x": "Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments. We apply the same feature set as Sun et al. (2011) but remove the entity and mention type information 2 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_7",
  "x": "We use the ACE 2005 corpus for DA experiments (as in<cite> Plank and Moschitti (2013)</cite> ). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_8",
  "x": "We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different. ---------------------------------- **EVALUATION OF WORD EMBEDDING FEATURES**",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_9",
  "x": "It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different. ----------------------------------",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_10",
  "x": "**DOMAIN ADAPTATION WITH WORD** Embeddings This section examines the effectiveness of word representations for RE across domains. We evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set. For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as<cite> Plank and Moschitti (2013)</cite> did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC). (i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_11",
  "x": "(i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our baseline performance is worse than that of<cite> Plank and Moschitti (2013)</cite> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system. However, the performance order across domains of the two baselines are the same. Besides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2).",
  "y": "differences"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_12",
  "x": "This is because the RCV1 corpus used to induce the word embeddings (Turian et al., 2010) does not cover spoken language words in cts very well. (v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations<cite> (Plank and Moschitti, 2013)</cite> . ---------------------------------- **DOMAIN ADAPTATION WITH REGULARIZATION** All the experiments we have conducted so far do not apply regularization for training.",
  "y": "differences"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_0",
  "x": "Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012) . <cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_1",
  "x": "Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012) . <cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_2",
  "x": "features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_3",
  "x": "features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_4",
  "x": "Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K\u00fcbler et al. (2009) ----------------------------------",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_5",
  "x": "We used the English (Hockenmaier and Steedman, 2007) and <cite>Hindi CCGbanks</cite> (Ambati et al., 1 http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). ---------------------------------- **SUPERTAGGERS** We used Clark and Curran (2004) 's supertagger for English, and <cite>Ambati et al. (2013)</cite> 's supertagger for Hindi.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_6",
  "x": "We used the English (Hockenmaier and Steedman, 2007) and <cite>Hindi CCGbanks</cite> (Ambati et al., 1 http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). ---------------------------------- **SUPERTAGGERS** We used Clark and Curran (2004) 's supertagger for English, and <cite>Ambati et al. (2013)</cite> 's supertagger for Hindi.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_7",
  "x": "In addition to the above mentioned features, <cite>Ambati et al. (2013)</cite> employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. ---------------------------------- **DEPENDENCY PARSERS** There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012) .",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_8",
  "x": "MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLIN-EAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002) . For Hindi, we also did all our experiments using automatic features <cite>Ambati et al. (2013)</cite> ).",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_9",
  "x": "For Hindi, we also did all our experiments using automatic features <cite>Ambati et al. (2013)</cite> ). (POS, chunk and morphological information) extracted using a Hindi shallow parser 2 . ---------------------------------- **CCG CATEGORIES AS FEATURES** Following <cite>Ambati et al. (2013)</cite> , we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_10",
  "x": "But for longer distances, 6\u221210, and >10, there was significant improvement of 1.3% and 1.3% respectively for MST. <cite>Ambati et al. (2013)</cite> reported similar improvements for Malt as well. ---------------------------------- **DISCUSSION** Though valency is a useful feature in dependency parsing (Zhang and Nivre, 2011) , Zhang and Nivre (2012) showed that providing valency information dynamically, in the form of the number of dependencies established in a particular state during parsing, did not help Malt.",
  "y": "similarities"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_11",
  "x": "---------------------------------- **CONCLUSION** We have shown that informative CCG categories, which contain both local subcategorization information and capture long distance dependencies elegantly, improve the performance of two dependency parsers, Malt and MST, by helping in recovering long distance relations for Malt and local verbal arguments for MST. This is true both in the case of English (a fixed word order language) and Hindi (free word order and morphologically richer language), extending the result of <cite>Ambati et al. (2013)</cite> . The result is particularly interesting in the case of Malt which cannot directly use valency information, which CCG categories provide indirectly.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_12",
  "x": "**ANALYSIS: HINDI** In the case of Hindi, for MST, providing CCG categories gave an increment of 0.5%, 0.4% and 0.3% for ROOT, SUBJ and OBJ labels respectively in F-score over the baseline. <cite>Ambati et al. (2013)</cite> showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1\u22125, 6\u221210 and >10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1\u22125).",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_13",
  "x": "We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories. ---------------------------------- **ANALYSIS: ENGLISH**",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_14",
  "x": "In case of Hindi, fine-grained supertags gave larger improvements for MST. We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_0",
  "x": "Riezler et al. (2014) showed how to use response-based learning to adapt an SMT system to a semantic parser for the Geoquery domain. The state-of-the-art in semantic parsing on Geoquery achieves a parsing accuracy of over 82% (see Andreas et al. (2013) for an overview), while the state-of-the-art in semantic parsing on the Free917 data (Cai and Yates, 2013) achieves 68.5% accuracy (Berant and Liang, 2014) . This is due to the lexical variability of Free917 (2,036 word types) compared to Geoquery (279 word types). In this paper, we compare different ways of scaling up state-of-the-art semantic parsers for Freebase by adding synonyms and paraphrases. First, we consider Berant and Liang (2014) 's own extension of the semantic parser of <cite>Berant et al. (2013)</cite> by using paraphrases.",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_1",
  "x": "**RELATED WORK** Our work is most closely related to Riezler et al. (2014) . We extend their application of responsebased learning for SMT to a larger and lexically more diverse dataset and show how to perform model selection in the environment from which response signals are obtained. In contrast to their work where a monolingual SMT-based approach (Andreas et al., 2013 ) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013) , <cite>Berant et al. (2013)</cite> , Goldwasser and Roth (2013) , inter alia).",
  "y": "background"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_2",
  "x": "---------------------------------- **SCALING SEMANTIC PARSING TO OPEN-DOMAIN DATABASE QUERIES** The main challenge of grounding SMT in semantic parsing for Freebase lies in scaling the semantic parser to the lexical diversity of the open-domain database. Our baseline system is the parser of <cite>Berant et al. (2013)</cite> , called SEMPRE. We first consider the approach presented by Berant and Liang (2014) to scale the baseline to open-domain database queries:",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_3",
  "x": "The translation of the English queries in FREE917 into German, in order to provide a set of source sentences for SMT, was done by the authors. The SMT framework used is CDEC (Dyer et al., 2010) with standard dense features and additional sparse features as described in Simianer et al. (2012) 4 . Training of the baseline SMT system was performed on the COMMON CRAWL 5 (Smith et al., 2013 ) dataset consisting of 7.5M parallel English-German segments extracted from the web. Response-based learning for SMT uses the code described in Riezler et al. (2014) 6 . For semantic parsing we use the SEMPRE and PARASEMPRE tools of <cite>Berant et al. (2013)</cite> and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus 7 .",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_0",
  "x": "The authors in (Malon, 2018) use TF-IDF along with exact matching of the page titles with the claim's named entities. The UCL team (Yoneda et al., 2018) highlights the pages titles, and detect them in the claims. They rank pages by logistic regression and extra features like capitalization, sentence position and token matching. Keyword matching along with page-view statistics are used in (Nie et al., 2019) . UKP-Athene <cite>(Hanselowski et al., 2018)</cite> , the highest document retrieval scoring team, uses MediaWiki API 1 to search the Wikipedia database for the claims noun phrases.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_1",
  "x": "In order to extract evidence sentences, (Thorne et al., 2018 ) use a TF-IDF approach similar to their document retrieval. The UCL team (Yoneda et al., 2018) trains a logistic regression model on a heuristically set of features. Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) with some small modifications has been used in (Nie et al., 2019;<cite> Hanselowski et al., 2018)</cite> . ESIM encodes premises and hypotheses using one Bidirectional Long Short-Term Memory (BiLSTM) with shared weights. The encoded sentences are later aligned by a bidirectional attention mechanism.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_2",
  "x": "The encoded and aligned sentences are combined, and another shared BiL-STM matches the two representations. Finally, a softmax layer classifies the max and mean pooled representations of the second BiLSTM. The UKP-Athene team <cite>(Hanselowski et al., 2018)</cite> achieved the highest sentence retrieval recall using ESIM and pairwise training. Their model takes a claim and a pair of positive and negative sentences and predicts a similarity score for each sentence. To train the model, they use a modified Hinge loss function and a random neg-ative sampling strategy.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_3",
  "x": "The Papelo team (Malon, 2018) employs transformer networks with pre-trained weights (Radford et al., 2018) . ESIM has been widely used among the FEVER challenge participants (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018)</cite> . UNC (Nie et al., 2019) , the winner of the competition, proposes a modified ESIM that takes the concatenation of the retrieved evidence sentences and claim along with ELMo embedding and three additional token-level features: Word-Net, number embedding, and semantic relatedness score from the document retrieval and sentence retrieval steps. Dream (Zhong et al., 2019) has the state of the art FEVER score. The authors use a graph reasoning method based on XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) , the two new BERT variants that are supposed to provide better pre-trained embeddings.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_4",
  "x": "---------------------------------- **DOCUMENT RETRIEVAL** In the document retrieval step, the Wikipedia documents containing the evidence supporting or refuting the claim are retrieved. Following the UKP-Athene promising document retrieval component <cite>(Hanselowski et al., 2018)</cite> , which results in more than 93% development set document recall, we exactly use their method to collect a set of top documents D c l top for the claim c l . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_5",
  "x": "In addition, we experiment with the modified Hinge loss functions like <cite>(Hanselowski et al., 2018)</cite> : At testing time, for both pairwise loss functions, we sort the sentences by their output value o and similarly choose S c l top for the claim c l . ---------------------------------- **HARD NEGATIVE MINING** The ratio of negative (non-evidence) to positive (evidence) sentences is high, thus it is not reasonable to train on all the negative samples.",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_8",
  "x": "Although the pairwise Ranknet with HNM has the best recall score, we cannot conclude that pairwise methods are necessarily better for this task. This is more clear in Figure 5 , which plots the recall-precision trade-off by applying a decision threshold on the output scores. The pointwise Model FEVER Score(%) Label Accuracy(%) DREAM (Zhong et al., 2019) 70 (Nie et al., 2019) 64.21 68.21 UCL (Yoneda et al., 2018) 62.52 67.62 UKP-Athene <cite>(Hanselowski et al., 2018)</cite> 61.58 65.46 Figure 5 : Recall and precision results on the development set. x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018</cite>; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance. Figure 5 also shows that HNM enhances both pairwise methods trained by the Ranknet and Hinge loss functions and preserves the pointwise performance.",
  "y": "differences"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_9",
  "x": "x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018</cite>; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance. Figure 5 also shows that HNM enhances both pairwise methods trained by the Ranknet and Hinge loss functions and preserves the pointwise performance. In Table 2 , we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component <cite>(Hanselowski et al., 2018)</cite> , the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predic-tions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system.",
  "y": "differences"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_0",
  "x": "For example, in Indonesian, the word aku (\"I\") can be written as: aq, akuw, akuh, q. People also tend to use non-standard words to represent named entities. This creative use of language results in numerous word variations which may increase the number out-of-vocabulary (OOV) words (Baldwin et al., 2013) . The most common approach to handle the OOV problem is by representing each OOV word with a single vector representation (embedding). However, this treatment is not optimal because it ignores the fact that words can share similar morphemes which can be exploited to estimate the OOV word embedding better. Meanwhile, word representation models based on subword units, such as characters or word segments, have been shown to perform well in many NLP tasks such as POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015) , language modeling (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017) , machine translation (Vylomova et al., 2016; Lee et al., 2016; Sennrich et al., 2016) , dependency parsing (Ballesteros et al., 2015) , and sequence labeling<cite> (Rei et al., 2016</cite>; Lample et al., 2016) .",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_1",
  "x": "In this paper, we report the ability of a neural network-based approach for Indonesian NER in conversational data. We employed the neural sequence labeling model of<cite> (Rei et al., 2016)</cite> and experimented with two word representation models: word-level and character-level. We evaluated all models on relatively large, manually annotated Indonesian conversational texts. We aim to address the following questions: 1) How do the character models perform compared to word embedding-only models on NER in Indonesian conversational texts? 2) How much can we gain in terms of performance from using the character models on OOV cases?",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_2",
  "x": "For the CRF model, we used an implementation provided by Okazaki (2007) 3 . Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_3",
  "x": "For the CRF model, we used an implementation provided by Okazaki (2007) 3 . Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_4",
  "x": "Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> . 4 In their implementation, all the LSTMs have only one layer. Dropout (Srivastava et al., 2014 ) is used as the regularizer but only applied to the final word embedding as opposed to the LSTM outputs as proposed by Zaremba et al. (2015) . The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in<cite> (Rei et al., 2016)</cite> but discussed in the subsequent work (Rei, 2017) . Thus, their implementation essentially does multi-task learning with sequence labeling as the primary task and language modeling as the auxiliary task.",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_5",
  "x": "4 In their implementation, all the LSTMs have only one layer. Dropout (Srivastava et al., 2014 ) is used as the regularizer but only applied to the final word embedding as opposed to the LSTM outputs as proposed by Zaremba et al. (2015) . The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in<cite> (Rei et al., 2016)</cite> but discussed in the subsequent work (Rei, 2017) . Thus, their implementation essentially does multi-task learning with sequence labeling as the primary task and language modeling as the auxiliary task. We used an almost identical setting to<cite> Rei et al. (2016)</cite> : words are lowercased, but characters are not, digits are replaced with zeros, singleton words in the training set are converted into unknown tokens, word and character embedding sizes are 300 and 50 respectively.",
  "y": "similarities uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_6",
  "x": "**CONCLUSION AND FUTURE WORK** We reported an empirical evaluation of neural sequence labeling models by<cite> Rei et al. (2016)</cite> on NER in Indonesian conversational texts. The neural models, even without character embedding, outperform the CRF baseline, which is a typical model for Indonesian NER. The models employing character embedding have an improvement up to 4 F 1 points compared to the word embeddingonly counterpart. We demonstrated that by using character embedding, we could gain improvement as high as 15 F 1 points on entities having OOV words.",
  "y": "uses"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_0",
  "x": "To circumvent this problem,<cite> Khapra et al. (2009)</cite> proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available. This is achieved by projecting Wordnet and corpus parameters from another language to the language in question. The approach is centered on a novel synset based multilingual dictionary (Mohanty et al., 2008) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked. For example, the word W L 1 belonging to synset S of language L 1 will be manually cross-linked to the word W L 2 of the corresponding synset in language L 2 to indicate that W L 2 is the best substitute for W L 1 according to an experienced bilingual speaker's intuition. We extend their work by addressing the following question on the economics of annotation, lexicon building and performance:",
  "y": "motivation"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_1",
  "x": "The remainder of this paper is organized as follows. In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of<cite> Khapra et al. (2009)</cite> on parameter projection for multilingual WSD. Section 5 is on the economics of multilingual WSD.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_2",
  "x": "Recent work by<cite> Khapra et al. (2009)</cite> has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available. However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language. Some similar work has been done in the area of domain adaptation where Chan et al. (2007) showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data. Similarly, Agirre and de Lacalle (2009) reported a 22% error reduction when source and target data were combined for training a classifier, compared to the case when only the target data was used for training the classifier. However, such combining of training statistics has not been tried in cases where the source data is in one language and the target data is in another language.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_3",
  "x": "The second factor is the cost of sense annotated data from the target language. The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself. The work of<cite> Khapra et al. (2009)</cite> as described above does not attempt to reach an optimal costbenefit point in this economic system. They place their bets on manual cross-linking only and settle for the accuracy achieved thereof. Specifically, they do not explore the inclusion of small amount of annotated data from the target language to boost the accuracy (as mentioned earlier, supervised systems which use annotated data from the target language are known to perform better).",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_4",
  "x": "Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by<cite> Khapra et al. (2009)</cite> . ---------------------------------- **OPTIMAL CROSS-LINKING** As mentioned earlier, in some cases where bilingual lexicographers are expensive we might be interested in reducing the effort of manual crosslinking. For such situations, we propose that only a small number of words, comprising of the most frequently appearing ones should be manually cross linked and the rest of the words should be cross-linked using a probabilistic model.",
  "y": "extends differences"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_5",
  "x": "As mentioned earlier, in some cases where bilingual lexicographers are expensive we might be interested in reducing the effort of manual crosslinking. For such situations, we propose that only a small number of words, comprising of the most frequently appearing ones should be manually cross linked and the rest of the words should be cross-linked using a probabilistic model. The rationale here is simple: invest money in words which are bound to occur frequently in the test data and achieve maximum impact on the accuracy. In the following paragraphs, we explain our probabilistic cross linking model. The model proposed by <cite>Khapra et al. (2009</cite> ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_6",
  "x": "The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine. We used the same dataset as described in<cite> Khapra et al. (2009)</cite> for all our experiments. The data was collected from two domains, viz., Tourism and Health. The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi. Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi.",
  "y": "similarities uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_0",
  "x": "In this paper, we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by <cite>Rudolph and Giesbrecht (2010)</cite> , and show a close correspondence between this matrix-space model and weighted finite automata. We conclude that the problem of learning compositional matrix-space models can be mapped to the problem of learning weighted finite automata over the real numbers. ---------------------------------- **INTRODUCTION** Quantitative models of language have recently received considerable research attention in the field of Natural Language Processing (NLP).",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_1",
  "x": "To overcome the limitations of VSMs, <cite>Rudolph and Giesbrecht (2010)</cite> proposed Compositional Matrix-Space Models (CMSM) as a recent alternative model to work with distributional approaches. These models employ matrices instead of vectors and make use of iterated matrix multiplication as the only composition operation. They show that these models are powerful enough to subsume many known models, both quantitative (vector-space models with diverse composition operations) and qualitative ones (such as regular languages). It is also proved theoretically that this framework is an elegant way to model compositional, symbolic and distributional aspects of natural language. However, in practical cases, methods are needed to automatically acquire the token-to-matrix assignments from available data.",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_2",
  "x": "In this paper, we are concerned with Graded Matrix Grammars, a variant of the Matrix Grammars of <cite>Rudolph and Giesbrecht (2010)</cite> , where instead of the \"yes or no\" decision, if a sequence is part of a language, a real-valued score is assigned. This is a popular task in NLP, used, e.g., in sentiment analysis settings (Yessenalina and Cardie, 2011) . Generally, in many tasks of NLP, we need to estimate functions which map arbitrary sequence of words (e.g. sentences) to some semantical space. Using Weighted Finite Automata (WFA), an extensive class of these functions can be defined, which assign values to these sequences (Balle and Mohri, 2012) . Herein, inspired by the definition of weighted finite automata (Sakarovitch, 2009) and their applications in NLP (Knight and May, 2009 ), we show a tight correspondence between graded matrix grammars and weighted finite automata.",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_4",
  "x": "**COMPOSITIONALITY AND COMPOSITIONAL MATRIX-SPACE MODEL** The general principle of compositionality is that the meaning of a complex expression is a function of the meaning of its constituent tokens and some rules used to combine them (Frege, 1884) . More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers. The mapping function \u00b7 maps the tokens into matrices so that the semantics of simple tokens is expressed by matrices.",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_5",
  "x": "More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers. The mapping function \u00b7 maps the tokens into matrices so that the semantics of simple tokens is expressed by matrices. Then, using the standard matrix multiplication as the only composition operation , the semantics of complex phrases are also described by matrices. <cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models).",
  "y": "uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_6",
  "x": "Then, using the standard matrix multiplication as the only composition operation , the semantics of complex phrases are also described by matrices. <cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models). ---------------------------------- **GRADED MATRIX GRAMMARS AND WEIGHTED FINITE AUTOMATA** In some applications of NLP, we need to derive the meaning of a sequence of words in a language, which can be done with CMSMs as described in Section 2.2.",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_7",
  "x": "<cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models). ---------------------------------- **GRADED MATRIX GRAMMARS AND WEIGHTED FINITE AUTOMATA** In some applications of NLP, we need to derive the meaning of a sequence of words in a language, which can be done with CMSMs as described in Section 2.2. In this section, we introduce the notion of a graded matrix grammar which constitutes a slight variation of matrix grammars as introduced by <cite>Rudolph and Giesbrecht (2010)</cite> .",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_8",
  "x": "---------------------------------- **RELATED WORK** An application of CMSM has been shown in the work of Yessenalina and Cardie (2011) . They proposed a learning-based approach for phraselevel sentiment analysis. Inspired by the work of <cite>Rudolph and Giesbrecht (2010)</cite> they use CMSMs to model composition, and present an algorithm for learning a matrix for each word via ordered logistic regression, which is evaluated with promising results.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_0",
  "x": "Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. <cite>Xiang et al. (2013)</cite> formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_1",
  "x": "<cite>Xiang et al. (2013)</cite> formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012) , which is a recent development. As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method<cite> (Xiang et al., 2013)</cite> for Chinese empty category detection as well as linguistically-motivated manually written rule-based method similar to (Campbell, 2004 ). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_2",
  "x": "We also use<cite> Xiang et al's (2013)</cite> model as another baseline. It formulates empty category detection as the classification of IP nodes. For example, in Figure 1 , empty nodes in the left tree are removed and encoded as additional labels with its position information to IP nodes in the right tree. As we can uniquely decode them from the extended IP labels, the problem is to predict the labels for the input tree that has no empty nodes. Let T = t 1 t 2 \u00b7 \u00b7 \u00b7 t n be the sequence of nodes produced by the post-order traversal from root node, and e i be the empty category tag associated with t i .",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_3",
  "x": "Let T = t 1 t 2 \u00b7 \u00b7 \u00b7 t n be the sequence of nodes produced by the post-order traversal from root node, and e i be the empty category tag associated with t i . The probability model of<cite> (Xiang et al., 2013)</cite> is formulated as MaxEnt model: where \u03c6 is a feature vector, \u03b8 is a weight vector to \u03c6 and Z is normalization factor: where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_4",
  "x": "where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the decisions on IPs in lower levels depend on those on higher levels in the tree. Second, empty category features are extracted from ancestor IP nodes, not from descendant IP nodes, in accordance with the first change.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_5",
  "x": "where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the decisions on IPs in lower levels depend on those on higher levels in the tree. Second, empty category features are extracted from ancestor IP nodes, not from descendant IP nodes, in accordance with the first change.",
  "y": "extends"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_6",
  "x": "Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the<cite> (Xiang et al., 2013)</cite> with ablation test. We find that the conjunction features left-sibling label or POS tag (up to two siblings) 10 right-sibling label or POS tag (up to two siblings) Lexical features 11 left-most word under the current node 12 right-most word under the current node 13 word immediately left to the span of the current node 14 word immediately right to the span of the current node 15 head word of the current node 16 head word of the parent node 17 is the current node head child of its parent? (binary) Empty category features 18 predicted empty categories of the left sibling 19* the set of detected empty categories of ancestor nodes Conjunction features 20 current node label with parent node label 21* current node label with features computed from ancestor nodes 22 current node label with features computed from leftsibling nodes 23 current node label with lexical features<cite> (Xiang et al., 2013)</cite> 68.2 \u22120.40 modified<cite> (Xiang et al., 2013)</cite> 68.6 -\u2212 Tree label 68.6 \u22120.00 \u2212 Empty category 68.3 \u22120.30 \u2212 Lexicon 68.6 \u22120.00 \u2212 Conjunction 58.5 \u221210.1 Table 2 : Ablation result of<cite> (Xiang et al., 2013)</cite> are highly effective compared to the three other features. This observation leads to the model proposed in the next section. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_8",
  "x": "We evaluated them using the word-position-level identification metrics described in<cite> (Xiang et al., 2013)</cite> . It projects the predicted empty category tags to the surface level. An empty node is regarded as correctly predicted surface position in the sentence, type (T or pro) and function (SBJ, OB1 and so on) are matched with the reference. To evaluate the effectiveness of the proposed 1 There are two models available in HARUNIWA, namely the BitPar model (Schmid, 2004) and Berkeley Parser binary branching model (Petrov and Klein, 2007) . The output of the later is first flattened, then added disambiguation tags and empty categories using tsurgeon script (Levy and Andrew, 2006) .",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_9",
  "x": "In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. Among the proposed models, the combination of path feature and child feature (PATH \u00d7 CHILD) even outperformed the baselines.",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_10",
  "x": "Table 4 shows the accuracies of various empty category detection methods, for both gold parse and system parse. In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. We also implemented the third baseline based on (Johnson, 2002) . Minimal unlexicalized tree fragments from empty node to its antecedent were extracted as pattern rules based on corpus statistics. For *pro*, which has no antecedent, we used the statistics from empty node to the root.",
  "y": "uses"
 },
 {
  "id": "27aeffca1f7a9a6b40743284a2871d_0",
  "x": "It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models. After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases. The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (Johnson et al., 2007) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (Johnson, 2008), but it has been shown to have applications in text data mining and information retrieval as well <cite>Hardisty et al., 2010</cite>) . We'll see how learning the referents of words and learning the roles of social cues in language acquisition (Johnson et al., 2012) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk. ----------------------------------",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_0",
  "x": "Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b</cite>; Boltu\u017ei\u0107 and\u0160najder, 2014; Peldszus and Stede, 2015b) . Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014) . The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in \u00a75.3) can help characterize the argumentative relations that connect pairs of argument components. Reconsidering the example in Figure 1 , without knowing the content \"horrendous images are displayed on the cigarette boxes\" in sentence 3, one cannot easily tell that \"reduction in the number of smokers\" in sentence 4 supports the \"pictures can influence\" claim in sentence 2. We expect that such content relatedness can be revealed from a discourse analysis, e.g., the appearance of a discourse connective \"As a result\".",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_1",
  "x": "We also derive features using an argument and domain word lexicon automatically created by post-processing an essay's topic model. Experimental results show that our proposed contextual features help significantly improve performance in two argumentative relation classification tasks. ---------------------------------- **RELATED WORK** Unlike argument component identification where textual inputs are typically sentences or clauses (Moens et al., 2007;<cite> Stab and Gurevych, 2014b</cite>; Levy et al., 2014; Lippi and Torroni, 2015) , textual inputs of argumentative relation mining vary from clauses (Stab and Gurevych, 2014b; Peldszus, 2014 ) to multiple-sentences (Biran and Rambow, 2011; Cabrio and Villata, 2012; Boltu\u017ei\u0107 an\u010f Snajder, 2014) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_2",
  "x": "Qazvinian and Radev (2010) used the term \"context sentence\" to refer to sentences surrounding a citation that contained information about the cited source but did not explicitly cite it. In our study, we only require that the context sentences of an argument component must be in the same paragraph and adjacent to the component. Prior work in argumentative relation mining has used argument component labels to provide constraints during relation identification. For example, when an annotation scheme (e.g., (Peldszus and Stede, 2013; Stab and Gurevych, 2014a) ) does not allow relations from claim to premise, no relations are inferred during relation mining for any argument component pair where the source is a claim and the target is a premise. In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_3",
  "x": "In our study, we only require that the context sentences of an argument component must be in the same paragraph and adjacent to the component. Prior work in argumentative relation mining has used argument component labels to provide constraints during relation identification. For example, when an annotation scheme (e.g., (Peldszus and Stede, 2013; Stab and Gurevych, 2014a) ) does not allow relations from claim to premise, no relations are inferred during relation mining for any argument component pair where the source is a claim and the target is a premise. In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining. We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> .",
  "y": "differences"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_4",
  "x": "We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) . Nguyen and Litman (2015) post-processed LDA (Blei et al., 2003) output to extract a lexicon of argument and domain words from development data. Their semi-supervised approach exploits the topic context through essay titles to guide the extraction. Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_5",
  "x": "Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. ---------------------------------- **PERSUASIVE ESSAY CORPUS** Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available.",
  "y": "similarities"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_6",
  "x": "---------------------------------- **PERSUASIVE ESSAY CORPUS** Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available. 3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016) , we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt the model developed by <cite>Stab and Gurevych (2014b)</cite> to serve as the baseline for evaluating our proposed approach. Three experts identified possible argument components of three types within each sentence in the corpus (MajorClaim -writer's stance toward the writing topic, Claim -controversial statements that support or attack MajorClaim, and Premiseevidence used to underpin the validity of Claim), and also connected the argument components using two argumentative relations (Support and Attack).",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_7",
  "x": "The three experts achieved inter-rater accuracy 0.88 for component labels and Krippendorff's \u03b1 U 0.72 for component boundaries. Given the annotated argument components, the three experts obtained Krippendorff's \u03b1 0.81 for relation labels. The number of relations are shown in Table 1. 4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_8",
  "x": "4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions. We use this split to train and test our proposed models, and directly compare our models' performance to the reported performance in<cite> (Stab and Gurevych, 2014b)</cite> . ---------------------------------- **TASK 2: SUPPORT VS. ATTACK**",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_9",
  "x": "For this task, we assume that the relation (i.e., attachment (Peldszus, 2014) ) between two components is given, and aim at identifying the argumentative function of the relation. Because we remove the paragraph constraint in this task, we obtain more Support relations than in Task 1. As shown in Table 1 , of the total 1473 relations, we have 1312 (89%) Support and 161 (11%) Attack relations. Because this task was not studied in<cite> (Stab and Gurevych, 2014b)</cite> , we adapt Stab and Gurevych's model to use as the baseline. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_10",
  "x": "**BASELINE** We adapt (Stab and Gurevych, 2014b ) to use as a baseline for evaluating our approach. Given a pair of argument components, we follow<cite> (Stab and Gurevych, 2014b)</cite> by first extracting 3 feature sets: structural (e.g., word counts, sentence position), lexical (e.g., word pairs, first words), and grammatical production rules (e.g., S\u2192NP,VP). Because a sentence may have more than one argument component, the relative component positions might provide useful information (Peldszus, 2014) . Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_11",
  "x": "Because a sentence may have more than one argument component, the relative component positions might provide useful information (Peldszus, 2014) . Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions. <cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_12",
  "x": "<cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_13",
  "x": "We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych. For later presentation purposes, we name the set of all features from this section except word pairs and production rules as the common features.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_14",
  "x": "<cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych. For later presentation purposes, we name the set of all features from this section except word pairs and production rules as the common features. While word pairs and grammatical production rules were the most predictive features in<cite> (Stab and Gurevych, 2014b)</cite> , we hypothesize that this large and sparse feature space may have negative impact on model robustness (Nguyen and Litman, 2015) . Most of our proposed models replace word pairs and production rules with different combinations of new contextual features.",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_16",
  "x": "Thus, we use relations at type-level as features. For RST-DTB relations, we use only relation labels, but ignore the nucleus and satellite labels of components as they do not provide more information given the component order in the pair. Because temporal relations were shown not helpful for argument mining tasks (Biran and Rambow, 2011;<cite> Stab and Gurevych, 2014b)</cite> , we exclude them here. Discourse marker: while the baseline model only considers discourse markers within the argument components, we define a boolean feature for each discourse marker classifying whether the marker is present before the covering sentence of the source and target components or not. This implementation aims to characterize the discourse of the preceding and following text segments of each argument component separately.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_17",
  "x": "window-size in range [0, 8] 8 that yields the best F1 score in 10-fold cross validation. We use the training set as determined in<cite> (Stab and Gurevych, 2014b)</cite> to train/test 9 the models using LibLINEAR algorithm (Fan et al., 2008) without parameter or feature optimization. Cross-validations are conducted using Weka (Hall et al., 2009) . We use Stanford parser (Klein and Manning, 2003) to perform text processing. As shown in Figure 4 , while increasing the window-size from 2 to 3 improves performance (significantly), using window-sizes greater than 3 does not gain further improvement.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_18",
  "x": "We also compare our baseline to the reported performance (REPORT) for Support vs. Non-support classification in<cite> (Stab and Gurevych, 2014b)</cite> . The learning algorithm with parameters are kept the same as in the window-size tuning experiment. Given the skewed class distribution of this data, Accuracy and F1 of Non-support (the major class) are less important than Kappa, F1, and F1 of Support (the minor class). To conduct T-tests for performance significance, we split the test data into subsets by essays' ID, and record prediction performance for individual essays. We first notice that the performances of our baseline model are better than (or equal to) RE-PORTED, except the Macro Recall.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_0",
  "x": "State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010) , paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1 . Recent approaches to this task have been based on slot-filling (Yang et al., 2011;<cite> Elliott and Keller, 2013)</cite> , combining web-scale ngrams , syntactic tree substitution (Mitchell et al., 2012) , and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013) . Image description has been compared to translating an image into text or summarising an image",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_1",
  "x": "Spearman's \u03c1 is a non-parametric correlation coefficient that restricts the ability of outlier data points to skew the co-efficient value. The automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness. ---------------------------------- **DATA** We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013) , and the data set of <cite>Elliott and Keller (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_2",
  "x": "The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013) . Each image-description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1-4. We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_3",
  "x": "The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. ----------------------------------",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_4",
  "x": "We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_5",
  "x": "More formally, Unigram BLEU without a brevity penalty has been reported by ), Ordonez et al. (2011 , and Kuznetsova et al. (2012) ; to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is <cite>Elliott and Keller (2013)</cite> . In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE-SU*. The skip-bigram calculation is parameterised with d skip , the maximum number of tokens between the words in the skip-bigram.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_6",
  "x": "Unigram BLEU is also only weakly correlated against human judgements, even though it has been reported extensively for this task. Finally, Meteor is most strongly correlated measure against human judgements. A similar pattern is observed in the <cite>Elliott and Keller (2013)</cite> data set, though the correlations are lower across all measures. This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set. Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness.",
  "y": "similarities"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_7",
  "x": "In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but <cite>Elliott and Keller (2013)</cite> report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_8",
  "x": "We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges. ---------------------------------- **CONCLUSIONS** In this paper we performed a sentence-level correlation analysis of automatic evaluation measures against expert human judgements for the automatic image description task.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_0",
  "x": "Another line of research aims at eliciting semantic similarity measures directly from freely available corpora, based on the distributional similarity assumption (Harria, 1968) . In this domain, vector-space methods give state-ofthe-art performance (Pad\u00f3 and Lapata, 2007) . Previously, a graph based framework has been proposed that models word semantic similarity from parsed text <cite>(Minkov and Cohen, 2008)</cite> . The underlying graph in this case describes a text corpus as connected dependency structures, according to the schema shown in Figure 1 . The toy graph shown includes the dependency analysis of two sentences: \"a major environmental disaster is under way\", and \"combat the environmental catastrophe\".",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_1",
  "x": "Notably, different edge types, as well as the paths traversed, may have varying importance for different types of similarity sought. For example, in the parsed text domain, noun similarity and verb similarity are associated with different syntactic phenomena (Resnik and Diab, 2000) . To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> . PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_2",
  "x": "PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework. Our results are encouraging: the PCW model yields superior results to the dependency vectors approach. Further, we show that learning specialized similarity measures per word type (nouns, verbs and adjectives) is preferable to applying a uniform model for all word types.",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_3",
  "x": "For example, in the parsed text domain, noun similarity and verb similarity are associated with different syntactic phenomena (Resnik and Diab, 2000) . To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> . PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_4",
  "x": "Further, we show that learning specialized similarity measures per word type (nouns, verbs and adjectives) is preferable to applying a uniform model for all word types. ---------------------------------- **PATH CONSTRAINED GRAPH WALKS** PCW is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences (paths) <cite>(Minkov and Cohen, 2008)</cite> . In this approach, rather than assume fixed (possibly, uniform) edge weight parameters \u0398 for the various edge types in the graph, the probability of following an edge of type \u2113 from node x is evaluated dynamically, based on the history of the walk up to x.",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_5",
  "x": "The path trees were constructed using the paths leading to the node known to be a correct answer, as well as to the otherwise irrelevant top-ranked 10 terms. We required the paths considered by PCW to include exactly 6 segments (edges). Such paths represent distributional similarity phenomena, allowing a direct comparison against the DV method. In conducting the constrained walk, we applied a threshold of 0.5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work <cite>(Minkov and Cohen, 2008)</cite> . We implemented DV using code made available by its authors, 3 where we converted the syntactic patterns specified to Stanford dependency parser conventions.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_0",
  "x": "There are also many research conducted on automatic detection and prevention of cyberbullying, which we will address in more details in the next section, but this problem is still far from resolved and there is the need for further improvements towards having a concrete solution. Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_1",
  "x": "In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value. We expand our work by re-implementing the models on a new dataset.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_2",
  "x": "Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_3",
  "x": "Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. We expand our work by re-implementing the models on a new dataset.",
  "y": "extends"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_4",
  "x": "In this study, we have first reproduced the experiments conducted in <cite>[11]</cite> on the datasets used by the authors namely, Formspring [12] , Wikipedia [14] , and Twitter [13] . We have used the same models and experimental setup for our implementations. In this section, we have briefly introduced the datasets and explained the models and other experiment components. For further details please see the reference literature. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_5",
  "x": "---------------------------------- **TRANSFER LEARNING** Transfer learning is the process of using a model which has been trained on one task for another related task. Following <cite>[11]</cite> we also implemented the transfer learning procedure to evaluate to what extent the DNN models trained on a social network, here Twitter, Formspring, and Wiki, can successfully detect cyberbullying posts in another social network, i.e., YouTube. For this purpose we used the BLSTM with attention model and experimented with three different approaches.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_6",
  "x": "YouTube, Formspring and (Table 6) was not significant compared to the feature level learning approach. This indicates that the transfer of network weights is not as essential to cyberbullying detection as the learned word embeddings. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this study, we successfully reproduced the reference literature <cite>[11]</cite> for detection of cyberbullying incidents in social media platforms using DNN based models.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_0",
  "x": "Fu et al. (2018) explore two models for style transfer which use multiple decoders or style embeddings to augment the encoded representations. <cite>Prabhumoye et al. (2018)</cite> propose to transfer style through backtranslation. The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer. We introduce two approaches which extend the back-translation models proposed by <cite>Prabhumoye et al. (2018)</cite> exploring back-translation setups that preserve the content of the sentence better.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_1",
  "x": "Fu et al. (2018) explore two models for style transfer which use multiple decoders or style embeddings to augment the encoded representations. <cite>Prabhumoye et al. (2018)</cite> propose to transfer style through backtranslation. The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer. We introduce two approaches which extend the back-translation models proposed by <cite>Prabhumoye et al. (2018)</cite> exploring back-translation setups that preserve the content of the sentence better.",
  "y": "extends"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_3",
  "x": "---------------------------------- **GROUNDING MEANING IN BACK-TRANSLATION** While the previous work (<cite>Prabhumoye et al., 2018</cite>) focuses on creating a representation by translating to a pivot language, preserving meaning in the generated sentences is still an unsolved question. In this work, we try to tackle this question by extending their model in two directions: (1) To improve the latent representation such that it grounds the meaning better and (2) Providing the generative models with a feedback which represents how good the generator performs in preserving the meaning.",
  "y": "motivation"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_4",
  "x": "We denote the sentences of X 1 transferred to style s 2 asX 12 = {x (1) 12 , . . . ,x (n) 12 } and the sentences of X 2 transferred to style s 1 b\u0177 Style transfer through Back-translation. <cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_5",
  "x": "<cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_6",
  "x": "<cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation. Johnson et al. (2017) showed that multi-lingual neural machine translation systems using one-to-many and many-to-one frameworks can perform zero-shot learning.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_7",
  "x": "<cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation. Johnson et al. (2017) showed that multi-lingual neural machine translation systems using one-to-many and many-to-one frameworks can perform zero-shot learning.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_8",
  "x": "We have trained a one to many translation system (Johnson et al., 2017) where we have a encoder-decoder network for one source language and two target languages. We also train a many to one translation system (Johnson et al., 2017) where we have a encoder-decoder network for two source languages and one target language. We use these translation systems for training the style specific decoders following the procedure in (<cite>Prabhumoye et al., 2018</cite>) . Specifically, a sentence is first translated from English to two pivot languages. We create the latent representation of the sentence by encoding the sentence in both pivot languages using the many to one translation system.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_9",
  "x": "---------------------------------- **STYLE TRANSFER TASKS** We use three tasks described in (<cite>Prabhumoye et al., 2018</cite>) to evaluate our models. The three tasks correspond to: (1) gender transfer: we transfer the style of writing reviews of Yelp from male and female authors (Reddy and Knight, 2016) . (2) political slant transfer: we transfer the style of addressing comments to the two political parties namely democratic and republican (Voigt et al., 2018) and (3) sentiment modification: here we focus on only two sentiments -positive and negative.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_11",
  "x": "**EVALUATION TASKS** Style Transfer Accuracy. We measure the accuracy of style transfer as described in (Shen et al., 2017) . We have reproduced the classifiers described in (<cite>Prabhumoye et al., 2018</cite>) . The classifier has an accuracy of 82% for the gender- annotated corpus, 92% accuracy for the political slant dataset and 93.23% accuracy for the sentiment dataset.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_12",
  "x": "We create separate language models for each of the three tasks using only the training data. We use only ngrams up to an order of 3 to create the language model 2 . Meaning Preservation. We follow the procedure described in (Bennett, 2005) to perform A/B testing. We reuse the instructions provided by (<cite>Prabhumoye et al., 2018</cite>) for the three tasks. But unlike (<cite>Prabhumoye et al., 2018</cite>), we perform our evaluation on Amazon Mechanical Turk.",
  "y": "differences uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_14",
  "x": "We annotate 200 samples per task and ask 3 unique workers to annotate each sample. We take the majority vote as the final label. The results in (<cite>Prabhumoye et al., 2018</cite>) were reproduced for comparing the CAE model with the <cite>BST</cite> model. As reported by <cite>them</cite>, the <cite>BST</cite> model performs better in preservation of meaning for the tasks of gender and political slant transfer. We present the results for the comparison between <cite>BST</cite> and MBST models; and the MBST and the MBST+F models.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_16",
  "x": "MBST+F performs better than the other models in 2 out of 3 tasks and MBST performs the best in one task -political slant. The over-all averaged scores for the two models MBST and MBST+F is the same 3.08, whereas it is much lower 2.79 for <cite>BST</cite> and 2.57 for CAE. Table 4 : Fluency in generated sentences. ---------------------------------- **MODEL**",
  "y": "differences"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_0",
  "x": "Finally, Section 5 describes conclusions. ---------------------------------- **RELATED WORK** Machine Learning methods have been widely applied for sentiment analysis (Pang et al. 2008;<cite> Pang et al. 2002</cite>; Tan et al. 2008 ). Pang and Lee (2004) experimented with various features like unigrams, bi-grams and adjectives for sentiment classification of movie reviews using different machine learning algorithms namely Na\u00efve Bayes (NB), Support Vector Machines (SVM), and Maximum-Entropy (ME).",
  "y": "background"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_1",
  "x": "We used product reviews of books, DVD and electronics for experiments. Each domain has 1000 positive and 1000 negative labelled reviews. Documents are initially pre-processed as follows: (i) Negation handling is performed as<cite> Pang et al. (2002)</cite> , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence. (ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text.",
  "y": "similarities uses"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_2",
  "x": "Each domain has 1000 positive and 1000 negative labelled reviews. Documents are initially pre-processed as follows: (i) Negation handling is performed as<cite> Pang et al. (2002)</cite> , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence. (ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text. In addition, there is no need of using separate discretisation method in case of binary weighting scheme as required by RSAR feature selection algorithm.",
  "y": "background motivation"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_3",
  "x": "Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers are the mostly used for sentiment classification <cite>(Pang et al. 2002</cite>; Tan et al. 2008) . Therefore, we report the classification results of SVM and NB classifier for classifying review documents into positive or negative sentiment polarity. For the evaluation of proposed methods 10 fold cross validation method is used. Fmeasure value is reported as a performance measure of various classifiers (Agarwal et al. 2013) ----------------------------------",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_0",
  "x": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000;<cite> Henderson, 2003)</cite> have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. (Gildea and Jurafsky, 2002 ) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. They use learning features such as phrase type, position, voice, and parse tree path.",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_1",
  "x": "(See (Hale and Keyser, 1993; Levin and Rappaport Hovav, 1995) among many others.) If the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure, and that learning semantic role labels at the same time as parsing will be beneficial to parsing accuracy. We present work to test the hypothesis that a current statistical parser <cite>(Henderson, 2003)</cite> can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where more complex labels comprising both syntactic labels and semantic roles are taken into account. These results have several consequences. First, we show that it is possible to build a single integrated system successfully.",
  "y": "uses"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_2",
  "x": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of state-of-the-art history-based statistical parsers, the Simple Synchrony Network (SSN) parsers <cite>(Henderson, 2003)</cite> , which use a form of left-corner parse strategy to map parse trees to sequences of derivation steps. These parsers do not impose any a priori independence assumptions, but instead smooth their parameters by means of the novel SSN neural network architecture. This architecture is capable of inducing a finite history representation of an unbounded sequence of derivation steps, which we denote is computed from a set f of handcrafted features of the derivation move d i\u22121 , and from a finite set D of recent history representations h(d 1 , . . . , d j ), where j < i \u2212 1. Because the history representation computed for the move i \u2212 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.",
  "y": "uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_0",
  "x": "For instance, the utterance \"The diseases that vaccination can protect you from are horrible\" expresses a stance for the pre-chosen target \"vaccination\", while expressing a negative sentiment towards the sentiment-target \"diseases\". ---------------------------------- **BACKGROUND** This definition of stance is used in several stance detection studies. For instance, in studies performed on the text genres web debate forums (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Hasan and Ng, 2013) , news paper text (Ferreira and Vlachos, 2016; Fake News Challenge, 2017) and tweets (Augenstein et al., 2016;<cite> Mohammad et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_1",
  "x": "Stance detection is generally considered more difficult than sentiment analysis and thereby a task for which currently available methods achieve lower results. This was, for instance, shown by a recent shared task on three-category stance classification of tweets, where an F-score of 0.59 was achieved by a classifier that outperformed submissions from 19 shared task teams <cite>(Mohammad et al., 2017)</cite> . For the task of stance classification of posts of two-sided discussion threads, an F-score of 0.70 is the best result we have been able to find in previous research (Hasan and Ng, 2013) Previous studies on attitudes towards vaccination do not make use of the term stance, but discuss negative/positive sentiment towards vaccination. There are a number of such sentiment detection studies conducted on tweets, while studies on online forums, to the best of our knowledge, are limited to the task of topic modelling (Tangherlini et al., 2016) .",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_2",
  "x": "**ANNOTATION** The 1,190 posts to include in the experiment were presented for manual classification in a random order, without revealing who the debater was or which thread the post belonged to. The annotation was performed by one of the authors of the paper. Following the principle of the guidelines by<cite> Mohammad et al. (2017)</cite> , we classified the posts as taking a stance against or for vaccination, or to be undecided. The third category was applied to posts in which the debater explicitly declared to be undecided, as well as to posts for which the debater's stance towards vaccination could not be determined.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_3",
  "x": "This follows the approach of<cite> Mohammad et al. (2017)</cite> , as well as of many of the previously performed vaccine sentiment studies. The model was trained on all tokens in the training data, as well as on 2-, 3-and 4-grams that occurred at least twice in the data. The standard NLTK stop word list for English was used for removing non-content words when constructing one set of n-grams. An additional set of n-grams was generated with a reduced version of this stop word list, which mainly consisted of articles, forms of copula, and forms of \"it\", \"have\" and \"do\". The reason for using a reduced list was that negations, pronouns etc.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_4",
  "x": "Our decision, to classify debate posts as against vaccination when they opposed an official vaccination policy, was based on that debaters often implicitly argue against such a policy. In addition, a system for surveilling increases in vaccine hesitancy is likely to take an official policy as its point of departure. The eight annotators that classified each tweet in the study by<cite> Mohammad et al. (2017)</cite> were employed through a crowdsourcing platform, which was made possible by that the stance targets were chosen with the criterion that they should be commonly known in the United States. For annotating stance on vaccination, however, annotators with some amount of prior knowledge of vaccine debate topics and vaccine controversies might be preferred. Crowdsourcing might therefore not be a viable option for this annotation task.",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_5",
  "x": "---------------------------------- **MACHINE LEARNING DECISIONS** The choice of machine learning model was primarily based on that a linear support vector machine was successful on data from the previously mentioned shared task of stance detection of tweets <cite>(Mohammad et al., 2017)</cite> . This model outperformed submissions from teams that used methods which might intuitively be better adapted to the task, i.e., an LSTM classifier (Augenstein et al., 2016) . Support vector machines have also been used in many of the previous vaccine sentiment studies.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_6",
  "x": "For instance, features constructed using an arguing lexicon (Somasun-daran and Wiebe, 2010), or word embeddings constructed in an unsupervised fashion using a large corpus from the same text genre as the text to classify <cite>(Mohammad et al., 2017)</cite> . Apart from making a decision on what type of classifier and features to use, it must also be decided on how to gather more training data. Strategies for reducing the manual labelling effort should be investigated, in particular since the annotation task, as discussed above, might not be suitable for crowdsourcing. One possible approach would be to use weakly supervised data. Posts from vaccine-related discussion threads contrasted with posts from other discussion threads might be used as weakly supervised data for classifying posts as either taking a stance on vaccination or being undecided.",
  "y": "similarities"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_0",
  "x": "Since its publication in 2016, SQuAD has been targeted by many research groups, and the proposed models are gradually approaching (even overcoming) human-level performances. All but a few of these models rely on Recurrent Neural Networks (RNNs), which currently dominate the stateof-the-art in most Natural Language Processing (NLP) tasks. However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_1",
  "x": "All but a few of these models rely on Recurrent Neural Networks (RNNs), which currently dominate the stateof-the-art in most Natural Language Processing (NLP) tasks. However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context. Inspired by the positive results of Vaswani et al. in machine translation, we have applied a similar architecture to the domain of question-answering, a model that we have named Fully Attention-Based Information Retriever (FABIR).",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_2",
  "x": "We validated our model in the SQuAD dataset, which proved that FABIR not only achieves competitive results (F1:77.6%, EM:67.7%) but also has fewer parameters and is faster at both training and testing times than competing methods. Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible: \u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). \u2022 Column-wise cross-attention: we modify the crossattention operation by <cite>[2]</cite> and propose a new technique that is better suited to question-answering.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_3",
  "x": "Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible: \u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). \u2022 Column-wise cross-attention: we modify the crossattention operation by <cite>[2]</cite> and propose a new technique that is better suited to question-answering. This article is organized as follows.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_4",
  "x": "The vast majority of papers that address the SQuAD dataset have adopted RNN-based models [3] - [26] . They all follow a similar pipeline, with pre-trained word-embeddings that are processed by bidirectional RNNs. Question and passage are processed independently, and their interaction is modeled by attention mechanisms [27] to produce an answer. There are slight differences in how each model employs attention, but they all calculate it over the hidden states of an RNN. Vaswani et al. were the first to apply attention directly over the word-embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state-ofthe-art results in machine translation <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_5",
  "x": "In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation <cite>[2]</cite> , [27] and natural language inference [28] , [29] . Indeed, most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage. Attention can be defined as a mechanism that gives a score \u03b1 i to a vector p i from a set P = [p 1 , ..., p m ] with respect to a vector q j from Q = [q 1 , ..., q n ]. This score is a function of both P and Q and is shown in its most general form in (1) . where s i and \u03b1 i are scalars and f is a score function that measures the importance of p i relative to q j .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_6",
  "x": "**B. GOOGLE'S TRANSFORMER** The Transformer is a machine translation model introduced in <cite>[2]</cite> that achieved state-of-the-art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position-encoded embedding vectors. It defines three different matrices U, K and V that are associated with queries, keys, and values, respectively. Every attention operation in the Transformer is performed by multiplying these matrices as shown in (4) . where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word.",
  "y": "background uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_7",
  "x": "Every attention operation in the Transformer is performed by multiplying these matrices as shown in (4) . where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <cite>[2]</cite> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as where W U,i , W K,i , W V,i \u2208 R d model \u00d7d head are again learnable weight matrices and d head is the embedding dimension of each head. Finally, attention is computed by the concatenation of every head attention att i , followed by an affine transformation:",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_8",
  "x": "This type of attention is often called \"self-attention\" or \"self-alignment\" <cite>[2]</cite> , [21] . Conversely, if one seeks the relationship between words from two different passages, then U represents one, while K and V represent the other. In that case, we talk about \"cross-attention\". ---------------------------------- **C. OTHER RNN-FREE MODELS**",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_9",
  "x": "In that case, we talk about \"cross-attention\". ---------------------------------- **C. OTHER RNN-FREE MODELS** We also identified another QA model [32] that is inspired by the architecture introduced by Vaswani et al. <cite>[2]</cite> . Their model differs from ours in that it heavily relies on convolutions (46 layers against 2 in FABIR), which approximates it to other CNN NLP models [33] , rather than purely attention based models.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_10",
  "x": "---------------------------------- **III. FABIR** In this section, we present FABIR's architecture and the main design decisions we have made to develop a lighter and faster question-answering model. In particular, we introduce the convolutional attention, the column-wise cross-attention, and the reduction layer, which build on the Transformer model <cite>[2]</cite> to enable its application to question-answering. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_11",
  "x": "Finally, we squeeze \u03c9 c values to [\u22121, 1] using a hyperbolic tangent activation function and pass the concatenation of \u03c9 w and tanh(\u03c9 c ) through a two-layer Highway-Network [36] to obtain the final representation of a word. ---------------------------------- **B. ENCODER** In contrast to an RNN, FABIR does not process words in sequence, and hence needs to model the position of each word in a sentence differently. We add positional information to each word embedding using a trigonometric encoder as proposed in <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_12",
  "x": "We add positional information to each word embedding using a trigonometric encoder as proposed in <cite>[2]</cite> . Therefore, given a sequence of embedding vectors of even size d model , the position of the i th word is encoded in a vector e i as follows: where f k are scalars, which were chosen according to <cite>[2]</cite> . The encoding of an embedding matrix \u2126 is represented by E and the whole operation can be summarized as where d model is the size of each position encoding, which is not necessarily equal to d input .",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_13",
  "x": "The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\". The architecture of this type of layer is addressed further on.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_14",
  "x": "The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\". The architecture of this type of layer is addressed further on.",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_15",
  "x": "**C. CONVOLUTIONAL ATTENTION** In FABIR the attention mechanism is inspired by the Transformer model introduced in <cite>[2]</cite> . However, we hypothesize the word-to-word relationship in (1a) fails to capture the complexity of expressions involving groups of words. To facilitate the modeling of the interdependence of surrounding words, we redefine s i,j as where h and w are the height and width of a convolution kernel.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_16",
  "x": "In FABIR, self-attention is a sublayer that applies such operation via convolutional attention and is defined as att self (P ) = att conv (P, P, P ). 2) Column-wise Cross-attention: Cross-attention (att cross ) differs from other types of attention by relating two different pieces of text. Given P and Q, cross-attention of Q over P is defined as In contrast to Vaswani et al. <cite>[2]</cite> , where the softmax in (12d) is applied in a row-wise manner, we suggest column-wise cross-attention. More precisely, we sum over i instead of j in (1b).",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_17",
  "x": "In contrast, the column-wise softmax attributes greater weights to passage words that are more closely related to the respective question word, which seems appropriate for the SQuAD task. Many question-answering models employ cross-attention in both directions: att cross (P, Q) and att cross (Q, P ) [7] , [21] , [22] . However, in FABIR we have observed better results when only the former is used. 3) Feedforward: The feedforward sublayer is solely composed of a neural network with a single hidden layer, which is applied vector-wise. Following the architecture suggested by Vaswani et al. <cite>[2]</cite> , the feedforward sublayer is implemented in (15) with a two-layer neural network:",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_18",
  "x": "Nonetheless, we observed that the new architecture introduced by Vaswani et al. <cite>[2]</cite> is more susceptible to overfitting than RNNs when presented with large embedding sizes. Hence, we needed a method to compress the word representations, and thus facilitate and speed up training by reducing the number of parameters. A straightforward method to reduce the input embedding size is to multiply it by a matrix with the required dimensions: where \u03c9 model \u2208 R 1\u00d7d model , \u03c9 input \u2208 R 1\u00d7dinput are the embedding vectors, and W Reduction \u2208 R d model \u00d7dinput is a weight matrix, to which we refer as \"reduction matrix\". Although matrix reduction is quite simple, it discards information before any processing and hence might hamper performance by preventing the network from using some relevant data.",
  "y": "motivation"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_19",
  "x": "As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer. In the character-level embedding process, a dropout of 0.75 was added before the convolution. Additionally, a dropout of 0.8 was added before each convolutional layer in the answer selector. We set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed-forward hidden size to 200 in processing layers and 400 in the reduction layer.",
  "y": "differences uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_20",
  "x": "We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM. We developed our model in Tensorflow [39] and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297/ for replicability. We pre-processed the texts with the NLTK Tokenizer [40] . As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_21",
  "x": "This analysis confirms the effectiveness of char-embeddings, as its addition increased the F1 and EM scores, by 2.7% and 3.1%, respectively. Most importantly, when the convolutional attention was replaced by the standard attention mechanism proposed in <cite>[2]</cite> , the performance dropped by 2.4% in F1 and 2.5% in EM. That validates the contribution of this new attention method in building elaborate contextual representations. Moreover, the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings. Indeed, when we replaced that layer by a standard feedforward layer with the same reduction ratio, there was a drop of 2.1% and 2.5% in the F1 and EM scores, respectively.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_22",
  "x": "FABIR also brings three significant contributions to this new class of neural network architectures. The convolutional attention, the reduction layer, and the column-wise crossattention individually increased the model's F1 and EM scores by more than 2%. Moreover, being thoroughly compatible with the Transformer <cite>[2]</cite> , these new mechanisms are valuable assets to further developments in attention models. In fact, an intriguing line for future research is to evaluate their impact on other NLP tasks, such as machine translation or parsing. Although FABIR is still far from surpassing the models at the top of the SQuAD leaderboard (Table III) , we believe that its faster and lighter architecture already make it an attractive alternative to RNN-based models, especially for applications with limited processing power or that require low-latency.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_0",
  "x": "**INTRODUCTION** There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given comparable accuracies to the best statictical models. With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_1",
  "x": "\u89d2(corner)\" (Zheng et al., 2013) , which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016) .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_2",
  "x": "\u89d2(corner)\" (Zheng et al., 2013) , which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016) .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_3",
  "x": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016) . Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011) , and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012) .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_4",
  "x": "To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and<cite> Zhang et al. (2016b)</cite> , we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016) , which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy (Collobert et al., 2011) , casting each external source of information as a auxiliary classification task, sharing a five-character window network.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_5",
  "x": "On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our results show a similar degree of error reduction compared to theirs by using external data. Our model inherits from previous findings on context representations, such as character windows Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016) . Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context.",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_6",
  "x": "Our results show a similar degree of error reduction compared to theirs by using external data. Our model inherits from previous findings on context representations, such as character windows Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016) . Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Figure 1 : Overall model.",
  "y": "similarities differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_7",
  "x": "Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions. Different from previous work, the structure of Figure 2: Deduction system, where \u2295 denotes string concatenation. our scoring network is shown in Figure 1 . It consists of three main layers.",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_8",
  "x": "For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014) , using five-character window [c \u22122 , c \u22121 , c 0 , c 1 , c 2 ] to represent incoming characters. Shown in Figure 3 , a multi-layer perceptron (MLP) is employed to derive a five-character window vector D C from single-character vector rep- For the latter, we follow recent work (Chen et al., 2015b;<cite> Zhang et al., 2016b)</cite> , using a bidirectional LSTM to encode input character sequence. 3 In particular, the bi-directional LSTM ] of the next incoming character c 0 is used to represent the coming characters [c 0 , c 1 , ...] given a state.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_9",
  "x": "A hidden layer is employed to derive a two-word vector X W from single word embeddings e w (w \u22122 ) and e w (w \u22121 ). For the latter, we follow<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , using an uni-directional LSTM on words that have been recognized. ---------------------------------- **PRETRAINING** Neural network models for NLP benefit from pretraining of word/character embeddings, learning distributed sementic information from large raw texts for reducing sparsity.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_10",
  "x": "On the other hand, if the goldstandard hypothesis does not fall out of the agenda until the full sentence has been segmented, a final update is made between the highest scored hypothesis\u0177 (non-gold standard) in the agenda and the gold-standard y i , using exactly the same loss function. Pseudocode for the online learning algorithm is shown in Algorithm 1. We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate \u03b1. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight \u03bb and a dropout rate p. All the parameters in our model are randomly initialized to a value (\u2212r, r), where r = 6.0 f an in +f anout (Bengio, 2012). We fine-tune character and character bigram embeddings, but not word embeddings, acccording to<cite> Zhang et al. (2016b)</cite> .",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_11",
  "x": "Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only w \u22121 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors <cite>(Zhang et al., 2016b</cite>; Cai and Zhao, 2016) . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_12",
  "x": "This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors <cite>(Zhang et al., 2016b</cite>; Cai and Zhao, 2016) . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily With both w \u22122 and w \u22121 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by offering more contextual information. On the other hand, when w \u22123 is also considered, the F-score does not improve further.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_13",
  "x": "With automatically-segmented data 6 , heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013) . Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. Zhang et al. (2016b) Both our model and<cite> Zhang et al. (2016b)</cite> use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training.",
  "y": "differences similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_14",
  "x": "With automatically-segmented data 6 , heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013) . Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. Zhang et al. (2016b) Both our model and<cite> Zhang et al. (2016b)</cite> use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_15",
  "x": "Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. Zhang et al. (2016b) Both our model and<cite> Zhang et al. (2016b)</cite> use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> .",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_16",
  "x": "As shown in Figure 5 , the models give different error distributions, with our models being more robust to the sentence length compared with<cite> Zhang et al. (2016b)</cite> . Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model. ---------------------------------- **FINAL RESULTS**",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_17",
  "x": "This shows the relative advantages of our model. ---------------------------------- **FINAL RESULTS** Our final results on CTB6 are shown in Table 7 , which lists the results of several current state-ofthe-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of<cite> Zhang et al. (2016b)</cite> , which gives the best accuracies among pure neural segments on this dataset.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_18",
  "x": "Our final results compare favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011) , and those leveraging joint POS and syntactic information (Zhang et al., 2014) . In addition, it also outperforms the best neural models, in particular<cite> Zhang et al. (2016b)</cite> Table 7 : Main results on CTB6. ual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the first time a pure neural network model outperforms all existing methods on this dataset, allowing the use of external data 7 .",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_19",
  "x": "Hong Kong corpora, respectively. We map them into simplified Chinese before segmentation. The Weibo corpus is in a yet different genre, being social media text. Xia et al. (2016) achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Similar to Table 7 , our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of<cite> Zhang et al. (2016b)</cite> by 0.2%.",
  "y": "differences"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_0",
  "x": "Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product. Leveraging the recent advancements (Vaswani et al., 2017; Devlin et al., 2019) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ).",
  "y": "background"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_1",
  "x": "Such agents may act as mediators or can be helpful for pedagogical purposes (Johnson et al., 2019) . Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
  "y": "background motivation"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_2",
  "x": "Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product.",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_3",
  "x": "Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set<cite> (He et al., 2018</cite> ) along with our model predictions in Table 1 . ---------------------------------- **PROBLEM SETUP**",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_4",
  "x": "Later, we formalize our problem definition. Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by<cite> He et al. (2018)</cite> . Instead of focusing on the previously studied game environments (Asher et al., 2016; Lewis et al., 2017) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 . The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table 1 ). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics.",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_5",
  "x": "This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others. The models also show evidence of capturing buyer interest. By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (Lewis et al., 2017; <cite>He et al., 2018)</cite> .",
  "y": "future_work"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_0",
  "x": "Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier<cite> (Denis and Baldridge, 2007)</cite> , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_1",
  "x": "Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently,<cite> Denis and Baldridge (2007)</cite> utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity. The goal of the present work is simply to show that transitivity constraints are a useful source of information, which can and should be incorporated into an ILP-based coreference system.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_2",
  "x": "---------------------------------- **COREFERENCE RESOLUTION** For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by<cite> Denis and Baldridge (2007)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_3",
  "x": "Prior work (Soon et al., 2001; <cite>Denis and Baldridge, 2007)</cite> has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. The COREF-ILP model of<cite> Denis and Baldridge (2007)</cite> took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_4",
  "x": "The COREF-ILP model of<cite> Denis and Baldridge (2007)</cite> took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent. This is equivalent to finding the most likely assignment to each x i,j in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent. Our SOON-STYLE baseline used the same training and testing regimen as Soon et al. (2001) .",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_5",
  "x": "Our SOON-STYLE baseline used the same training and testing regimen as Soon et al. (2001) . Our D&B-STYLE baseline used the same test time method as<cite> Denis and Baldridge (2007)</cite> , however at training time we created data for all mention pairs. ---------------------------------- **INTEGER LINEAR PROGRAMMING TO ENFORCE TRANSITIVITY** Because of the ad-hoc manner in which transitivity is enforced in our baseline systems, we do not necessarily find the most probable legal clustering.",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_6",
  "x": "**RESULTS** Our results are summarized in Table 1 . We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREF-ILP system of<cite> Denis and Baldridge (2007)</cite> , which was also based on a na\u00efve pairwise classifier. They used an ILP solver to find an assignment for the variables, but as they note at the end of Section 5.1, it is equivalent to taking all links for which the classifier returns a probability \u2265 0.5, and so the ILP solver is not really necessary.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_0",
  "x": "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. We define complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De Belder and Moens, 2010) . This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation.",
  "y": "uses differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_1",
  "x": "<cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_2",
  "x": "However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification. We extract sentences from a balanced corpus and control sentences to have only one complex word.",
  "y": "background motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_3",
  "x": "In contrast, such a parallel corpus does not exist in Japanese. <cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows:",
  "y": "extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_4",
  "x": "In addition, Horn et al. (2014) extracted simplification candidates and constructed an evaluation dataset using English Wikipedia and Simple English Wikipedia. In contrast, such a parallel corpus does not exist in Japanese. <cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "extends motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_5",
  "x": "**RELATED WORK** The evaluation dataset for the English Lexical Simplification task (Specia et al., 2012) Figure 1: A part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007) . They asked university students to rerank substitutes according to simplification ranking. Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_6",
  "x": "Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts. These contexts were randomly selected from a newswire corpus.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_7",
  "x": "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words. Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_8",
  "x": "These contexts were randomly selected from a newswire corpus. The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words. <cite>They</cite> gathered substitutes from five annotators using crowdsourcing. These procedures were the same as for De Belder and Moens (2012) . During the simplification ranking task, annotators were asked to reorder the target word and its substitutes in a single order without allowing ties.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_9",
  "x": "<cite>They</cite> used crowdsourcing to find five annotators different from those who performed the substitute extraction task. Simplification ranking was integrated on the basis of the average of the simplification ranking from each annotator to generate a gold-standard ranking that might include ties. During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332. Spearman's score of this work was lower than that of Specia et al. (2012) by 0.064. Thus, there was a big blur between annotators, and the simplification ranking collected using crowdsourcing tended to have a lower quality.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_10",
  "x": "Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) .",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_11",
  "x": "Thus, there was a big blur between annotators, and the simplification ranking collected using crowdsourcing tended to have a lower quality. Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_12",
  "x": "Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) .",
  "y": "background motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_13",
  "x": "For example, when three complex words which have 10 substitutes each in a sentence, annotators should consider 10 3 patterns. Thus, it is desired that a sentence includes only simple words after the target word is substituted. Therefore, in this work, we extract sentences containing only one complex word. Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) .",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_14",
  "x": "This deteriorates ranking consistency if some substitutes have a similar simplicity. De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than Specia et al. (2012) . The method of ranking integration is na\u00efve. <cite>Kajiwara and Yamamoto (2015)</cite> and Specia et al. (2012) use an average score to integrate rankings, but it might be biased by outliers. De Belder and Moens (2012) report a slight increase in agreement by greedily removing annotators to maximize the agreement score.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_15",
  "x": "We create a balanced dataset for the evaluation of Japanese lexical simplification. Figure 2 illustrates how we constructed the dataset. It follows the data creation procedure of <cite>Kajiwara and Yamamoto's (2015)</cite> dataset with improvements to resolve the problems described in Section 3. We use a crowdsourcing application, Lancers, 3 3 http://www.lancers.jp/ Figure 3 : Example of annotation of extracting substitutes. Annotators are provided with substitutes that preserve the meaning of target word which is shown bold in the sentence.",
  "y": "uses extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_16",
  "x": "Annotators' rankings were integrated into one ranking, using a maximum likelihood estimation (Matsui et al., 2014) to penalize deceptive annotators as was done by De Belder and Moens (2012) . This method estimates the reliability of annotators in addition to determining the true order of rankings. We applied the reliability score to exclude extraordinary annotators. Table 1 shows the characteristics of our dataset. It is about the same size as previous work (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) .",
  "y": "similarities"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_17",
  "x": "The baseline integration ranking used an average score (<cite>Kajiwara and Yamamoto, 2015</cite>) . Our proposed method excludes outlier annotators by using a reliability score calculated using the method developed by Matsui et al. (2014) . Pairwise agreement is calculated between each pair of sets (p 1 , p 2 \u2208 P ) from all the possible pairings (P) (Equation 1). The agreement among annotators from the substitute evaluation phase was 0.669, and agreement among the students is 0.673, which is similar to the level found in crowdsourcing. This score is almost the same as that from Kajiwara and Yamamoto (2015) .",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_18",
  "x": "This score is higher than that from <cite>Kajiwara and Yamamoto (2015)</cite> by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task. Table 2 shows the results of the ranking integration. Our method achieved better accuracy in ranking integration than previous methods (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) and is similar to the results from De Belder and Moens (2012) . This shows that the reliability score can be used for improving the quality.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_19",
  "x": "This score is higher than that from <cite>Kajiwara and Yamamoto (2015)</cite> by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task. Table 2 shows the results of the ranking integration. Our method achieved better accuracy in ranking integration than previous methods (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) and is similar to the results from De Belder and Moens (2012) . This shows that the reliability score can be used for improving the quality.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_20",
  "x": "In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_21",
  "x": "In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_22",
  "x": "We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ---------------------------------- **LEXICAL SIMPLIFICATION SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_23",
  "x": "We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_24",
  "x": "Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_25",
  "x": "Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_26",
  "x": "We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "differences extends"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_0",
  "x": "These models are shallow and limited in terms of modeling capacity. Furthermore, most of these classifiers are trained to extract PIO elements one by one which is suboptimal since this approach does not allow the use of shared structure among the individual classifiers. Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels (Mikolov et al., 2013; Le and Mikolov, 2014; Peters et al., 2017; <cite>Devlin et al., 2018</cite>; Logeswaran and Lee, 2018; Radford et al., 2018) .",
  "y": "background"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_1",
  "x": "To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in (Jin and Szolovits, 2018) . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model<cite> (Devlin et al., 2018)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_2",
  "x": "BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017;<cite> Devlin et al., 2018)</cite> . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has been pre-trained on a multitude of corpora.",
  "y": "background motivation"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_3",
  "x": "In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release<cite> (Devlin et al., 2018)</cite> . This model is pre-trained on the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT (Lee et al., 2019) , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words).",
  "y": "uses"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_4",
  "x": "In order to insure that independent probabilities are assigned to the labels, as a loss function we have chosen the binary cross entropy with logits (BCEWithLogits) defined by where t and y are the target and output vectors, respectively; n is the number of independent targets (n=3). The outputs are computed by applying the logistic function to the weighted sums of the last hidden layer activations, s, For the original BERT model, we have chosen the smallest uncased model, Bert-Base. The model has 12 attention layers and all texts are converted to lowercase by the tokenizer<cite> (Devlin et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_0",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_1",
  "x": "---------------------------------- **INTRODUCTION** In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_2",
  "x": "**INTRODUCTION** In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_3",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_5",
  "x": "The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_6",
  "x": "The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_7",
  "x": "While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) . More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_8",
  "x": "We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification. In all three of these tasks, we are able to improve performance past the state of the art. ---------------------------------- **METHOD** ----------------------------------",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_9",
  "x": "In total, this dataset is comprised of more than 12 million tokens. The data used for the classification tasks was from the same Stanford MOOCPosts dataset. The posts are annotated by domain experts and given scores for sentiment (the degree of emotionality exhibited by the post), confusion expressed by the student and urgency for the post to receive a response from an instructor. Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) .",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_10",
  "x": "The data used for the classification tasks was from the same Stanford MOOCPosts dataset. The posts are annotated by domain experts and given scores for sentiment (the degree of emotionality exhibited by the post), confusion expressed by the student and urgency for the post to receive a response from an instructor. Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) . Both models are initialized from their <cite>base model</cite> and finetuned on educational data, using the Transformers library .",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_11",
  "x": "To encourage easily comparable results, we evaluated the models on three wellexplored classification tasks on the StandfordMOOC dataset. Following previous work by Guo et al. (2019) , we split the data into a 2/3 training set and 1/3 test set and consider a post to express sentiment, urgency or confusion if and only if its respective score is \u2265 4. We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_12",
  "x": "To encourage easily comparable results, we evaluated the models on three wellexplored classification tasks on the StandfordMOOC dataset. Following previous work by Guo et al. (2019) , we split the data into a 2/3 training set and 1/3 test set and consider a post to express sentiment, urgency or confusion if and only if its respective score is \u2265 4. We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_13",
  "x": "Following previous work by Guo et al. (2019) , we split the data into a 2/3 training set and 1/3 test set and consider a post to express sentiment, urgency or confusion if and only if its respective score is \u2265 4. We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training. Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_14",
  "x": "Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) . Here too, all the pretraining approaches outperform the SoA. EduDistilBERT obtains the best results on both urgency and confusion prediction while EduBERT performs the best for sentimentality classification. However, EduDistilBERT has a lower memory footprint and is noticeably faster at inference time, allowing for a 30% speedup.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_15",
  "x": "However, EduDistilBERT has a lower memory footprint and is noticeably faster at inference time, allowing for a 30% speedup. ---------------------------------- **RESULTS & DISCUSSIONS** . EduBERT and EduDistilBERT are fine-tuned on millions of tokens, in contrast to the billions of tokens required to make the most of the architecture potential (<cite>Devlin et al., 2019</cite>) . We are actively seeking more data to train models even more capable of producing contextualized word representations in the educational domain.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_0",
  "x": "With the availability and abundance of linguistic data that captures different avenues of human social interactions, there is an unprecedented opportunity to expand NLP to not only understand language, but also to understand the people who speak it and the social relations between them. Social power structures are ubiquitous in human interactions, and since power is often reflected through language, computational research at the intersection of language and power has gained interest recently. This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012) , blogs (Rosenthal, 2014) as well as workplace interactions <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012; Prabhakaran, 2015) . The corporate environment is one social context in which power dynamics have a clearly defined structure and shape the interactions between individuals, making it an interesting case study on how language and power interact. Organizations stand to benefit greatly from being able to detect power dynamics within their internal interactions, in order to address disparities and ensure inclusive and productive workplaces.",
  "y": "background motivation"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_1",
  "x": "While early work <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014) . However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. ---------------------------------- **ARXIV:1807.06557V1 [CS.CL] 17 JUL 2018** based features extracted from this concatenated text to infer power relations.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_2",
  "x": "However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. based features extracted from this concatenated text to infer power relations. They ignore the fact that the text comes from separate emails, and that there is a sequential order to them.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_3",
  "x": "The participants in these pairs are 1) interacting: at least one message exists in the thread such that either A is the sender and B is a recipient or vice versa, and 2) related: A and B are related by a dominance relation (either superior or subordinate) based on the organizational hierarchy. As in (Prabhakaran and Rambow, 2014) , we exclude pairs of employees who are peers, and we use the same train-dev-test splits so our results are comparable. Grouped: Here, we group all emails A sent to B across all threads in the corpus, and vice versa, and use these sets of emails to predict the power relation between A and B. This formulation is similar those in <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) , but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012) ; however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures. ---------------------------------- **METHODS**",
  "y": "similarities extends differences"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_4",
  "x": "We then merge the resulting output of the two LSTMs with the non-lexical features from each individual's emails, pass it on to a dense layer with ReLU activation, and then to a sigmoid classifier for the final prediction. ---------------------------------- **EXPERIMENTS AND RESULTS** We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014;<cite> Bramsen et al., 2011</cite>; Gilbert, 2012) . We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ).",
  "y": "background uses"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_5",
  "x": "**EXPERIMENTS AND RESULTS** We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014;<cite> Bramsen et al., 2011</cite>; Gilbert, 2012) . We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ). For each of our neural net models, we trained for 30-70 epochs until the performance on the development set stopped improving, in order to avoid overfitting. We used Hyperas to tune hyperparameters on our development dataset for the same set of parameter options for each task formulation, varying activation functions, hidden layer size, batch size, dropout, number of filters, and number of words to include per email.",
  "y": "differences extends"
 },
 {
  "id": "310272015a781b05c42015c0559b18_0",
  "x": "Several computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages. Saffran et al. (1996) assume transitional probability, but<cite> Brent (1999a)</cite> claims mutual information (MI) is more appropriate. Both assume predictability is measured locally, relative to neighboring segment-pairs.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_1",
  "x": "However, the experimental design does not fully specify how predictability is best measured or modeled in a simulation. Saffran et al. (1996) assume transitional probability, but<cite> Brent (1999a)</cite> claims mutual information (MI) is more appropriate. Both assume predictability is measured locally, relative to neighboring segment-pairs. This paper replicates<cite> Brent's (1999a)</cite> mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold. Brent's finding regarding the superiority of MI is confirmed; the relative performance of local comparisons and global thresholds depends on the evaluation metric.",
  "y": "background differences similarities"
 },
 {
  "id": "310272015a781b05c42015c0559b18_2",
  "x": "While the infant studies discussed above focus primarily on the properties of particular cues, computational studies of word-segmentation must also choose between various implementations, which further complicates comparisons. Several models (e.g., Batchelder, 2002;<cite> Brent's (1999a)</cite> MBDP-1 model; Davis, 2000; de Marcken, 1996; Olivier, 1968) simultaneously address the question of vocabulary acquisition, using previously learned word-candidates to bootstrap later segmentations. (It is beyond the scope of this paper to discuss these in detail; see Brent 1999a,b for a review.) Other models do not accumulate a stored vocabulary, but instead rely on the degree of predictability of the next syllable (e.g., Saffran et al., 1996) or segment (e.g., Christiansen et al., 1998) . The intuition here, first articulated by Harris (1954) , is that word boundaries are marked by a spike in unpredictability of the following phoneme.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_3",
  "x": [
   "Brent (1999a) points out one type of ambiguity, namely that Saffran and colleagues' (1996) results can be modeled as favoring word-breaks at points of either low transitional probability or low mutual information. Brent reports results for models relying on each of these measures. It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same. Brent (1999a) compares these two models in terms of word tokens correctly segmented (see Section 3 for exact criteria), reporting approximately 40% precision and 45% recall for transitional probability (TP) and 50% precision and 53% recall for mutual information (MI) on the first 1000 utterances of his corpus (with improvements given larger corpora). Indeed, their performance on word tokens is surpassed only by Brent's main model (MBDP-1), which seems to have about 73% precision and 67% recall for the same range."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_4",
  "x": [
   "Brent reports results for models relying on each of these measures. It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same. Brent (1999a) compares these two models in terms of word tokens correctly segmented (see Section 3 for exact criteria), reporting approximately 40% precision and 45% recall for transitional probability (TP) and 50% precision and 53% recall for mutual information (MI) on the first 1000 utterances of his corpus (with improvements given larger corpora). Indeed, their performance on word tokens is surpassed only by Brent's main model (MBDP-1), which seems to have about 73% precision and 67% recall for the same range. ----------------------------------"
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_5",
  "x": "**1** Another question which Saffran et al. (1996) leave unanswered is whether the segmentation depends on local or global comparisons of predictability. Saffran et al. assume implicitly, and<cite> Brent (1999a)</cite> explicitly, that the proper comparison is local-in Brent, dependent solely on the adjacent pairs of segments. However, predictability measures for segmental bigrams (whether TP or MI) may be compared in any number of ways. One straightforward alternative to the local comparison is to compare the predictability measures compare to some global threshold.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_6",
  "x": "The global comparison, taken on its own, seems a rather simplistic and inflexible heuristic: for any pair of phonemes xy, either a word boundary is always hypothesized between x and y, or it never is. Clearly, there are many cases where x and y sometimes straddle a word boundary and sometimes do not. The heuristic also takes no account of lengths of possible words. However, the local comparison may take length into account too much, disallowing words of certain lengths. In order to see that, we must examine<cite> Brent's (1999a)</cite> suggested implementation of Saffran et al. (1996) more closely.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_7",
  "x": "In the local comparison, given some string \u2026wxyz\u2026, in order for a word boundary to be inserted between x and y, the predictability measure for xy must be lower than both that of wx and of yz. It follows that neither wx nor yz can have word boundaries between them, since they cannot simultaneously have a lower predictability measure than xy. This means that, within an utterance, word boundaries must have at least two segments between them, so this heuristic will not correctly segment utterance-internal one-phoneme words. 3 Granted, only a few one-phoneme word types exist in either English or Greek (or other languages). However, these words are often function words and so are less likely to appear at edges of utterances (e.g., ends of utterances for articles and prepositions; beginnings for postposed elements). Neither<cite> Brent's (1999a)</cite> implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_8",
  "x": "Neither<cite> Brent's (1999a)</cite> implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned. Brent (1999a) himself points out another lengthrelated limitation-namely, the relative difficulty that the 'local comparison' heuristic has in segmenting learning longer words. The bigram MI frequencies may be most strongly influenced byand thus as an aggregate largely encode-the most frequent, shorter words. Longer words cannot be memorized in this representation (although common ends of words such as prefixes and suffixes might be). In order to test for this, Brent proposes that precision for word types (which he calls \"lexicon precision\") be measured as well as for word tokens.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_9",
  "x": [
   "Longer words cannot be memorized in this representation (although common ends of words such as prefixes and suffixes might be). In order to test for this, Brent proposes that precision for word types (which he calls \"lexicon precision\") be measured as well as for word tokens. While the word-token metric emphasizes the correct segmentation of frequent words, the word-type metric does not share this bias. Brent defines this metric as follows: \"After each block [of 500 utterances], each word type that the algorithm produced was labeled a true positive if that word type had occurred anywhere in the portion of the corpus processed so far; otherwise it is labeled a false positive. \" Measured this way, MI yields a word type precision of only about 27%; transitional probability yields a precision of approximately 24% for the first 1000 utterances, compared to 42% for MBDP-1. He does not measure word type recall. This same limitation in finding longer, less frequent types may apply to comparisons against a global threshold as well. This is also in need of testing. It seems that both global and local comparisons, used on their own as sole or decisive heuristics, may have serious limitations. It is not clear a priori which limitation is most serious; hence both comparisons are tested here."
  ],
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_10",
  "x": "While in its general approach the study reported here replicates the mutual-information and transitional-probability models in<cite> Brent (1999a)</cite> , it differs slightly in the details of their use. First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables. While Modern Greek syllabic structure is not as complicated as English's, it is still more complicated than the CV structure assumed in Saffran et al. (1996) ; hence, access to syllabification cannot be assumed.",
  "y": "background differences"
 },
 {
  "id": "310272015a781b05c42015c0559b18_11",
  "x": "---------------------------------- **OUTLINE OF CURRENT RESEARCH** While in its general approach the study reported here replicates the mutual-information and transitional-probability models in<cite> Brent (1999a)</cite> , it differs slightly in the details of their use. First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above).",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_12",
  "x": "4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables. While Modern Greek syllabic structure is not as complicated as English's, it is still more complicated than the CV structure assumed in Saffran et al. (1996) ; hence, access to syllabification cannot be assumed. ---------------------------------- **CORPUS DATA**",
  "y": "background similarities"
 },
 {
  "id": "310272015a781b05c42015c0559b18_13",
  "x": "Since the finite-state framework selects the best path over the whole utterance, it also allows for optimization over a sequence of decisions, rather than optimizing each local decision separately. 6 Unlike Belz (1998) , where the actual FSM structure (including classes of phonemes that could be group onto one arc) was learned, here the structure of each FSM is determined in advance. Only the weight on each arc is derived from data. No attempt is made to combine phonemes to produce more minimal FSMs; each phoneme (and phoneme-pair) is modeled separately. Like<cite> Brent (1999a)</cite> and indeed most models in the literature, this model assumes (for sake of convenience and simplicity) that the child hears each segment produced within an utterance without error. This assumption translates into the finitestate domain as a simple acceptor (or equivalently, an identity transducer) over the segment sequence for a given utterance.",
  "y": "background motivation uses"
 },
 {
  "id": "310272015a781b05c42015c0559b18_14",
  "x": "This boundary measure may be more conservative than that reported by other authors, but is easily convertible into other metrics. The second metric, the percentage of word tokens detected, is the same as<cite> Brent (1999a)</cite> . In order for a word to be counted as correctly found, three conditions must be met: (a) the word's beginning (left boundary) is correctly detected, (b) the word's ending (right boundary) is correctly detected, and (c) these two are consecutive (i.e., no false boundaries are posited within the word). The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match.",
  "y": "similarities"
 },
 {
  "id": "310272015a781b05c42015c0559b18_15",
  "x": "The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> . ----------------------------------",
  "y": "motivation extends"
 },
 {
  "id": "310272015a781b05c42015c0559b18_16",
  "x": "In order for a word to be counted as correctly found, three conditions must be met: (a) the word's beginning (left boundary) is correctly detected, (b) the word's ending (right boundary) is correctly detected, and (c) these two are consecutive (i.e., no false boundaries are posited within the word). The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> .",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_17",
  "x": "**COMPARING THE FOUR VARIANTS** The findings here confirm<cite> Brent's (1999a)</cite> contention that mutual information is a better measure of predictability than is transitional probability-at least for the task of identifying words, not just boundaries. This is particularly true in the global comparison. Transitional probability finds more word boundaries in the 'local comparison' model, but this does not carry over to the task of pulling out the word themselves, which is arguably the infant's main concern. This result should be kept in mind when interpreting or replicating (Saffran et al., 1996) or similar studies.",
  "y": "similarities"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_0",
  "x": "The only existing work on topic segmentation in dialogue, Galley et al. (2003) , segments recorded speech between multiple persons using both lexical cohesion and dis-tinctive textual and acoustic markers. The present work differs from Galley et al. (2003) in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue. In this study we apply the methods of Foltz et al. (1998) ,<cite> Hearst (1994</cite> Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. All three are vector space methods that measure lexical cohesion to determine topic shifts. Our results show that the new using an orthonormal basis significantly outperforms the other methods.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_1",
  "x": "**PREVIOUS WORK** Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation. An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_2",
  "x": "However,<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_3",
  "x": "However,<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_4",
  "x": "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text. As stated previously, these comparisons reflect the cohesion between units of text. In order to use these comparisons to segment text, however, one must have a criterion in place. Foltz et al. (1998) , noting mean cosines of .16 for boundaries and .43 for non-boundaries, choose a threshold criterion of .15, which is two standard deviations below the boundary mean of .43.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_5",
  "x": "The average number of utterances per topic, 16 utterances, and the average number of words per utterance, 32 words, were calculated to determine the parameters of the segmentation methods. For example, a moving window size greater than 16 utterances implies that, in the majority of occurrences, the moving window straddles three topics as opposed to the desired two. To replicate Foltz et al. (1998) , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window. Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (Choi, 1999 ) Java software. A variant of<cite> Hearst (1994</cite> Hearst ( , 1997 was created by using LSA instead of the standard vector space method.",
  "y": "differences"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_6",
  "x": "**HEARST (1994, 1997)** The JTextTile software was used to implement<cite> Hearst (1994)</cite> on dialogue. As with Foltz et al. (1998) , a text unit and window size had to be determined for dialogue. Hearst (1994) recommends using the average paragraph size as the window size. Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_7",
  "x": "Hearst (1994) recommends using the average paragraph size as the window size. Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 . The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units. This combination matches<cite> Hearst (1994)</cite> 's heuristic of choosing the window size to be the average paragraph length. On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by Hearst (1997) , .70.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_9",
  "x": "However, comparing the Hearst algorithm with the Hearst + LSA algorithm indicates that a 57% improvement stems from the addition of LSA, keeping all other factors constant. While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue. Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain. These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not). It may be that<cite> Hearst (1994</cite> Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_0",
  "x": "However, factoring the term-context matrix means throwing away a considerable amount of information, as the original matrix of size M \u00d7 N (number of instances by number of features) is factored into two smaller matrices of size M \u00d7 K and N \u00d7 K, with K M, N . If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004) , this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF;<cite> Guo and Diab, 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_2",
  "x": "1 unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance Table 1 : Fine-grained features for paraphrase classification, selected from prior work (Wan et al., 2006) . ---------------------------------- **SUPERVISED CLASSIFICATION** While previous work has performed paraphrase classification using distance or similarity in the latent space<cite> (Guo and Diab, 2012</cite>; Socher et al., 2011) , more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences v 1 and v 2 into a sample vector,",
  "y": "background"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_4",
  "x": "In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work<cite> (Guo and Diab, 2012)</cite> , the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007) . To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error.",
  "y": "similarities uses"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_5",
  "x": "As in prior work<cite> (Guo and Diab, 2012)</cite> , the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007) . To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. Table 2 compares the accuracy of a number of different configurations.",
  "y": "similarities uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_0",
  "x": "---------------------------------- **INTRODUCTION** Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models <cite>(Sperber et al., 2017)</cite> . Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice.",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_1",
  "x": "However, such a scenario has drawbacks. From a practical perspective, it requires access to the original speech utterances and transcriptions, which can be unrealistic if a user needs to employ an out-ofthe-box ASR system. From a theoretical perspective, intermediate representations such as lattices can be enriched through external, textual resources such as monolingual corpora or dictionaries. <cite>Sperber et al. (2017)</cite> proposes a lattice-tosequence model which, in theory, can address both problems above. However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining.",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_2",
  "x": "<cite>Sperber et al. (2017)</cite> proposes a lattice-tosequence model which, in theory, can address both problems above. However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure. Together with the standard batching strategies provided by graph networks, we are able to decrease training time by two orders of magnitude, enabling us to match their translation performance under the same training speed constraints without relying on gold transcriptions.",
  "y": "motivation"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_3",
  "x": "The BPE segmentation can lead to redundant nodes in the lattice. Our next transformation step is a minimisation procedure, where such nodes are joined into a single node in the graph. To perform this step, we leverage an efficient algorithm for automata minimisation (Hopcroft, 1971) , which traverses the graph detecting redundant nodes by using equivalence classes, running in O(n log n) time, where n is the number of nodes. 1 This procedure is also done in <cite>Sperber et al. (2017)</cite> . The final step adds reverse and self-loop edges to the lattice, where these new edges have specific parameters in the encoder.",
  "y": "similarities"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_4",
  "x": "We use the original release by Post et al. (2013) , containing both 1-best and pruned lattice outputs from an ASR system for each Spanish utterance. 2 The Fisher corpus contain 150K instances and we use the original splits provided with the datasets. Following previous work (Post et al., 2013; <cite>Sperber et al., 2017)</cite> , we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. Models and Evaluation All our models are trained on the Fisher training set.",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_5",
  "x": "Table 1 : Out-of-the-box scenario results, in BLEU scores. \"L\" corresponds to word lattice inputs, \"L+S\" and \"L+S+M\" correspond to lattices after subword segmentation and after minimisation, respectively. Each model is trained using 5 different seeds and we report BLEU (Papineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in <cite>Sperber et al. (2017)</cite> ), while for subword models we do not perform any threshold pruning. We report all results on the Fisher \"dev2\" set. 4",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_6",
  "x": "We keep the dev and test sets with lattices only, as this emulates test time conditions. The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_7",
  "x": "With this, we can simply take the union of transcriptions and lattices into a single training set. We keep the dev and test sets with lattices only, as this emulates test time conditions. The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_8",
  "x": "We keep the dev and test sets with lattices only, as this emulates test time conditions. The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_9",
  "x": "Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only. This means that we can obtain state-of-the-art performance even in an out-of-thebox scenario, under the same training speed constraints. While there are other constraints that may be considered (such as parameter budget), we nevertheless believe this is an encouraging result for real world scenarios. ---------------------------------- **ADDING LATTICE SCORES**",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_10",
  "x": "As a simple first approach to incorporate scores, we embed them using a multilayer perceptron, using the score as the input. This however did not produce good results: performance dropped to 32.9 BLEU in the single model setting and 38.4 for the ensemble. It is worth noticing that <cite>Sperber et al. (2017)</cite> has a more principled approach to incorporate scores: by modifying the attention module. This is arguably a better choice, since the scores can directly inform the decoder about the ambiguity in the lattice. Since this approach does not affect the encoder, it is theoretically possible to combine our GGNN encoder with their attention module, we leave this avenue for future work.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_11",
  "x": "We show promising results and outperform baselines in speech translation, particularly in out-of-the-box ASR scenarios, when one has no access to transcriptions. For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ). Another important avenue is to explore this approach in low-resource scenarios such as ones involving endangered languages (Adams et al., 2017; Anastasopoulos and Chiang, 2018) .",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_12",
  "x": "We show promising results and outperform baselines in speech translation, particularly in out-of-the-box ASR scenarios, when one has no access to transcriptions. For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ). Another important avenue is to explore this approach in low-resource scenarios such as ones involving endangered languages (Adams et al., 2017; Anastasopoulos and Chiang, 2018) .",
  "y": "future_work"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_0",
  "x": "Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; <cite>Kann et al., 2018)</cite> . While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009 ), or Arapaho (Littell, 2018 Moeller et al., 2018) ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson et al., 2007) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN)<cite> (Kann et al., 2018)</cite> . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 .",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_1",
  "x": "Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; <cite>Kann et al., 2018)</cite> . While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009 ), or Arapaho (Littell, 2018 Moeller et al., 2018) ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson et al., 2007) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN)<cite> (Kann et al., 2018)</cite> . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 .",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_2",
  "x": "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_3",
  "x": "We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4). ---------------------------------- **LANGUAGES AND DATASETS** Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed as what is considered by native speakers to be just one word.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_4",
  "x": "Kann et al. (2018) have made a first step by releasing a small set of morphologically segmented datasets although even in these carefully curated datasets, the distinction between affix and clitic is not always indicated. We use these datasets in an unsupervised fashion (i.e., we use the unsegmented words). These datasets were taken from detailed descriptions in the Archive of Indigenous Languages collection for MX (Canger, 2001 ), NH (de Su\u00e1rez, 1980) , WX (G\u00f3mez and L\u00f3pez, 1999) , and YN (Freeze, 1989) . They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_5",
  "x": "They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13. ---------------------------------- **USING ADAPTOR GRAMMARS FOR POLYSYNTHETIC LANGUAGES**",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_6",
  "x": "These datasets were taken from detailed descriptions in the Archive of Indigenous Languages collection for MX (Canger, 2001 ), NH (de Su\u00e1rez, 1980) , WX (G\u00f3mez and L\u00f3pez, 1999) , and YN (Freeze, 1989) . They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13. ----------------------------------",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_7",
  "x": "However, since affixes and stems are not distinguished in the training annotations from<cite> Kann et al. (2018)</cite> , we only consider the first and last morphemes that appear at least five times. We call this setup AG Scholar BestL . Multilingual Training. Since the vocabulary in<cite> Kann et al. (2018)</cite> for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually. We call this setup AG M ulti .",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_8",
  "x": "We call this setup AG Scholar BestL . Multilingual Training. Since the vocabulary in<cite> Kann et al. (2018)</cite> for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually. We call this setup AG M ulti . Data Augmentation.",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_9",
  "x": "We denote this setup as AG Aug . ---------------------------------- **EVALUATION AND DISCUSSION** We evaluate the different AG setups on the blind test set from<cite> Kann et al. (2018)</cite> and compare our AG approaches to state-of-the-art unsupervised systems as well as supervised models including the best supervised deep learning models from<cite> Kann et al. (2018)</cite> . As the metric, we use the segmentation-boundary F1-score, which is standard for this task (Virpioja et al., 2011) .",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_13",
  "x": "As can be seen in Table  4 , our unsupervised AG-based approaches outperform the best supervised approaches for NH and YN with absolute F1-scores of 0.010 and 0.012, respectively. An interesting observation is that for YN we only used the words in the training set of<cite> Kann et al. (2018)</cite> (unsegmented) , without any data augmentation. For MX and WX, the neural models from<cite> Kann et al. (2018)</cite> (BestMTT and BestDA), outperform our unsupervised AG-based approaches. Error Analysis. For the purpose of error analysis, we train our unsupervised segmentation on the training sets and perform the analysis of results on the output of the development sets based on our best unsupervised models AG BestL .",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_14",
  "x": "An interesting observation is that for YN we only used the words in the training set of<cite> Kann et al. (2018)</cite> (unsegmented) , without any data augmentation. For MX and WX, the neural models from<cite> Kann et al. (2018)</cite> (BestMTT and BestDA), outperform our unsupervised AG-based approaches. Error Analysis. For the purpose of error analysis, we train our unsupervised segmentation on the training sets and perform the analysis of results on the output of the development sets based on our best unsupervised models AG BestL . Since there is no distinction between stems and affixes in the labeled data, we only consider the morphemes that appear at least three times in order to eliminate open-class morphemes in our statistics.",
  "y": "differences"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_0",
  "x": "(But) The last thing they needed was another drag-down blow.\" (Implicit, Comparison. Contrast) Previous studies show that the presence of connectives can greatly help with classification of the relation and can be disambiguated with 0.93 accuracy (4-ways) solely on the discourse relation connectives (Pitler et al., 2008) . In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features <cite>(Lin et al., 2009</cite>; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) . For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen in-stances, so that it's important to make maximal use of both the data set for training and testing.",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_1",
  "x": "In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features <cite>(Lin et al., 2009</cite>; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) . For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen in-stances, so that it's important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015; Rutherford et al., 2017) , which contains only about 761 implicit relations. This small size implies that a gain of 1 percentage point in accuracy corresponds to just classifying an additional 7-8 instances correctly. This paper therefore aims to demonstrate the degree to which conclusions about the effectiveness of including certain features would depend on whether one evaluates on the standard test section only, or performs cross validation on the whole dataset for second-level discourse relation classification.",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_2",
  "x": "The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective while the implicit one does not. The PDTB provides a three level hierarchy of relation tags for its annotation. Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 .",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_3",
  "x": "The PDTB provides a three level hierarchy of relation tags for its annotation. Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 . We follow the preprocessing method in <cite>(Lin et al., 2009</cite>; Rutherford et al., 2017) . If the instance is annotated with two relations, we adopt the first one shown up, and remove those relations with too few instances.",
  "y": "uses"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_4",
  "x": "---------------------------------- **MODEL** The task is to predict the discourse relation given the two arguments of an implicit instance. As a label set, we use 11-way distinction as proposed in<cite> Lin et al., (2009)</cite>; Ji and Eisenstein (2015) . Word Embeddings are trained with the Skip-gram architecture in Word2Vec (Mikolov et al., 2013) , which is able to capture semantic and syntactic patterns with an unsupervised method, on the training sections of WSJ data.",
  "y": "uses"
 },
 {
  "id": "3477c0225d6a0e55365242d95a3dc9_0",
  "x": "**** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, <cite>9]</cite> . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_0",
  "x": "Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region. In this paper we propose a text-based geolocation method based on neural networks. Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model<cite> (Rahimi et al., 2015a)</cite> and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines. ----------------------------------",
  "y": "background extends"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_1",
  "x": "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009 ), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012 , or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014) . Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions<cite> (Rahimi et al., 2015a)</cite> as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> . Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_2",
  "x": "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009 ), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012 , or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014) . Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions<cite> (Rahimi et al., 2015a)</cite> as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> . Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_3",
  "x": "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets). To test the applicability of the model's embeddings in dialectology, we created DAREDS.",
  "y": "differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_4",
  "x": "The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016) . Following Cheng (2010) and Eisenstein (2010) , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_5",
  "x": "Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of <cite>Rahimi et al. (2015a)</cite> , and improved upon their work. We analysed the Table 2 : Nearest neighbours of place names. in Figure 3 . The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas).",
  "y": "background extends"
 },
 {
  "id": "373795850c8f182051214a8ee09461_0",
  "x": "**INTRODUCTION** Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954) , that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010) . In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks Levy et al., 2015; Nguyen et al., 2016) , or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; <cite>Kiela et al., 2014</cite>; Lazaridou et al., 2015) . While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015) , WordSim (Finkelstein et al., 2002) , etc.",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_1",
  "x": "Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters<cite> (Kiela et al., 2014)</cite> , and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the most imaginable targets are added, or (b) the textual modality performs well, and all available -potentially noisy-images are added. In addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi-modality than for nouns, presumably because they are more difficult to grasp. ----------------------------------",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_2",
  "x": "---------------------------------- **LEXICAL, EMPIRICAL AND VISUAL FILTERS** The experiments compare the predictions of compositionality across all targets in the gold standards. 2 Furthermore, we zoom into factors that might influence the quality of predictions: (A) the impact of lexical and empirical target properties, i.e., ambiguity (relying on the DUDEN dictionary 3 , frequency (as provided by the gold standards), abstractness and imageability (as taken from K\u00f6per and Schulte im Walde (2016)); (B) optimisation of the visual space: (i) In accordance with human concept processing (Paivio, 1990) , including image representations should be more useful for words which are visual. We therefore apply the dispersion-based filter suggested by<cite> Kiela et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "373795850c8f182051214a8ee09461_3",
  "x": "Zooming into target subsets, the predictions for monosemous targets are better than those for ambiguous targets (significant for GS-NN), see Figure 3 ; ditto for low-frequency vs. high-frequency targets. Taking frequency as an indicator of ambiguity, these differences are presumably due to the difficulty of distinguishing between multiple senses in vector spaces that subsume the features of all word senses within one vector, which applies to our textual and multi-modal models. The gold standard predictions strongly differ regarding the influence of target abstractness, imageability and compositionality. For GS-NN, the compositionality of concrete and imaginable targets is predicted better than for abstract and less imaginable targets, as one would expect and has been shown by<cite> Kiela et al. (2014)</cite> ; for GS-PV, the opposite is the case. Similarly, while for GS-NN highly compositional targets are predicted worse than low-and mid-compositional targets, for GS-PV mid-compositional targets are predicted much worse than low-and high-compositional targets.",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_0",
  "x": "In addition, newly given real-world datasets may include a lot of unseen words and phrases. Practically, OOV words in a corpus are replaced with a special token representing OOV. The larger OOV rate in a corpus affects the accuracies of downstream tasks (Sun et al., 2005) . In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words.",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_1",
  "x": "Practically, OOV words in a corpus are replaced with a special token representing OOV. The larger OOV rate in a corpus affects the accuracies of downstream tasks (Sun et al., 2005) . In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_2",
  "x": "The larger OOV rate in a corpus affects the accuracies of downstream tasks (Sun et al., 2005) . In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_3",
  "x": "In this paper, we propose a simple unsupervised method of character n-gram embedding for unsegmented languages, where the segmentation step is completely omitted thus words, phrases and sentences are treated seamlessly. Our model considers all possible character n-grams as embedding targets in a corpus. Each n-gram is explicitly modeled as a composition of its sub-n-grams just like each word is modeled as a composition of sub-words in the subword information skipgram model (SISG)<cite> (Bojanowski et al., 2017)</cite> . Our segmentation-free compositional n-gram embedding is referred to as SCNE in this paper. This kind of approach that does not consider any word boundaries for unsegmented languages may sound reckless since the embedding targets can include a lot of wrong boundaries.",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_5",
  "x": "We will make the C++ implementation of our method and pre-trained models available open-source. ---------------------------------- **BASELINE SYSTEMS** As baseline systems, we use C-BOW, Skipgram (Mikolov et al., 2013) , Subword Information Skip-gram (SISG)<cite> (Bojanowski et al., 2017)</cite> and Segmentation-free word embedding for unsegmented languages (Sembei) (Oshikiri, 2017) for the word-level tasks. For the sentence-level task, baselines are PV-DBOW, PV-DM (Le and Mikolov, 2014) and Sent2vec (Pagliardini et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_6",
  "x": "For pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by Spearman rank correlation. Most of the settings are the same as that of <cite>Bojanowski et al. (2017)</cite> . Two widely-used benchmark datasets are used: Chinese word similarity dataset (Jin and Wu, 2012) , which contains 297 pairs of words, and Japanese word similarity dataset (Sakaizawa and Komachi, 2017) , which contains 4429 pairs of words. Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data. In contrast, SISG and our model can compute representations for almost all words, since both methods learn compositional n-gram features.",
  "y": "similarities uses"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_7",
  "x": "Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data. In contrast, SISG and our model can compute representations for almost all words, since both methods learn compositional n-gram features. In order to show comparable results, we use the null vector for these OOV words following <cite>Bojanowski et al. (2017)</cite> . Noun category prediction task: We use 100MB of SNS data, Sina Weibo for Chinese and Twitter for Japanese and Korean, as training corpora. For evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from Wikidata with the predetermined semantic category set 2 .",
  "y": "similarities uses"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_0",
  "x": "We introduce an unsupervised method based on MT for GEC that does not almost use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018) , <cite>Artetxe et al. (2018b)</cite> , and Lample et al. (2018) . These methods are based on phrase-based statistical machine translation (SMT) and two phrase table refinements, i.e., forward and backward refinement. Forward refinement simply arguments a learner corpus with automatic corrections whereas backward refinement expends both source-side and target-side data to train GEC model using backtranslation (Sennrich et al., 2016a) . Unsupervised MT techniques do not require a parallel but a comparable corpus as training data.",
  "y": "similarities uses"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_1",
  "x": "We also verified our GEC system through experiments for a low resource track of the shared task at Building Educational Applications 2019 (BEA2019). The experimental results show that our system achieved an F 0.5 score of 28.31 points in the low resource track of the shared task at BEA2019. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from <cite>Artetxe et al. (2018b)</cite> . First, the cross-lingual phrase embeddings are acquired.",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_2",
  "x": "The backward phrase translation probability \u03c6(e|f ) was determined in a similar manner. The source-to-target lexical translation model lex(f |e) considers the word with the highest translation probability in a target phrase for each word in a source phrase. The score of the lexical translation model was calculated based on the product of respective phrase translation probabilities. \u01eb is a constant term for the case where no alignments are found. As in <cite>Artetxe et al. (2018b)</cite> , the term was set to 0.001.",
  "y": "similarities"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_3",
  "x": "A new phrase table P (1) s\u2192t was then created with this target synthetic corpus. This operation was executed N times. For backward refinement<cite> (Artetxe et al., 2018b)</cite> , source synthetic data were generated from the target monolingual data using the target to source phrase table P (0) t\u2192s and source language model LM s . A new source to target phrase table P (1) s\u2192t was created with this source synthetic parallel corpus.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_4",
  "x": "(1) t\u2192s was built using this target synthetic data. Construction of a comparable corpus This unsupervised method is based on the assumption that the source and target corpora are comparable. In fact, Lample et al. (2018) , <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as 3 Experiment of low resource GEC 3.1 Experimental setting Table 1 shows the training and development data size. Unless mentioned otherwise, Finnish News Crawl 2014-2015 translated into English was used as source training data and English News Crawl 2017 was used as target training data.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_5",
  "x": "For CoNLL-14 and JFLEG test set, NLTK (Bird, 2006) tokenizer was used. We used moses truecaser for the training data; this truecaser model was learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016b) learned from processed English News Crawl; the number of operations was 50K. The implementation made by <cite>Artetxe et al. (2018b)</cite> 6 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model (Durrani et al., 2013) 7 , and 9-gram word class language model 8 , similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features.",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_6",
  "x": "Difference between forward and backward refinements To examine how different the refinement methods are, we counted the number of corrections predicted by each method. The number of USMT forward in iter 1 and iter 2 is 3,437 and 3,257, respectively, whereas that of USMT backward in iter 1 and iter 2 is 4,092 and 2,789. As for USMT backward , the number of corrections from iter 1 to iter 2 decreases by 1,303. <cite>Artetxe et al. (2018b)</cite> and Lample et al. (2018) reported that the BLEU score (Papineni et al., 2002) of unsupervised MT with backward-refinement improves with increasing iterations. In GEC, increasing the iterations of USMT backward improves the GEC accuracy by predicting less corrections.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_8",
  "x": "It is considered that our system is mostly not able to correct the word usage errors based on the context because the phrase table is still noisy. Although we observed some usage error examples of 'watch' in the synthetic source data, our model was not able to replace 'watch' to 'see' based on the context. both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT<cite> (Artetxe et al., 2018b)</cite> . In this study, we apply the USMT method of <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments.",
  "y": "similarities uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_0",
  "x": "In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000) . Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000;<cite> Kudo et al., 2004)</cite> . However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called \"pointwise\" prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010) . While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_1",
  "x": "Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000;<cite> Kudo et al., 2004)</cite> . However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called \"pointwise\" prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010) . While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach<cite> (Kudo et al., 2004)</cite> on in-domain data, and is significantly more robust to out-of-domain data.",
  "y": "differences"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_2",
  "x": "---------------------------------- **JAPANESE MORPHOLOGICAL ANALYSIS** Japanese MA takes an unsegmented string of characters x I 1 as input, segments it into morphemes w J 1 , and annotates each morpheme with a part of speech t J 1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004) , or as a single joint process of finding a morpheme/POS string from unsegmented text <cite>(Kudo et al., 2004</cite>; Nakagawa, 2004; Kruengkrai et al., 2009) . In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_3",
  "x": "**TYPE** Feature Strings tire sentences as in Figure 1 (a). The CRF-based method presented by<cite> Kudo et al. (2004)</cite> is generally accepted as the state-of-the-art in this paradigm. CRFs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations. The model is able to take into account arbitrary features, as well as the context between neighboring tags.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_4",
  "x": "We follow<cite> Kudo et al. (2004)</cite> in defining our feature set, as summarized in Table 1 1 . Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are wordbased features, and information about transitions between POS tags is included. When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs. 1 More fine-grained POS tags have provided small boosts in accuracy in previous research<cite> (Kudo et al., 2004)</cite> , but these increase the annotation burden, which is contrary to our goal.",
  "y": "uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_5",
  "x": "We follow<cite> Kudo et al. (2004)</cite> in defining our feature set, as summarized in Table 1 1 . Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are wordbased features, and information about transitions between POS tags is included. When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs. 1 More fine-grained POS tags have provided small boosts in accuracy in previous research<cite> (Kudo et al., 2004)</cite> , but these increase the annotation burden, which is contrary to our goal.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_7",
  "x": "As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006) , an open source implementation of<cite> Kudo et al. (2004)</cite> 's CRF-based method (we will call this JOINT). For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR). In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word segmentation and POS tagging) to examine the contribution of context information (2-CRF). To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data 3 . As an evaluation measure, we follow Nagata (1994) and<cite> Kudo et al. (2004)</cite> and use Word/POS tag pair Fmeasure, so that both word boundaries and POS tags must be correct for a word to be considered correct.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_1",
  "x": "For example, consider the sentence in Figure 1 . It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer's purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) .",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_2",
  "x": "Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) . To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations between Figure 2 : Lexicalized syntactic tree used by SPADE.",
  "y": "motivation"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_3",
  "x": "The results show that this approach improved the results to achieve nearhuman perfomance, even with the use of automatic tools (syntax parser and discourse segmenter). 2 Related Work 2.1 Supervised Discourse Parsing<cite> Soricut and Marcu (2003)</cite> use two probabilistic models to perform a sentence-level analysis, one for segmentation and other to identify the relations and build the rhetorical structure. The parser is named SPADE (Sentence-level Parsing of DiscoursE) and the authors base their model on lexical and syntactic information, extracting features from a lexicalized syntactic tree. They assume that the features extracted at the jointing point of two discursive segments are the most indicative information to identify the rhetorical structure of the sentence. For example, in Figure 2 , the circled nodes correspond to the most indicative cues to identify the structure and relation between each two adjacent segments.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_4",
  "x": "The needed data is widely and freely available on the web. Their architecture runs 24 hours per day, forever, obtaining new information and performing a learning task. With the aim of surpassing the limitation of labeled RST in Portuguese to develop a good DP, we employ SSNEL in the task by adapting the work of<cite> Soricut and Marcu (2003)</cite> and Hernault et al. (2010) . This choice for SSLNEL was made considering the large and free availability of news texts on the web. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_5",
  "x": "This work focuses on the identification of rhetorical relations at the sentence level, and as is common since the work of<cite> Soricut and Marcu (2003)</cite> , fine-grained relations were grouped: 29 sentence-level rhetorical relations were found and grouped into 16 groups. The imbalance of the relations is a natural characteristic in discourse and, to avoid overfitting of a learning model on the lessfrequent relations, no balancing was made. The relation Summary, for example, occurs only 2 times, and Elaboration occurs 1491 times, making very difficult the identification of the Summary relation. ---------------------------------- **ADAPTED MODELS**",
  "y": "background similarities uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_6",
  "x": "Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser. In this adaptation, only pairs of adjacent segments at sentence-level were considered, and nuclearity was not considered, in order to avoid sparseness in the data. Training the adapted model (here called SPADE-PT) using the RST-DT-PT achieved F-measure of 0.30. The precision was 0.69, but the recall was only 0.19.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_7",
  "x": "Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser. In this adaptation, only pairs of adjacent segments at sentence-level were considered, and nuclearity was not considered, in order to avoid sparseness in the data. Training the adapted model (here called SPADE-PT) using the RST-DT-PT achieved F-measure of 0.30. The precision was 0.69, but the recall was only 0.19.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_8",
  "x": "In the SSNEL for English, only decision-tree classifiers were used to classify new instances. For Portuguese, a symbolic model (lexical patterns) was also used together with the classifiers. The improved results presented in Table 2 and 3 are very different due to differing evaluation strategies. Using separated test data, we tried to avoid possible overfitting on training data, but the size of test data may not lead to a fair evaluation<cite> Soricut and Marcu (2003)</cite> or Joty et al. (2012) , since HILDA-PT used different corpora (RST-DT-PT instead of RST-DT), and some reported results are for the complete DP. However, our results show the potential of the SSNEL workflow when not enough labeled data is available for supervised learning, since the same approach for relation identification of Hernault et al. (2010) was used in HILDA-PT and 0.531 was initially obtained.",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_0",
  "x": "Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. But use of hierarchical decoders has not solved the DE construction translation problem. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005;<cite> Wang et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_1",
  "x": "For example <cite>Wang et al. (2007)</cite> introduced a set of rules to decide if a (DE) construction should be reordered or not before translating to English: \u2022 For DNPs (consisting of\"XP+DEG\"): -Reorder if XP is PP or LCP; -Reorder if XP is a non-pronominal NP \u2022 For CPs (typically formed by \"IP+DEC\"): -Reorder to align with the \"that+clause\" structure of English. Although this and previous reordering work has led to significant improvements, errors still remain. Indeed, <cite>Wang et al. (2007)</cite> found that the precision of their NP rules is only about 54.6% on a small human-judged set. One possible reason the (DE) construction remains unsolved is that previous work has paid insufficient attention to the many ways the (DE) construction can be translated and the rich structural cues to the translation.",
  "y": "background motivation"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_3",
  "x": "**DE CLASSIFICATION** The Chinese character DE serves many different purposes. According to the Chinese Treebank tagging guidelines (Xia, 2000) , the character can be tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to<cite> (Wang et al., 2007)</cite> , we only consider the majority case when the phrase with (DE) is a noun phrase modifier. The DEs in NPs have a part-of-speech tag of DEC (a complementizer or a nominalizer) or DEG (a genitive marker or an associative marker).",
  "y": "background similarities"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_4",
  "x": "**CLASS DEFINITION** The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of <cite>Wang et al. (2007)</cite> where they use rules to decide if a certain DE and the words next to it will need to be reordered. Some NPs are translated into a hybrid of these categories, or just don't fit into one of the five categories, for instance, involving an adjectival premodifier and a relative clause. In those cases, they are put into an \"other\" category.",
  "y": "background similarities"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_6",
  "x": "2 As a baseline, we use the rules introduced in <cite>Wang et al. (2007)</cite> to decide if the DEs require reordering or not. However, since their rules only decide if there is reordering in an NP with DE, their classification result only has two classes. So, in order to compare our classifier's performance with the rules in <cite>Wang et al. (2007)</cite> , we have to map our five-class results into two classes. We mapped our five-class results into two classes. So we mapped B preposition A and relative clause into the class \"reordered\", and the other three classes into \"not-reordered\".",
  "y": "extends"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_8",
  "x": "However, since we are using automatic parses instead of gold-standard ones, the DEPOS feature might have other values than just DEC and DEG. From Table 2 , we can see that with this simple feature, the 5-class accuracy is low but at least better than simply guessing the majority class (47.92%). The 2-class accuracy is still lower than using the heuristic rules in<cite> (Wang et al., 2007)</cite> , which is reasonable because their rules encode more information than just the POS tags of DEs. A-pattern: Chinese syntactic patterns appearing before Secondly, we want to incorporate the rules in<cite> (Wang et al., 2007)</cite> as features in the log-linear classifier.",
  "y": "uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_9",
  "x": "true if A+DE is a DNP which is in the form of \"QP+DEG\". 3. A is pronoun: true if A+DE is a DNP which is in the form of \"NP+DEG\", and the NP is a pronoun. 4. A ends with VA: true if A+DE is a CP which is in the form of \"IP+DEC\", and the IP ends with a VP that's either just a VA or a VP preceded by a ADVP. Features 1-3 are inspired by the rules in<cite> (Wang et al., 2007)</cite> , and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000) . 3 We call these four features A-pattern.",
  "y": "similarities uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_11",
  "x": "We have two different settings as baseline experiments. Also, we reorder the training data, the tuning and the test sets with the NP rules in<cite> (Wang et al., 2007)</cite> and compare our results with this second baseline (WANG-NP). The NP reordering preprocessing (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT.",
  "y": "background uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_12",
  "x": "Our approach DE-Annotated reorders the Chinese sentence, which is similar to the approach proposed by <cite>Wang et al. (2007)</cite> (WANG-NP). However, our focus is on the annotation on DEs and how this can improve translation quality. Table 7 shows an example that contains a DE construction that translates into a relative clause in English. 12 The automatic parse tree of the sentence is listed in Figure 3 . The reordered sentences of WANG-NP and DE-Annotated appear on the top and bottom in Figure 4 .",
  "y": "differences similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_0",
  "x": "this setting, we apply the methodology used by <cite>Shoemark et al. (2017)</cite> in the context of the 2014 Scottish independence referendum to a dataset of tweets related to the Catalonian referendum. We use the phenomenon of code-switching between Catalan and Spanish to pursue the following research questions in order to understand the choice of language in the context of the referendum: 1. Is a speaker's stance on independence strongly associated with the rate at which they use Catalan? 2. Does Catalan usage vary depending on whether the discussion topic is related to the referendum, and on the intended audience? For the first question, our findings are similar to those in the Scottish case: pro-independence tweets are more likely to be written in Catalan than anti-independence tweets, and pro-independence Twitter users are more likely to use Catalan than anti-independence Twitter users (Section 4).",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_1",
  "x": "A relatively unexplored area of code-switching behavior is politically-motivated code-switching, which we assume has a different set of constraints compared to everyday code-switching. With respect to political separatism, <cite>Shoemark et al. (2017)</cite> studied the use of Scots, a language local to Scotland, in the context of the 2014 Scotland independence referendum. They found that Twitter users who openly supported Scottish independence were more likely to incorporate words from Scots in their tweets. They also found that Twitter users who tweeted about the referendum were less likely to use Scots in referendum-related tweets than in non-referendum tweets. This study considers the similar scenario which took place in 2017 vis-\u00e0-vis the semi-autonomous region of Catalonia.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_2",
  "x": "Our main methodological divergence from <cite>Shoemark et al. (2017)</cite> relates to the linguistic phenomenon at hand: while Scots is mainly manifested as interleaving individual words within English text (code-mixing), Catalan is a distinct language which, when used, usually replaces Spanish altogether for the entire tweet (code-switching). ---------------------------------- **DATA** The initial set of tweets for this study, T , was drawn from a 1% Twitter sample mined between January 1 and October 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath. 2 The first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_3",
  "x": "From these co-occurring hashtags, we selected a set of 46 hashtags and divided it into pro-independence, anti-independence, and neutral hashtags, based on translations of associated tweet content. 3 After including ASCII-equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings. Next, all tweets containing any referendum hashtag were extracted from T , yielding 190,061 tweets. After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in <cite>Shoemark et al. (2017)</cite> ). 36 referendum-related hashtags appear in the filtered dataset.",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_4",
  "x": "Next, all tweets containing any referendum hashtag were extracted from T , yielding 190,061 tweets. After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in <cite>Shoemark et al. (2017)</cite> ). 36 referendum-related hashtags appear in the filtered dataset. They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_5",
  "x": "They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf. the 693,815 control tweets in Table 6 of <cite>Shoemark et al. (2017)</cite> ). The CT dataset is very balanced with respect to the number of tweets per user: only four users contribute over ten tweets (max = 14) and only 16 have more than five. The XT dataset also has only a few \"power\" users, such that nine users have over 1,000 tweets (max = 3,581) and a total of 173 have over 100 tweets.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_6",
  "x": "To measure Catalan usage, let n To determine significance, the users are randomly shuffled between the two groups to recompute d over 100,000 iterations. The p-value is the proportion of permutations in which the randomized test statistic was greater than or equal to the original test statistic from the unpermuted data. Results. Catalan is used more often among the pro-independence users compared to the antiindependence users, across both the hashtagonly and all-tweet conditions. Table 3 shows that the proportion of tweets in Catalan for proindependence users (p pro ) is significantly higher than the proportion for anti-independence users (p anti ). This is consistent with <cite>Shoemark et al. (2017)</cite> , who found more Scots usage among proindependence users (d = 0.00555 for pro/anti tweets, d = 0.00709 for all tweets).",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_7",
  "x": "Our second result is the opposite of the prior finding that there were significantly fewer Scots words in referendum-related tweets than in control tweets (cf. Table 7 in <cite>Shoemark et al. (2017)</cite> ; d u = \u22120.0015 for all controls). This suggests that Catalan may serve a different function than Scots in terms of political identity expression. Rather than suppressing their use of Catalan in broadcast tweets, users increase their Catalan use, perhaps to signal their Catalonian identity to a broader audience. This is supported by literature highlighting the integral role Catalan plays in the Catalonian national narrative (Crameri, 2017) , as well as the relatively high proportion of Catalan speakers in Catalonia: 80.4% of the population has speaking knowledge of Catalan (Government of Catalonia, 2013) , versus 30% population of Scotland with speaking knowledge of Scots (Scots Language Centre, 2011) .",
  "y": "differences"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_0",
  "x": "Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2, 4, 5, <cite>6,</cite> 10, 11, 13, 14] . The chunking task was extended to the CoNLL-2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3] . Most previous works with relatively high performance in English used machine learning methods for chunking [4, 13] . Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_1",
  "x": "A number of conditional models recently have been developed for use. They showed better performance than generative models as they can handle many arbitrary and overlapping features of input sequence [12] . A number of methods are applied to chunking in Korean texts. Unlike English, a rule-based chunking method [7, 8] is predominantly used in Korean because of its well-developed function words, which contain information such as grammatical relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_2",
  "x": "Unlike English, a rule-based chunking method [7, 8] is predominantly used in Korean because of its well-developed function words, which contain information such as grammatical relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts. CRFs are undirected graphical models trained to maximize conditional probabilities of label sequence given input sequence.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_3",
  "x": "Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts. CRFs are undirected graphical models trained to maximize conditional probabilities of label sequence given input sequence. It takes advantage of generative and conditional models.",
  "y": "background uses"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_4",
  "x": "For example, when the part-of-speech of current word is one of determiner, pronoun and noun, the following seven rules for NP chunking in Table 1 can find most NP chunks in text, with about 89% accuracy<cite> [6]</cite> . For this reason, boundaries of chunks are easily found in Korean, compared to other languages such as English or Chinese. This is why a rule-based chunking method is predominantly used. However, with sophisticated rules, the rule-based chunking method has limitations when handling exceptional cases. Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5,<cite> 6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_5",
  "x": "For this reason, boundaries of chunks are easily found in Korean, compared to other languages such as English or Chinese. This is why a rule-based chunking method is predominantly used. However, with sophisticated rules, the rule-based chunking method has limitations when handling exceptional cases. Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5,<cite> 6]</cite> . Many recent machine learning techniques can capture hidden characteristics for classification.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_6",
  "x": "Abney was the first to use the term 'chunk' to represent a non-recursive core of an intra-clausal constituent, extending from the beginning of constituent to its head. In Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)<cite> [6]</cite> . As function words such as postposition or ending are well-developed, the number of chunk types is small compared to other languages such as English or Chinese. Like the CoNLL-2000 dataset, we use three types of chunk border tags, indicating whether a word is outside a chunk (O), starts a chunk (B), or continues a chunk (I). Each chunk type XP has two border tags: B-XP and I-XP.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_7",
  "x": "Park et al. reported the performance of various chunking methods<cite> [6]</cite> . We add the experimental results of the chunking methods using HMMs-bigram and CRFs. In Table 6 , F-score of chunking using CRFs in Korean texts is 97.19%, the highest performance of all. It significantly outperforms all others, including machine learning methods, rule-based methods and hybrid methods. It is because CRFs have a global optimum solution hence overcoming the label bias problem.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_8",
  "x": "However, the performance of NP chunk type is lowest, 94.27% because the border of NP chunk type is very ambiguous in case of consecutive nouns. Using more features such as previous chunk tag should be able to improve the performance of NP chunk type. Park et al. reported the performance of various chunking methods<cite> [6]</cite> . We add the experimental results of the chunking methods using HMMs-bigram and CRFs. In Table 6 , F-score of chunking using CRFs in Korean texts is 97.19%, the highest performance of all.",
  "y": "similarities"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_0",
  "x": "Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention. The existing work on building chatbots includes generation-based methods and retrieval-based methods. In this paper, we focus on the second type and study the problem of multi-turn response selection. This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017 ).",
  "y": "background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_1",
  "x": "The generation-based methods synthesize a response with a natural language generation model by maximizing its generation probability given the previous conversation context. This approach enables the incorporation of rich context when mapping between consecutive dialogue turns (Shang et al., 2015; Serban et al., 2016; . Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (Li et al., 2016; Zhou et al., 2018a) . Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate. This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017 EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EB  EB  EB  EB  EB   E0  E1  E2  E3  E4  E5  E18  E19  E6  E7  E8  E9  E10  E11  E17  E12  E13  E14  E15  E16  E20  E21  E22  E23  E24  E25 [ E0  E0  E0  E0  E0  E0  E0  E0  E1  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.",
  "y": "background similarities"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_2",
  "x": "Finally, the classifier returns a score to denote the matching degree of this context-response pair. We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1<cite> (Lowe et al., 2015)</cite> , Ubuntu Dialogue Corpus V2 (Lowe et al., 2017) , Douban Conversation Corpus (Wu et al., 2017) , E-commerce Dialogue Corpus (Zhang et al., 2018b) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan Kim, 2019). The first four datasets have been disentangled in advance and our proposed speaker-aware disentanglement strategy has been applied to only the last DSTC 8-Track 2-Subtask 2 Corpus. Ubuntu Dialogue Corpus V1, V2 and DSTC 8-Track 2-Subtask 2 Corpus contain multi-turn dialogues about Ubuntu system troubleshooting in English. Here, we adopted the version of Ubuntu Dialogue Corpus V1 shared in , in which numbers, paths and URLs were replaced by placeholders.",
  "y": "background uses"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_3",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **EVALUATION METRICS** We used the same evaluation metrics as those used in previous work<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017; Zhang et al., 2018b; Seokhwan Kim, 2019) .",
  "y": "uses"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_0",
  "x": "Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and <cite>(Grave et al., 2018)</cite> , capture syntactic or semantic information as measured by analogy tasks. Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens. Therefore, in this work, we introduce KaWAT (Kata Word Analogy Task), an Indonesian word analogy task dataset, and new Indonesian word embeddings pretrained on 160M tokens of online news corpus. We evaluated these embeddings on KaWAT, and also tested them on POS tagging and text summarization as representatives of syntactic and semantic downstream task respectively.",
  "y": "background"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_1",
  "x": "In total, we have 15K syntactic and 19K semantic analogy queries. KaWAT is available online. 1 One of the goals of this work is to evaluate and compare existing Indonesian pretrained word embeddings. We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and <cite>(Grave et al., 2018)</cite> , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively. We refer to them as Wiki/fastText and CC/fastText hereinafter.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_0",
  "x": "**INTRODUCTION** Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; . Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014) , where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; . Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD <cite>(Levy et al., 2015)</cite> and GloVe (Pennington et al., 2014) , both achieving state-of-the-art performance on a variety of tasks. * This is a preprint of the paper that will be presented at the 54th Annual Meeting of the Association for Computational Linguistics.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_1",
  "x": "Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements M wc is defined as the number of times a target word w and the context word c co-occurred in the corpus within the window. The PMI matrix is defined as where '*' represents the summation of the corresponding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) .",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_2",
  "x": "The PMI matrix is defined as where '*' represents the summation of the corresponding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) . As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_3",
  "x": "As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices. where W w andW c are d-dimensional row vectors corresponding to vector embeddings for the target and context words. Using the truncated SVD of size d yields the factorization U \u03a3T \u22a4 with the lowest possible L 2 error (Eckert and Young, 1936) . Levy et al. (2015) recommend using W = U \u03a3 p as the word representations, as suggested by Bullinaria and Levy (2012) , who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis. Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015) , we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments <cite>(Levy et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_4",
  "x": "All models were trained on a dump of Wikipedia from June 2015, split into sentences, with punctuation removed, numbers converted to words, and lower-cased. Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embeddings of 300 dimensions. The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in <cite>(Levy et al., 2015)</cite> and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 \u22125 (Mikolov et al., 2013b) .",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_5",
  "x": "2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b) , a window of size 10 (<cite> Levy et al., 2015)</cite> , for 5 iterations with initial learning rate set to the default 0.025. GloVe is run with a window of size 10, x max = 100, \u03b2 = 3/4, for 50 iterations and initial learning rate of 0.05 (Pennington et al., 2014) . In LexVec two window sampling alternatives are compared: W S P P M I , which keeps the same fixed size win = 2 as used to create the P P M I * matrix; or W S SGN S , which adopts identical SGNS settings (win = 10 with size randomization). We run LexVec for 5 iterations over the training corpus. All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following<cite> Levy et al. (2015)</cite> , and W and W +W for LexVec.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_6",
  "x": "All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following<cite> Levy et al. (2015)</cite> , and W and W +W for LexVec. For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) , using 3CosAdd and 3CosM ul . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_7",
  "x": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) , using 3CosAdd and 3CosM ul . ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_8",
  "x": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies. In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_9",
  "x": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies. In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_10",
  "x": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies. In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "40d370558d873499e493a83f106f17_0",
  "x": "For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. ---------------------------------- **INTRODUCTION** Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994;<cite> Lin, 1998</cite>; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_1",
  "x": "**INTRODUCTION** Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994;<cite> Lin, 1998</cite>; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures<cite> (Lin, 1998</cite>; Curran and Moens, 2002; Ferret, 2010) , identifying and demoting bad neighbors (Ferret, 2013) , or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013) .",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_2",
  "x": "**RELATED WORK** In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) .",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_3",
  "x": "**RELATED WORK** In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) .",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_4",
  "x": "Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs.",
  "y": "background motivation"
 },
 {
  "id": "40d370558d873499e493a83f106f17_5",
  "x": "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_6",
  "x": "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) .",
  "y": "background motivation"
 },
 {
  "id": "40d370558d873499e493a83f106f17_7",
  "x": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.",
  "y": "motivation"
 },
 {
  "id": "40d370558d873499e493a83f106f17_8",
  "x": "The thesauri were constructed using <cite>Lin's (1998)</cite> method. Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013) . In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin's measure, we apply them to remove potentially noisy triples: \u2022 Threshold (th): we remove triples that occur less than a threshold th.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_0",
  "x": "It is thus imperative to evaluate the extent to which they exhibit social and intersectional bias. May et al. <cite>[21]</cite> establish a preliminary study of social bias in BERT, but their analysis relies only on sentence level encodings. Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models. We also evaluate intersectional (gender + race) bias, since the lived experience of groups with multiple minority identities is cumulatively worse than that of each of the groups with a singular minority identity [9] . We adapt the Sentence Encoder Association Test (SEAT) <cite>[21]</cite> to evaluate how these techniques displays bias in contextual word representations.",
  "y": "background motivation"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_1",
  "x": "Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models. We also evaluate intersectional (gender + race) bias, since the lived experience of groups with multiple minority identities is cumulatively worse than that of each of the groups with a singular minority identity [9] . We adapt the Sentence Encoder Association Test (SEAT) <cite>[21]</cite> to evaluate how these techniques displays bias in contextual word representations. This effectively gives a new metric that considers a word embedding and its bias in context. We find that the standard corpora for pre-training contextual word models exhibit significant gender bias imbalances.",
  "y": "extends"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_2",
  "x": "Zhao et al. [35] and Basta et al. [1] demonstrate gender bias in ELMo [25] word embeddings, whereas May et al. <cite>[21]</cite> evaluate various models of contextual word representations on a sentential generalization of WEAT. Our work extends such analyses in two ways: 1) we consider a wide variety of contextual word embedding tools including state-of-the-art approaches such as BERT and GPT-2, 2) we extend the evaluation to consider contextual word representations as opposed to prior work, which either used word embeddings without context or used sentence-level embeddings that can have additional confounds and obscure bias, and 3) we include additional embedding association tests targeting gender, race and intersectional identities. 1 and BooksCorpus (800M words) [36] . ELMo [25] was trained on the 1 Billion Word Benchmark (1,000M words) [7] . GPT [26] was trained on BooksCorpus, and GPT-2 was trained on WebText [27] .",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_3",
  "x": "**SOCIAL AND INTERSECTIONAL BIAS USING EMBEDDING ASSOCIATION TESTS** ---------------------------------- **EMBEDDING ASSOCIATION TESTS** We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations. Caliskan et al. [5] proposed Word Embedding Association Tests (WEATs), which follows human implicit association tests [14] in measuring the association between two target concepts and two attributes.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_4",
  "x": "We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations. Caliskan et al. [5] proposed Word Embedding Association Tests (WEATs), which follows human implicit association tests [14] in measuring the association between two target concepts and two attributes. We follow May et al. <cite>[21]</cite> in describing WEATs and SEATs. Let X and Y be equal-size sets of target concept embeddings, and A and B be sets of attribute embeddings. These embeddings are obtained after encoding a set of words which define the concept or attribute.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_5",
  "x": "To compute the significance of the association between (A, B) and (X, Y ), a permutation test on s(X, Y, A, B) is used. where the probability is computed over the space of partitions (X i , Y i ) of X \u222a Y so that X i and Y i are of equal size. The effect size is defined to be A larger effect size corresponds to more severe pro-stereotypical representations, controlling for significance. May et al. <cite>[21]</cite> adopt the WEAT tests [5] into Sentence Encoder Association Tests (SEATs) to test biases using sentence encodings.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_6",
  "x": "We refer to WEATs and SEATs as Caliskan tests [5] . ---------------------------------- **EXTENSION OF EMBEDDING ASSOCIATION TESTS TO CONTEXTUAL WORD REPRESENTATIONS** May et al. <cite>[21]</cite> suggest that although they find less bias in sentence encoders than context free word embeddings, the sentence templates may not be as semantically bleached as expected, and that a lack of evidence of bias should not be taken as a lack of bias. We propose to assess bias at the contextual word level.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_7",
  "x": "To investigate social and intersectional bias, we introduce new embedding association tests to more comprehensively target race, gender and intersectional identities. The new tests are prefixed by a \"+\" in Tables 3, 4 and 5. For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> . The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_8",
  "x": "For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> . The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_9",
  "x": "Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] . 5 Empirical Analysis",
  "y": "background uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_10",
  "x": "Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] . 5 Empirical Analysis",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_11",
  "x": "5 Empirical Analysis ---------------------------------- **EXPERIMENTS** We investigate biases in GPT-2 [27] , one of the state-of-the-art models for contextual word representations, in both its 117M and 345M versions. For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_12",
  "x": "For all association tests, we use p = 0.01 for significance testing. We use PyTorch, as well as the framework and code from May et al. <cite>[21]</cite> , to conduct the experiments 6 . ---------------------------------- **OVERALL ANALYSIS** We report the proportion of tests with significant effects in Table 2 . Note that all instances of significant effects had positive effect sizes.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_13",
  "x": "Combining the above de-biasing techniques for contextual word models remains a crucial direction for future work. Furthermore, methods for de-biasing specifically across race, gender, and intersectional identities remains a challenging open question. Similar to May et al. <cite>[21]</cite> , in the continuous bag of words (CBoW) model we encode sentences as the average of word embeddings using 300-dimensional GloVe vectors 7 trained on the Common Crawl corpus [24] . There is no corresponding contextual word level equivalent since GloVe is context-free. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_14",
  "x": "---------------------------------- **A.2 ELMO** Following May et al. <cite>[21]</cite> , the sentence encoding of ELMo is a sequence of vectors, one for each token. We use mean-pooling over the tokens, followed by summation over aggregated layer outputs to obtain the final 1024-dimensional sentence encoding. At the contextual word level, we do not apply mean-pooling over tokens.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_15",
  "x": "We conduct experiments on both the 768-dimensional bert-base-cased (bbc) and 1024dimensional bert-large-cased (blc) versions of BERT. We use the implementations of BERT from Hugging Face 9 . ---------------------------------- **A.4 GPT** Similar to May et al. <cite>[21]</cite> and the original word [26] , we use the representation corresponding to the last word in the sequence as the sentence encoding.",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_16",
  "x": "Both encoding types are 768-dimensional. Different from May et al. <cite>[21]</cite> , we use the implementation of GPT from Hugging Face, and not the jiant project 10 . prefix weat but sentence level tests have the prefix sent-weat. The Caliskan Tests are named weat1 to weat10and sent-weat1 to sent-weat10. Alternate versions of the Caliskan Tests using group terms instead of names and vice versa are denotetd by the suffix b. The additional tests are named weat+11 to weat+13 and sent-weat+11 to sent-weat+13, as well as weat+i1 to weat+i5 and sent-weat+i1 to sent-weat+i5, and lastly weat+occ and sent-weat+occ.",
  "y": "differences"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_17",
  "x": "The double bind tests targeting gender are denoted with the prefix weat_hdb or sent-weat_hdb. The double bind tests targeting race are denoted with the prefix weat_r_hdb or sent-weat_r_hdb. The tests regarding the angry black woman stereotype are denoted with the prefix weat_angry or sent-weat_angry. The Caliskan Tests are detailed in Caliskan et al. [5] , and the double bind tests are detailed in May et al. <cite>[21]</cite> . The additional tests are detailed in the sections below.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_18",
  "x": "In the full results presented in Tables 7 and 8 , we also report results for neutral tests (C1-2) and tests regarding disability and age (C9-10). In both the race and gender double bind tests, sentence templates that are either bleached of semantic meaning or unbleached are used. Following May et al. <cite>[21]</cite> , examples of such templates (non-exhaustive) are as below. ---------------------------------- **B.3 INTERSECTIONAL**",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_0",
  "x": "---------------------------------- **INTRODUCTION** In NLP, Neural language model pre-training has shown to be effective for improving many tasks [12, 26] . Transformer<cite> [34]</cite> is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely. At present, this model has received extensive attentions and plays an key role in many neural language models, such as BERT [12] , GPT [27] and Universal Transformer [10] .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_1",
  "x": "However, the new structure after compressing can not be integrated into the model. In Transformer, the multi-head attention is a key part and it is constructed by a large number of parameters. Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_2",
  "x": "Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer. First, the self-attention function in Transformer is a non-linear function, which makes it difficult to compress. In order to address this challenge, we first prove that the output of the attention function of the self-attention model<cite> [34]</cite> can be linearly represented by a group of orthonormal base vectors.",
  "y": "motivation"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_3",
  "x": "Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer. First, the self-attention function in Transformer is a non-linear function, which makes it difficult to compress. In order to address this challenge, we first prove that the output of the attention function of the self-attention model<cite> [34]</cite> can be linearly represented by a group of orthonormal base vectors.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_4",
  "x": "This process can lead to reduce many parameters. The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] . In order to address this challenge, there are three steps as follows. First, the average of each block tensor can be computed; Second, some matrices can be given by tensor split. Third, the concatenation of these matrices can serve as the input to the next layer network in Transformer.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_5",
  "x": "The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] . In order to address this challenge, there are three steps as follows. First, the average of each block tensor can be computed; Second, some matrices can be given by tensor split. Third, the concatenation of these matrices can serve as the input to the next layer network in Transformer. After that, it can be integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] and trained end-to-end.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_6",
  "x": "**PRELIMINARIES** Multi-linear attention is carried out in this paper. The analysis of Multi-linear attention relies on these concepts and results from the field of tensor decomositon and multi-head attention. We cover below in Section 2.1 basic background on Block-Term tensor decomposition [9] . Then, we describe in Section 2.2 multi-head attention<cite> [34]</cite> .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_7",
  "x": "In Transformer, the attention function is named as \"Scaled Dot-Product Attention\". In practice, Transformer<cite> [34]</cite> processes query, keys and values as matrices Q, K, and V respectively. The attention function can be written as follows: where d is the number of columns of Q and K. In these work <cite>[34,</cite> 12, 7] , they all use the multi-head attention, as introduced in<cite> [34]</cite> , where matrices W Q i and",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_8",
  "x": "In practice, Transformer<cite> [34]</cite> processes query, keys and values as matrices Q, K, and V respectively. The attention function can be written as follows: where d is the number of columns of Q and K. In these work <cite>[34,</cite> 12, 7] , they all use the multi-head attention, as introduced in<cite> [34]</cite> , where matrices W Q i and In this work<cite> [34]</cite> , multiple groups of parameters (W Q i , W K i and W V i ) are used, which results in a large number of redundant parameters.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_9",
  "x": "A diagram which is about the incorporating of Multi-linear attention in partial Transformer structure is given in Supplementary Materials E.1. ---------------------------------- **ANALYSIS OF COMPRESSION AND COMPLEXITY** Compression Our focus is on the compression of the multi-head mechanism in the multi-head attention of Transformer. Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_10",
  "x": "Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings. We use three linear ma for matrices Q, K and V , respectively. For the output of three mappings, we choose to share them which are considered as three factor matrices in reconstructing the Multi-linear attention. This process is shown in Figure 2 (left) . h is the number of heads in<cite> [34]</cite> , and d is the dimension of factor matrices.",
  "y": "similarities"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_11",
  "x": "We use three linear ma for matrices Q, K and V , respectively. For the output of three mappings, we choose to share them which are considered as three factor matrices in reconstructing the Multi-linear attention. This process is shown in Figure 2 (left) . h is the number of heads in<cite> [34]</cite> , and d is the dimension of factor matrices. The compression ratios can be computed by (3 \u00d7 h \u00d7 d)/(3 \u00d7 d + h).",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_12",
  "x": "However, we can reorder the computations to reduce the model complexity O(R 2 d), where R is the rank of the tensor which can be set in our experiments. In our experiments, R is set as the number between 10 and 18 which is smaller than N . The minimum number of sequential operations in Multi-linear attention for different layers is lower than that of the self-attention in Transformer<cite> [34]</cite> . ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_13",
  "x": "---------------------------------- **RELATED WORK** The field of language modeling has witnessed many significant advances. Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing. Transformer networks have a potential of learning long-term dependency, but are limited by a fixed-length context in the setting of language modeling.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_14",
  "x": "Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing. Transformer networks have a potential of learning long-term dependency, but are limited by a fixed-length context in the setting of language modeling. Vaswani et al.<cite> [34]</cite> uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question. BERT [12] is a kind of bidirectional encoder representations from transformers. It is designed to pre-train deep bidirectional representation and obtains new SoTA on some NLP tasks.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_15",
  "x": "In this task, we have trained the Transformer model<cite> [34]</cite> on WMT 2016 English-German dataset [30] . Sentences were tokenized using the SentencePiece 3 . For our experiments, we have replaced each of the attention layers with Multi-linear attention. For evaluation we used beam search with a beam size of 5 and length penalty \u03b1=0.6. In this section, we only compared the results with Transformer<cite> [34]</cite> .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_16",
  "x": "Sentences were tokenized using the SentencePiece 3 . For our experiments, we have replaced each of the attention layers with Multi-linear attention. For evaluation we used beam search with a beam size of 5 and length penalty \u03b1=0.6. In this section, we only compared the results with Transformer<cite> [34]</cite> . Our results are summarized in Table 3 .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_17",
  "x": "* indicates that the result is our own implementation. In Table 3 , we select two baseline models. The Base-line [30] is first model in WMT 2016 English-German dataset. For the other baseline, we use the basic Transformer architecture<cite> [34]</cite> . The BLEU score is 34.5 for the basic architecture.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_0",
  "x": "These are in contrast to 'supervised' dialogue systems, which we define as those that explicitly incorporate some supervised signal such as task completion or user satisfaction 1 . Unsupervised systems can be roughly separated into response generation systems that attempt to produce a likely response given a conversational context, and retrieval-based systems that attempt to select a response from a (possibly large) list of utterances in a corpus. While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; , it has recently been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (Liu et al., 2016) . Retrieval-based systems are of interest because they admit a natural evaluation metric, namely the recall and precision measures. First introduced for evaluating user simulations by Schatzmann et al. (2005) , such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015; Dodge et al., 2015) .",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_1",
  "x": "NUC is useful for several reasons: 1) the performance (i.e. loss or error) is easy to com-pute automatically, 2) it is simple to adjust the difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems , and 5) models trained with NUC can be converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016) . In this case, NUC additionally allows for making hard constraints on the allowable outputs of the system (to prevent offensive responses), and guarantees that the responses are fluent (because they were generated by humans). Thus, NUC can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the bAbI tasks for language understanding , and as a useful framework for building chatbots. With the huge size of current dialogue datasets that contain millions of utterances<cite> (Lowe et al., 2015a</cite>; Banchs, 2012; Ritter et al., 2010) and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans. However, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task.",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_2",
  "x": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012) , the Twitter Corpus (Ritter et al., 2010) , and the Ubuntu Dialogue Corpus<cite> (Lowe et al., 2015a)</cite> . Since conversations in the Ubuntu Dialogue Corpus are highly technical, we recruit 'expert' humans who are adept with the Ubuntu terminology, whom we compare with a state-of-the-art machine learning agent on all datasets. We find that there is indeed a significant separation between machine and expert hu- man performance, suggesting that NUC is a useful intermediate task for measuring progress. ---------------------------------- **TECHNICAL BACKGROUND ON NUC**",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_3",
  "x": "Next-Utterance-Classification (NUC, see Figure 1 ) is a task, which is straightforward to evaluate, designed for training and validation of dialogue systems. They are evaluated using the metric of Recall@k, which we define in this section. In NUC, a model or user, when presented with the context of a conversation and a (usually small) pre-defined list of responses, must select the most appropriate response from this list. This list includes the actual next response of the conversation, which is the desired prediction of the model. The other entries, which act as false positives, are sampled from elsewhere in the corpus. Note that no assumptions are made regarding the number of utterances in the context: these can be fixed or sampled from arbitrary distributions. Performance on this task is easy to assess by measuring the success rate of picking the correct next response; more specifically, we measure Recall@k (R@k), which is the percentage of correct responses (i.e. the actual response of the conversation) that are found in the top k responses with the highest rankings according to the model. This task has gained some popularity recently for evaluating dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015) .",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_4",
  "x": "For AMT respondents, the questions were approximately evenly distributed across the three datasets, while for the lab experts, half of the questions were related to Ubuntu and the remainder evenly split across Twitter and movies. Each question had 1 correct response, and 4 false responses drawn uniformly at random from elsewhere in the (same) corpus. Participants had a time limit of 40 minutes. Conversations were extracted to form NUC conversation-response pairs as described in Sec. 2. The number of utterances in the context were sampled according to the procedure in<cite> (Lowe et al., 2015a)</cite> , with a maximum context length of 6 turns -this was done for both the human trials and ANN model.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_5",
  "x": "The human results are separated into AMT nonexperts, consisting of paid respondents who have 'Beginner' or no knowledge of Ubuntu terminology; AMT experts, who claimed to have 'Intermediate' or 'Advanced' knowledge of Ubuntu; and Lab experts. We also presents results on the same task for a state-of-the-art artificial neural network (ANN) dialogue model (see<cite> (Lowe et al., 2015a)</cite> for implementation details). We first observe that subjects perform above chance level (20% for R@1) on all domains, thus the task is doable for humans. Second we observe difference in performances between the three domains. The Twitter dataset appears to have the best predictability, with a Recall@1 approximately 8% points higher than for the movie dialogues for AMT workers, and 18% higher for lab experts.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_0",
  "x": "The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005) , which states roughly that contexts in which a narrow term x may appear (\"cat\") should be a subset of the contexts in which a broader term y (\"animal\") may appear. Intuitively, the DIH states that we should be able to replace any occurrence of \"cat\" with \"animal\" and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system<cite> (Shwartz et al., 2017)</cite> . Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016) . While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora.",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_1",
  "x": "Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012;<cite> Shwartz et al., 2017)</cite> . Here, we compare to two methods with strong empirical results. As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces. First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of x which are included in the set of a broader term's features, y: Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term.",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_2",
  "x": "Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. In particular, let denote the degree of inclusion of x in y as proposed by Clarke (2009) . To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014;<cite> Shwartz et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_3",
  "x": "This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in <cite>Shwartz et al. (2017)</cite> . Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by Kiela et al. (2015) : On BLESS, the task is to predict the direction for all 1337 positive pairs in the dataset. Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. score(x, y) > score(y, x). We reserve 10% of the data for validation, and test on the remaining 90%.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_4",
  "x": "We also remove any pair (y, x) if p(y, x) < p(x, y). The final corpus contains roughly 4.5M matched pairs, 431K unique pairs, and 243K unique terms. For SVD-based models, we select the rank from r \u2208 {5, 10, 15, 20, 25, 50, 100, 150, 200, 250 , 300, 500, 1000} on the validation set. The other pattern-based models do not have any hyperparameters. Distributional models: For the distributional baselines, we employ the large, sparse distributional space of <cite>Shwartz et al. (2017)</cite> , which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks.",
  "y": "uses"
 },
 {
  "id": "45723171ec398550e687c57d42e7cc_0",
  "x": "22, 46.8, 42.4, and 85.53 on tasks 1, 2, 3, and 4 respectively. ---------------------------------- **INTRODUCTION** The increasing use of social media platforms world wide offers an interesting application of natural language processing tools for monitoring public health and health-related events on the social media. The social media mining for health applications (SMM4H) shared task<cite> (Weissenbacher et al., 2018)</cite> hosts four tasks aiming to identify mentions of different aspects medication use on Twitter.",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_0",
  "x": "The agents systems use a calendar management system for displaying to their owners the results of the appointment negotiations. The users can enter their appointment constraints via a graphical user interface and receive the results either by e-mail or via their electronic calendar. Agent systems are thus hooked up to e-mail, to a calendar manager and to the dialogue server. The server interface is command-driven. A client may connect to the server and open up a dialogue (see Figure 1 in <cite>(Busemann et al., 1997)</cite> ).",
  "y": "uses"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_1",
  "x": "The use of SMES in COSMA, semantic analysis and inference, the dialogue model mapping between human and machine dialogue structures, utterance generation, the architectural framework of the server, and the PASHA agent system are described in <cite>(Busemann et al., 1997)</cite> . Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> . In particular, the systems to be demonstrated can process counterproposals, which form an important part of efficient and cooperative scheduling dialogues. ----------------------------------",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_2",
  "x": "Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> . In particular, the systems to be demonstrated can process counterproposals, which form an important part of efficient and cooperative scheduling dialogues. ---------------------------------- **THE DEMONSTRATION SCENARIO**",
  "y": "extends"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_0",
  "x": "Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models such as those proposed in<cite> Lin et al. (2017)</cite> , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases. On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism. On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism.",
  "y": "background motivation similarities"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_1",
  "x": [
   "Lin et al. (2017) proposed a scalar structure/multi-head self-attention method for sentence embedding. The multi-head self-attention is calculated by a MLP with only LSTM states as input. There are two main differences from our proposed method; i.e., (1) they used scalar attention instead of vectorial attention, (2) we propose different penalization terms which is suitable for vector-based multi-head self-attention, while their penalization term on attention matrix is only designed for scalar multi-head self-attention. Choi et al. (2018) proposed a fine-grained attention mechanism for neural machine translation, which also extend scalar attention to vectorial attention. Shen et al. (2017) proposes multi-dimensional/vectorial self-attention pooling on the top of self-attention network instead of BiLSTM."
  ],
  "y": "background differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_2",
  "x": "Instead of using AA T \u2212 I 2 F to encourage the diversity for scalar attention matrix as in<cite> Lin et al. (2017)</cite> , we propose the following formula to encourage the diversity for vectorial attention matrices. The penalization term on attention matrices is where \u03bb and \u00b5 are hyper-parameters which need to be tuned based on a development set. Intuitively, we try to encourage the diversity of any two different A i under the threshold \u03bb. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_3",
  "x": "We use the same data split as in Williams et al. (2017) , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing. Note that, we do not use SNLI as an additional training/development set in our experiments. Age Dataset To compare our models with that of<cite> Lin et al. (2017)</cite> , we use the same Age dataset in our experiment here, which is an Author Profiling dataset. The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter. The task is to predict the age range of authors of input tweets.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_4",
  "x": "We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing. ---------------------------------- **YELP DATASET** The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_5",
  "x": "The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. ---------------------------------- **TRAINING DETAILS** We implement our algorithm with Theano (Theano Development Team, 2016) framework.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_7",
  "x": "In addition, the results on cross-domain test set yield a new state of the art at an accuracy of 74.0%, which is better than 73.6% of shortcut-stacked BiLSTM (Nie and Bansal, 2017) . Table 3 shows the results of different models for the Yelp and the Age dataset. The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> . We also show that the results of baseline with mean pooling and last pooling, in which mean pooling achieves the best result on the Yelp dataset among three baseline models and max pooling achieves the best on the Age dataset among three baselines.",
  "y": "background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_8",
  "x": "The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> . We also show that the results of baseline with mean pooling and last pooling, in which mean pooling achieves the best result on the Yelp dataset among three baseline models and max pooling achieves the best on the Age dataset among three baselines. Our proposed generalized pooling method obtains further improvement on these already strong baselines, achieving 66.55% on the Yelp dataset and 82.63% on the Age dataset (statistically significant p < 0.00001 against best baselines), ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_10",
  "x": "The green lines indicate scalar selfattention pooling added on top of the BiLSTMs, same as in<cite> Lin et al. (2017)</cite> , and the blue lines indicate vector-based attention used in our generalized pooling methods. It is obvious that the vector-based attention achieves improvement over scalar attention. Different line styles are used to indicate selfattention using different numbers of multi-head, ranging from 1, 3, 5, 7 to 9. For vector-based attention, the 9-head model achieves the best accuracy of 86.8% on the development set. For scalar attention, the 7-head model achieves the best accuracy of 86.4% on the development set.",
  "y": "background differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_11",
  "x": "Specifically the proposed model aims to use vectors to enrich the expressiveness of attention mechanism and leverage proper penalty terms to reduce redundancy in multi-head attention. We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification. The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper. Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from<cite> Lin et al. (2017)</cite> .",
  "y": "background future_work"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_0",
  "x": "Fashioning a good story is an act of creativity and developing algorithms to replicate this has been a long running challenge. Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline. In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity. Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; <cite>Huang et al., 2016)</cite> . The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_1",
  "x": "A more direct dataset was recently released<cite> (Huang et al., 2016)</cite> , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums. This requires us to either generate a story from all of the album's photos or to learn selection mechanisms to identify representative photos and then generate stories from those summary photos.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_2",
  "x": "A more direct dataset was recently released<cite> (Huang et al., 2016)</cite> , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums. This requires us to either generate a story from all of the album's photos or to learn selection mechanisms to identify representative photos and then generate stories from those summary photos.",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_3",
  "x": "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling<cite> (Huang et al., 2016</cite>; Park and Kim, 2015) . While (Park and collects data by mining Blog Posts,<cite> (Huang et al., 2016)</cite> collects stories using Mechanical Turk, providing more directly relevant stories. ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_4",
  "x": "While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014) . Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling<cite> (Huang et al., 2016</cite>; Park and Kim, 2015) . While (Park and collects data by mining Blog Posts,<cite> (Huang et al., 2016)</cite> collects stories using Mechanical Turk, providing more directly relevant stories.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_5",
  "x": "Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo. We first extract the 2048-dimensional visual representation f i \u2208 R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album. Following<cite> (Huang et al., 2016)</cite> , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence. The sequence output at each time step encodes the local album context for each photo (from both directions). Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_6",
  "x": "We then apply a max-margin ranking loss to encourage correctly-ordered stories: L rank (S, S ) = max(0, m\u2212log p(S )+log p(S)). The final loss is then a combination of the generation and ranking losses: ---------------------------------- **EXPERIMENTS** We use the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> , consisting of 10,000 albums with 200,000 photos.",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_0",
  "x": "We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by <cite>Klinger et al. (2018)</cite> for the task. ---------------------------------- **INTRODUCTION** Emotion is essential in human experience and communication, lending special significance to natural language processing systems aimed at learning it.",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_1",
  "x": "Alhuzali et al. (2018) introduce a third effective approach that leverages firstperson seed phrases like \"I'm happy that\" to collect emotion data. <cite>Klinger et al. (2018)</cite> propose yet a fourth method for collecting emotion data that depends on the existence of the expression \"emotion-word + one of the following words (when, that or because)\" in a tweet, regardless of the position of the emotion word. In the \"Implicit Emotion\" shared task 1 , participants were provided data representing the 6 emotions in the set (anger, disgust, fear, joy, sad, surprise). The trigger word was removed from each tweet. To illustrate, the task is to predict the emotion in a tweet like \"Boys who like Starbucks make me [#TRIGGERWORD#] because we can go on cute coffee dates\" (with the triggered word labeled as joy).",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_2",
  "x": "The training and validation sets were provided early for system development, while the test set was released one week before the deadline of system submission. The full details of the dataset can be found in <cite>Klinger et al. (2018)</cite> . We now describe our methods in the nesxt section. ---------------------------------- **METHODS**",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_3",
  "x": "As an additional baseline, we compare to <cite>Klinger et al. (2018)</cite> who propose a model based on Logistic Regression with a bag of word unigrams (BOW). All our deep learning models are based on variations of recurrent neural networks (RNNs), which have proved useful for several NLP tasks. RNNs are able to capture sequential dependencies especially in time series data, of which language can be seen as an example. One weakness of RNNs, however, lies in gradient either vanishing or exploding during training. Longshort term memory (LSTM) networks were developed to target this problem, and hence we employ these in our work.",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_4",
  "x": "For our hyper-parameters for this iteration of experiments, we identify them on our validation set. We list the network architectures and hyper-parameters for this set of experiments in Table 2 . Table 2 : Hyper-parameters for our submitted system exploiting fine-tuned language models from Howard and Ruder (2018) . Table 3 shows results of all our models in F-score. As the Table shows, all our models achieve sizable gains over the logistic regression model introduced by<cite> (Klinger et al., 2018)</cite> as a baseline for the competition (F-score = 60%).",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_5",
  "x": "Figure 3 shows the impact of different percentages of training examples on model performance. We test model performance for this analysis on our validation data. Interestingly, as Figure 3 shows, the model exceeds the baseline model reported by the task organizers<cite> (Klinger et al., 2018)</cite> when trained on only 10% of the training data. Additionally, the model outperforms the fastText and ELMo models by only seeing 40% of the training data. Once the model has access to 80% of the training data, its gains start to increase relatively slowly.",
  "y": "differences"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_0",
  "x": "Why aren't you giving it the same treatment you do to evolution?\" Because it doesn't carry the same weight. ;P bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011) ; (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003) ; and (3) online social and political public forums (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Wang and Ros\u00e9, 2010; Biran and Rambow, 2011) . Debates in online public forums (e.g. Fig. 1 ) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi-vidual making the argument, and whether s/he favors emotive or factual modes of expression, e.g. Let me answer.... NO! (P2 in Fig. 3 ).",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_1",
  "x": "These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics <cite>(Somasundaran and Wiebe, 2010</cite> Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT.",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_2",
  "x": "For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT. We model SOURCE relations with a graph, and add this information to classifiers operating on the text of a post.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_0",
  "x": "Automatic essay scoring (AES) is the task of assigning grades to essays written in an educational setting, using a computer-based system with natural language processing capabilities. The aim of designing such systems is to reduce the involvement of human graders as far as possible. AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; <cite>Phandi et al., 2015</cite>) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task (Dong and Zhang, 2016) . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_1",
  "x": "The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . ---------------------------------- **METHOD** String kernels. Kernel functions (Shawe-Taylor and Cristianini, 2004) capture the intuitive notion of similarity between objects in a specific domain.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_2",
  "x": "Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As Dong and Zhang (2016), we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . For the in-domain experiments, we use 5-fold cross-validation.",
  "y": "similarities uses motivation"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_3",
  "x": "As Dong and Zhang (2016), we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%.",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_4",
  "x": "For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (<cite>Phandi et al., 2015;</cite> Dong and Zhang, 2016) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data.",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_5",
  "x": "For the cross-domain experiments, we use the same source\u2192target domain pairs as (<cite>Phandi et al., 2015;</cite> Dong and Zhang, 2016) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0.",
  "y": "similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_6",
  "x": "We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_7",
  "x": "As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_8",
  "x": "For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases. We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation. The best QWK score (among the machine learning systems) for each prompt is highlighted in bold. brary (Vedaldi and Fulkerson, 2008) for the other steps involved in the BOSWE approach, such as the k-means clustering and the randomized forest of k-d trees.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_9",
  "x": "We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_10",
  "x": "Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_11",
  "x": "For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_12",
  "x": "For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_13",
  "x": "Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_14",
  "x": "We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_15",
  "x": "We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional. We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (<cite>Phandi et al., 2015</cite>) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . The best QWK scores for each source\u2192target domain pair are highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_16",
  "x": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it.",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_17",
  "x": "---------------------------------- **CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_0",
  "x": "I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem: Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of Philomel [97] [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared. Our previous work focused on only the segmentation part of the voice identification task <cite>(Brooke et al., 2012)</cite> . Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task.",
  "y": "background motivation"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_1",
  "x": "Beyond literature, the tasks of stylistic inconsistency detection (Graham et al., 2005; Guthrie, 2008) and intrinsic (unsupervised) plagiarism detection (Stein et al., 2011) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (Stamatatos, 2009 ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed. Our work here is built on our earlier work <cite>(Brooke et al., 2012)</cite> . Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches. Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (Pevzner and Hearst, 2002) indicated that it was well above various baselines. ----------------------------------",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_2",
  "x": "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans <cite>(Brooke et al., 2012)</cite> . Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (Brants and Franz, 2006) , lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010) , formality score (Brooke et al., 2010) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009) ; in previous work (Brooke et al., 2010) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see<cite> Brooke et al. (2012)</cite> .",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_3",
  "x": "Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (Brants and Franz, 2006) , lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010) , formality score (Brooke et al., 2010) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009) ; in previous work (Brooke et al., 2010) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see<cite> Brooke et al. (2012)</cite> . All the features are normalized to a mean of zero and a standard deviation of 1.",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_4",
  "x": "We consider three segmentations: the segmentation of our gold standard (Gold), the segmentation predicted by our segmentation model (Automatic), and a segmentation which consists of equallength spans (Even), with the same number of spans as in the gold standard. The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance. For the automatic segmentation model, we use the settings from<cite> Brooke et al. (2012)</cite> . We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster. For these latter two methods, which both have a random component, we averaged our metrics over 50 runs.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_0",
  "x": "More recent approach is proposed by [22] and is based on jointly learning recipe-text and image representations in a shared latent space. Recurrent Neural Networks (RNN) and CNN are mainly used to map text and image into the shared space. To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. <cite>[3]</cite> proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. [22] , they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup.",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_1",
  "x": "We chose 512-dimensional word embedding for our model with self-attention, whereas [17] and <cite>[3]</cite> chose a vector length of 300. In the following sections, more details about the aforementioned paths are presented. ---------------------------------- **ATTENTION MECHANISMS** The instruction encoder follows a transformer based encoder, as suggested by [24] .",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_2",
  "x": "We have trained our model using cosine similarity loss with margin as in [17] and with the triplet loss proposed by <cite>[3]</cite> . Both objective functions and the semantic regularization by [17] aim at maximizing intra-class correlation and minimizing inter-class correlation. Let us define the text query embedding as \u03d5 q and the embedding of the image query as \u03d5 d , then the cosine embedding loss can be defined as follows: where cos (x,y) is the normalized cosine similarity and \u03b1 is a margin (\u22121 \u2a7d \u03b1 \u2a7d 1), that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum \u03b1 similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_3",
  "x": "Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective [17] and for the triplet loss <cite>[3]</cite> . ---------------------------------- **EXPERIMENTAL SETUP AND RESULTS** Recipe1M is already distributed in three parts, the training, validation and testing sets. We did not make any changes to these partitions.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_4",
  "x": "By filtering out noisy instructions sentences (e.g. instructions containing only punctuation) we increased the effective dataset size to 254,238 samples for the training set and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to [17] and <cite>[3]</cite> , we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank.",
  "y": "similarities"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_6",
  "x": "Recall rates are averaged over the evaluation batches. Image to Recipe MedR R@1 R@5 R@10 1k samples Random [17] 500.0 0.001 0.005 0.01 JNE [17] 5.0 \u00b1 0. Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] . This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's.",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_7",
  "x": "Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] . This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's. By using online-instruction encoding with the self-attention encoder, we were able train the model for its main task in under 30 hours. Furthermore, the proposed approach offers more flexibility for dataset alterations. Figure 4 : Ingredient-Attention based focus on instruction sentences.",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_9",
  "x": "Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure 5 we present a few typical results on the intended recipe retrieval task. AdaMine <cite>[3]</cite> creates more distinct class clusters than in [17] . In Figure 3 , we demonstrate the difference in cluster formation using the aforementioned Methods for our Ingredient Attention. We visualize the top ten most common recipe classes in Recipe1M using t-SNE [23] .",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_10",
  "x": "---------------------------------- **CONCLUSIONS** In this paper, we have introduced self-attention for instruction encoding in the context of the recipe retrieval task and ingredient attention for disclosing ingredient dependent meal preparation steps. Our main contribution is the aforementioned ingredient attention, empowering our model to solve the recipe retrieval without any upstream skip instruction embedding, as well as the light-weight architecture provided by the transformer-like instruction encoder. On the recipe retrieval task, our method performs similarly to our baseline implementation of <cite>[3]</cite> .",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_0",
  "x": "Before applying tools trained on specific languages, one must determine the language of the text. It has attracted considerable attention in recent years [1, 2,<cite> 3,</cite> 4, 5, 6, 7, 8] . Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem. Generally speaking, language identification between different languages is a task that can be solved at a high accuracy. For example, Simoes et al. [9] achieved 97% accuracy for discriminating among 25 unrelated languages.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_1",
  "x": "Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem. Generally speaking, language identification between different languages is a task that can be solved at a high accuracy. For example, Simoes et al. [9] achieved 97% accuracy for discriminating among 25 unrelated languages. However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example). To deal with this problem, Huang and Lee<cite> [3]</cite> proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_3",
  "x": "To expand these representative words for each dialect, we extract more coarse-grained (n-gram with n \u2265 4) words using a word alignment technology, and then propose word alignment-based feature (dictionary for each dialect with n-gram under n \u2265 4). In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously. The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR. In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend <cite>3</cite> dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. In fact, the more dialects there are, the more difficult the dialects discrimination becomes.",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_5",
  "x": "Huang and Lee<cite> [3]</cite> presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore. Zampieri and Gebre [4] found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination. Tiedemann and Ljubesic [5] ; Ljubesic and Kranjcic [6] showed that the Na\u00efve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification. Grefenstette [11] ; Lui and Cook [12] found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties. Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_7",
  "x": "Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes.",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_8",
  "x": "Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_9",
  "x": "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially. While in this paper, besides the traditional uni-gram feature, we propose some novel features, such as character form, PMI-based and word alignment-based features. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_10",
  "x": "The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially. While in this paper, besides the traditional uni-gram feature, we propose some novel features, such as character form, PMI-based and word alignment-based features.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_11",
  "x": "**N-GRAM FEATURES** According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages. Compared with English, no space exists between words in Chinese sentence. Therefore, we use character uni-grams, bi-grams and tri-grams as features. However, Huang and Lee<cite> [3]</cite> did not use character-level n-grams.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_19",
  "x": "(1) <cite>3</cite>-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters. Baseline system 1: As mentioned in Section 2, we take the Huang and Lee<cite> [3]</cite> 's top-bag-of-word similarity-based approach as one of our baseline system. We re-implement their method in this paper using the similar <cite>3</cite>-way news dataset. ---------------------------------- **BASELINE SYSTEM 2:**",
  "y": "uses"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_22",
  "x": "If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task. Instead, bi-gram and word segmentation based features are better than uni-gram one. Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance. Also the bi-gram and word segmentation based features are better than the Huang and Lee<cite> [3]</cite> 's method (baseline system 1) for 6-way, <cite>3</cite>-way and 2-way dialect identification in the GCR. Obviously, the random method does not work for the GCR dialect identification.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_28",
  "x": "Similar to Huang and Lee<cite> [3]</cite> 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features. Table 6 shows the experimental results for the dialect identification in the GCR. As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset. Also, the word alignment-based features can contribute about <cite>3</cite>.<cite>3</cite>2% performance improvement. This also confirms our intuition that the word alignment-based information is helpful to discriminate dialects in the GCR, which shows the effectiveness of both fine-grained and coarsegrained characteristic of word alignment based features.",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_0",
  "x": "**ABSTRACT** Verb-noun idiomatic combinations (VNICs) are idioms consisting of a verb with a noun in its direct object position. Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of <cite>Fazly et al. (2009)</cite> , respectively.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_1",
  "x": "Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014) . Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) . In this study we focus on English verb-noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) .",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_2",
  "x": "Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) . In this study we focus on English verb-noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_3",
  "x": "They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_4",
  "x": "<cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_5",
  "x": "<cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) .",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_6",
  "x": "<cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_7",
  "x": "<cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_8",
  "x": "<cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) . Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_9",
  "x": "In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>. ---------------------------------- **MODELS FOR VNIC IDENTIFICATION BASED ON WORD EMBEDDINGS** The following subsections propose supervised and unsupervised approaches to VNIC identification based on word embeddings.",
  "y": "differences uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_10",
  "x": "Finally, to form the feature vector representing a VNIC instance, we subtract e from c, and append to this vector a single binary feature representing whether the VNIC instance occurs in its canonical form, as determined by <cite>Fazly et al. (2009)</cite> . The feature vectors are then used to train a supervised classifier; in our experiments we use the linear SVM implementation from Pedregosa et al. (2011) . The motivation for the subtraction is to capture the difference between the context in which a VNIC instance occurs ( c) and a type-level representation of that expression ( e), to potentially represent VNIC instances such that the classifier is able to generalize across expressions (i.e., to generalize to MWE types that are unseen during training). The canonical form feature is included because it is known to be highly informative as to whether an instance is idiomatic or literal. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_11",
  "x": "Our unsupervised approach combines the word embedding-based representation used in the supervised approach (without relying on training a supervised classifier, of course) with the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> . In this approach, we first represent each token instance of a given VNIC type as a feature vector, using the same representation as in Section 2.1. 3 We then apply k-means clustering to form k clusters of the token instances. 4 All instances in each cluster are then assigned a single class, idiomatic or literal, depending on whether the majority of token instances in a cluster are in that VNIC's canonical form or not, respectively. In the case of ties the method backs off to a most-frequent class (idiomatic) baseline.",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_12",
  "x": "**SUPERVISED RESULTS** Following <cite>Fazly et al. (2009)</cite> , the supervised approach was evaluated using a leave-one-token-out strategy. That is, for each MWE, a single token instance is held out, and the classifier is trained on the remaining instances. The trained model is then used to classify the held out instance. This is Table 1 for a variety of settings of window size and number of dimensions for the word embeddings.",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_13",
  "x": "All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>. That a window size of just 1 performs well is interesting. A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_14",
  "x": "This is Table 1 for a variety of settings of window size and number of dimensions for the word embeddings. The results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small. The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_15",
  "x": "They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>. That a window size of just 1 performs well is interesting. A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_16",
  "x": "A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1. ---------------------------------- **GENERALIZATION TO UNSEEN VNICS**",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_17",
  "x": "For these experiments we consider the setup that performed best on average over DEV and TEST in the previous experiments (i.e., a window size of 1 and 300 dimensional vectors). The macroaveraged accuracy on DEV and TEST is 68.9% and 69.4%, respectively. Although this is a substantial improvement over the most-frequent class baseline, it is well-below the accuracy for the previously-considered leave-one-token-out setup. Moreover, the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> gives substantially higher accuracies than this supervised approach. The limited ability of this model to generalize to unseen MWE types further motivates exploring unsupervised approaches to this task.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_18",
  "x": "**UNSUPERVISED RESULTS** The k-means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. The average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of Table 2 . For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered. Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of <cite>Fazly et al.</cite>, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset. We now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_19",
  "x": "For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered. Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of <cite>Fazly et al.</cite>, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset. We now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages. In the right panel of Table 2 we show results for an oracle approach that always selects the best label for each cluster. In this case, as the number of clusters increases, so too will the accuracy.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_20",
  "x": "A word's predominant sense is known to be a powerful baseline in word-sense disambiguation, and prior work has addressed automatically identifying predominant word senses (McCarthy et al., 2007; Lau et al., 2014) . The findings here suggest that methods for determining whether a set of usages of a VNIC are predominantly literal or idiomatic could be leveraged to give further improvements in unsupervised VNIC identification. ---------------------------------- **CONCLUSIONS** In this paper we proposed supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs that performed better than the supervised approach, and unsupervised CFORM approach, of <cite>Fazly et al. (2009)</cite> , respectively.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_0",
  "x": "**PRIOR WORK** Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications (Blodgett and O'Connor, 2017; Hovy and Spruit, 2016; Tatman, 2017) . Mitigating such biases is a difficult problem, and researchers have created many ways to make fairer NLP applications. Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) . More specifically, word embeddings has been an area of focus for evaluating unintended bias.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_1",
  "x": "Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) . More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_2",
  "x": "Mitigating such biases is a difficult problem, and researchers have created many ways to make fairer NLP applications. Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) . More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_3",
  "x": "More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general. Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_4",
  "x": "We evaluate three pretrained embedding models: GloVe (Pennington et al., 2014) , Word2vec (Mikolov et al., 2013 ) (trained on the large Google News corpus), and ConceptNet . GloVe and Word2vec embeddings have been shown to contain unintended bias in (Bolukbasi et al., 2016a;<cite> Caliskan-Islam et al., 2016)</cite> . ConceptNet has been shown to be less biased than these models (Speer, 2017) due to the mixture of curated corpora used for training. As part of our pipeline, we also use a labeled positive/negative sentiment training set (Hu and Liu, 2004) . This dataset has been shown to be a trustworthy lexicon for negative and positive sentiment words (Pang et al., 2008; Liu, 2012; Wilson et al., 2005) .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_5",
  "x": "**RESULTS AND DISCUSSION** We evaluate our framework and metric on two cases studies: National Origin Discrimination and Religious Discrimination. For each case study, we create a set of the most frequent identity terms from the protected groups in the Wikipedia word corpus and analyze bias with respect to these terms via our framework. First, we compare the RNSB metric for 3 pretrained word embeddings, showing that our metric is consistent with other word embedding analysis like WEAT<cite> (Caliskan-Islam et al., 2016)</cite> . We then show that our framework enables an insightful view into word embedding bias.",
  "y": "similarities"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_6",
  "x": "Although the RNSB metric is not directly comparable to WEAT scores, these results are still consistent with some of the bias predicted by<cite> (Caliskan-Islam et al., 2016)</cite> . The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European-American names are more correlated with positive sentiment than AfricanAmerican names. RNSB captures the same types of biases, but has a clear and larger scope, measuring discrimination with respect to more than two demographics within a protected group. Table 1: Table showing our RNSB metric for various word embeddings on two case studies. Our metric effectively predicts the unintended demographic bias in the presented word embeddings with respect to negative sentiment.",
  "y": "similarities"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_0",
  "x": "However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005;<cite> Liu et al., 2007</cite>; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Bod, 2007) . The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.",
  "y": "background motivation"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_1",
  "x": "Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks. For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006;<cite> Liu et al., 2007)</cite> . Zhang et al. (2008a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007) . In these models, various kinds of rules can be employed.",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_2",
  "x": "A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) . <cite>Liu et al. (2007)</cite> differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR). The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes.",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_3",
  "x": "In other words, some comprehensive rule classifications are necessary to make the rule analyses feasible. The motivation of this paper is to present such a rule classification. ---------------------------------- **RELATED WORKS** A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) .",
  "y": "background motivation"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_4",
  "x": "**RULES CLASSIFICATIONS** Currently, there have been several classifications in SMT research community. Generally, the rules can be classified into two main groups according to whether syntax information is involved: bilingual phrases (Phrase) and syntax rules (Syntax). Further, the syntax rules can be divided into three categories according to the lexicalization levels<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a source and target sides are non-lexicons (nonterminals) 3) Partially lexicalized (PLex): otherwise.",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_5",
  "x": "Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in<cite> (Liu et al., 2007)</cite> or in (Zhang et al., 2008a) . Taking such a system as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories.",
  "y": "future_work"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_0",
  "x": "Specifically, we focus on a new reading comprehension dataset called DROP <cite>(Dua et al., 2019)</cite> , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer. Unlike previous benchmarks such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) that have been well solved Devlin et al., 2019) , DROP is substantially more challenging in three ways. First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings. Therefore, various kinds of prediction strategies are required to successfully find the answers. Second, rather than restricting the answer to be a span of text, DROP loosens the constraint so that answers may be a set of multiple text strings.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_1",
  "x": "Second, rather than restricting the answer to be a span of text, DROP loosens the constraint so that answers may be a set of multiple text strings. Third, for questions that require discrete reasoning, a system must have a more comprehensive understanding of the context and be able to perform numerical operations such as addition, counting, or sorting. Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_2",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types. Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer. Second, previous reading comprehension models (Wang et al., 2017; Yu et al., 2018; Hu et al., 2018) are designed to produce one single span as the answer. But for some questions such as \"Which ancestral groups are smaller than 11%?\", there may exist several spans as correct answers (e.g., \"Italian\", \"English\", and \"Polish\"), which can not be well handled by these works.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_3",
  "x": "However, they have not fully considered all potential types. Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer. Second, previous reading comprehension models (Wang et al., 2017; Yu et al., 2018; Hu et al., 2018) are designed to produce one single span as the answer. But for some questions such as \"Which ancestral groups are smaller than 11%?\", there may exist several spans as correct answers (e.g., \"Italian\", \"English\", and \"Polish\"), which can not be well handled by these works. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_4",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_5",
  "x": "cessfully find the answer, some discrete reasoning abilities, such as sorting (A 1 ), subtraction (A 3 ), and negation (A 4 ), are required. Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning. Our model uses BERT (Devlin et al., 2019) as encoder: we map word embeddings into contextualized representations using pre-trained Transformer blocks (Vaswani et al., 2017 ) ( \u00a73.1). Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( \u00a73.2). Following<cite> Dua et al. (2019)</cite> , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_6",
  "x": "Here, we omit a detailed introduction of the block architecture and refer readers to Vaswani et al. (2017) for more details. ---------------------------------- **MULTI-TYPE ANSWER PREDICTOR** Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text). Following<cite> Dua et al. (2019)</cite> , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_7",
  "x": "Answer type prediction Inspired by the Augmented QANet model <cite>(Dua et al., 2019)</cite> , we use the contextualized token representations from the last four blocks (H L\u22123 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively. To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token. Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively: where h P 2 is computed in a similar way over P 2 . Next, we calculate a probability distribution to represent the choices of different answer types as:",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_8",
  "x": "Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as: where \u2297 denotes the outer product between the vector g and each token representation in M. Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to<cite> Dua et al. (2019)</cite> . As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated. Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = (u 1 , ..., u N ) \u2208 R N \u00d72 * D where N numbers exist.",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_9",
  "x": "**TRAINING AND INFERENCE** Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in Berant et al. (2013);<cite> Dua et al. (2019)</cite> . We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans. We use simple rules to search over all mentioned numbers to find potential negations. That is, if 100 minus a number is equal to the answer, then a negation occurs on this number.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_10",
  "x": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) <cite>(Dua et al., 2019)</cite> prehensive understanding of the context as well as the ability of numerical reasoning are required. Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to Devlin et al. (2019) for details on model sizes. We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train. The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively. A dropout probability of 0.1 is used unless stated otherwise.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_11",
  "x": "Baselines Following the implementation of Augmented QANet (NAQANet) <cite>(Dua et al., 2019)</cite> , we introduce a similar baseline called Augmented BERT (NABERT). The main difference is that we replace the encoder of QANet (Yu et al., 2018) with the pre-trained Transformer blocks (Devlin et al., 2019) . Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number. ---------------------------------- **MAIN RESULTS**",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_12",
  "x": "Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved Devlin et al., 2019) . Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. Saxton et al. (2019) introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities. Neural reading models Previous neural reading models, such as BiDAF (Seo et al., 2017) , R-Net (Wang et al., 2017) , QANet (Yu et al., 2018) , Reinforced Mreader (Hu et al., 2018) , are usually designed to extract a continuous span of text as the answer.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_13",
  "x": "While early research used cloze-style tests (Hermann et al., 2015; Hill et al., 2016) , most of recent works (Rajpurkar et al., 2016; Joshi et al., 2017) are designed to extract answers from the passage. Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved Devlin et al., 2019) . Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. Saxton et al. (2019) introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.",
  "y": "uses"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_0",
  "x": "However, it is not always an easy task to write an exact regular expression for those who do not have a deep knowledge of regular expressions or when the expression is very complicated, and incorrect or sloppy regular expressions may cause unexpected consequences in practice (Bispo et al., 2006; Zhang et al., 2018) . Indeed, even a single character difference between regular expressions can cause them * Now at Google. to represent completely different sets of strings. As such, researchers have begun working on a system than generates a regular expression from a natural language description provided by a human while reducing possible errors caused by incorrect regular expressions (Liu et al., 2019) . Recently, <cite>Locascio et al. (2016)</cite> designed the Deep-Regex model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014) using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from NLs. Later, Zhong et al. (2018a) improved the performance by training on not only syntactic content of the expressions (i.e. the exact textual representation of the expression that was used), but also the semantic content (the regular language described by the expression).",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_1",
  "x": "Kushman and Barzilay (2013) built a parsing model that translates a natural language sentence to a regular expression, and provided a dataset which is now a popular benchmark dataset for related research. <cite>Locascio et al. (2016)</cite> proposed the Deep-Regex model based on Seq2Seq for generating regular expressions from natural language descriptions together with a dataset of 10,000 NL-RX pairs. However, due to the limitations of the standard Seq2Seq model, the Deep-Regex model can only generate regular expressions similar in shape to the training data. The SemRegex model improved the Deep-Regex model by using reinforcement learning based on the determinisitic finite automata (DFA) equivalence oracle (which determines if two regular expressions describe the same language) as a reward function. This model can generate correct regular expressions that may not resemble the ground truth answer.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_2",
  "x": "---------------------------------- **EQ REG MODEL** Datasets: <cite>Locascio et al. (2016)</cite> created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form. We define the depth of a regular expression in this dataset as the depth of the tree that generated the NL-RX pair (see Appendix A). Similar to <cite>Locascio et al. (2016)</cite> , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_3",
  "x": "We define the depth of a regular expression in this dataset as the depth of the tree that generated the NL-RX pair (see Appendix A). Similar to <cite>Locascio et al. (2016)</cite> , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair. We sample approximately 200,000 pairs using this method with a ratio of equivalent and non-equivalent pairs of about 2:1. We prepare three sets of data having different depths for test data. One set is made up of only regular expressions with depth at most 3, which is the same as the training data (10,000 pairs).",
  "y": "similarities"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_4",
  "x": "**SOFTREGEX MODEL** Datasets: We use three public datasets to compare SoftRegex with the-state-of-the-art model, Sem-Regex. The KB13 (Kushman and Barzilay, 2013) dataset was constructed by regex experts and is relatively small. On the other hand, NL-RX-Synth is data generated automatically and NL-RX-Turk is made from ordinary people by paraphrasing NL descriptions in NL-RX-Synth using Mechanical Turk<cite> (Locascio et al., 2016)</cite> . Both datasets have 10,000 pairs of NL-RX data.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_5",
  "x": "Though the speedup described in experimental result may appear constant, our softened equivalence approximately decides a PSPACE-complete problem in linear time to the length of regular expressions, which would otherwise take exponential time. Zhong et al. (2018b) pointed out some problems in the NL-RX datasets. Specifically, there are some ambiguities since <cite>Locascio et al. (2016)</cite> tried to obtain data from machine-generated sentences. Thus, there are situations that even expert humans cannot accurately classify. On the other hand, the NL-RX-Turk dataset is unreliable in that it is generated by non-experts who are paraphrasing previously generated data.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_6",
  "x": "---------------------------------- **A SUPPLEMENTAL MATERIAL** We now describe how <cite>Locascio et al. (2016)</cite> generated their synthetic regular expression data. To begin, they manually mapped primitive regular expression operations, such as union and concatenation, to natural language. They then defined a small alphabet on which the operations could be performed.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_0",
  "x": "BERT <cite>(Devlin et al., 2018)</cite> , for example, trains on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, for a combined 3,200M words. Other iterations increased the amount of knowledge used during pre-training, such as RoBERTa (Liu et al., 2019) . Training large-scale models on these massive datasets has drawbacks, such as significantly increased carbon pollution and harm to the environment (Schwartz et al., 2019; Strubell et al., 2019) . We present a methodology of combining queries from commonsense knowledge bases with contextual embeddings, BIG MOOD -BERT Infused Graphs: Matching Over Other embeDdings, and abbreviated for its relationship to human knowledge awareness. Our methodology achieves a increase without significant additional fine-tuning or pre-training.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_1",
  "x": "Our model has three major components: language model adaptation, knowledge graph embeddings, and attention for classification. ---------------------------------- **DATA PREPROCESSING** Before model usage, we preprocess the data in two ways to make it easier for the model to un-derstand. For language modeling, we create training data similar to those in BERT <cite>(Devlin et al., 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_2",
  "x": "Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. First, we preprocess the tokens with WordPiece embeddings (Wu et al., 2016) . Then, we append special [CLS] and [SEP ] to each datum. We append [CLS] to the beginning of each datum, and [SEP ] to separate the question with the answer, as such: Then, we randomly mask 15% of all WordPiece embeddings.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_3",
  "x": "Then, we randomly mask 15% of all WordPiece embeddings. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> . 80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction. 10% of the time, we replace the word with a random word.",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_4",
  "x": "Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> . 80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction. 10% of the time, we replace the word with a random word. 10% of the time, we keep the word unchanged.",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_5",
  "x": "80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction. 10% of the time, we replace the word with a random word. 10% of the time, we keep the word unchanged. Combined with the above cloze task, we process the data for next sentence prediction. We do this process after the cloze task masking, similar to<cite> Devlin et al. (2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_6",
  "x": "Contrary to<cite> Devlin et al. (2018)</cite> , we do language model fine-tuning in addition to classification finetuning. We find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset. For each prompt, we use the previous preprocessed data to create tasks for our model to predict. We do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture. For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_7",
  "x": "We find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset. For each prompt, we use the previous preprocessed data to create tasks for our model to predict. We do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture. For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> . For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_8",
  "x": "For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> . ---------------------------------- **TOKEN REALIGNMENT** We do a word-level fusion to incorporate knowledge embeddings into the BERT model. First, we collect word embeddings from BERT.",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_9",
  "x": "We also allow the knowledge embeddings to be modified through this back-propagation. Hyperparameters are noted in Section 4.1. We also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings. For BERT baselines, we use the process in<cite> Devlin et al. (2018)</cite> , and use the [CLS] token, without attention, for classification. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_10",
  "x": "Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> .",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_0",
  "x": "Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers<cite> (Luo et al., 2004)</cite> . Moreover, these methods employ similar feature sets. The clusterization phase is different across current approaches. For example, there are several linking decisions for clusterization. (Soon et al., 2001 ) advocate the link-first decision, which links a mention to its closest candidate referent, while (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_1",
  "x": "Both these clustering decisions are locally optimized. In contrast, globally optimized clustering decisions were reported in<cite> (Luo et al., 2004)</cite> and (DaumeIII and Marcu, 2005a) , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeIII and Marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text. We argue that a more adequate clusterization phase for coreference resolution can be obtained by using a graph representation. In this paper we describe a novel representation of the coreference space as an undirected edge-weighted graph in which the nodes represent all the mentions from a text, whereas the edges between nodes constitute the confidence values derived from the coreference classification phase. In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity.",
  "y": "background motivation"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_2",
  "x": "In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity. We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (Stoer and Wagner, 1994) . BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 . Moreover, we have slightly modified the Min-Cut procedures. BESTCUT replaces the bottom-up search in a tree representation (as it was performed in<cite> (Luo et al., 2004)</cite> ) with the top-down problem of obtaining the best partitioning of a graph.",
  "y": "background extends differences"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_4",
  "x": "Based on the data seen, a maximum entropy model (Berger et al., 1996) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j . where g k (m i , m j , C) is a feature and \u03bb k is its weight; Z(m i , m j ) is a normalizing factor. We created the training examples in the same way as<cite> (Luo et al., 2004)</cite> , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files. ---------------------------------- **FEATURE REPRESENTATION**",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_5",
  "x": "We duplicated the statistical model used by<cite> (Luo et al., 2004)</cite> , with three differences. First, no feature combination was used, to prevent long running times on the large amount of ACE data. Second, through an analysis of the validation data, we implemented seven new features, presented in Table 1. Third, as opposed to<cite> (Luo et al., 2004)</cite> , who represented all numerical features quantized, we translated each numerical feature into a set of binary features that express whether the value is in certain intervals. This transformation was necessary because our maximum entropy tool performs better on binary features.",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_7",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS** The clusterization algorithms that we implemented to evaluate in comparison with our method are<cite> (Luo et al., 2004)</cite> 's Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002) . The features used were described in section 2.2. We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_8",
  "x": "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F<cite> (Luo et al., 2004)</cite> and the MUC P, R and F scores (Vilain et al., 1995) . In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types. The results obtained are tabulated in Table 4 . As can be observed, when it has prior knowledge of the mention types BESTCUT performs significantly better than the other two systems in the ECM-F score and slightly better in the MUC metrics. The more knowledge it has about the mentions, the better it performs.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_9",
  "x": "If BESTCUT has no information about the mentions, its performance ranks significantly under the LinkBest and Belltree algorithms in ECM-F and MUC R. Surprisingly enough, the Belltree algorithm, a globally optimized algorithm, performs similarly to Link-Best in most of the scores. Despite not being as dramatically affected as BESTCUT, the other two algorithms also decrease in performance with the decrease of the mention information available, which empirically proves that mention detection is a very important module for coreference resolution. Even with an F-score of 77.2% for detecting entity types, our mention detection system boosts the scores of all three algorithms when compared to the case where no information is available. It is apparent that the MUC score does not vary significantly between systems. This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods-the MUC metric has been found too indulgent by researchers (<cite> (Luo et al., 2004)</cite> , (Baldwin et al., 1998) Table 4 : Comparison of results between three clusterization algorithms on ACE Phase 2.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_10",
  "x": "The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations. Table 5 : Impact of feature categories on BEST-CUT on MUC6. Baseline system has the<cite> (Luo et al., 2004)</cite> features. The system was tested on key mentions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_12",
  "x": "(Luo et al., 2004) do the clusterization through a beam-search in the Bell tree using either a mention-pair or an entity-mention model, the first one performing better in their experiments. Despite the fact that the Bell tree is a complete representation of the search space, the search in it is optimized for size and time, while potentially losing optimal solutions-similarly to a Greedy search. Moreover, the fact that the two implementations are comparable is not inconceivable once we consider that<cite> (Luo et al., 2004</cite> ) never compared their system to another coreference resolver and reported their competitive results on true mentions only. (Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. The overall performance of the system is limited by the performance of its best component.",
  "y": "background motivation"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_0",
  "x": "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017) . The main contributions of the paper are: \u2022 Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification. \u2022 Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost.",
  "y": "similarities"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_1",
  "x": "The main contributions of the paper are: \u2022 Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017 ).",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_2",
  "x": "We conduct our experimental evaluation on two dialog act benchmark datasets. \u2022 SWDA: Switchboard Dialog Act Corpus (Godfrey et al., 1992; Jurafsky et al., 1997) is a popular open domain dialogs corpus between two speakers with 42 dialogs acts. \u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004 ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017) .",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_3",
  "x": "For both datasets we used the following: 2-layer SGNN (P T =80,d=14 \u00d7 FullyConnected 256 \u00d7 FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (Loshchilov and Hutter, 2016) . Unlike prior approaches <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017 ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors. These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning. Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (Sutskever et al., 2013) , run for 1M steps.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_4",
  "x": "**RESULTS** ---------------------------------- **BASELINES** We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute.",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_5",
  "x": "**RESULTS** ---------------------------------- **BASELINES** We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_6",
  "x": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to (Ortega and Vu, 2017) , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_7",
  "x": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to (Ortega and Vu, 2017) , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
  "y": "background"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_8",
  "x": "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017) . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_10",
  "x": "Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Ortega and Vu, 2017) . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly. In the future, we are interested in extending this approach to more natural language tasks. For instance, we built a multilingual SGNN model for customer feedback classification (Liu et al., 2017) and obtained 73% on Japanese, close to best performing system on the challenge (Plank, 2017) .",
  "y": "differences"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_1",
  "x": "The DPMM used in<cite> Kawahara et al. (2014)</cite> for clustering verb senses. M is the number of verb senses, and N is the sum total of slot counts for that verb sense. ---------------------------------- **PRIOR WORK** Parisien and Stevenson (2011) and<cite> Kawahara et al. (2014)</cite> showed distinct ways of applying the Hierarchical Dirichlet Process (Teh et al., 2006) to uncover the latent clusters from cluster examples.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_2",
  "x": "In<cite> Kawahara et al. (2014)</cite> , two identical DPMM's were used. The first clustered verb instances into senses, and one such model was trained for each verb. These verb-sense clusters are available publicly, and are used unmodified in this paper. The second DPMM clusters verb senses into VerbNet-like clusters of verbs. The result is a resource that, like Verbnet, inherently captures the inherent polysemy of verbs.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_3",
  "x": "The result is a resource that, like Verbnet, inherently captures the inherent polysemy of verbs. We focus our improvements on this second step, and try to derive verb clusters that more closely align to VerbNet. ---------------------------------- **DIRICHLET PROCESS MIXTURE MODELS** The DPMM used in<cite> Kawahara et al. (2014)</cite> is shown in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_4",
  "x": "By separating the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to <cite>(Kawahara et al., 2014)</cite> , the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective. This aligns with Levin's hypothesis of diathesis alternations -the syntactic contexts are sufficient for the clustering. In this paper, we re-create the second stage clustering with the same features, but add supervision.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_5",
  "x": "Because the product-of-experts is a discrete probability distribution, it is easy to marginalize out this variable when sampling k, using Either way, once a cluster is selected, we should update the \u03c1 and \u03b8. So, once a cluster is selected, we still sample a discrete y. We compare performance for sampling k with assigned y and with marginalized y. When incorporating supervision, we flatten VerbNet, using only the top-level categories, simplifying the selection process for y. In<cite> Kawahara et al. (2014)</cite> , slot features were most effective features at producing a VerbNet-like structure; we follow suit. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_6",
  "x": "For evaluation, we compare using the same dataset and metrics as<cite> Kawahara et al. (2014)</cite> . There, the authors use the polysemous verb classes of Korhonen et al. (2003) , a subset of frequent polysemous verbs. This makes the test set a sort of miniVerbNet, suitable for evaluation. They also define a normalized modified purity and normalized inverse purity for evaluation, explained below. The standard purity of a hard clustering averages, for each cluster's majority gold standard class, the percentage of clustered items of that class.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_7",
  "x": "where This nmPU is analagous to clustering precision: it measures, on average, how well the clustering avoids matching items that should not be clustered. We also define a recall analogue, the normalized inverse purity (niPU), as, This measures how well each gold standard cluster is recovered. We report each metric, and the F1 score combining them, to compare the clustering accuracy with respect to the gold standard G. We use the clustering from<cite> Kawahara et al. (2014)</cite> as a baseline for comparison.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_8",
  "x": "This measures how well each gold standard cluster is recovered. We report each metric, and the F1 score combining them, to compare the clustering accuracy with respect to the gold standard G. We use the clustering from<cite> Kawahara et al. (2014)</cite> as a baseline for comparison. However, for evaluation, the authors only clustered senses of verbs in the evaluation set. Since we would like to test the effectiveness of adding supervision, we treat all verbs in the evaluation set as unsupervised, with no initialization of \u03b8. Therefore, to compare apples-to-apples, we calculate the nPU, niPU, and F1 of the<cite> Kawahara et al. (2014)</cite> full clustering against the evaluation set.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_9",
  "x": "Swapping out the Dirichlet process for a Pitman-Yor process may bring finer control over the number of clusters. We have expanded the work in<cite> Kawahara et al. (2014)</cite> by explicitly modeling a VerbNet class for each verb sense, drawn from a product of experts based on the cluster and verb. This allowed us to leverage data from SemLink with VerbNet annotation, to produce a higher-quality clustering. It also allows us to describe each cluster in terms of alignment to VerbNet classes. Both of these improvements bring us closer to extending VerbNet's usefulness, using only automated dependency parses of corpora.",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_0",
  "x": "In order to rank the the 25-best candidate disfluency analyses of the NCM and select the most suitable one, we apply the MaxEnt reranker proposed by . We use the feature set introduced by<cite> Zwarts and Johnson (2011)</cite> , but instead of n-gram scores, we apply the LSTM language model probabilities. The features are so good that the reranker without any external language model is already a state-of-the-art system, providing a very strong baseline for our work. The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 . Our reranker optimizes the expected f-score approximation described in<cite> Zwarts and Johnson (2011)</cite> with L2 regularisation.",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_1",
  "x": "In order to alleviate this problem, we follow<cite> Zwarts and Johnson (2011)</cite> by training LMs on different corpora, but we apply state-ofthe-art recurrent neural network (RNN) language models. ---------------------------------- **LSTM** We use a long short-term memory (LSTM) neural network for training language models. LSTM is a particular type of recurrent neural networks which has achieved state-of-the-art performance in many tasks including language modelling (Mikolov et al., 2010; Jozefowicz et al., 2016) .",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_2",
  "x": "The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 . Our reranker optimizes the expected f-score approximation described in<cite> Zwarts and Johnson (2011)</cite> with L2 regularisation. ---------------------------------- **CORPORA FOR LANGUAGE MODELLING** In this work, we train forward and backward LSTM language models on Switchboard (Godfrey and Holliman, 1993) and Fisher (Cieri et al., 2004) corpora.",
  "y": "uses"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_3",
  "x": "Since the bigram language model of the NCM is trained on this corpus, we cannot directly use Switchboard to build LSTM LMs. The reason is that if the training data of Switchboard is used both for predicting language fluency and optimizing the loss function, the reranker will overestimate the model-based features 1-2. forward & backward LSTM LM scores 3-7. This is because the fluent sentence itself is part of the language model <cite>(Zwarts and Johnson, 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_0",
  "x": "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by<cite> Barbu (2015)</cite> through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish. ---------------------------------- **INTRODUCTION**",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_1",
  "x": "Exploring methods to handle the latter -clearly more challenging than removing duplicate and outdated material -motivated us to participate in the First Shared Task on Translation Memory Cleaning (Barbu et al., 2016) . Going forward, we strive to make human translation more efficient (by showing translators less erroneous fuzzy matches) and machine translation more accurate (by reducing noise in training data). In this paper, we describe our submitted system for distinguishing correct from incorrect TUs. Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by<cite> Barbu (2015)</cite> and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_2",
  "x": "In Section 3, we describe our method and, in Section 4, show how it compares to <cite>Barbu's (2015)</cite> approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5. ---------------------------------- **BACKGROUND** TM cleaning functionality in commercial tools is mostly rule-based, centering around the removal of duplicate entries, ensuring markup validity (e.g., no unclosed tags), or controlling for client or project specific terminology .",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_3",
  "x": "The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\"<cite> (Barbu, 2015)</cite> . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in <cite>Barbu's (2015)</cite> experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_4",
  "x": "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in <cite>Barbu's (2015)</cite> experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively. Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as \"buy\" and \"purchase\") or phrases (such as \"despite\" and \"in spite of\") should not be punished. We also experiment with part-of-speech information to identify spurious translation units. With closely related languages in particular, the rationale would be that adjectives (to name an example) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation.",
  "y": "background"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_5",
  "x": "**CLASSIFICATION** Our feature extraction pipeline, including <cite>Barbu's (2015)</cite> as well as our own features (see Section 3.1), is implemented in Scala. This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011) . From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below). ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_6",
  "x": "**FEATURE SELECTION** For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages. We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) . From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from<cite> Barbu, 2015)</cite> . Evaluation results are given in the next section.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_7",
  "x": "**BINARY CLASSIFICATION (II)** Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training. While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_8",
  "x": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par. Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data.",
  "y": "similarities uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_9",
  "x": "**FINE-GRAINED CLASSIFICATION** Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task. Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect. Again, we compared our system's performance to <cite>Barbu's (2015)</cite> method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation). The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs.",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_0",
  "x": "Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, <cite>Lau et al. (2014)</cite> proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_1",
  "x": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_2",
  "x": "The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases. We also test the PMI methodology (Newman et al., 2010) and make the same observation.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_3",
  "x": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases.",
  "y": "background motivation"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_4",
  "x": "This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases. We also test the PMI methodology (Newman et al., 2010) and make the same observation. To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_5",
  "x": "This has broad implications for topic model evaluation. ---------------------------------- **DATASET AND GOLD STANDARD** To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated coherence scores (Newman et al., 2010; Aletras and Stevenson, 2013;<cite> Lau et al., 2014</cite>; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_6",
  "x": "Following <cite>Lau et al. (2014)</cite> , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword). We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections. We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_7",
  "x": "We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_8",
  "x": "We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_9",
  "x": "**AUTOMATED METHOD -WORD INTRUSION** <cite>Lau et al. (2014)</cite> proposed an automated approach to the word intrusion task. The methodology computes pairwise word association features for the top-N words, and trains a support vector regression model to rank the words. The top-ranked word is then selected as the predicted intruder word. Note that even though it is supervised, no manual annotation is required as the identity of the true intruder word is known.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_10",
  "x": "This result is consistent with previous studies<cite> (Lau et al., 2014)</cite> . We see that correlation decreases systematically as N increases, implying that N has high impact on topic coherence evaluation and that if a single value of N is to be used, a lower value is preferable. To test whether we can leverage the additional information from the different values of N , we aggregate the model precision values and human ratings per-topic before computing the correlation (Table 3 : Cardinality = \"Avg\"). We also test the significance of difference for each N with the aggregate correlation using the Steiger Test (Steiger, 1980) The correlation improves substantially. In fact, for NEWS using in-domain features, the correlation is higher than that of any individual cardinality setting.",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_11",
  "x": "The other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top-N words. Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further. To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI methodology. We compute the topic coherence using the full collection of WIKI and NEWS, respectively, for varying N . Results are presented in Table 4 .",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_12",
  "x": "**AUTOMATED METHOD -NPMI** The other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top-N words. Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further. To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI methodology. We compute the topic coherence using the full collection of WIKI and NEWS, respectively, for varying N .",
  "y": "uses"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_0",
  "x": "Knowledge Bases (KB's) are structured knowledge sources widely used in applications like question answering (Kwiatkowski et al., 2013; Berant et al., 2013; Bordes et al., 2014) and search engines like Google Search and Microsoft Bing. This has led to the creation of large KB's like Freebase (Bollacker et al., 2008) , YAGO (Suchanek et al., 2007) and NELL (Carlson et al., 2010 ). KB's contains millions of facts usually in the form of triples (entity1, relation, entity2). However, KB's are woefully incomplete (Min et al., 2013) , missing important facts, and hence limiting their usefulness in downstream tasks. Figure 1: The two paths above consist of the same relations (locatedIn \u2192 locatedIn) and, hence, the model of <cite>Neelakantan (2015)</cite> will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport.",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_1",
  "x": "For example, we can infer nationality of a person from their place of birth. A common approach in many KBC methods for relation extraction is reasoning on individual relations (single-hop reasoning) to predict new relations (Mintz et al., 2009; Bordes et al., 2013; Socher et al., 2013) . For example, predicting Nationality(X, Y) from BornIn(X, Y). The performance of relation extraction methods have been greatly improved by incorporating selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, both in sentence level (Roth and Yih, 2007; Singh et al., 2013) and KB relation extraction (Chang et al., 2014) , and in learning entailment rules (Berant et al., 2011) . Another line of work in relation extraction performs reasoning on the paths (multi-hop reasoning on paths of length \u2265 1) connecting an entity pair (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014;<cite> Neelakantan et al., 2015</cite>; Guu et al., 2015) .",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_2",
  "x": "All these methods utilize only the relations in the path and do not include any information about the entities. In this work, we extend the method of <cite>Neelakantan (2015)</cite> by incorporating entity type information. Their method can generalize to paths unseen in training by composing embeddings of relations in the path non-linearly using a Recurrent Neural Network (RNN) (Werbos, 1990) . While entity type information has been successfully incorporated into relation extraction methods that perform single hop reasoning, here, we include them for multi-hop relation extraction. For example, Figure 1 illustrates an example where reasoning without type information would score both the paths equally although the latter path should receive a lesser score since there is an entity type mismatch for the first entity.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_3",
  "x": "This paper extends the Recurrent Neural Network model of <cite>Neelakantan (2015)</cite> by jointly reasoning over the relations and entity types occurring in the paths between an entity pair. Paths are represented Figure 2: The encoder network for a path between an entity pair. The inputs to the network are embeddings of entities, entity types and relations. This architecture corresponds to equation 4 below. The network for other equations can be obtained by setting the appropriate input embeddings to zeros.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_4",
  "x": "Let v r (\u03b4) \u2208 R d denote the vector representation of relation type \u03b4. Let v e (e) \u2208 R m denote the vector representation of an entity e and v et (e) \u2208 R n denote the combined representation of the types of e obtained by taking the sum of the representation of its top l types. Let \u03c0 be a path between the entity pair (e1, e2) containing the relation types \u03b4 1 , \u03b4 2 , . . . , \u03b4 N . In the following section, we first briefly describe the model proposed by <cite>Neelakantan (2015)</cite> (RNN model henceforth) followed by our extensions to it. ----------------------------------",
  "y": "background extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_5",
  "x": "We rank the entity pairs in the test set based on their scores and calculate the Mean Average Precision (MAP) score for the ranking following previous work<cite> Neelakantan et al., 2015)</cite> . Table 2 lists the MAP scores of both the models averaged over 12 freebase relation types. Incorporating selectional preferences by adding entity types gives a significant boost in scores (17.67 % over the baseline model.). However, we see a drop in performance on adding just entities. This is primarily because during test time we encounter a lot of previously unseen entities and hence we do Table 2 : Mean Average Precision scores averaged over 12 relations.",
  "y": "uses"
 },
 {
  "id": "4f0dec0ce2d7639c250be00d5efee4_0",
  "x": "**INTRODUCTION** Word collocation is one source of information that has been proposed as a useful tool to post-process word recognition results( <cite>[1,</cite> 4] ). It can be considered as a constraint on candidate selection so that the word candidate selection problem can be formalized as an instance of constraint satisfaction. Relaxation is a typical method for constraint satisfaction problems. One of the advantages of relaxation is that it can achieve a global effect by using local constraints.",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_1",
  "x": "ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (Danescu-NiculescuMizil et al., 2009 ). Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes<cite> (Kim et al., 2006)</cite> ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. ---------------------------------- **DATA AND FEATURES**",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_2",
  "x": "The arguments were sorted up into paragraphs, keeping the area of interest clear, but be careful about bringing up new things at the end and then simply leaving them there without elaboration (ie black sterilization at the end of the paragraph). An unhelpful peer review of average-rating 1: Your paper and its main points are easy to find and to follow. As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by<cite> Kim et al. (2006)</cite> . While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_3",
  "x": "**EXPERIMENT AND RESULTS** Following<cite> Kim et al. (2006)</cite> , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (Joachims, 1999) . We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness. Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ). Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews.",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_4",
  "x": "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_5",
  "x": "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_6",
  "x": "4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness. ---------------------------------- **ANALYSIS OF THE SPECIALIZED FEATURES**",
  "y": "similarities"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_7",
  "x": "Evaluation of the specialized features is shown in Table 3 , where all features examined are signifi- 4 The best performing single feature type reported<cite> (Kim et al., 2006)</cite> was review unigrams: r = 0.398 and rs = 0.593. cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coefficients ([0.43, 0.51] ) than the best generic features, these differences are not significant, and the specialized features have the potential advantage of being theory-based. The use of features related to meaningful dimensions of writing has contributed to validity and greater acceptability in the related area of automated essay scoring (Attali and Burstein, 2006) . When combined with some generic features, the specialized features improve the model's performance in terms of both r and r s compared to the best performance in Section 4.1 (the baseline).",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_8",
  "x": "**DISCUSSION** Despite the difference between peer reviews and other types of reviews as discussed in Section 2, our work demonstrates that many generic linguistic features are also effective in predicting peer-review helpfulness. The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews<cite> (Kim et al., 2006)</cite> , this is an important consideration.",
  "y": "differences motivation"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_9",
  "x": "Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_10",
  "x": "Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> . ---------------------------------- **CONCLUSIONS AND FUTURE WORK** The contribution of our work is three-fold: 1) Our work successfully demonstrates that techniques used in predicting product review helpfulness ranking can be effectively adapted to the domain of peer reviews, with minor modifications to the semantic and metadata features.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_11",
  "x": "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews<cite> (Kim et al., 2006)</cite> , this is an important consideration. Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews.",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_0",
  "x": "Springing forth from the pages of science fiction and capturing the daydreams of weary chore-doers everywhere, the promise and potential of general-purpose robotic assistants that follow natural language instructions has been long understood. Taking a small step towards this goal, recent work has begun developing artificial agents that follow natural language navigation instructions in perceptually-rich, simulated environments<cite> [4,</cite> 6] . An example instruction might be \"Go down the hall and turn left at the wooden desk. Continue until you reach the kitchen and then stop by the kettle.\" and agents are evaluated by their ability to follow the described path in (potentially novel) simulated environments. Many of these tasks have been developed from datasets of panoramic images captured in real scenes -e.g.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_1",
  "x": "Many of these tasks have been developed from datasets of panoramic images captured in real scenes -e.g. Google StreetView images in Touchdown [6] or Matterport3D panoramas captured in homes in Vision-and-Language Navigation (VLN)<cite> [4]</cite> . This paradigm enables efficient data collection and high visual fidelity compared to 3D scanning or creating synthetic environments; however, scenes are only observed from a sparse set of points relative to the full 3D environment (\u223c117 viewpoints per environment in VLN). As a consequence, environments in these tasks are defined in terms of a navigation graph (or nav-graph for short) (a) Vision-and-Language Navigation (VLN) (b) VLN in Continuous Environments (VLN-CE) Fig. 1 . The VLN setting (a) operates on a fixed topology of panoramic images (shown in blue) -assuming perfect navigation between nodes (often meters apart) and precise localization.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_2",
  "x": "In this work, we develop a continuous setting that enables these types of studies and take a first step towards integrating VLN agents with control via low-level actions. Vision-and-Language Navigation in Continuous Environments. In this work, we focus in on the Vision-and-Language Navigation (VLN)<cite> [4]</cite> task and lift these implicit assumptions by instantiating it in continuous 3D environments rendered in a high-throughput simulator [19] . Consequently, we call this task Vision-and-Language Navigation in Continuous Environments (VLN-CE). Agents in our task are free to navigate to any unobstructed point through a set of low-level actions (e.g. move forward 0.25m, turn-left 15 degrees) rather than teleporting between fixed nodes.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_5",
  "x": "Language-guided Visual Navigation Tasks. Language-guided visual navigation tasks require agents to follow navigation directions in simulated environments. There have been a number of recent tasks proposed in this space<cite> [4,</cite> 6, 13, 20] . Chen et al. [6] introduce the Touchdown task which studies outdoor language-guided navigation in Google Street View panoramas. Hermann et al. [13] investigates the same setting; however, the instructions are automatically generated from Google Map directions rather than being crowdsourced from human annotators.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_6",
  "x": "Hermann et al. [13] investigates the same setting; however, the instructions are automatically generated from Google Map directions rather than being crowdsourced from human annotators. Both adopt a nav-graph setting due to the source data being panoramic images -constraining agent navigation to fixed points. Misra et al. [20] introduce a simulated environment with unconstrained navigation and a dataset of crowdsourced instructions; however, the environments are unrealistic, synthetic scenes. Most related to our work is the Vision-and-Language Navigation (VLN) task of Anderson et al.<cite> [4]</cite> . VLN provides nav-graph trajectories and crowdsourced instructions in Matterport3D [5] environments as the Room-to-Room (R2R) dataset.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_7",
  "x": "Given a natural language navigation instruction, an agent must navigate from a start position to the described goal in a continuous 3D environment by executing a sequence of low-level actions based on egocentric perception alone. In overview, we develop this setting by transferring nav-graph-based Room-to-Room (R2R)<cite> [4]</cite> trajectories to reconstructed continuous Matterport3D environments in the Habitat simulator [19] . We discuss the task specification and the details of this transfer process in this section. Continuous Matterport3D Environments in Habitat. We set our problem in the Matterport3D (MP3D) [5] dataset, a collection of 90 environments captured through over 10,800 high-definition RGB-D panoramas.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_8",
  "x": "In contrast to the simulator used in VLN<cite> [4]</cite> , Habitat allows agents to navigate freely in the continuous environments. Observations and Actions. We select observation and action spaces to emulate a ground-based, zero-turning radius robot with a single, forward-mounted RGBD camera, similar to a LoCoBot [1] . Agents perceive the world through egocentric RGBD images from the simulator with a resolution of 256\u00d7256 and a horizontal field-of-view of 90 degrees. Note that this is similar to the egocentric RGB perception in the original VLN task<cite> [4]</cite> but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] .",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_9",
  "x": "In contrast to the simulator used in VLN<cite> [4]</cite> , Habitat allows agents to navigate freely in the continuous environments. Observations and Actions. We select observation and action spaces to emulate a ground-based, zero-turning radius robot with a single, forward-mounted RGBD camera, similar to a LoCoBot [1] . Agents perceive the world through egocentric RGBD images from the simulator with a resolution of 256\u00d7256 and a horizontal field-of-view of 90 degrees. Note that this is similar to the egocentric RGB perception in the original VLN task<cite> [4]</cite> but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] .",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_10",
  "x": "Agents perceive the world through egocentric RGBD images from the simulator with a resolution of 256\u00d7256 and a horizontal field-of-view of 90 degrees. Note that this is similar to the egocentric RGB perception in the original VLN task<cite> [4]</cite> but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] . While the simulator is quite flexible in terms of agent actions, we consider four simple, low-level actions for agents in VLN-CE -move forward 0.25m, turn-left or turn-right 15 degrees, or stop to declare that the goal position has been reached. These actions can easily be implemented on robotic agents with standard motion controllers. In contrast, actions to move between panoramas in<cite> [4]</cite> traverse 2.25m on average and can include avoiding obstacles.",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_11",
  "x": "Rather than collecting a new dataset of trajectories and instructions, we instead transfer those from the nav-graph-based Room-to-Room dataset to our continuous setting. Doing so enables us to compare existing nav-graph-based techniques with our methods that operate in continuous environments on the same instructions. Matterport3D Simulator and the Room-to-Room Dataset. The original VLN task is based on panoramas from Matterport3D (MP3D) [5] . To enable agent interaction with these panoramas, Anderson et al.<cite> [4]</cite> developed the Matterport3D Simulator.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_12",
  "x": "Agents act by teleporting between adjacent nodes in this graph. Based on this simulator, Anderson et al.<cite> [4]</cite> collect the Roomto-Room (R2R) dataset containing 7189 trajectories each with three humangenerated instructions on average. These trajectories consist of a sequence of nodes \u03c4 = [v 1 , . . . , v T ] with length T averaging between 4 and 6 nodes. Converting Room-to-Room Trajectories to Habitat. Given a mapping between the coordinate frames of Matterport3D Simulator and MP3D in Habitat, it is seemingly simple to transfer the Room-to-Room trajectories -after all, each node has a corresponding xyz location.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_13",
  "x": "---------------------------------- **INSTRUCTION-GUIDED NAVIGATION MODELS IN VLN-CE** We develop two models for VLN-CE. A simple sequence-to-sequence baseline and a more powerful cross-modal attentional model. While there are many differences in the details, these models are conceptually similar to early<cite> [4]</cite> and more recent [29] work in the nav-graph based VLN task.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_14",
  "x": "---------------------------------- **AUXILIARY LOSSES AND TRAINING REGIMES** Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] . We explore some of these directions for VLN-CE, but note that this is not an exhaustive accounting of impactful techniques. Particularly, we suspect that methods addressing exposure bias and data sparsity in VLN will help in the VLN-CE setting where these problems may be amplified by lengthy action sequences.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_15",
  "x": "The second recurrent network then takes a concatenation of these features as input (including an action encoding and the first recurrent network's hidden state) and predicts an action. ---------------------------------- **AUXILIARY LOSSES AND TRAINING REGIMES** Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] . We explore some of these directions for VLN-CE, but note that this is not an exhaustive accounting of impactful techniques.",
  "y": "motivation"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_16",
  "x": "We observe a similar effect in early experiments and apply inflection weighting in all our experiments. Coping with Exposure Bias. Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_17",
  "x": "We observe a similar effect in early experiments and apply inflection weighting in all our experiments. Coping with Exposure Bias. Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_18",
  "x": "We report standard metrics for visual navigation tasks defined in [2, <cite>4,</cite> 18] -trajectory length in meters (TL), navigation error in meters from goal at termination (NE), oracle success rate (OS), success rate (SR), success weighted by inverse path length (SPL), and normalized dynamic-time warping (nDTW). For our discussion, we will examine success rate and SPL as the primary metrics for performance and use NDTW to describe how paths differ in shape from ground truth trajectories. For full details on these metrics, see [2, <cite>4,</cite> 18] . Implementation Details. We utilize the Adam optimizer [15] with a learning rate of 2.5 \u00d7 10 \u22124 and a batch size of 5 full trajectories.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_19",
  "x": "For full details on these metrics, see [2, <cite>4,</cite> 18] . Implementation Details. We utilize the Adam optimizer [15] with a learning rate of 2.5 \u00d7 10 \u22124 and a batch size of 5 full trajectories. We set the inflection weighting coefficient [30] to 3.2 (inverse frequency of inflections in our groundtruth paths). We train on all ground-truth paths until convergence on val-unseen (at most 30 epochs).",
  "y": "uses"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_1",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_0",
  "x": "This framework holds that meaning can be inferred from the linguistic context of the word, usually seen as co-occurrence data. The context of usage is even more crucial for characterizing meanings of ambiguous or polysemous words: a definition that does not take disambiguating context into account will be of limited use <cite>(Gadetsky et al., 2018)</cite> . We argue that definition modeling should preserve the link between the definiendum and its context of occurrence. The most natural approach to this task is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it (cf. sections 3 & 4) .",
  "y": "motivation"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_1",
  "x": "Related to these works, specifically tackle definition modeling in the context of Chinese-whereas all previous works on definition modeling studied English. In a Transformer-based architecture, they incorporate \"sememes\" as part of the representation of the definiendum to generate definitions. On a more abstract level, definition modeling is related to research on the analysis and evaluation of word embeddings (Levy and Goldberg, 2014a,b; Arora et al., 2018; Batchkarov et al., 2016; Swinger et al., 2018, e.g.) . It also relates to other works associating definitions and embeddings, like the \"reverse dictionary task\" (Hill et al., 2016 )-retrieving the definiendum knowing its definition, which can be argued to be the opposite of definition modeling-or works that derive embeddings from definitions (Wang et al., 2015; Tissier et al., 2017; Bosc and Vincent, 2018) . 3 Definition modeling as a sequence-to-sequence task<cite> Gadetsky et al. (2018)</cite> remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_2",
  "x": "3 Definition modeling as a sequence-to-sequence task<cite> Gadetsky et al. (2018)</cite> remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum. The disambiguation that<cite> Gadetsky et al. (2018)</cite> proposed was based on a contextual cue-ie. a short text fragment. As notes, the cues in Gadetsky et al.'s (2018) dataset do not necessarily contain the definiendum or even an inflected variant thereof. For instance, one training example disambiguated the word \"fool\" using the cue \"enough horsing around-let's get back to work!\".",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_3",
  "x": "Despite some key differences, all of the previously proposed architectures we are aware of (Noraset et al., 2017;<cite> Gadetsky et al., 2018</cite>; followed a pattern similar to sequence-to-sequence models. They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia. In the case of Noraset et al. (2017) , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\". Gadetsky et al. (2018) used a sigmoid-based gating module to tweak the definiendum embedding. The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_4",
  "x": "In the case of the Transformer architecture, this is equivalent to using a multiplicative marking on the encoded representations: vectors that have been zeroed out are ignored during attention and thus cannot influence the behavior of the decoder. This SELECT approach may seem intuitive and naturally interpretable, as it directly controls what information is passed to the decoder-we carefully select only the contextualized definiendum, thus the only remaining zone of uncertainty would be how exactly contextualization is performed. It also seems to provide a strong and reasonable bias for training the definition generation system. Such an approach, however, is not guaranteed to excel: forcibly omitted context could contain important information that might not be easily incorporated in the definiendum embedding. Being simple and natural, the SELECT approach resembles architectures like that of<cite> Gadetsky et al. (2018)</cite> and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context; in that, the encoder may be seen as a dedicated contextualization sub-module.",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_5",
  "x": "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling. As a consequence, our experiments focus on the English language. The dataset of Noraset et al. (2017) (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here. In the dataset of<cite> Gadetsky et al. (2018)</cite> (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence. D Nor contains on average shorter definitions than D Gad .",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_6",
  "x": "Perplexity measures for Noraset et al. (2017) and<cite> Gadetsky et al. (2018)</cite> are taken from the authors' respective publications. All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (Vaswani et al., 2017) , which is known to perform well on semantic tasks (Radford, 2018; Cer et al., 2018; Devlin et al., 2018; Radford et al., 2019, eg.) . Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results.",
  "y": "uses"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_7",
  "x": "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (Vaswani et al., 2017) , which is known to perform well on semantic tasks (Radford, 2018; Cer et al., 2018; Devlin et al., 2018; Radford et al., 2019, eg.) . Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_8",
  "x": "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (Vaswani et al., 2017) , which is known to perform well on semantic tasks (Radford, 2018; Cer et al., 2018; Devlin et al., 2018; Radford et al., 2019, eg.) . Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_9",
  "x": "As for POS-mismatches, we do note that the work of Noraset et al. (2017) had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda. Another possibility would be to pre-train the model, as was done by<cite> Gadetsky et al. (2018)</cite> : in our case in particular, the encoder could be trained for POS-tagging or lemmatization. Lastly, one important kind of mistakes we observed is hallucinations. Consider for instance this production by the ADD model trained on D Ctx , for the word \"beta\": \"the twentieth letter of the Greek alphabet (\u03ba), transliterated as 'o'.\".",
  "y": "future_work"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_10",
  "x": "Building on the distributional hypothesis, we naturally treat definition generation as a sequence-to-sequence task of mapping the word's context of usage (input sequence) into the contextappropriate definition (output sequence). We showed that our approach is competitive against a more naive 'contextualize and select' pipeline. This was demonstrated by comparison both to the previous contextualized model by<cite> Gadetsky et al. (2018)</cite> and to the Transformerbased SELECT variation of our model, which differs from the proposed architecture only in the context encoding pipeline. While our results are encouraging, given the existing benchmarks we were limited to perplexity measurements in our quantitative evaluation. A more nuanced semantically driven methodology might be useful in the future to better assess the merits of our system in comparison to alternatives.",
  "y": "similarities"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_0",
  "x": "Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002) , predicting unseen syntax (Parisien and Stevenson, 2010) , argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010) . While Levin's classification can be extended manually (Kipper-Schuler, 2005) , a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002) . There has also been some success incorporating selectional preferences<cite> (Sun and Korhonen, 2009)</cite> . Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004) .",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_1",
  "x": "2 In the example in Table 2 , the frame pair \"PP(on) PP(on)\" will always have the same value as the \"PP(on)\" frame in F1. We extracted the SCFs using the system of Preiss et al. (2007) which classifies each corpus occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008) , and the 17 classes set in Sun et al. (2008) . We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_2",
  "x": "We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008) , and the 17 classes set in Sun et al. (2008) . We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> . To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.'s test set 7-9) and 100 features for the 7-17 way classifications.",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_3",
  "x": "To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.'s test set 7-9) and 100 features for the 7-17 way classifications. In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as in<cite> Sun and Korhonen (2009)</cite> which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters.",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_4",
  "x": "The result of F2 is lower than that of F3, and even lower than that of F1 for 3-6 way classification. This indicates that the frame independence assumption is a poor assumption. F3 yields substantially better result than F2 and F1. The result of F3 is 6.4% higher than the result (F=63.28) reported in<cite> Sun and Korhonen (2009)</cite> using the F1 feature. This experiment shows, on two datasets, that DA features are clearly more effective than the frame features for verb clustering, even when relaxations are used.",
  "y": "differences"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_5",
  "x": "In the future, we plan to evaluate the performance of DA features in a larger scale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007) . Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008) . Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data <cite>(Sun and Korhonen, 2009</cite> ) rather than WordNet, and use all argument head data in all frames.",
  "y": "uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_0",
  "x": "**INTRODUCTION** We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation <cite>(Koehn et al., 2003)</cite> . The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005) . Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions.",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_1",
  "x": "This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. In a phrase-based statistical translation <cite>(Koehn et al., 2003)</cite> , a bilingual text is decomposed as K phrase translation pairs (\u0113 1 ,f\u0101 1 ), (\u0113 2 ,f\u0101 2 ), ...: The input foreign sentence is segmented into phrasesf K 1 , mapped into corresponding English\u0113 K 1 , then, reordered to form the output English sentence according to a phrase alignment index mapping\u0101. In a hierarchical phrase-based translation (Chiang, 2005) , translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969) : X \u2192 \u03b3, \u03b1, \u223c where X is a non-terminal, \u03b3 and \u03b1 are strings of terminals and non-terminals. \u223c is a one-to-one correspondence for the non-terminals appeared in \u03b3 and \u03b1.",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_2",
  "x": "**PHRASE/RULE EXTRACTION** The phrase extraction algorithm is based on those presented by<cite> Koehn et al. (2003)</cite> . First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003) , in both directions and by combining the results based on a heuristic (Och and Ney, 2004) . Second, phrase translation pairs are extracted from the word aligned corpus <cite>(Koehn et al., 2003)</cite> . The method exhaustively extracts phrase pairs ( f j+m j , e i+n i ) from a sentence pair ( f J 1 , e I 1 ) that do not violate the word alignment constraints a.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_3",
  "x": "**PHRASE/RULE EXTRACTION** The phrase extraction algorithm is based on those presented by<cite> Koehn et al. (2003)</cite> . First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003) , in both directions and by combining the results based on a heuristic (Och and Ney, 2004) . Second, phrase translation pairs are extracted from the word aligned corpus <cite>(Koehn et al., 2003)</cite> . The method exhaustively extracts phrase pairs ( f j+m j , e i+n i ) from a sentence pair ( f J 1 , e I 1 ) that do not violate the word alignment constraints a.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_4",
  "x": "\u03b3 = \u03b3 \u2032f \u03b3 \u2032\u2032 and \u03b1 = \u03b1 \u2032\u0113 \u03b1 \u2032\u2032 constitutes a rule: ---------------------------------- **DECODING** The decoder for the phrase-based model is a left-toright generation decoder with a beam search strategy synchronized with the cardinality of already translated foreign words. The decoding process is very similar to those described in <cite>(Koehn et al., 2003)</cite> : It starts from an initial empty hypothesis.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_5",
  "x": "In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006) . The major difference to the phrase-based model's decoder is the handling of non-terminals, or holes, in each rule. ---------------------------------- **FEATURE FUNCTIONS** Our phrase-based model uses a standard pharaoh feature functions listed as follows <cite>(Koehn et al., 2003)</cite> :",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_6",
  "x": "\u2022 Distortion model that counts the number of words skipped. \u2022 The number of words in English-side and the number of phrases that constitute translation. For details, please refer to<cite> Koehn et al. (2003)</cite> . In addition, we added three feature functions to restrict reorderings and to represent globalized insertion/deletion of words: \u2022 Lexicalized reordering feature function scores whether a phrase translation pair is monotonically translated or not :",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_7",
  "x": "Second, corpora were transformed by a Porter's algorithm based multilingual stemmer (stem) 1 . Third, mixed-cased corpora were truncated to the prefix of four letters of each word (prefix4). For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of \"grow-diagfinal\" <cite>(Koehn et al., 2003)</cite> . Different preprocessing yields quite divergent alignment points as illustrated in Table 1 . The table also shows the numbers for the intersection and union of three alignment annotations.",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_0",
  "x": "Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy. Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000) , English<cite> (Yamada and Matsumoto, 2003)</cite> , Turkish (Oflazer, 2003) , and Swedish (Nivre et al., 2004) . For English, the interest in dependency parsing has been weaker than for other languages. To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (Marcus et al., 1993) , is annotated primarily with constituent analysis. On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000) , use statistical models for disambiguation that make crucial use of dependency relations.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_1",
  "x": "Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_2",
  "x": "On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000) , use statistical models for disambiguation that make crucial use of dependency relations. Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_3",
  "x": "The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case. Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995) , while we instead rely on memory-based learning (Daelemans, 1999) .",
  "y": "extends differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_4",
  "x": "This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case. Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995) , while we instead rely on memory-based learning (Daelemans, 1999) . Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types. As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner, 1996; <cite>Yamada and Matsumoto, 2003)</cite> , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002) . The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_5",
  "x": "We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999) . This permits us to make exact comparisons with the parser of<cite> Yamada and Matsumoto (2003)</cite> , but also the parsers of Collins (1997) and Charniak (2000) , which are evaluated on the same data set in<cite> Yamada and Matsumoto (2003)</cite> . One problem that we had to face is that the standard conversion of phrase structure trees to dependency trees gives unlabeled dependency trees, whereas our parser requires labeled trees. Since the annotation scheme of the Penn Treebank does not include dependency types, there is no straightforward way to derive such labels. We have therefore experimented with two different sets of labels, none of which corresponds to dependency types in a strict sense.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_6",
  "x": "We use the following metrics for evaluation: The proportion of non-root words that are assigned the correct head<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of root words that are analyzed as such<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of sentences whose unlabeled dependency structure is completely correct<cite> (Yamada and Matsumoto, 2003)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_7",
  "x": "We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended. On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set. Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> . We believe that there are mainly three reasons for this.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_8",
  "x": "It is interesting to note that this effect holds even in the case where the dependency labels are mostly derived from phrase structure categories. We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended. On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set. Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> .",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_9",
  "x": "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> . We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by<cite> Yamada and Matsumoto (2003)</cite> (96.1% vs. 97.1%) . Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_10",
  "x": "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference. Secondly, since 5 The information in the first three rows is taken directly from<cite> Yamada and Matsumoto (2003)</cite> . our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank. Indirect support for this assumption can be gained from previous experiments with Swedish data, where almost the same accuracy (85% unlabeled attachment score) has been achieved with a treebank which is much smaller but which contains proper dependency annotation (Nivre et al., 2004) . A third important factor is the relatively low root accuracy of our parser, which may reflect a weakness in the one-pass parsing strategy with respect to the global structure of complex sentences.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_11",
  "x": "6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the<cite> (Yamada and Matsumoto, 2003)</cite> labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not. ---------------------------------- **CONCLUSION** This paper has explored the application of a datadriven dependency parser to English text, using data from the Penn Treebank. The parser is deterministic and uses a linear-time parsing algorithm, guided by memory-based classifiers, to construct labeled dependency structures incrementally in one pass over the input.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_0",
  "x": "We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models. ---------------------------------- **INTRODUCTION** Attention-based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning (Xu et al., 2015) , and speech recognition (Chorowski et al., 2015) . Benefiting from the availability of large-scale benchmark datasets such as SQuAD (Rajpurkar et al., 2016) , the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors (Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017;<cite> Hu et al., 2017</cite>; Pan et al., 2017) .",
  "y": "background"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_2",
  "x": "---------------------------------- **ENCODER LAYERS** The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations. Here we choose a bi-directional Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997 ) to obtain more abstract representations for words in passages and questions. Different from the commonly used approaches that every single model has exactly one question and passage encoder (Seo et al., 2017;<cite> Hu et al., 2017)</cite> , our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_4",
  "x": "As shown in Table 2 which summarizes the performance of single models, we achieve steady improvements when 1) additional question encoders are used to extend the passage-question attention function, denoted as QPAtt+, as detailed in Section 2.1 and Section 2.2, and 2) on top of that, using PhaseCond making our model better than using Iterative Aligner. Specifically, PhaseCond's computational path for two question-aware passage attention layers On the other hand, Iterative Aligner builds path in turn through different kinds of attention layers: The performance of our models and published results of competing attention-based architectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo et al., 2017) and RNET from their recently published papers instead of using the up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline is one implementation of MReader, re-named as Iterative Aligner which has very similar results as those of MReader <cite>(Hu et al., 2017)</cite> 71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader <cite>(Hu et al., 2017)</cite> N As shown in Table 3 , in the single model setting, our model PhaseCond is clearly more effective than all the single-layered models (BiDAF and RNET) and multi-layered models (MReader and Iterative Aligner).",
  "y": "similarities"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_0",
  "x": "Automatic essay scoring (AES) is the task of building a computer-based grading system, with the aim of reducing the involvement of human raters as far as possible. AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002) , regression (Attali and Burstein, 2004;<cite> Phandi et al., 2015)</cite> , or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013) , addressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc. Some grammatical features are considered to assess the quality of essays (Yannakoudakis et al., 2011) .",
  "y": "background"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_1",
  "x": "**BASELINE** Bayesian Linear Ridge Regression (BLRR) and Support Vector Regression (SVR) (Smola and Vapnik, 1997 ) are chosen as state-of-art baselines. Feature templates follow <cite>(Phandi et al., 2015)</cite> , extracted by EASE 1 , which are briefly listed in Table 1 . \"Useful n-grams\" are determined using the Fisher test to separate the good scoring essays and bad scoring essays. Good essays are essays with a score greater than or equal to the average score, and the remainder are considered as bad scoring essays. The top 201 ngrams with the highest Fisher values are chosen as the bag of features and these top 201 n-grams constitute useful n-grams.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_2",
  "x": "The POS tags that are not included in the correct POS tags are treated as bad POS tags, and these bad POS tags make up the \"bad POS n-grams\" features. The features tend to be highly useful for the in-domain task since the discrete features of same prompt data share the similar statistics. However, for different prompts, features statistics vary significantly. This raises challenges for discrete feature patterns. ML-\u03c1 <cite>(Phandi et al., 2015)</cite> was proposed to address this issue.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_3",
  "x": "The settings of data preparation follow <cite>(Phandi et al., 2015)</cite> . We use quadratic weighted kappa (QWK) as the metric. For domainadaptation (cross-domain) experiments, we follow <cite>(Phandi et al., 2015)</cite> , picking four pairs of essay prompts, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928, where 1\u21922 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization. Word embeddings are randomly initialized and the hyper-parameter settings are listed in Table 3 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_4",
  "x": "For domainadaptation (cross-domain) experiments, we follow <cite>(Phandi et al., 2015)</cite> , picking four pairs of essay prompts, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928, where 1\u21922 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization. Word embeddings are randomly initialized and the hyper-parameter settings are listed in Table 3 . ---------------------------------- **RESULTS** In-domain The in-domain results are shown in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "57af9690eb41ff3f9217da6138425f_0",
  "x": "Part 6 also looks at the place of centering in a general theory of anaphora resolution and proposes a new hybrid theory that integrates Grosz and Sidner's discourse structure theory with a dynamic theory of semantic interpretation (Roberts). Finally in the last chapter of Part 6, it is argued that the restriction of centering to operate within a discourse segment should be abandoned in favor of a new model integrating centering and the global discourse structure and to this end it is proposed that a model of attentional state, the cache model, be integrated with the centering algorithm (Walker). Centering has proved to be a powerful tool for accounting for discourse coherence and has been used successfully in anaphora resolution; however, as with every theory in linguistics, it has its limitations. Some chapters suggest extensions of or amendments to the centering theory with a view to achieving a more comprehensive and successful model (e.g., the chapters by Kameyama, Roberts, and Walker). Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997) , that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions <cite>(Strube 1998)</cite> .",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_0",
  "x": "This includes both supervised (Blodgett and O'Connor, 2017; Tatman, 2017; Kiritchenko and Mohammad, 2018; De-Arteaga et al., 2019) and unsupervised natural language processing systems (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) . Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube. The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets<cite> (Waseem, 2016</cite>; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_1",
  "x": "For example, if we mistakenly consider speech by a targeted minority group as abusive we might unfairly penalize the victim, but if we fail to identify abuse against them we will be unable to take action against the perpetrator. Although no model can perfectly avoid such problems, we should be particularly concerned about the potential for such models to be systematically biased against certain social groups, particularly protected classes. A number of studies have shown that false positive cases of hate speech are associated with the presence of terms related to race, gender, and sexuality (Kwok and Wang, 2013; Burnap and Williams, 2015; . While not directly measuring bias, prior work has explored how annotation schemes and the identity of the annotators<cite> (Waseem, 2016</cite> ) might be manipulated to help to avoid bias. Dixon et al. (2018) directly measured biases in the Google Perspective API classifier, 1 trained on data from Wikipedia talk comments, finding that it tended to give high toxicity scores to innocuous statements like \"I am a gay man\".",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_2",
  "x": "To account for potential bias in the previous dataset,<cite> Waseem (2016)</cite> relabeled 2876 tweets in the dataset, along with a new sample from the tweets originally collected. The tweets were annotated by \"feminist and anti-racism activists\", based upon the assumption that they are domain-experts. A fourth category, racism and sexism was also added to account for the presence of tweets which exhibit both types of abuse. The dataset contains 6,909 tweets. collected tweets containing terms from the Hatebase, 2 a crowdsourced hate speech lexicon, then had a sample coded by crowdworkers located in the United States.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_3",
  "x": "On the other hand, this classifier is 1.7 times more likely to classify tweets in the black-aligned corpus as sexist. For<cite> Waseem (2016)</cite> we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low. Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus. Moving onto , we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set. Similarly, 17% of black-aligned tweets are predicted to contain offensive language compared to 6.5% of whitealigned tweets.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_4",
  "x": "The results of Experiment 2 are consistent with the previous results, although there are some notable differences. In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive. Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_5",
  "x": "Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_6",
  "x": "Classifiers trained on data from Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> only predicted a small fraction of the tweets to be racism. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators<cite> (Waseem, 2016)</cite> .",
  "y": "background similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_7",
  "x": "First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016;<cite> Waseem, 2016</cite>; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) . Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives Founta et al., 2018) . In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities. In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories.",
  "y": "background uses"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_8",
  "x": "For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled. Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists<cite> (Waseem, 2016)</cite> . Even the classifier trained on expert-labeled data<cite> (Waseem, 2016)</cite> flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",
  "y": "extends differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_0",
  "x": "**INTRODUCTION** Extracting entities (Florian et al., 2006 (Florian et al., , 2010 and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013 ) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004) . Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011) , predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; . End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_1",
  "x": "<cite>Miwa and Bansal (2016)</cite> were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, <cite>they</cite> used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encode syntactic information, given the output of a parser. The resulting representations are then used for making local decisions for entity and relation extraction incrementally, leading to much improved results compared with the best statistical model (Li and Ji, 2014) . This demonstrates the strength of neural representation learning for end-to-end relation extraction. On the other hand, <cite>Miwa and Bansal (2016)</cite> 's model is trained locally, without considering structural correspondences between incremental decisions.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_2",
  "x": "This demonstrates the strength of neural representation learning for end-to-end relation extraction. On the other hand, <cite>Miwa and Bansal (2016)</cite> 's model is trained locally, without considering structural correspondences between incremental decisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014) . As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001) , which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016) .",
  "y": "background motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_3",
  "x": "We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) . However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_4",
  "x": "We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016) . In particular, we follow Miwa and Sasaki (2014) , casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014) , yet has shown to be more flexible and accurate (Miwa and Sasaki, 2014) . We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_5",
  "x": "However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of <cite>Miwa and Bansal (2016)</cite> must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015) .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_6",
  "x": "Our method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of <cite>Miwa and Bansal (2016)</cite> must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015) . Second, <cite>Miwa and Bansal (2016)</cite> did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; . We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016) .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_7",
  "x": "We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016) . Evaluation on two benchmark datasets shows that our method outperforms previous methods of <cite>Miwa and Bansal (2016)</cite> , Li and Ji (2014) and Miwa and Sasaki (2014) , giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on discrete parser outputs. We make our code publicly As shown in Figure 1 , the goal of relation extraction is to mine relations from raw texts.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_8",
  "x": "We follow recent studies and recognize entities and relations as one single task. ---------------------------------- **METHOD** We follow Miwa and Sasaki (2014) and , treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to <cite>Miwa and Bansal (2016)</cite> by performing the task end-to-end. Formally, given a sentence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , we maintain a table T n\u00d7n , where T (i, j) denotes the relation between w i and w j .",
  "y": "similarities"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_9",
  "x": "**METHOD** We follow Miwa and Sasaki (2014) and , treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to <cite>Miwa and Bansal (2016)</cite> by performing the task end-to-end. Formally, given a sentence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , we maintain a table T n\u00d7n , where T (i, j) denotes the relation between w i and w j . When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>) .",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_10",
  "x": "Following <cite>Miwa and Bansal (2016)</cite> , we use a neural network to learn the vector representation of T i\u22121 , and then use Equation 1 to rank candidate next labels. There are two types of input features, including the word sequence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , and the readily filled label sequence l 1 l 2 \u00b7 \u00b7 \u00b7 l i\u22121 . We build a neural network to represent T i\u22121 . ---------------------------------- **WORD REPRESENTATION**",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_11",
  "x": "Two different forms of embeddings are used based on the word form, one being obtained by using a randomly initialized look-up table E w , 2 We remove the illegal table-filling labels during decoding for training and testing. The above two components have also been used by <cite>Miwa and Bansal (2016)</cite> . We further enhance the word representation by using its character sequence Lample et al., 2016) , taking a convolution neural network (CNN) to derive a character-based word representation h char , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014) .",
  "y": "extends"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_12",
  "x": "**LSTM FEATURES** We follow <cite>Miwa and Bansal (2016)</cite> , learning global context representations using LSTMs. Three basic LSTM structures are used: a leftto-right word LSTM ( and a left-to-right entity boundary label LSTM ( \u2212 \u2212\u2212\u2212 \u2192 LSTM e ). Each LSTM derives a sequence of hidden vectors for inputs.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_13",
  "x": "and a left-to-right entity boundary label LSTM ( \u2212 \u2212\u2212\u2212 \u2192 LSTM e ). Each LSTM derives a sequence of hidden vectors for inputs. For example, for Different from <cite>Miwa and Bansal (2016)</cite> , who use the output hidden vectors {h i } of LSTMs to represent words, we exploit segment representations as well. In particular, for a segment of text [i, j] , the representation is computed by using LSTM-Minus (Wang and Chang, 2016) , shown by Figure 4 , where h j \u2212 h i\u22121 in a left-to-right LSTM and h i \u2212 h j+1 in a right-to-left LSTM are used to represent the segment [i, j].",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_14",
  "x": "**SYNTACTIC FEATURES** Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005) . For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) . Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016) , where the encoder represents the syntactic features of the input sentences.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_15",
  "x": "---------------------------------- **SYNTACTIC FEATURES** Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005) . For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) . Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_16",
  "x": "Denoting the dumped syntactic features on each word as h In this paper, we exploit the parser of Dozat and Manning (2016) , since it achieves the current best performance for dependency parsing. Our method can be easily generalized to other parsers, which are potentially useful for our task as well. For example, we can use a constituent parser in the same way by dumping the implicit encoder features. Our exploration of syntactic features has two main advantages over the method of <cite>Miwa and Bansal (2016)</cite> , where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_17",
  "x": "For example, we can use a constituent parser in the same way by dumping the implicit encoder features. Our exploration of syntactic features has two main advantages over the method of <cite>Miwa and Bansal (2016)</cite> , where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by using our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic entity detection outputs.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_18",
  "x": "Previous work (<cite>Miwa and Bansal, 2016</cite>; trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , its neural representation h T is first obtained, and then compute the next label scores {l 1 , l 2 , \u00b7 \u00b7 \u00b7 , l s } using Equation 1. The output scores are regularized into a probability distribution {p l 1 , p l 2 , \u00b7 \u00b7 \u00b7 , p ls } by using a softmax layer. The training objective is to minimize the cross-entropy loss between this output distribution with the gold-standard distribution: where l g i is the gold-standard next label for T , and \u0398 is the set of all model parameters.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_19",
  "x": "---------------------------------- **DATA AND EVALUATION** We evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04) , respectively. The ACE05 dataset defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 dataset defines four entity types and five relation categories. For the ACE05 dataset, we follow Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> , splitting and preprocessing the dataset into training, development and test sets.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_20",
  "x": "In particular, we exploit pre-training techniques (Wiseman and Rush, 2016 ) to learn better model parameters. For the local model, we follow <cite>Miwa and Bansal (2016)</cite> , training parameters only for entity detection during the first 20 iterations. For the global model, we pretrain our model using local optimization for 40 iterations, before conducting beam global optimization. ---------------------------------- **DEVELOPMENT EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_21",
  "x": "Compared with <cite>Miwa and Bansal (2016)</cite> features for entity detection. Feature ablation experiments are conducted for the two types of features. Table 3 shows the experimental results, which demonstrate that the character-level features and the segment features we use are both useful for relation extraction. ---------------------------------- **LOCAL V.S. GLOBAL TRAINING**",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_22",
  "x": "---------------------------------- **LOCAL V.S. GLOBAL TRAINING** We study the influence of training strategies for relation extraction without using syntactic features. For the local model, we apply scheduled sampling (Bengio et al., 2015) , which has been shown to improve the performance of relation extraction by <cite>Miwa and Bansal (2016)</cite> . Table 4 shows the results.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_23",
  "x": "**SYNTACTIC FEATURES** We examine the effectiveness of the proposed implicit syntactic features. Table 5 shows the development results using both local and global optimization. The proposed features improve the relation performances significantly under both settings (p < 10 \u22124 ), demonstrating that our use of syntactic features is highly effective. We also compare our feature integration method with the traditional methods based on syntactic outputs which <cite>Miwa and Bansal (2016)</cite> and all previous methods use.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_24",
  "x": "Similarly, we extract segment features but by using max pooling instead over the sequential outputs of the feed-forward layer, since the vector minus is nonsense here. The final relation results are 53.1% and 53.9% for the local and global models, respectively, which have no significantly differences compared with our models. On the other hand, our method is relatively more efficient, and flexible to the grammar formalism. <cite>Miwa and Bansal (2016)</cite> , who exploit end-to-end LSTM neural networks with local optimization, and L&J (2014) and M&S (2014) refer to Li and Ji (2014) and Miwa and Sasaki (2014) , respectively, which are both globally optimized models using discrete features, giving the top F-scores among statistical models. 7 Overall, neural models give better performances than statistical models, and global optimization can give improved performances as well.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_25",
  "x": "In addition, the accuracy decreases sharply as the sentence length increases, with the local model suffering more severely from larger sentences. To understand the effectiveness of the proposed syntactic features, we examine the relation Fscores with respect to entity distances. <cite>Miwa and Bansal (2016)</cite> exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis-tance, thus facilitating relation extraction. We verify whether the proposed syntactic features can benefit our model similarly. As shown in Figure 7 , the F-scores of entity-pairs with large distances see apparent improvements, demonstrating that our use of syntactic features has a similar effect compared to the shortest dependency path.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_26",
  "x": "Entity recognition (Florian et al., 2004 (Florian et al., , 2006 Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016) . Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) . Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_27",
  "x": "The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016) . Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) . Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; <cite>Miwa and Bansal, 2016</cite>) and sentiment analysis .",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_0",
  "x": "Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009 ; Patwardhan 1 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5. 4.3.pdf and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b;<cite> Li et al., 2013b</cite>; Venugopal et al., 2014) . Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011;<cite> Li et al., 2013b)</cite> , it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains.",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_1",
  "x": "Although this approach has achieved the top performance (Hong et al., 2011;<cite> Li et al., 2013b)</cite> , it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum\u00e9 III, 2007; McClosky et al., 2010) ), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014) , search query retrieval (Shen et al., 2014) , semantic matching (Hu et al., 2014) , sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), name tagging and semantic role labeling (Collobert et al., 2011) , relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015) .",
  "y": "motivation"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_2",
  "x": "**MODEL** We formalize the event detection problem as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not<cite> (Li et al., 2013b)</cite> ? The current token along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In order to prepare for the CNNs, we limit the context to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary.",
  "y": "background uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_3",
  "x": "We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010;<cite> Li et al., 2013b)</cite> . The ACE 2005 corpus has 33 event subtypes that, along with one class \"None\" for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED.",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_4",
  "x": "The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011;<cite> Li et al., 2013b)</cite> . In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well.",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_5",
  "x": "Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality.",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_6",
  "x": "More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality. Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> . Note that CNN1 is eligible for this comparison as it does not utilize any external features, thus avoiding usage of the name tagger and the information extraction system to identify entity mentions and types. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_7",
  "x": "This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality. Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> . Note that CNN1 is eligible for this comparison as it does not utilize any external features, thus avoiding usage of the name tagger and the information extraction system to identify entity mentions and types.",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_9",
  "x": "First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008;<cite> Li et al., 2013b)</cite> do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error Table 4 : In-domain (first column) and Out-of-domain Performance (columns two to four). Cells marked with \u2020designate CNN models that significantly outperform (p < 0.05) all the reported feature-based methods on the specified domain. propagation and be more robust to domain shifts.",
  "y": "differences"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_0",
  "x": "1 mosesdecoder/scripts/ ---------------------------------- **RTM PREDICTION MODELS AND OPTIMIZATION** We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch\u00f6lkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009 ), or PLS after FS (FS+PLS).",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_1",
  "x": "The learning rate updates the weight values with weights in the range [a, b] using the following function taking error rate as the input: Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: ---------------------------------- **TRAINING RESULTS** We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bi\u00e7ici, 2015;<cite> Bi\u00e7ici, 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_2",
  "x": "Rank lists the overall ranking in the task out of about 9 submissions. We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties. RTMs with FS followed by PLS and learning with SVR is able to achieve the top rank in this task. Task 2: Prediction of Word-level Translation Quality Task 2 is about binary classification of word-level quality. We develop individual RTM models for each subtask and use GLMd model <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) , for predicting the quality at the word-level.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_3",
  "x": "**RTMS ACROSS TASKS AND YEARS** We compare the difficulty of tasks according to MRAER levels achieved. In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14 (Bi\u00e7ici and Way, 2014) , and QET13<cite> (Bi\u00e7ici, 2013)</cite> . The best results when predicting HTER are obtained this year. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_4",
  "x": [
   "---------------------------------- **CONCLUSION** Referential translation machines achieve top performance in automatic, accurate, and language independent prediction of document-, sentence-, and word-level statistical machine translation (SMT) performance. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations. RTMs achieve top performance when predicting translation performance."
  ],
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_0",
  "x": "A record of earlier decisions could also help users to identify outstanding issues for discussion, and to therefore make better use of the remainder of the meeting. Our approach to decision detection uses an annotation scheme which distinguishes between different types of utterance based on the roles which they play in the decision-making process. Such a scheme facilitates the detection of decision discussions<cite> (Fern\u00e1ndez et al., 2008)</cite> , and by indicating which utterances contain particular types of information, it also aids their summarization. To automatically detect decision discussions, we use what we refer to as hierarchical classification. Here, independent binary sub-classifiers detect the different decision dialogue acts, and then based on the sub-classifier hypotheses, a super-classifier determines which dialogue regions are decision discussions.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_1",
  "x": "User studies (Lisowska et al., 2004; Banerjee et al., 2005) have confirmed that meeting participants consider this to be the case, and Whittaker et al. (2006) found that the development of an automatic decision detection component is critical to the re-use of meeting archives. As a result, with the new availability of substantial meeting corpora such as the ISL (Burger et al., 2002) , ICSI (Janin et al., 2004) and AMI Meeting Corpora, recent years have seen an increasing amount of research on decision-making dialogue. This recent research has tackled issues such as the automatic detection of agreement and disagreement (Hillard et al., 2003; Galley et al., 2004) , and of the level of involvement of conversational participants (Wrede and Shriberg, 2003; Gatica-Perez et al., 2005) . In addition, Verbree et al. (2006) created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and<cite> (Fern\u00e1ndez et al., 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_2",
  "x": "Unlike Hsueh and Moore (2007),<cite> Fern\u00e1ndez et al. (2008)</cite> made an attempt at modelling the structure of decision-making dialogue. They designed an annotation scheme that takes account of the different roles which different utterances play in the decision-making process -for example, their scheme distinguishes between decision DAs (DDAs) which initiate a discussion by raising a topic/issue, those which propose a resolution, and those which express agreement for a proposed resolution and cause it to be accepted as a decision. The authors applied the annotation scheme to a portion of the AMI corpus, and then took what they refer to as a hierarchical classification approach in order to automatically identify decision discussions and their component DAs. Here, one binary Support Vector Machine (SVM) per DDA class hypothesized occurrences of that DDA class, and then based on the hypotheses of these socalled sub-classifiers, a super-classifier, (a further SVM), determined which regions of dialogue represented decision discussions. This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_3",
  "x": "Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time. ---------------------------------- **DATA**",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_4",
  "x": "This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_5",
  "x": "This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_6",
  "x": "---------------------------------- **MODELLING DECISION DISCUSSIONS** We use the same annotation scheme as<cite> (Fern\u00e1ndez et al., 2008</cite> ) in order to model decision-making dialogue. As stated in Section 2, this scheme distinguishes between a small number of dialogue act types based on the role which they perform in the formulation of a decision. Recall that using this scheme in conjunction with hierarchical classification produced better decision detection than a \"flat classification\" approach with a single \"decision-related\" DA class.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_7",
  "x": "**EXPERIMENTAL DATA FOR REAL-TIME DECISION DETECTION** Originally, two individuals used the annotation scheme described above in order to annotate the manual transcripts of 9 and 10 meetings respectively. The annotators overlapped on two meetings, and their kappa inter-annotator agreement ranged from 0.63 to 0.73 for the four DDA classes. (2007) are part of the AMI corpus, and are for the manual transcriptions. The reader can find a comparison between these annotations and our own manual transcript annotations in<cite> (Fern\u00e1ndez et al., 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_9",
  "x": "For 1 and 2, we use the same lenient-match metric as<cite> (Fern\u00e1ndez et al., 2008</cite>; Hsueh and Moore, 2007) , which allows a margin of 20 seconds preceding and following a hypothesized DDA. Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis. ---------------------------------- **RESULTS AND ANALYSIS**",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_10",
  "x": "Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis. ---------------------------------- **RESULTS AND ANALYSIS** Here, Section 6.3.1 will present results for different values of i, the number of old/new utterances processed in a single run.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_11",
  "x": "Combining the output from each of the independent sub-classifiers might compensate somewhat for any decreases in their individual accuracy, as there was here for the I and RP sub-classifiers. The hierarchical real-time detector's F1-score is also 10 points higher than a flat classifier (.54 vs. .44). Hence, while<cite> Fern\u00e1ndez et al. (2008)</cite> demonstrated that the hierarchical classification approach could improve off-line decision detection, we have demonstrated here that it can also improve realtime decision detection. Table 5 shows the results when an off-line detector is applied to real-time ASR transcripts. Here, the super-classifier obtains an F1-score of .55, one point higher than the real-time detector, but again, the difference is not statistically significant.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_12",
  "x": "7 Conclusion<cite> (Fern\u00e1ndez et al., 2008)</cite> described an approach to decision detection in multi-party meetings and demonstrated how it could work relatively well in an off-line system. The approach has two defining characteristics. The first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision-making process. The second is its use of hierarchical classification, whereby binary sub-classifiers detect instances of each of the decision DAs (DDAs), and then based on the sub-classifier hypotheses, a super-classifier determines which regions of dialogue are decision discussions. In this paper then, we have taken the same basic approach to decision detection as<cite> Fern\u00e1ndez et al. (2008)</cite> , but changed the way in which it is implemented so that it can work effectively in realtime.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_13",
  "x": "The second is its use of hierarchical classification, whereby binary sub-classifiers detect instances of each of the decision DAs (DDAs), and then based on the sub-classifier hypotheses, a super-classifier determines which regions of dialogue are decision discussions. In this paper then, we have taken the same basic approach to decision detection as<cite> Fern\u00e1ndez et al. (2008)</cite> , but changed the way in which it is implemented so that it can work effectively in realtime. Our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances. The fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate hypothesized decision discussions are possible. We have therefore added facilities to merge overlapping hypotheses and to remove duplicates.",
  "y": "background differences"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_0",
  "x": "The results are used in sophisticated tasks including summarizations and recommendations. In the past several years, sequential neural models such as long-short term memory (LSTM) have been applied to NER. They have outperformed the conventional models (Huang et al., 2015) . Recently, Convolutional Neural Network (CNN) was introduced into many models for extracting sub-word information from a word (Santos and Guimaraes, 2015;<cite> Ma and Hovy, 2016)</cite> . The models achieved higher performance because CNN can capture capitalization, suffixes, and prefixes (Chiu and Nichols, 2015) .",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_1",
  "x": "Above all, BLSTM-CNNs-CRF<cite> (Ma and Hovy, 2016</cite> ) achieved state-of-theart performance on the standard English corpus: CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) . Character-based Neural Models: Kuru et al. (2016) proposed a character-based neural model. This model, which inputs only characters, exhibits good performance on the condition that no external knowledge is used. This model predicts a tag for each character and forces that predicted tags in a word are the same. Therefore, it is unsuitable for languages in which boundary conflicts occur.",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_2",
  "x": "In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus. Figure 1 presents the architecture of this model. This word-based model combines CNN, BLSTM, and CRF layers. We describe each layer of this model as the following. CNN Layer: This layer is aimed at extracting subword information.",
  "y": "motivation"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_3",
  "x": "In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus. Figure 1 presents the architecture of this model. This word-based model combines CNN, BLSTM, and CRF layers. We describe each layer of this model as the following. CNN Layer: This layer is aimed at extracting subword information. The inputs are character embeddings of a word. This layer consists of convolution and pooling layers. The convolution layer produces a matrix for a word with consideration of the sub-word. The pooling layer compresses the matrix for each dimension of character embedding. BLSTM Layer: BLSTM (Graves and Schmidhuber, 2005 ) is an approach to treat sequential data. The output of CNN and word embedding are concatenated as an input of BLSTM. CRF Layer: This layer was designed to select the best tag sequence from all possible tag sequences with consideration of outputs from BLSTM and correlations between adjacent tags. This layer introduces a transition score for each transition pattern between adjacent tags. The objective function is calculated using the sum of the outputs from BLSTM and the transition scores for a sequence.",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_4",
  "x": "We conduct parameter tuning using the development dataset. We choose the unit number of LSTM as 300, the size of word embedding as 500, that of character embedding as 50, the maximum epoch as 20, and the batch size as 60. We use Adam (Kingma and Ba, 2014) , with the learning rate of 0.001 for optimization. We use MeCab (Kudo, 2005) for word segmentation. Other conditions are the same as those reported for an earlier study<cite> (Ma and Hovy, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_0",
  "x": "Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community <cite>(Kim and Hovy, 2006</cite>; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) . A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Cardie, 2012, 2013; Katiyar and Cardie, 2016 ). Here we focus on opinion role labeling (ORL) (Marasovi\u0107 and Frank, 2018) , which identifies opinion holders and * Corresponding author. We want to resolve all issues peacefully targets assuming that the opinion expressions are given. Figure 1 shows an example of the task.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_1",
  "x": "Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. For example, opinion expressions are different from verb/noun predicates in SRL, and meanwhile, opinion holders and targets may not always correspond to semantic agents (ARG0) and patients (ARG1), respectively. We can exploit machine learning based method to solve the mismatching problem between ORL and SRL. With a small number of annotated ORL corpus, we can feed the SRL outputs as inputs to build a statistical model for ORL.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_2",
  "x": "Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. For example, opinion expressions are different from verb/noun predicates in SRL, and meanwhile, opinion holders and targets may not always correspond to semantic agents (ARG0) and patients (ARG1), respectively. We can exploit machine learning based method to solve the mismatching problem between ORL and SRL. With a small number of annotated ORL corpus, we can feed the SRL outputs as inputs to build a statistical model for ORL.",
  "y": "motivation"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_3",
  "x": "Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method. All the codes and datasets are released publicly available for research purpose under Apache Licence 2.0 at https://github.com/zhangmeishan/SRL4ORL. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_4",
  "x": "For fine-grained evaluations, the final model outperforms the baseline model consistently on opinion holders and targets. The tendencies are similar by exploiting the binary and proportional matching methods. The results show that SRL information is very helpful for ORL, which is consistent with previous studies <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . The implicit SRL-SAWR method is highly effective to integrate SRL information into the ORL model. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_5",
  "x": "In addition, we compare this method with two other representative methods of SRL integration as well: one uses discrete SRL outputs as features directly for ORL and the other one exploits a multi-tasklearning (MTL) framework to benefit ORL by SRL information. Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method. All the codes and datasets are released publicly available for research purpose under Apache Licence 2.0 at https://github.com/zhangmeishan/SRL4ORL.",
  "y": "similarities uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_6",
  "x": "The semantic roles agent (ARG0) and patient (ARG1) are often corresponding to the opinion holder and target, respectively. Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR). Figure 3 shows the overall architectures of our SRL integration method.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_7",
  "x": "Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR). Figure 3 shows the overall architectures of our SRL integration method. Instead of using the discrete outputs from the SRL model, the SRL-SAWR method exploits the intermediate encoder outputs as inputs for ORL, which can alleviate the problems in the above two methods.",
  "y": "extends"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_8",
  "x": "As shown, the results are consistent with our intuition. Thus SRL and ORL are highly correlative. Considering the much larger scale of annotated SRL corpora, SRL can benefit ORL potentially. According to the above findings, we design a simple system by mapping SRL outputs into ORL directly <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . We simply convert the semantic role ARG0 into holder, and ARG1 into target.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_0",
  "x": "Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005) , information extraction (Yakushiji et al., 2006) , analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (\u00d8vrelid et al., 2009 ). There were several shared tasks organized on dependency parsing (CoNLL 2006 (CoNLL -2007 and labeled dependencies (CoNLL 2008 (CoNLL -2009 ) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007) , and extrinsically, e.g. (Wu et al., 2012) . In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two 'classic' representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012) ; (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007) , MST (McDonald et al., 2005) and the parser of <cite>Bohnet and Nivre (2012)</cite> ; (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types).",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_2",
  "x": "**PARSERS** In the experiments described in Section 4 we used parsers that adopt different approaches and implement various algorithms. Malt (Nivre et al., 2007) : transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005) : graph-based dependency parser with global near-exhaustive search. <cite>Bohnet and Nivre (2012)</cite> parser: transitionbased dependency parser with joint tagger that implements global learning and beam search.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_3",
  "x": "**EXPERIMENTS** In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser. ---------------------------------- **SETUP** For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_4",
  "x": "For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing. Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the <cite>Bohnet and Nivre (2012)</cite> parser we set the beam parameter to 80 and otherwise employed the default setup. With regards to evaluation metrics we use labelled attachment score (LAS), unlabeled attachment score (UAS) and label accuracy (LACC) excluding punctuation. Our results cannot be directly compared to the state-of-the-art scores on the Penn Treebank because we train on sections 0-13 and test on section 15 of WSJ. Also our results are not strictly inter-comparable because the setups we are using are different.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_5",
  "x": "Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the <cite>Bohnet and Nivre (2012)</cite> parser we set the beam parameter to 80 and otherwise employed the default setup. With regards to evaluation metrics we use labelled attachment score (LAS), unlabeled attachment score (UAS) and label accuracy (LACC) excluding punctuation. Our results cannot be directly compared to the state-of-the-art scores on the Penn Treebank because we train on sections 0-13 and test on section 15 of WSJ. Also our results are not strictly inter-comparable because the setups we are using are different. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_6",
  "x": "From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags). This is explained by the fact that the <cite>Bohnet and Nivre (2012)</cite> parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST \"loses\" more than Malt when parsing SB with gold supertags (Table 1, Gold supertags) . This parser exploits context features \"POS tag of each intervening word between head and dependent\" (McDonald et al., 2006) .",
  "y": "background"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_7",
  "x": "Below we will look at these three angles in detail. From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags). This is explained by the fact that the <cite>Bohnet and Nivre (2012)</cite> parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST \"loses\" more than Malt when parsing SB with gold supertags (Table 1, Gold supertags) .",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_8",
  "x": "This parser exploits context features \"POS tag of each intervening word between head and dependent\" (McDonald et al., 2006) . Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags).",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_9",
  "x": "This parser exploits context features \"POS tag of each intervening word between head and dependent\" (McDonald et al., 2006) . Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags).",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_10",
  "x": "This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ).",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_11",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_12",
  "x": "As discussed in section 3.4, they contain bits of information that are different. For this reason their combination results in slight increase of accuracy for all three parsers on all dependency formats (Table 1 , Gold PTB tags + gold supertags, and Table 2 , Predicted PTB + gold supertags and Predicted supertags + gold PTB). The <cite>Bohnet and Nivre (2012)</cite> parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrest\u00f8l, 2011) . When we consider punctuation in the evaluation, all scores raise significantly for DT and at the same time decrease for SB and CD for all three parsers. This is explained by the fact that punctuation in DT is always attached to the nearest token which is easy to learn for a statistical parser.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_13",
  "x": "**ERROR ANALYSIS** Using the CoNLL-07 evaluation script 2 on our test set, for each parser we obtained the error rate distribution over CPOSTAG on SB, CD and DT. VBP, VBZ and VBG. VBP (verb, non-3rd person singular present), VBZ (verb, 3rd person singular present) and VBG (verb, gerund or present participle) are the PTB tags that have error rates in 10 highest error rates list for each parser (Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser) with each dependency format (SB, CD and DT) and with each PoS tag set (PTB PoS and supertags) when PTB tags are included as CPOSTAG feature. We automatically collected all sentences that contain 1) attachment errors, 2) label errors, 3) attachment and label errors for VBP, VBZ and VBG made by Malt parser on DT format with PTB PoS. For each of these three lexical categories we manually analyzed a random sample of sentences with errors and their corresponding gold-standard versions.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_14",
  "x": "Coordination. The error rate of Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags). It means that there are many errors on incoming dependency arcs for coordinating conjunctions when parsing DT. On outgoing arcs parsers also make more mistakes on DT than on SB and CD. This is related to the difference in choice of annotation principle (see Figure 1) .",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_15",
  "x": "b) Proceeds from the sale will be used for remodelling and reforbishing projects, as well as for the planned MGM Grand hotel/casino and theme park. In the sentence a) \"the national fast-food\" refers only to the conjunct \"chains\", while in the sentence b) \"the planned\" refers to both conjuncts and \"MGM Grand\" refers only to the first conjunct. The <cite>Bohnet and Nivre (2012)</cite> parser succeeds in finding the correct conjucts (shown in bold font) on DT and makes mistakes on SB and CD in some difficult cases like the following ones: a) <. . . > investors hoard gold and help underpin its price <. . . > b) Then take the expected return and subtract one standard deviation.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_16",
  "x": "CD and SB wrongly suggest \"gold\" and \"help\" to be conjoined in the first sentence and \"return\" and \"deviation\" in the second. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags. From the parser perspective, the <cite>Bohnet and Nivre (2012)</cite> parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_17",
  "x": "**CONCLUSIONS AND FUTURE WORK** In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags. From the parser perspective, the <cite>Bohnet and Nivre (2012)</cite> parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST. From the dependency format perspective, DT appeares to be a more difficult target dependency representation than SB and CD. This suggests that the expressivity that we gain from the grammar theory (e.g. for coordination) is harder to learn with state-of-the-art dependency parsers.",
  "y": "differences"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_0",
  "x": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase 1 relations.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_1",
  "x": "A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011) . To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data.",
  "y": "motivation"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_2",
  "x": "As shown in Figure 2 , we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model trained on the same data. We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001) , this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) . We present a reliable and novel way to address these issues and achieve significant improvement over the <cite>MULTIR</cite> system (<cite>Hoffmann et al., 2011</cite>) , increasing recall from 47.7% to 61.2% at comparable precision.",
  "y": "background motivation"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_3",
  "x": "We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) . We present a reliable and novel way to address these issues and achieve significant improvement over the <cite>MULTIR</cite> system (<cite>Hoffmann et al., 2011</cite>) , increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998) : an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall. Our work is developed in parallel with Min et al. (2013) , who take a very different approach by adding additional latent variables to a multi-instance multi-label model (Surdeanu et al., 2012) to solve this same problem. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_4",
  "x": "Another negative data set with more conservative sense N EG(r) is defined as the set of sentences which contain the primary entity e1 (e.g. person in any CEO-of relation in the knowledge base) and any secondary entity e2 of required type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. ---------------------------------- **DISTANTLY SUPERVISED PASSAGE RETRIEVAL** We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_5",
  "x": "---------------------------------- **DISTANTLY SUPERVISED PASSAGE RETRIEVAL** We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_6",
  "x": "We select the entity pairs whose average score of the sentences they are involved in is greater than p, where p is a parameter tuned on development data. 3 The relation extraction model is then trained using (\u03a3, E, R, \u2206 ) with a more complete database than the original knowledge base \u2206. ---------------------------------- **DISTANTLY SUPERVISED RELATION EXTRACTION** We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_7",
  "x": "We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component. <cite>MULTIR</cite> is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009 ) by allowing overlapping relations. <cite>MULTIR</cite> uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_8",
  "x": "<cite>MULTIR</cite> uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. ---------------------------------- **EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_9",
  "x": "Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_10",
  "x": "---------------------------------- **EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_11",
  "x": "**EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_12",
  "x": "We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> . With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_13",
  "x": "With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation. It underestimates the improvements of the newly developed systems in this paper.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_14",
  "x": "Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation. It underestimates the improvements of the newly developed systems in this paper. We therefore also created a new test set of 1000 sentences by sampling from the union of Freebase matches and sentences where MULTIR-LEX or IRMIELEX extracted a relation. Table 1 shows the overall precision and recall computed against these two test datasets, with and without adding lexical features into multi-instance learning models. The performance improvement by using pseudo-feedback is significant (p < 0.05) in McNemar's test for both datasets.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_0",
  "x": "Generation of referring expression (GRE) is an important task in the field of Natural Language Generation (NLG) systems (Reiter and <cite>Dale, 1995)</cite> . The task of any GRE algorithm is to find a combination of properties that allow the audience to identify an object (target object) from a set of objects (domain or environment). The properties should satisfy the target object and dissatisfy all other objects in the domain. We sometimes call it distinguishing description because it helps us to distinguish the target from potential distractors, called contrast set. When we generate any natural language text in a particular domain, it has been observed that the text is centered on certain objects for that domain.",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_1",
  "x": "For example the expression \"The black dog\" suffices if the other dogs in the environment are all non black. Grice, an eminent philosopher of language, has stressed on brevity of referential communication to avoid conversational implicature. Dale (1992) developed Full Brevity algorithm based on this observation. It always generates shortest possible referring description to identify an object. But Reiter and<cite> Dale (1995)</cite> later proved that Full Brevity requirement is an NP-Hard task, thus computationally intractable and offered an alternative polynomial time Incremental Algorithm. This algorithm adds properties in a predetermined order, based on the observation that human speakers and audiences prefer certain kinds of properties when describing an object in a domain (Krahmer et al. 2003) .",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_2",
  "x": "**MODELING GRE USING TRIE STRUCTURE** In this section, it is shown how a scene can be represented using a trie data structure. The scheme is based on Incremental algorithm<cite> (Reiter and Dale 1995)</cite> and incorporates the attractive properties (e.g. speed, simplicity etc) of that algorithm. Later it is extended to take care of different refinements (like relational, boolean description etc) that could not be handled by Incremental algorithm. Reiter and<cite> Dale (1995)</cite> pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_3",
  "x": "Later it is extended to take care of different refinements (like relational, boolean description etc) that could not be handled by Incremental algorithm. Reiter and<cite> Dale (1995)</cite> pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set. We assume that the initial description of an entity is following this sequence (e.g. \"The large black dog\") then the later references will be some subset of initial description (like \"The dog\" or \"The large dog\") which is defined as the prefix of the initial description. So, we have to search for a prefix of the initial full length description so that it is adequate to distinguish the target object. Following the Incremental version we will add properties one by one from the 'PreferredAttributes' list.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_0",
  "x": "Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task. The training data is generated by distant supervision, which assumes that if an entity has a type in knowledge bases (KBs), then all sentences containing this entity will express this type. This method inevitably introduces irrelevant types to the context. For example, the entity \"Donald Trump\" has types \"person\", \"businessman\" and \"politician\" in KBs, thus all three types are annotated for its mentions in the training corpora.",
  "y": "background"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_1",
  "x": "---------------------------------- **INTRODUCTION** Fine-grained entity typing aims to assign types (e.g., \"person\", \"politician\", etc.) to entity mentions in the local context (a single sentence), and the type set constitutes a treestructured hierarchy (i.e., type hierarchy). Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task.",
  "y": "background motivation"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_2",
  "x": "Supposing that there are n sentences containing entity e, i.e., S e = {s 1 , s 2 , ..., s n }, and T e is the automatically labeled types based on KBs. Firstly PAN employs LSTM to generate representations of sentences s i following<cite> (Shimaoka et al. 2016)</cite> , where s i \u2208 R d is the semantic representation of s i , i \u2208 {1, 2, ..., n}. Afterwards, we build path-based attention \u03b1 i,t over sentences s i for each type t \u2208 T e , which is expected to focus on relevant sentences to type t. Then, the representation of sentence set S e for type t, denoted by s e,t \u2208 R d , is calculated through weighted sum of vectors of sentences. Finally, we obtain predicted types through a classification layer. The architecture of PAN for given entity e, type t More precisely, given e, an attention \u03b1 i,t is learned to score how well sentence s i matches type t, i.e., , where A \u2208 R d\u00d7d is a weighted diagonal matrix.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_3",
  "x": "To optimize the model, a multi-type loss is defined according to the cross entropy as follows, J = \u2212 e t [I t ln P (t|s e,t ) + (1 \u2212 I t ) ln(1 \u2212 P (t|s e,t ))], where I t is indicator function to indicate whether t is the annotated type of entity e, i.e., t \u2208 T e . ---------------------------------- **EXPERIMENTS AND CONCLUSION** Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD)<cite> (Shimaoka et al. 2016)</cite> . The statistics of the datasets are listed in Table1.",
  "y": "differences"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_4",
  "x": "Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD)<cite> (Shimaoka et al. 2016)</cite> . The statistics of the datasets are listed in Table1. We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following <cite>(Shimaoka et al. 2016</cite> ). Specifically, \"Strict\" evaluates on the type set of each entity mention, while \"Loose\" on each type. \"Marco\" is the geometric average over all mentions, while \"Micro\" is the arithmetic average. The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM<cite> (Shimaoka et al. 2016)</cite> , and other methods shown in Table2.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_5",
  "x": "Specifically, \"Strict\" evaluates on the type set of each entity mention, while \"Loose\" on each type. \"Marco\" is the geometric average over all mentions, while \"Micro\" is the arithmetic average. The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM<cite> (Shimaoka et al. 2016)</cite> , and other methods shown in Table2. (2) Predicting types using clean data by denoising ahead, i.e., H PLE and F PLE (Ren et al. 2016) . To prove the superiority of path-based attention, we also directly apply the attention neural model in relation extraction (Lin et al. 2016) without using type hierarchy (AN). The results of baselines are the best results reported in their papers.",
  "y": "uses"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_0",
  "x": "This is an extension and further validation of the results achieved by <cite>Manion and Sainudiin (2014)</cite>. SUDOKU's three submissions are incremental in the use of the two aforementioned constraints. Run1 has no constraints and disambiguates all lemmas in one pass. Run2 disambiguates lemmas at increasing degrees of polysemy, leaving the most polysemous until last. Run3 is identical to Run2, with the additional constraint of disambiguating all named entities and nouns first before other types of open-class words (verbs, adjectives, and adverbs).",
  "y": "extends"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_2",
  "x": "In-Degree Centrality as implemented in<cite> (Manion and Sainudiin, 2014)</cite> observes F-Score improvement (F + \u2206F) by applying the iterative approach. The author found in the investigations of his thesis (Manion, 2014) that the iterative approach performed best on the SemEval 2013 Multilingual WSD Task (Navigli et al., 2013) , as opposed to earlier tasks such as SensEval 2004 English All Words WSD Task (Snyder and Palmer, 2004) and the SemEval 2010 All Words WSD task on a Specific Domain (Agirre et al., 2010) . While these earlier tasks also experienced improvement, F-Scores remained lower overall. Table 1 Depicted above are distributions for each domain and language, detailing the probability (y-axis) of specific parts of speech at increasing degrees of polysemy (x-axis). These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word.",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_3",
  "x": "These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word. Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored. Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution. Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (Fellbaum, 1998) or an alternative equivalent, whereas SemEval 2013 Task 12 WSD and this task (Moro and Navigli, 2015) included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (Navigli and Ponzetto, 2012) . Secondly, as shown by <cite>Manion and Sainudiin (2014)</cite> with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document.",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_4",
  "x": "---------------------------------- **SYSTEM DESCRIPTION & IMPLEMENTATION** Run1 (SUDOKU-1) is the conventional approachno constraints are applied. Formalised in<cite> (Manion and Sainudiin, 2014)</cite> , this run can act as a baseline to gauge any improvement for Run2 and Run3 that apply the iterative approach. Run2 (SUDOKU-2) has the constraint of words being disambiguated in order of increasing polysemy, leaving the most polysemous to last.",
  "y": "background"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_0",
  "x": "**DATA SET** For our experiments we use the dataset described in<cite> (DeVault et al., 2011b)</cite> . It contains 19 Wizard of Oz dialogues with a virtual human called Amani . The user plays the role of an Army commander whose unit has been attacked by a sniper. The user interviews Amani, who was a witness to the incident and has some information about the sniper.",
  "y": "uses"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_2",
  "x": "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric. This score is .79; see <cite>DeVault et al. (2011b)</cite> . ---------------------------------- **BASELINE SYSTEMS** We consider two existing baseline systems in our experiments here.",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_0",
  "x": "This study proves that fast and accurate ensemble parsers can be built with minimal effort. ---------------------------------- **INTRODUCTION** Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell'Orletta, 2009) , and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; <cite>Hall et al., 2007</cite>; Attardi and Dell'Orletta, 2009 ).",
  "y": "background"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_1",
  "x": "parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre's arceager (AE), Nivre's arc-standard (AS), and Covington's non-projective model (CN)), and the parsing direction (left to right (\u2192) or right to left (\u2190)), similar to<cite> (Hall et al., 2007)</cite> . The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> . The MST parser was used with the default configuration. Table 1 shows the performance of these models in the development and test partitions. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_2",
  "x": "The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> . The MST parser was used with the default configuration. Table 1 shows the performance of these models in the development and test partitions. ---------------------------------- **EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_3",
  "x": "---------------------------------- **ON RE-PARSING ALGORITHMS** To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006;<cite> Hall et al., 2007)</cite> . 6 However, it is not clear that this step is necessary. In other words, how many sentences are not wellformed if one uses a simple word-by-word voting scheme?",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_0",
  "x": "We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use wordword co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> humour classification datasets using only 10% of known labels.",
  "y": "motivation"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_1",
  "x": "---------------------------------- **INTRODUCTION** Recognizing humor automatically is an important step for natural human-computer interaction (Shahaf et al., 2015) . While early works tend to frame humor recognition as a binary classification task (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , the last few years have seen the emergence of humor recognition as a pairwise relative ranking task (Cattle and Ma, 2016; Shahaf et al., 2015) . In addition to pairwise ranking, SemEval 2017 Task 6 also includes a global ranking subtask.",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_2",
  "x": "Only Yan and Pedersen (2017) attempt to predict global rankings directly, ranking documents inversely to their probability according to an n-gram language model. State-of-the-art humor recognition algorithms usually require a considerable amount of training data with labels to learn effective features<cite> (Yang et al., 2015)</cite> . However, such data are difficult to obtain -especially fine-grained humor annotations. First, the humor judgments differ from individual to individual. Thus, collecting perceptually consistent human labels is expensive and time-consuming.",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_3",
  "x": "In this paper, considering the importance of lexical information for humor recognition (Radev et al., 2015) , we propose a tensor decomposition method to capture the contextual nuances of a corpus. This allows us to model the lexical similarity of sentences regardless of the size of the corpus. In this way, we can rank the degree of humor effectively via lexical centrality (Radev et al., 2015) , namely, regarding the distance to the lexical center as an indicator of the degree of humor. Experimental results on the SemEval 2017 Task 6 dataset (Potash et al., 2017) show that without external training data, the tensor embedding method can achieve performance equivalent to the second place on SemEval 2017 Task 6B without the need for any external training corpus. In addition, by applying a semi-supervised label propagation procedure (Zhou et al., 2003) , we can also use the tensor embedding method for small sample humor recognition, achieving about 0.7 accuracy with only 10% of known labels on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> datasets.",
  "y": "background motivation"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_4",
  "x": "Previous works tend to use a combination of phonological, stylistic, semantic, and content-based features. Phonological features include acoustic features extracted from sitcom audio tracks (Bertero and Fung, 2016) and \"phonetic embeddings\" generated using a character-to-phoneme LSTM encoder-decoder (Donahue et al., 2017) . Stylistic features include alliteration, rhyming, negative sentiment, and adult slang (Mihalcea and Strapparava, 2005) as well as emotional scenarios (Reyes et al., 2012) . Semantic features range from attempts to measure incongruity (Cattle and Ma, 2018; Shahaf et al., 2015; <cite>Yang et al., 2015)</cite> to the use of word embeddings as inputs to neural models (Bertero and Fung, 2016; Donahue et al., 2017) . Content-based approaches include word frequency (Mihalcea and Strapparava, 2005) , n-gram probability (Yan and Pedersen, 2017) , and lexical centrality (Radev et al., 2015) .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_5",
  "x": "**SMALL SAMPLE HUMOR RECOGNITION** Once the humor features have been extracted, the next step is training a machine learning model to make predictions. Although learning-based methods have shown significant performance improvement recently<cite> (Yang et al., 2015)</cite> , one of their main bottlenecks is the lack of appropriate training corpora. While previous works have employed data crawled from websites (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , Twitter (Cattle and Ma, 2016; Reyes et al., 2012) , sitcom subtitles (Bertero and Fung, 2016; Purandare and Litman, 2006) , or the New Yorker Cartoon Caption Contest (Radev et al., 2015; Shahaf et al., 2015) , these datasets are generally not released publicly. Owing to the difficulty in obtaining fine-grained labeled humor data, it is critical to study how to recognize humor by a small training sample or even without labeled data.",
  "y": "differences"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_6",
  "x": "Considering a corpus D = {s 1 , s 2 , . . . , s D } with D sentences, we first build a vocabulary for it, namely, w 1 , w 2 , . . . , w V , where V is the number of words. For each sentence s in D, we count the wordword co-occurrence in a small window H, and build a frequency matrix W s \u2208 Z V \u00d7V , where Z denotes the set of integers. In particular, W s (i, j) indicates the frequency that word w i and w j cooccur in s within the window H. In this way, we can capture the lexical patterns of s in W s . We then stack all W s as a three-dimensional tensor W \u2208 Z V \u00d7V \u00d7D . The objective of tensor decomposition is to find an approximation\u0174 of W so<cite> Yang et al. (2015)</cite> that:\u0174",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_7",
  "x": "To show the effectiveness of label propagation of our tensor embedding method for small sample humor recognition, we conduct an experiment on two humor classification datasets 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> . Similarly, we run a grid search procedure to find optimal parameters, and set the rank as 10, window size as 5, neighbor number as 50, \u03b1 as 0.2. F (0) is set as a zero matrix initially. For each dataset, we randomly select 5%, 10%, 30%, and 90% of the data for training. We run a 10-fold procedure, and report the average accuracy, precision, recall, and F1 score values.",
  "y": "similarities uses"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_8",
  "x": "The results of humor classification are shown in 1 www.tensortoolbox.org Table 2 . Our own implementation of<cite> Yang et al. (2015)</cite> is included as a baseline. While<cite> Yang et al. (2015)</cite> uses a large portion of data for training and combine different features, we find that at similar portion of training data (90%), the results of our method are comparable to it. In addition, with only a small portion of training data, our method still achieves good results. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_0",
  "x": "The similarity module in SODA identifies the best adaptable source collection based on the similarity between the source and target collections. This is based on the observations from existing literature (Bhatt et al., 2015;<cite> Blitzer et al., 2007)</cite> which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar. The similarity module in SODA is capable of computing different kinds of lexical, syntactic, and semantic similarities between unlabeled target and labeled source collections. For this demonstration on sentiment categorization from social media data, it measures cosine similarity between the comments in each collection and computes sim as the similarity score. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_1",
  "x": "For this demonstration on sentiment categorization from social media data, it measures cosine similarity between the comments in each collection and computes sim as the similarity score. ---------------------------------- **DOMAIN ADAPTATION** The heart of SODA is the adaptation module that works on two principles, generalization and adaptation. During generalization, it learns shared common representation<cite> (Blitzer et al., 2007</cite>; Ji et al., 2011; Pan et al., 2010) which minimizes the divergence between two collections.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_2",
  "x": "**DOMAIN ADAPTATION** The heart of SODA is the adaptation module that works on two principles, generalization and adaptation. During generalization, it learns shared common representation<cite> (Blitzer et al., 2007</cite>; Ji et al., 2011; Pan et al., 2010) which minimizes the divergence between two collections. We leverage one of the widely used structural correspondence learning (SCL) approach<cite> (Blitzer et al., 2007)</cite> to compute shared representations. The idea adhered here is that a model learned on the shared feature representation using labeled data from the source collection will also generalize well on the target collection.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_3",
  "x": "Coll-1 to Coll-8 refer to collections pertaining to marketing & sales, comcast support, 2 https://www.youtube.com/watch?v= zKnP5QEHVAE Figure  3 compares the effect of adding labeled comments in batches of 25 comments at-a-time. When there is no labeled data in the target collection, in-domain classifier can not be applied while SODA still yields good classification accuracy. Moreover, SODA consistently performs better than the in-domain classifier with same amount of labeled data. We also evaluated the performance of domain adaptation (DA) module of SODA on the Amazon review dataset<cite> (Blitzer et al., 2007)</cite> which is a benchmark dataset for sentiment categorization. It has 4 domains, namely, books(B), dvds(D), electronics(E), and kitchen(K) each with 2000 reviews divided equally into positive and negative reviews.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_4",
  "x": "It has 4 domains, namely, books(B), dvds(D), electronics(E), and kitchen(K) each with 2000 reviews divided equally into positive and negative reviews. Table 2 shows that DA module of SODA outperforms 1) a widely used domain adaptation technique , namely, structural correspondence learning (SCL)<cite> (Blitzer et al., 2007</cite>; Blitzer et al., 2006) , 2) the baseline (BL) where a classifier trained on one domain is applied on another domain, and 3) the in-domain classifier. Note that in Table 2 , the performance of DA module of SODA is reported when it does not use any labeled instances from the target domain. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_0",
  "x": "We extend <cite>our</cite> previous work on constituency parsing <cite>(Kitaev and Klein, 2018)</cite> by incorporating pre-training for ten additional languages, and compare the benefits of no pre-training, ELMo , and BERT (Devlin et al., 2018). Pre-training is effective across all languages evaluated, and BERT outperforms ELMo in large part due to the benefits of increased model capacity. Our parser obtains new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1). ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_1",
  "x": "---------------------------------- **INTRODUCTION** There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language.",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_2",
  "x": "**INTRODUCTION** There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_3",
  "x": "**INTRODUCTION** There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) .",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_4",
  "x": "1 ---------------------------------- **MODEL** Our parser as described in <cite>Kitaev and Klein (2018)</cite> accepts as input a sequence of vectors corresponding to words in a sentence, transforms these repre-1 https://github.com/nikitakit/self-attentive-parser sentations using one or more self-attention layers, and finally uses these representations to output a parse tree. We incorporate BERT by taking the token representations from the last layer of a BERT model and projecting them to 512 dimensions (the default size used by our parser) using a learned projection matrix.",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_5",
  "x": "The extra layers on top of BERT use word-based tokenization instead of sub-words, apply the factored version of self-attention proposed in <cite>Kitaev and Klein (2018)</cite> , and are randomly-initialized instead of being pre-trained. We found that omitting these additional layers and using the BERT vectors directly hurt parsing accuracies. We also extend the parser to predict part-ofspeech tags in addition to constituent labels, a feature we include based on feedback from users of our previous parser. Tags are predicted using a small feed-forward network (with only one ReLU nonlinearity) after the final layer of self-attention. This differs slightly from Joshi et al. (2018) , where tags are predicted based on span representations instead.",
  "y": "uses"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_6",
  "x": "This differs slightly from Joshi et al. (2018) , where tags are predicted based on span representations instead. The tagging head is trained jointly with the parser by adding an auxiliary softmax crossentropy loss, averaged over all words present in a given batch. (2018) We train our parser with a learning rate of 5 \u00d7 10 \u22125 and batch size 32, where BERT parameters are fine-tuned as part of training. All other hyperparameters are unchanged from <cite>Kitaev and Klein (2018)</cite> and Devlin et al. (2018) . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_0",
  "x": "In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision<cite> (Zhang and Litman, 2015)</cite> . Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a revision on local text cohesion), and 2) transform revision purpose classification to a sequential labeling task to capture dependencies among revisions (as in Table 1 ).",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_1",
  "x": "In this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays , scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009 ). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision<cite> (Zhang and Litman, 2015)</cite> . Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors.",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_2",
  "x": "There are multiple works on the classification of revisions (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> . While different classification tasks were explored, similar approaches were taken by extracting features (location, text, meta-data, language) from the revised text to train a classification model (SVM, Random Forest, etc.) on the annotated data. One problem with prior works is that the contextual features used were typically shallow (location), while we cap- and sentence 2 acts as the Warrant for the Claim. Sentence 1 in Draft 1 is modified to sentence 1 (also acts as the Claim) of Draft 2. Sentence 2 in Draft 1 is deleted in Draft 2.",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_3",
  "x": "For example, a revision on the evidence argument can be just a correction of spelling mistakes. ---------------------------------- **DATA DESCRIPTION** Revision purposes. To label our data, we adapt the schema defined in<cite> (Zhang and Litman, 2015)</cite> as it can be reliably annotated and is argument- (Faigley and Witte, 1981) .",
  "y": "extends"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_4",
  "x": "As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision. Corpora. Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per<cite> (Zhang and Litman, 2015)</cite> , although the original annotations were modified as described above. It contains 47 paper draft pairs about placing contemporaries in Dante's Inferno.",
  "y": "motivation"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_5",
  "x": "As we focus on argumentative changes, we merge all the Surface subcategories into one Surface category. As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision. Corpora. Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per<cite> (Zhang and Litman, 2015)</cite> , although the original annotations were modified as described above.",
  "y": "uses"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_6",
  "x": "4 Utilizing Context ---------------------------------- **ADDING CONTEXTUAL FEATURES** Our previous work<cite> (Zhang and Litman, 2015)</cite> used three types of features primarily from prior work (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) for argumentative revision classification. Location features encode the location of the sentence in the paragraph and the location of the sentence's paragraph in the essay.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_0",
  "x": "**** later works proposed partially-or purely-convolutional CTC models [8] [9] [10] [11] and convolution-heavy encoder-decoder models [16] for ASR. However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_1",
  "x": "However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_2",
  "x": "**MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position.",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_3",
  "x": "Operations per layer ---------------------------------- **SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> . T is input length, d is no. of hidden units, and k is filter/context width.",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_4",
  "x": "Let H \u2208 R T \u00d7d h denote a sublayer's input. The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give where \u03c3 is row-wise softmax.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_5",
  "x": "Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters with the biases b1, b2 broadcasted over all T positions. This sublayer aggregates the multiple heads at time t into the attention layer's final output at t. All together, the layer is given by: ----------------------------------",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_6",
  "x": "Note that CTC will still require U \u2264 T /k, however. ---------------------------------- **POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_7",
  "x": "At train time, utterances are sorted by length: we exclude those longer than 1800 frames ( 1% of each training set). We take a window of 25ms, a hop of 10ms, and concatenate cepstral mean-variance normalized features with temporal first-and second-order differences. 1 We downsample by a factor of k = 3 (this also gave an ideal T /k \u2248 dh for our data; see Table 1 ). We perform Nesterov-accelerated gradient descent on batches of 20 utterances. As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> .",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_0",
  "x": "Recently, some preliminary studies have begun to incorporate certain side information (e.g., triplets from external knowledge base) into the model design of various NLP tasks, such as question answering [1] and conversation generation [18] . Generally, there are two lines of this work. The first line focuses on designing task-specific model structures [1, 15] , which exploit the retrieved concepts from external knowledge base for enhancing the representation. Recently, the other line has studied to pre-train a language model over large corpus to learn the inherent word-level knowledge in an unsupervised way<cite> [4,</cite> 8] , which achieves very promising performance. The first line of work is usually carefully designed for the target task, which is not widely applicable.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_1",
  "x": "Pre-trained language model such as BERT and GPT<cite> [4,</cite> 8] is also used as a kind of commonsense knowledge source. However, the LM method mainly captures the co-occurrence of words and phrases and cannot address some more complex problems which may require the reasoning ability. Unlike previous work, we incorporate external knowledge by jointly training MRC model with two auxiliary tasks which are relevant to commonsense knowledge. The model can learn to fill in the knowledge gap without changing the original model structure. ----------------------------------",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_2",
  "x": "**OVERALL FRAMEWORK** The proposed method can be roughly divided into three parts: a pre-trained LM encoder, a task-specific prediction layer for multichoice MRC and two relation-aware auxiliary tasks. The overall framework is shown in Figure 1 . The pre-trained LM encoder acts as the foundation of the model, which is used to capture the relationship between question, document and answer options. Here we utilize BERT <cite>[4]</cite> as the pretrained encoder for its superior performance in a range of natural language understanding tasks.",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_3",
  "x": "In the following, we will describe the two auxiliary tasks in detail. ---------------------------------- **INCORPORATING RELATION KNOWLEDGE** Task 1 is the relation-existence task. Following <cite>[4]</cite> , we first convert the concept to a set of BPE tokens tokens A and tokens B, with beginning index i and j in the input sequence respectively.",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_4",
  "x": "**IMPLEMENTATION DETAILS** We use the uncased BERT(base) <cite>[4]</cite> as pre-trained language model. We set the batch size to 24, learning rate to 2e-5. The maximum sequence length is 384 for SemEval-2018 Task 11 and 512 for Story Cloze Test. We fine-tune for 3 epochs on each dataset.",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_0",
  "x": "****QUADRATIC-TIME DEPENDENCY PARSING FOR MACHINE TRANSLATION**** **ABSTRACT** Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently,<cite> McDonald et al. (2005b)</cite> formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_1",
  "x": "In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ---------------------------------- **O(N 2 )-TIME DEPENDENCY PARSING FOR MT** We now formalize weighted non-projective dependency parsing similarly to<cite> (McDonald et al., 2005b)</cite> and then describe a modified and more efficient version that can be integrated into a phrasebased decoder. Given the single-head constraint, parsing an input sentence x = (x 0 , x 1 , \u00b7 \u00b7 \u00b7 , x n ) is reduced to labeling each word x j with an index i identifying its head word x i .",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_2",
  "x": "Extracting all these tags takes time O(n) for any arbitrary pair (i, j). Since i and j are both free variables, feature computation in<cite> (McDonald et al., 2005b)</cite> takes time O(n 3 ), even though parsing itself takes O(n 2 ) time. To make our parser genuinely O(n 2 ), we modified the set of in-between POS features in two ways. First, we restrict extraction of in-between POS tags to those words that appear within a window of five words relative to either the head or the modifier. While this change alone ensures that feature extraction is now O(1) for each word pair, this causes a fairly high drop of performance (dependency accuracy Table 4 : Dependency parsing experiments on test sentences of any length.",
  "y": "motivation"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_3",
  "x": "Indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance (Brants et al., 2007) , and the ability of phrase-based decoders to handle large-size, high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKY decoders, whose time complexity grows prohibitively large with higher-order language models. While context-free decoding algorithms (CKY, Earley, etc.) may sometimes appear too computationally expensive for high-end statistical machine translation, there are many alternative parsing algorithms that have seldom been explored in the machine translation literature. The parsing literature presents faster alternatives for both phrasestructure and dependency trees, e.g., O(n) shiftreduce parsers and variants ( (Ratnaparkhi, 1997; Nivre, 2003) , inter alia). While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n 2 ) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) ,<cite> McDonald et al. (2005b)</cite> present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) ).",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_4",
  "x": "Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) ,<cite> McDonald et al. (2005b)</cite> present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) ). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen-dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of<cite> McDonald et al. (2005b)</cite> for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008) , which use a 5-gram LM only during reranking.",
  "y": "extends"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_5",
  "x": "In this section, we review dependency parsing formulated as a maximum spanning tree problem<cite> (McDonald et al., 2005b)</cite> , which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008) . Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_6",
  "x": "The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999) . It runs in time O(n 3 ), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n 4 ) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999) ,<cite> McDonald et al. (2005b)</cite> show O(n 2 )-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig. 1 .",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_7",
  "x": "This relaxation is key to computational efficiency, since the parser does not need to keep track of whether dependencies assemble into contiguous spans. It is also linguistically desirable in the case of free word order languages such as Czech, Dutch, and German. Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1 . For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_8",
  "x": "Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1 . For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_9",
  "x": "While this change alone ensures that feature extraction is now O(1) for each word pair, this causes a fairly high drop of performance (dependency accuracy Table 4 : Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one implemented as in (McDonald et al., 2005a) , which is known as one of the top performing dependency parsers for English. The O(n 3 ) non-projective parser of <cite>(McDonald et al., 2005b</cite> ) is slightly more accurate than our version, though ours runs in O(n 2 ) time. \"Local classifier\" refers to non-projective dependency parsing without removing loops as a post-processing step. The result marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art dependency parser (**). on our test was down 0.9%).",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_10",
  "x": "In the MT setting, we need to use a less effective tagger, since we cannot afford to perform Viterbi inference as a by-product of phrase-based decoding. Hence, we use a simpler tagging model that assigns tag t i to word x i by only using features of words x i\u22123 \u00b7 \u00b7 \u00b7 x i , and that does not condition any decision based on any preceding or next tags (t i\u22121 , etc.). Its performance is 95.02% on the WSJ, and 95.30% on the English CTB. Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%. Table 4 shows that the accuracy of our truly O(n 2 ) parser is only .25% to .34% worse than the O(n 3 ) implementation of<cite> (McDonald et al., 2005b)</cite> .",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_11",
  "x": "The training data consists of about 28 million English words and 23.3 million 5 Note that our results on WSJ are not exactly the same as those reported in <cite>(McDonald et al., 2005b</cite> ), since we used slightly different head finding rules. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007) : LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E8, and LDC2006G05.",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_12",
  "x": "In this paper, we presented a non-projective dependency parser whose time-complexity of O(n 2 ) improves upon the cubic time implementation of<cite> (McDonald et al., 2005b)</cite> , and does so with little loss in dependency accuracy (.25% to .34%). Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring). We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements). We plan to pursue other research directions using dependency models discussed in this paper. While we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work to incorporate dependency features of both sourceand target side.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_0",
  "x": "****EXTENDING SENSE COLLOCATIONS IN INTERPRETING NOUN COMPOUNDS**** **ABSTRACT** This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by <cite>Moldovan et al. (2004)</cite> . Our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity of the classifiers by expanding the range of sense collocations via different semantic relations. Our method combines hypernyms, hyponyms and sister words of the component nouns, based on WordNet.",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_1",
  "x": "It has been shown that NCs with semantically similar compo-nents share the same SR ; this is encapsulated by the phrase coined as sense collocation in <cite>Moldovan et al. (2004)</cite> . For example, NCs such as apple pie have the same interpretation as banana cake, where the modifiers of both NCs are semantically similar (they are both classified as fruit), and the head nouns of both NCs are a type of baked edible concoction. Given that the modifier is a fruit and the head noun is a baked concoction, then we can interpret NCs with this sense collocation as having the PRODUCT-PRODUCER SR. However, unlike the method of , where new data was induced by substituting the components of the NCs with semantically similar terms, our approach adds related terms as features for the classifier. The related terms we add are the NC components' hypernyms, hyponyms, and sister words.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_2",
  "x": "---------------------------------- **RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category. A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_3",
  "x": "Finally, we conclude our work in Section 9. ---------------------------------- **RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_4",
  "x": "Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category. A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to her model. In contrast, utilise sense collocations in a different way: instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances (which is assumed to have the same SR as the original).",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_5",
  "x": "As mentioned above, <cite>Moldovan et al. (2004)</cite> showed that the sense collocation of NCs is a key feature when interpreting NCs. Further research in this area has shown that not only synonymous NCs share the same SR, but NCs whose components are replaced with similar words also have the same SR as the original NCs . For example, car factory, automobile factory and truck factory substituted with a synonym, hypernym and sister word, respectively, share the same SR of PRODUCT-PRODUCER. Figure 3 shows an example of semantic neighbours for the two NCs car key and apple pie. Car key can be interpreted as PRODUCT-PRODUCER by referring to the training NC automobile key, since they have the same sense collocation.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_6",
  "x": "The disadvantage of the method employed by is that noise will inevitably infect the training data, skewing the classifier performance. The original method described in <cite>Moldovan et al. (2004)</cite> only relies on observed sense collocations. The components of the NCs are represented as specific synsets in WordNet, and the model does not capture related words. Hence, in this paper, we aimed to develop a model that can take advantage of relatedness between WordNet synsets via hypernyms, hyponyms and sister words, without the risk of losing semantic granularity or introducing noisy training data. Note that in Kim and Baldwin (2007), we used synonyms, hypernyms and sister words.",
  "y": "motivation"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_7",
  "x": "---------------------------------- **METHOD** At first, we describe the principal idea of sense collocation method on NC interpretation and the probability model proposed in<cite> (Moldovan et al., 2004)</cite> . Then we present our method using hypernyms, hyponyms and sister words in order to extend sense collocation method. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_8",
  "x": "At first, we describe the principal idea of sense collocation method on NC interpretation and the probability model proposed in<cite> (Moldovan et al., 2004)</cite> . Then we present our method using hypernyms, hyponyms and sister words in order to extend sense collocation method. ---------------------------------- **SENSE COLLOCATION** The basic idea behind sense collocation method in <cite>Moldovan et al. (2004)</cite> was based on the \"pair-ofword-senses\" from the component nouns in noun compounds as features of the classifier.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_9",
  "x": "We extend the approach of <cite>Moldovan et al. (2004)</cite> by adding similar words as features focusing on hypernyms, hyponyms and sister words of the modifier and head noun. We accumulate the features for semantic relations based on different taxonomic relation types, from which we construct a feature vector to build a classifier over. The features of each taxonomic relation types are listed below. The first is features used in the original sense collocation method. The second, third and fourth are our experimental features added hypernyms, hyponyms and sister words respectively.",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_10",
  "x": "---------------------------------- **7-WAY CLASSIFICATION EXPERIMENT** We ran our first experiment over the 7-class dataset. The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_12",
  "x": "We ran our first experiment over the 7-class dataset. The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark. .217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = <cite>Moldovan et al. (2004)</cite> method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper. Table 3 shows that our method, combined with hypernyms outperforms the original sense collocation method, with the highest accuracy of .5880 achieved with 5th-degree ancestors of the head noun and modifier.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_13",
  "x": "Lauer (1995) proposed probabilistic models (based on dependency and adjacency analyses of the data). Later Nakov (2005) built upon this by adding linguistic features into these probabilistic models. Methods employed in word sense disambiguation (WSD) have also been used to enhance NC interpretation; the noun components that comprise the NCs are disambiguated using these WSD techniques (SparckJones, 1983; . carried out experiments on automatically modeling WSD and attested the usefulness of conducting analysis of the word senses in the NC in determining its SR. In the automatic interpretation of NCs, many claims have been made for the increase in performance, but these works make their own assumptions for interpretation (Barker and Szpakowicz, 1998;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Girju, 2007; Seaghdha, 2007) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_0",
  "x": "In finance, volatility is an empirical measure of risk and will vary based on a number of factors. This paper attempts to use text information in financial reports as factors to rank the risk of stock returns. Considering such a problem is a text ranking problem, we attempt to use learning-to-rank techniques to deal with the problem. Unlike the previous study<cite> (Kogan et al., 2009)</cite> , in which a regression model is employed to predict stock return volatilities via text information, our work utilizes learning-to-rank methods to model the ranking of relative risk levels directly. The reason of this practice is that, via text information only, predicting ranks among real-world quantities should be more reasonable than predicting their real values.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_1",
  "x": "The difficulty of predicting the values is partially because of the huge amount of noise within texts<cite> (Kogan et al., 2009</cite> ) and partially because of the weak connection between texts and the quantities. Regarding these issues, we turn to rank the relative risk levels of the companies (their stock returns). By means of learning-to-ranking techniques, we attempt to identify some key factors behind the text ranking problem. Our experimental results show that in terms of two different ranking correlation metrics, our ranking approach significantly outperforms the regression-based method with a confidence level over 95%. In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in<cite> (Kogan et al., 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_2",
  "x": "Regarding these issues, we turn to rank the relative risk levels of the companies (their stock returns). By means of learning-to-ranking techniques, we attempt to identify some key factors behind the text ranking problem. Our experimental results show that in terms of two different ranking correlation metrics, our ranking approach significantly outperforms the regression-based method with a confidence level over 95%. In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in<cite> (Kogan et al., 2009</cite> ). These words enable us to get more insight and understanding into financial reports.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_3",
  "x": "In addition to the conventional models, in recent years there have also been some attempts of using learning-based methods to solve the text ranking problem, such as (Freund et al., 2003; Burges et al., 2005; Joachims, 2006) , which subsequently brings about a new area of learning to rank in the fields of information retrieval and machine learning. Considering the prevalence of learning-to-rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk. In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; <cite>Kogan et al., 2009</cite>; Leidner and Schilder, 2010) . (Lin et al., 2008 ) use a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short-term stock price movements. In the work, a Hierarchical Agglomerative Clustering (HAC) method with K-means updating is employed to improve the purity of the prototypes of financial reports, and then the generated prototypes are used to predict stock price movements.",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_4",
  "x": "Let S t be the price of a stock at time t. Holding the stock for one period from time t \u2212 1 to time t would result in a simple net return: R t = S t /S t\u22121 (Tsay, 2005) . The volatility of returns for a stock from time t \u2212 n to t can be defined as . We now proceed to classify the volatilities of n stocks into 2\u2113 + 1 risk levels, where n, \u2113 \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 }. Let m be the sample mean and s be the sample standard deviation of the logarithm of volatilities of n stocks (denoted as ln(v)). The distribution over ln(v) across companies tends to have a bell shape<cite> (Kogan et al., 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_5",
  "x": "---------------------------------- **EXPERIMENTS AND ANALYSIS** In this paper, the 10-K Corpus<cite> (Kogan et al., 2009</cite> ) is used to conduct the experiments; only Section 7 \"management's discussion and analysis of financial conditions and results of operations\" (MD&A) is included in the experiments since typically Section 7 contains the most important forward-looking statements. In the experiments, all documents were stemmed by the Porter stemmer, and the documents in each year are indexed separately. In addition to the reports, the twelve months after the report volatility for each company can be calculated by Equation (1), where the price return series can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database.",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_6",
  "x": "For regression, linear kernel is adopted with \u03b5 = 0.1 and the trade-off C is set to the default choice of SVM light , which are the similar settings of<cite> (Kogan et al., 2009</cite> ). For ranking, linear kernel is adopted with C = 1, all other parameters are left for the default values of SVM Rank . Table 1 tabulates the experimental results, in which all reports from the five-year period preceding the test year are used as the training data (we denote the training data from the n-year period preceding the test year as T n hereafter). For example, the reports from year 1996 to 2000 constitute a training data T 5 , and the resulting model is tested on the reports of year 2001. As shown in the table, with the feature of TF-IDF, our results are significantly better than those of the baseline in terms of both two measures.",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_7",
  "x": "Table 1 tabulates the experimental results, in which all reports from the five-year period preceding the test year are used as the training data (we denote the training data from the n-year period preceding the test year as T n hereafter). For example, the reports from year 1996 to 2000 constitute a training data T 5 , and the resulting model is tested on the reports of year 2001. As shown in the table, with the feature of TF-IDF, our results are significantly better than those of the baseline in terms of both two measures. In addition to using T 5 as the training data, we also conduct other 4 sets of experiments with T 1 , T 2 , T 3 , T 4 to test the reports from Words with Negative Weights 1996-2000 1997-2001 1998-2002 1999-2003 2000-2004 2001-2005 Figure 1 Figure 1 illustrates the top positive and negative weighted terms appearing more than twice in the six T 5 models trained on TF-IDF; these terms (8 positive and 8 negative) constitute the radar chart in Figure 1 . Almost all the terms found by our ranking approach are financially meaningful; in addition, some of highly risk-correlated terms are not even reported in<cite> (Kogan et al., 2009)</cite> .",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_0",
  "x": "Translating Japanese to English is difficult because they belong to different language families. Na\u00efve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English. Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (<cite>Hoshino et al., 2013</cite>) , and extends <cite>their</cite> rules to handle abbreviation and passivization frequently found in scientific papers.",
  "y": "uses extends"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_1",
  "x": "Experimental results show that our proposed method improves performance of both (<cite>Hoshino et al., 2013</cite>) 's system and our phrase-based SMT baseline without preordering. ---------------------------------- **INTRODUCTION** Preordering method is one of the popular techniques in statistical machine translation. Preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and English, and thus improve performance of machine translation system.",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_2",
  "x": "Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_3",
  "x": "Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.",
  "y": "motivation"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_4",
  "x": "For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference. Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair. Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following (<cite>Hoshino et al., 2013</cite>) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_5",
  "x": "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following (<cite>Hoshino et al., 2013</cite>) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order. We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers. The main contribution of this work is as follows: \u2022 We propose an extension to (<cite>Hoshino et al., 2013</cite>) in order to deal with abbreviation and passivization frequently found in scientific papers.",
  "y": "motivation extends"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_6",
  "x": "Third, <cite>Hoshino et al. (2013)</cite> proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task. The first is sentence-level and the second is phrase-level. Furthermore, sentence-level preordering rules are divided into three parts. In total, sentences are reordered sequentially by four rules. Since <cite>this method</cite> is the one we re-implemented in this paper, we will describe <cite>their method</cite> in detail below.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_7",
  "x": "Since <cite>this method</cite> is the one we re-implemented in this paper, we will describe <cite>their method</cite> in detail below. Pseudo head-initialization Since Japanese is a head-final language but English is a head-initial language, this rule transforms a Japanese dependency tree as to become a head-initial phrase sequence. Concretely, we move the last phrase, which is a predicate of a Japanese sentence in almost all cases, to the beginning of the sentence. We then order each phrase as their children located immediately after them. Inter-chunk preordering We move a predicate of a sentence to an adequate place.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_8",
  "x": "It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words. 3 Extension to (<cite>Hoshino et al., 2013</cite>) Our proposed preordering model is based on (<cite>Hoshino et al., 2013</cite>) with three extensions to better handle academic writing in scientific papers. ---------------------------------- **PARENTHESIS PREORDERING** Scientific papers often include parenthetical expressions.",
  "y": "uses extends"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_9",
  "x": ". was explained\" or \"It was explained that . . .\".). <cite>Hoshino et al. (2013)</cite> proposed to move a predicate after the subject (inter-chunk preordering). However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases. Thus, we move a predicate to the end of the subjective phrase. Table 1d depicts how subject preordering moves a predicate in a sentence.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_10",
  "x": "---------------------------------- **EXPERIMENTS** We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data: \u2022 original data (baseline), \u2022 preordered data by our re-implementation of (<cite>Hoshino et al., 2013</cite>) , and",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_11",
  "x": "\u2022 preordered data by our re-implementation of (<cite>Hoshino et al., 2013</cite>) , and \u2022 preordered data by our proposed methods. We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence. Also, following (<cite>Hoshino et al., 2013</cite>) , we did not consider event nouns as predicates. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_12",
  "x": "In terms of BLEU, our re-implementation of (<cite>Hoshino et al., 2013</cite>) is below the baseline method while our proposed methods better than the baseline. In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score. ---------------------------------- **EXPERIMENTAL RESULTS** All methods including parenthesis preordering outperform the baseline method, and when we subtract three modifications one by one from proposed method, the parenthesis rule has the largest impact on the translation quality.",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_13",
  "x": [
   "All methods including parenthesis preordering outperform the baseline method, and when we subtract three modifications one by one from proposed method, the parenthesis rule has the largest impact on the translation quality. ---------------------------------- **DISCUSSION** Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis. We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods."
  ],
  "y": "uses"
 },
 {
  "id": "66b6283cf1f20977286f99ef21b3c7_0",
  "x": "Modeling efforts will remain crucial to the exploration of these new capabilities. When we build and assemble models of actions and interpretations, we get systems that can plan their own behavior simply by exploiting what they know about communication. These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress (Cassell et al., 2000; <cite>Koller and Stone, 2007)</cite> shows that there's still lots of hard work needed to develop suitable techniques.",
  "y": "future_work motivation"
 },
 {
  "id": "672d4299e60752e866293d72f97905_0",
  "x": "Psycholinguistic properties have been used in various approaches, such as for Lexical Simplification<cite> [12]</cite> , for Text Simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [18] , to predict the reading times (RTs) of each word in a sentence to assess sentence complexity [14] and also to create robust text level readability models [17] , which is also one of the purposes of this paper. Because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [2, 7, 8, 15] . For the English language, the most well known database of this kind is the MRC Psycholinguistic Database 4 , which contains 27 subjective psycholinguistic properties for 150,837 words. For BP for example, there is a psycholinguistic database 5 containing 21 columns of information for 215,175 words, but no subjective psycholinguistic properties. In this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, AoA and subjective frequency (similar to familiarity) for a large database of 26,874 BP words using a resource-light regression approach.",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_1",
  "x": "In this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, AoA and subjective frequency (similar to familiarity) for a large database of 26,874 BP words using a resource-light regression approach. As for the automatic inference, this work is strongly based on the results of<cite> [12]</cite> which proposed an automatic bootstrapping method for regression to populate the MRC Database. We explore here 3 research questions: (1) is it possible to achieve high Pearson and Spearman correlations values and low MSE values with a regression method using only word embedding features to infer the psycholinguistic properties for BP? (2) which size a database with psycholinguistic properties should have to be used in regression models? Does merging databases from different sources yield better correlation and lower MSE scores? (3) can the inferred values help in creating features that result in more reliable readability prediction models of BP texts for early school years (from 3rd to 6th grades)? Moreover, we assessed interrater reliability (Cronbach's alpha) between ratings generated by our method and the imageability and concreteness produced for 237 nouns by [9] .",
  "y": "similarities"
 },
 {
  "id": "672d4299e60752e866293d72f97905_2",
  "x": "To the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the MRC Database [4, <cite>12]</cite> . In order to solve limitations resulting from using word databases with human ratings, [4] proposes a computational model to predict word concreteness, by using linear regression with word attributes from WordNet [3] , Latent Semantic Analysis (LSA) and the CELEX Database 6 and use these attributes to simulate human ratings in the MRC database. Word concreteness is among the most important indices provided by CohMetrix, as comprehension is facilitated by virtue of more concrete words. The lexical features used were 19 lexical types from WordNet, 17 LSA dimensions, hypernymy information from WordNet, word frequencies from the CELEX Database, and word length (i.e., number of letters), totalling 39 attributes. The Pearson correlation between the estimated concreteness score and the concreteness score in the test set was 0.82.",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_3",
  "x": "The Pearson correlation between the estimated concreteness score and the concreteness score in the test set was 0.82. [<cite>12]</cite> automatically estimate missing psycholinguistic properties in the MRC Database through a bootstrapping algorithm for regression. Their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper-nyms and hyponyms for word in WordNet and also minimum, maximum and average distance between the word's senses in WordNet and the thesaurus' root sense. The Pearson correlation between the estimated score and the inferred score for familiarity was 0.846; 0.862 for AoA; 0.823 for imagenery and 0.869 for concretness, which is better than the results of [4] . ----------------------------------",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_4",
  "x": "---------------------------------- **A LIGHTWEIGHT REGRESSION METHOD TO INFER PSYCHOLINGUISTIC PROPERTIES OF WORDS** The fact that the methods developed by [4] and<cite> [12]</cite> are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\". Therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models. One critical difference between the strategy of<cite> [12]</cite> and ours is that they concatenate all features to train a regressor, while we take a different approach.",
  "y": "background motivation"
 },
 {
  "id": "672d4299e60752e866293d72f97905_5",
  "x": "One critical difference between the strategy of<cite> [12]</cite> and ours is that they concatenate all features to train a regressor, while we take a different approach. Although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them (e.g. word embeddings and word length). Instead, we adopted a more elegant solution, called Multi-View Learning [19] . In a Multi-View Learning, multiple regressors/classifiers are trained over different feature spaces and then combined to produce a single result. Here, the fusion stage is made by averaging the values predicted by the regressors [19] .",
  "y": "differences motivation"
 },
 {
  "id": "672d4299e60752e866293d72f97905_6",
  "x": "We used a linear least squares regressor with L2 regularization, which is also known as Ridge Regression or Tikhonov regularization [6] . We choose this regression method due to the promising results reported by<cite> [12]</cite> . We trained three regressors in different feature spaces: lexical features, Skip-Gram embeddings, and GloVe embeddings. ---------------------------------- **EVALUATION**",
  "y": "uses"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_0",
  "x": "M ost effort of unsupervised discovery of acoustic patterns considered only one level of phoneme-like acoustic patterns. However, it is well known that speech signals have multilevel structures including at least phonemes and words, and such structures are very helpful in analysing or decoding speech [12] . In a previous work, we proposed to discover the hierarchical structure of two-level acoustic patterns, including subword-like and word-like patterns. A similar two-level framework was also developed recently [18] . In a more recent attempt <cite>[19]</cite> , we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_1",
  "x": "The transcription of a signal decoded with these patterns can be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all distinct acoustic patterns can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic patterns represents the phonetic granularity. This gives a two-dimensional representation of the acoustic pattern configurations in terms of temporal and phonetic granularities as in Fig. 1 . Any point in this two-dimensional space in Fig. 1 corresponds to an acoustic pattern configuration. Note that in our previous work <cite>[19]</cite> , the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_2",
  "x": "There can be various applications for the acoustic patterns presented here. In this section we summarize the way to perform spoken term detection <cite>[19]</cite> . Let {pr, r = 1, 2, 3, .., n} denote the n acoustic patterns in the set of \u03c8=(m, n). We first construct a similarity matrix S of size n \u00d7 n off-line for every pattern set \u03c8=(m, n), for which the element S(i, j) is the similarity between any two pattern HMMs pi and pj in the set. The KL-divergence KL(i, j) between two pattern HMMs in (7) is defined as the symmetric KL-divergence between the states based on the variational approximation [22] summed over the states.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_4",
  "x": "However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because here we have jointly considered the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets (e.g. including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included <cite>[19]</cite> . The M \u00d7N relevance scores R(d, q) in (9) obtained with M \u00d7N pattern sets \u03c8=(m, n) are then averaged and the average scores are used in ranking all the documents for spoken term detection. It is also possible to learn the weights for different pattern sets to produce better results using a development set. But here we simply assume the detection is completely unsupervised without any annotation, and all pattern sets are equally weighted <cite>[19]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_0",
  "x": "---------------------------------- **INTRODUCTION** Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000;<cite> Wang et al., 2005</cite>; McDonald et al., 2005) . Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) .",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_1",
  "x": "Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005) , a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" (McDonald et al., 2005) -is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work.",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_2",
  "x": "For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" (McDonald et al., 2005) -is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work. ---------------------------------- **DEPENDENCY PARSING MODEL**",
  "y": "background motivation"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_3",
  "x": "where the score s can depend on any measurable property of \u00a4 and \u00a4 % $ within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models<cite> (Wang et al., 2005</cite>; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006) . For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where i are the weight parameters to be estimated during training.",
  "y": "uses"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_4",
  "x": "where i are the weight parameters to be estimated during training. ---------------------------------- **LEXICALISED DEPENDENCY PARSING** To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words<cite> (Wang et al., 2005)</cite> . The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_0",
  "x": "Owing to notable advances in deep learning and representation learning, important progress has been achieved on text classification, reading comprehension, and other NLP tasks. Recently, pretrained language representations with self-supervised objectives (Peters et al., 2018; <cite>Devlin et al., 2018</cite>; Radford et al., 2018) have further pushed forward the state-of-the-art on many English tasks. While these sorts of deep models can be trained on different languages, deep models typically require substantial amounts of labeled data for the specific domain of data. Unfortunately, the cost of acquiring new custom-built resources for each combination of language and domain is very high, as it typically requires human annotation. Available resources for domain-specific tasks are often imbalanced between different languages.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_1",
  "x": "The scarcity of non-English annotated corpora may preclude our ability to train language-specific machine learning models. In contrast, English-language annotations are often readily available to train deep models. Although translation can be an option, human translation is very costly and for many language pairs, any available domain-specific parallel corpora are too small to train high-quality machine translation systems. Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de Melo and Siersdorfer, 2007) , alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018;<cite> Devlin et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_2",
  "x": "Although translation can be an option, human translation is very costly and for many language pairs, any available domain-specific parallel corpora are too small to train high-quality machine translation systems. Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de Melo and Siersdorfer, 2007) , alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018;<cite> Devlin et al., 2018)</cite> . In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model<cite> (Devlin et al., 2018)</cite> on non-English data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019) .",
  "y": "extends"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_3",
  "x": "We tune the hyper-parameters for our neural network architecture based on each non-English validation set. For the encoder, we invoke the multilingual BERT model<cite> (Devlin et al., 2018)</cite> , which supports 104 languages 1 . It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space. Most model hyperparameters are the same as in pretraining, with the exception of the batch size, max. sequence length, and number of training epochs.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_4",
  "x": "With models such as ELMo (Peters et al., 2018) , GPT-2 (Radford et al., 2018) , and BERT<cite> (Devlin et al., 2018)</cite> , important progress has been made in learning improved sentence representations with context-specific encodings of words via a language modeling objective. The latter two approaches both rely on Transformer encoders, but BERT is trained using masked language modeling instead of right-to-left or left-to-right language modeling. Additionally, BERT also optimizes a next sentence classification objective. Recent work has also investigated cross-lingual extensions. Devlin et al. (2018) themselves published a multilingual version of BERT, following the same model architecture and training procedure, except that the union of 104 different language editions of Wikipedia serves as the training input.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_5",
  "x": [
   "Devlin et al. (2018) themselves published a multilingual version of BERT, following the same model architecture and training procedure, except that the union of 104 different language editions of Wikipedia serves as the training input. Lample and Conneau (2019) incorporate parallel text into BERT's architecture by training on a new supervised learning objective. Artetxe and Schwenk (2018) also show that the encoder from a pretrained sequence-to-sequence model can be used to produce cross-lingual sentence embeddings. All these methods are compatible with our self-learning framework, since they provide a shared sentence meaning representation across languages as needed by our approach. ----------------------------------"
  ],
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_0",
  "x": "**INTRODUCTION** Dialect Identification (DID) problem is a special case of the more general problem of Language Identification (LID). LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID. A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] . In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) <cite>[2]</cite> .",
  "y": "similarities"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_1",
  "x": "A Vector Space Model (VSM) is then constructed using a term-document matrix [4] , followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.1), giving a Phonotactic VSM. In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] . On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] . One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model<cite> [2,</cite> 11] . The extracted i-Vectors give an Acoustic VSM (Section 2.2).",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_2",
  "x": "At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made. This model combination approach has been shown to give performace improvements on the DID task <cite>[2]</cite> . This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM. In this work, we present a feature space combination approach. We form a combined VSM that incorporates useful information, necessary for DID, from both the Phonotactic and Acoustic VSMs.",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_3",
  "x": "We start by presenting the Phonotactic VSM, X P and Acoustic VSM, X A , used in this work, followed by the section on CCA VSM, Z C . ---------------------------------- **PHONOTACTIC VSM; X P** Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer. Details about the phone recognizer can be found in <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_4",
  "x": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works<cite> [2,</cite> 13] . Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons. Tied-phone states are used as the target to the DNNs. The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14] . Input to the DNN consists of 11 consecutive frames stacked together, where for each frame 23 fbank features along with pitch and voicing probability are extracted.",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_5",
  "x": "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] . In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional <cite>[2]</cite> . Finally, we construct the acoustic VSM, X A \u2208 R N \u00d7400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i . We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance<cite> [2,</cite> 11] .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_6",
  "x": "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional <cite>[2]</cite> . Finally, we construct the acoustic VSM, X A \u2208 R N \u00d7400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i . We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance<cite> [2,</cite> 11] . Here, we give a brief overview of the mathematical foundations of the CCA .",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_7",
  "x": "This value is the optimal VSM dimensionality that is experimentally decided. We also perform LDA and WCCN to increase the discriminability of the shared Vector space. ---------------------------------- **DATA USED** Training and test data used in this work is the same as used in <cite>[2]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_8",
  "x": "**DATA EGY GLF LAV NOR MSA** Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR. The test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality. The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal 1 . More details about the train and test data can be found in<cite> [2,</cite> 18] .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_0",
  "x": "Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (<cite>Tian et al., 2014</cite>) . It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual. There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover. For (i), it may not be easy to enumerate all types of logically sound transformations, but tree transformations can be seen as an approximation of logical inference. For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences.",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_1",
  "x": "It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual. There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover. For (i), it may not be easy to enumerate all types of logically sound transformations, but tree transformations can be seen as an approximation of logical inference. For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences. To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( \u00a72) How well can tree transformation approximate logical inference? ( \u00a73) With rigorous inference on DCS trees, where does logic contribute in the system of <cite>Tian et al. (2014)</cite> In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus (BarHaim et al., 2007a) .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_2",
  "x": "T: Hurricane Isabel, which caused significant damage, was a tropical storm when she entered Virginia. Note the coreference between Hurricane Isabel and she, suggesting us to copy the subtree of Hurricane Isabel to she, in a tree edit approach. This is not enough yet, because the head storm in T is not placed at the subject of cause. The issue is indeed very logical: from \"Hurricane Isabel = she\", \"Hurricane Isabel = storm\", \"she = subject of enter\" and \"Hurricane Isabel = subject of cause\", we can imply that \"storm = subject of enter = subject of cause\". 3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_3",
  "x": "The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) . This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed > 0.4. However, generally such kind of similarity is very noisy. <cite>Tian et al. (2014)</cite> used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_4",
  "x": "<cite>Tian et al. (2014)</cite> used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1 . Each strategy is described in the following. LexNoun + Inference The same system as above, except that we only align paths between lexically aligned nouns. Two nouns are aligned if and only if they are synonyms, hyponyms or derivatively related in WordNet.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_5",
  "x": "NoFilter + Coverage Same as above, but all paths alignments with similarity score > 0.4 are accepted. ---------------------------------- **HOW CAN LOGICAL INFERENCE HELP RTE?** Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_6",
  "x": "Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable. However, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes. The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words \"grant\", \"decertify\", and \"accuse\" in the above example.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_7",
  "x": "---------------------------------- **HOW CAN LOGICAL INFERENCE HELP RTE?** Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_8",
  "x": "We hope this approach can bring new progress to RTE and other semantic processing tasks. ---------------------------------- **EFFICIENCY OF ABSTRACT DENOTATIONS** To evaluate the efficiency of logical inference on abstract denotations, we took 110 true entailment pairs from RTE5 development set, which are also pairs that can be proven with on-the-fly knowledge. We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 .",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_9",
  "x": "As such, we use <cite>Tian et al. (2014)</cite> 's inference engine to pin down statements that are actually needed for proving H (usually just 2 or 3 statements), and try to prove H by Prover9 again, using only necessary statements. Proven pairs in 5 minutes then jump to 82% (the \"Red. 5 Min.\" column), showing that a large number of on-the-fly rules may drastically increase computation cost. Still, nearly 20% pairs cannot be proven even in this setting, suggesting that traditional FOL prover is not suited for textual inference. ---------------------------------- **CONCLUSION AND FUTURE WORK**",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_10",
  "x": "And after all, as there may be many possible variations of semantic representations, it is good to have an efficient inference framework that has the potential to connect them. It would be exciting if we can combine different types of structured data with natural language in semantic processing tasks. Directions of our future work are described below. Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_11",
  "x": "And after all, as there may be many possible variations of semantic representations, it is good to have an efficient inference framework that has the potential to connect them. It would be exciting if we can combine different types of structured data with natural language in semantic processing tasks. Directions of our future work are described below. Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure.",
  "y": "extends"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_12",
  "x": "**EFFICIENCY OF ABSTRACT DENOTATIONS** To evaluate the efficiency of logical inference on abstract denotations, we took 110 true entailment pairs from RTE5 development set, which are also pairs that can be proven with on-the-fly knowledge. We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 . The graph shows all pairs can be proven in 6 seconds, and proof time scales logarithmically on weight of statements. On the other hand, we converted statements on abstract denotations into FOL formulas, and tried to prove the same pairs using Prover9, 3 a popu-lar FOL theorem prover.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_0",
  "x": "In their targeted review of sentence-pair modeling, Lan and Xu (2018) likewise examine neural networks that abide by this paradigm. * Equal contribution. The NLP community is, however, witnessing a dramatic paradigm shift toward the pretrained deep language representation model, which achieves state of the art in question answering, sentiment classification, and similarity modeling, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> represents one of the latest developments in this line of work. It outperforms its predecessors, ELMo (Peters et al., 2018) and GPT (Radford et al.) , staggeringly exceeding state of the art by a wide margin on multiple natural language understanding tasks.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_1",
  "x": "The resulting models achieve state of the art in question answering, named-entity recognition, and natural language inference, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) . ---------------------------------- **MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_2",
  "x": "Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) . ---------------------------------- **MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> . To adapt BERT for document classifica- Figure 1 .",
  "y": "uses"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_3",
  "x": "---------------------------------- **MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> . To adapt BERT for document classifica- Figure 1 . During fine-tuning, we optimize the entire model end-to-end, with the additional softmax classifier parameters W \u2208 IR K\u00d7H , where H is the dimension of the hidden state vectors and K is the number of classes.",
  "y": "extends"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_4",
  "x": "We tune the number of epochs, batch size, learning rate, and maximum sequence length (MSL), the number of tokens that documents are truncated to. We observe that model quality is quite sensitive to the number of epochs, and thus the number must be tailored for each dataset. We finetune on Reuters, AAPD, and IMDB for 30, 20 and 4 epochs, respectively. Due to resource constraints, we fine-tune on Yelp for only one epoch. As is the case with<cite> Devlin et al. (2018)</cite> , we find that choosing a batch size of 16, learning rate of 2\u00d710 \u22125 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_5",
  "x": "As is the case with<cite> Devlin et al. (2018)</cite> , we find that choosing a batch size of 16, learning rate of 2\u00d710 \u22125 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets. Hyperparameter study. To gauge the improvement over the default hyperparameters, as well as to highlight the differences in fine-tuning BERT for document classification, we explore varying several key hyperparameters: namely, the number of epochs and the MSL. Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_6",
  "x": "Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic. Furthermore, while conducting our experiments, we find that even fine-tuning BERT is a computationally intensive task. We argue that it is important to study these two hyperparameters, as they are major determinants of the computational resources required to fine-tune BERT. BERT large , for example, requires eight V100s to fine-tune on our datasets, which is of course prohibitive.",
  "y": "motivation"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_7",
  "x": "---------------------------------- **MODEL QUALITY** Trending with<cite> Devlin et al. (2018)</cite> , BERT large achieves state-of-the-art results on all four datasets, followed by BERT base (see Table 2 , rows 11 and 12). The considerably simpler LSTM reg model (row 10) achieves a high F 1 and accuracy of 87.0 and 52.8, respectively, coming close to the quality of BERT base . Surprisingly, the LR and SVM baselines yield competitive results for the multi-label datasets.",
  "y": "background similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_8",
  "x": "Figure 2 shows that this is not the case, since truncating to even 256 tokens causes accuracy to fall lower than that of the much smaller LSTM reg (see Table 2 ). From these results, we conclude that any amount of truncation is detrimental in document classification, but the level of degradation may differ. Epoch analysis. The rightmost two subplots in Figure 2 illustrate the F 1 score of BERT fine-tuned using a various number of epochs for AAPD and Reuters. Contrary to<cite> Devlin et al. (2018)</cite> , who achieve state of the art on small datasets with only a few epochs of fine-tuning, we find that smaller datasets require many more epochs to converge.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_0",
  "x": "The end goal of a fact checking system is to provide a verdict on whether the claim is true, false, or mixed. Several organizations such as FactCheck.org and PolitiFact are devoted to such activities. The FEVER Shared task aims to evaluate the ability of a system to verify information using evidence from Wikipedia. Given a claim involving one or more entities (mapping to Wikipedia pages), the system must extract textual evidence (sets of sentences from Wikipedia pages) that supports or refutes the claim and then using this evidence, it must label the claim as Supported, Refuted or NotEnoughInfo. The dataset for the shared task was introduced by <cite>Thorne et al. (2018)</cite> and consists of 185,445 claims.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_1",
  "x": "Given a claim involving one or more entities (mapping to Wikipedia pages), the system must extract textual evidence (sets of sentences from Wikipedia pages) that supports or refutes the claim and then using this evidence, it must label the claim as Supported, Refuted or NotEnoughInfo. The dataset for the shared task was introduced by <cite>Thorne et al. (2018)</cite> and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_2",
  "x": "Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. <cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_3",
  "x": "<cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_4",
  "x": "<cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_5",
  "x": "\u2022 Combined: We use the union of the documents returned by the three approaches as the final set of relevant documents to be used by the Sentence Selection module. Table 2 shows the percentage of claims that can be fully supported or refuted by the retrieved docu-ments before sentence selection on the dev set. We see that our best approach (combined) achieved a high coverage 94.4% compared to the baseline<cite> (Thorne et al., 2018)</cite> of 55.3%. Because we do not have the gold evidences for the blind test set we cannot report the claim coverage using our pipeline . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_6",
  "x": "Because we do not have the gold evidences for the blind test set we cannot report the claim coverage using our pipeline . ---------------------------------- **SENTENCE SELECTION** For sentence selection, we used the modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning as proposed by<cite> (Thorne et al., 2018)</cite> . We extract the top 5 most similar sentences from the k-most relevant documents using the TF-IDF vector similarity.",
  "y": "similarities uses"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_7",
  "x": "We extract the top 5 most similar sentences from the k-most relevant documents using the TF-IDF vector similarity. Our evidence recall is 78.4 as compared to 45.05 in the development set of FEVER<cite> (Thorne et al., 2018)</cite> , which demonstrates the importance of document retrieval in fact extraction and verification. On the blind test set our sentence selection approach achieves an evidence recall of 75.89. However, even though TF-IDF proves to be a strong baseline for sentence selection we noticed on the dev set that using all 5 evidences together introduced additional noise to the entailment model. To solve this, we further filtered the top 3 evidences from the selected 5 evidences using distributed semantic representations.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_8",
  "x": "Unlike <cite>Thorne et al. (2018)</cite> , we did not concatenate evidences, but trained our model for each claim-evidence pair. For recognizing textual entailment we used the model introduced by Conneau et al. (2017) in their work on supervised learning of universal sentence representations. The architecture is presented in Figure 1 . We use bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) with max-pooling to encode the claim and the evidence. The text encoder provides dense feature representation of an input claim or evidence.",
  "y": "differences"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_0",
  "x": "**INTRODUCTION** Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011) . In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing<cite> (Zhang and Nivre, 2011</cite>; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied.",
  "y": "background"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_1",
  "x": "For example, the contribution of global learning in improving the accuracies has not been separately studied. It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of<cite> Zhang and Nivre (2011)</cite> to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar<cite> (Zhang and Nivre, 2011)</cite> by using the same range of rich feature definitions. In this paper, we answer the above questions empirically. First, we separate out global learning and beam-search, and study the effect of each technique by comparison with a local greedy baseline.",
  "y": "motivation"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_2",
  "x": "Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of<cite> Zhang and Nivre (2011)</cite> . Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by McDonald and Nivre (2007) . On the one hand, global beam-search parsing is more similar to global, exhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_3",
  "x": "On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based MSTParser (McDonald and Pereira, 2006) using the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , showing that the parsers give near identical overall accuracies, but have very different error distributions according to various metrics. While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar<cite> (Zhang and Nivre, 2011)</cite> as a representative system.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_4",
  "x": "Our experiments are performed using ZPar, which uses global learning and beam-search. To make comparisons with local learning under different settings, we make configurations and modifications to ZPar where necessary. Global learning is implemented in the same way as<cite> Zhang and Nivre (2011)</cite> , using the averaged perceptron algorithm (Collins, 2002) and early update (Collins and Roark, 2004) . This is a global learning method in the sense that it tries to maximize accuracy over the entire sentence and not on isolated local transitions. Unless explicitly specified, the same beam size is applied for training and testing when beam-search is applied.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_5",
  "x": "This is a global learning method in the sense that it tries to maximize accuracy over the entire sentence and not on isolated local transitions. Unless explicitly specified, the same beam size is applied for training and testing when beam-search is applied. Local learning is implemented as a multi-class classifier that predicts the next transition action given a parser configuration (i.e. a stack and an incoming queue), trained using the averaged perceptron algorithm. In local learning, each transition is considered in isolation and there is no global view of the transition sequence needed to parse an entire sentence. Figure 1 shows the UAS of ZPar under different settings, where 'global' refers to a global model trained using the same method as<cite> Zhang and Nivre (2011)</cite> , 'local' refers to a local classifier trained using the averaged perceptron, 'base features' refers to the set of base feature templates in<cite> Zhang and Nivre (2011)</cite> , and 'all features' refers to the set of base and all extended feature templates in<cite> Zhang and Nivre (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_6",
  "x": "This is a consequence of the interaction between learning and search: a large beam not only reduces search errors, but also enables a more complex model to be trained without overfitting. In contrast to a globally trained model, a local model cannot benefit as much from the power of rich features. With greedy local search, the UAS of a local model improves from 89.15% with base features to 89.28% with all features. Beam-search does not bring additional improvements. For further evidence, we add rich non-local features in the same increments as<cite> Zhang and Nivre (2011)</cite> to both ZPar and MaltParser, and evaluate UAS on the same development data set.",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_0",
  "x": "The pair HMM has been used before (Mackay and Kondrak, 2005; Wieling et al., 2007) for string similarity estimation, and is based on the notion of string Edit Distance (ED). String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task<cite> (Li et al., 2009)</cite> Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings. To generate transliterations using pair HMM parameters, WFST (Graehl, 1997) techniques are adopted. Transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used.",
  "y": "background uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_1",
  "x": "**DATA SETUP** The data used is divided according to the experimental runs that were specified for the NEWS 2009 shared transliteration task<cite> (Li et al., 2009</cite> ): a standard run and non-standard runs. The standard run involved using the transliteration system described above that uses pair HMM parameters combined with transformation rules. The English-Russian datasets used here were provided for the NEWS 2009 shared transliteration task (Kumaran and Kellner, 2009): 5977 pairs of names for training, 943 pairs for development, and 1000 for testing. For the non-standard runs, an additional English-Russian dataset extracted from the Geonames data dump was merged with the shared transliteration task data above to form 10481 pairs for training and development.",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_3",
  "x": "**RESULTS** Six measures were used for evaluating system transliteration quality. These include<cite> (Li et al., 2009)</cite> : Accuracy (ACC), Fuzziness in Top-1 (Mean F Score), Mean Reciprocal Rank (MRR), Mean Average Precision for reference transliterations (MAP_R), Mean Average Precision in 10 best candidate transliterations (MAP_10), Mean Average Precision for the system (MAP_sys). Table 1 shows the results obtained using only the data sets provided for the shared transliteration task. The system used for the standard run is \"phmm_rules\" described in section 2 to sub section 2.3. \"phmm_basic\" is the system in which pair HMM parameters are used for transliteration generation but there is no representation for bigrams as described for the system used in the standard run.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_0",
  "x": "These models have the additional advantage of being more language-independent than semantic methods, which often rely on parsers and other NLP tools. To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced<cite> (Wang et al., 2017</cite>; Koncel-Kedziorski et al., 2016) . In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving<cite> (Wang et al., 2017)</cite> . These powerful models have been shown to outperform other data-driven approaches in a variety of tasks. However, it is not obvious that solving word problems is best modeled as a sequence prediction task rather than a classification or retrieval task.",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_1",
  "x": "Another popular line of research is on purely data-driven solvers. Given enough training data, data-driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations. These models have the additional advantage of being more language-independent than semantic methods, which often rely on parsers and other NLP tools. To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced<cite> (Wang et al., 2017</cite>; Koncel-Kedziorski et al., 2016) . In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving<cite> (Wang et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_2",
  "x": "Downstream applications such as question answering or automated tutoring systems may never have to deal with arbitrarily complex or even unseen equation types, obviating the need for a sequence prediction model. These considerations beg the questions: how do data-driven approaches to math word problem solving compare to each other? How can datadriven approaches benefit from recent advances in neural representation learning? What are the limits of data-driven solvers? In this paper, we thoroughly examine datadriven techniques on three larger algebra word problem datasets (Huang et al., 2016; Koncel-Kedziorski et al., 2016;<cite> Wang et al., 2017)</cite> . We study classification, generation, and information retrieval models, and examine popular extensions to these models such as structured self-attention (Lin et al., 2017) and the use of pretrained word embeddings (Pennington et al., 2014; Peters et al., 2018) . Our experiments show that a well-tuned neural equation classifier consistently performs better than more sophisticated solvers.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_3",
  "x": "The nearest neighbor's equation template is then filled in with numbers from the test problem and solved. Following <cite>Wang et al. (2017)</cite> , we use Jaccard distance in this model. For test problem S and training problem T , the Jaccard similarity is computed as: jacc(S, T ) = S\u2229T S\u222aT . We also evaluate the use of a cosine similarity metric. Words from S and T are associated with pretrained vectors v(w i ) (Pennington et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_4",
  "x": "Following <cite>Wang et al. (2017)</cite> we evaluate a seq2seq with LSTMs as the encoder and decoder. We also evaluate the use of Convolutional Neural Networks (CNNs) in the encoder and decoder. ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_5",
  "x": "Datasets For comparison, we report solution accuracy on the Chinese language Math23K dataset<cite> (Wang et al., 2017)</cite> , and the English language DRAW (Upadhyay and Chang, 2015) and MAWPS (Koncel-Kedziorski et al., 2016) datasets. Math23K and MAWPS consist of single equation problems, and DRAW contains both single and simultaneous equation problems. Details on the datasets are shown in Table 1 . The Math23K dataset contains problems with possibly irrelevant quantities. To prune these quantities, we implement a significant number identifier (SNI) as discussed in <cite>Wang et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_6",
  "x": "To prune these quantities, we implement a significant number identifier (SNI) as discussed in <cite>Wang et al. (2017)</cite> . Our best accuracy for SNI is 97%, slightly weaker than previous results. each dataset. We also explore two modifications of the BiLSTM's embedding matrix W E , either by using pretrained GloVe embeddings (Pennington et al., 2014) or using the ELMo technique of (Peters et al., 2018) as implemented in the AllenNLP toolkit (Gardner et al.) with pretrained character embeddings. For seq2seq modeling, we use OpenNMT (Klein et al., 2017) with 500 dimensional hidden states and embeddings and a dropout rate of 0.3.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_7",
  "x": "Neither of these methods help over the English language data. It appears that the ELMo technique may require more training examples before it can improve solution accuracy. The previous state of the art model for the DRAW dataset is described in Upadhyay and Chang (2015) . The state of the art for Math23K, described in <cite>Wang et al. (2017)</cite> , uses a hybrid Jaccard retrieval and seq2seq model. All models shown here fall well short of the highest possible classification/retrieval accuracy, shown in Table 2 as \"Oracle\".",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_8",
  "x": "More recently, <cite>Wang et al. (2017)</cite> provide a large dataset of Chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components. The current work extends these approaches by exploring advanced techniques in data-driven solving. ---------------------------------- **CONCLUSION** We have thoroughly examined data-driven models for automatically solving algebra word problems, including retrieval, classification, and generation techniques.",
  "y": "extends"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_0",
  "x": "Automatic disentanglement could be used to provide more interpretable results when searching over chat logs, and to help users understand what is happening when they join a channel. Over a decade of research has considered conversation disentanglement (Shen et al., 2006) , but using datasets that are either small (2,500 messages,<cite> Elsner and Charniak, 2008)</cite> or not released (Adams and Martell, 2008) . * jkummerf@umich.edu We introduce a conversation disentanglement dataset of 77,563 messages of IRC manually annotated with reply-to relations between messages. 1 Our data is sampled from a technical support channel at 173 points in time between 2004 and 2018, providing a diverse set of speakers and topics, while remaining in a single domain.",
  "y": "background motivation"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_1",
  "x": "These cues are useful for understanding the discussion, but only around 48% of messages have them. System messages, which indicate actions like users entering the channel. These all start with ===, but not all messages starting with === are system messages, as shown by the second message in Figure 1 . 3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by Mehri and Carenini (2017) with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009; Mehri and Carenini, 2017; Jiang et al., 2018) .",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_2",
  "x": "3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Using our data we provide the first empirical evaluation of their method.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_3",
  "x": "Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods. Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data. Recent work has applied neural networks (Mehri and Carenini, 2017; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison. Our data is much larger than prior work, one of the only released sets, and the only one with context and adjudication. '+a' indicates there was an adjudication step to resolve disagreements. '?' indicates the value is not in the paper and the authors no longer have access to the data.",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_4",
  "x": "By removing these labels and mixing conversations they create a disentanglement problem. While convenient, this risks introducing a bias, as people write differently when explicit structure is defined, and only a few papers have released data (Abbott et al., 2016; Zhang et al., 2017; Louis and Cohen, 2015) . Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods. Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_5",
  "x": "Studies that do consider graphs for disentanglement have used small datasets (Dulceanu, 2016; Mehri and Carenini, 2017) that are not always released (Wang et al., 2008; Guo et al., 2017) . ---------------------------------- **DATA** We introduce a manually annotated dataset of 77,563 messages: 74,963 from the #Ubuntu IRC channel, 3 and 2,600 messages from the #Linux IRC channel. 4 Annotating the #Linux data enables comparison with <cite>Elsner and Charniak (2008)</cite> , while the #Ubuntu channel has over 34 million messages, making it an interesting largescale resource for dialogue research.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_6",
  "x": "Figure 2 shows ambiguous examples from our data to provide some intuition for the source of disagreements. In both examples the disagreement involves one link, but the conversation structure in the second case is substantially changed. Some disagreements in our data are mistakes, where one annotation is clearly incorrect, and some are ambiguous cases, such as these. In Channel Two, we also see mistakes and ambiguous cases, including a particularly long discussion about a user's financial difficulties that could be divided in multiple ways (also noted by <cite>Elsner and Charniak (2008)</cite> ). Graphs: We measure agreement on the graph structure annotation using Cohen (1960) 's \u03ba.",
  "y": "similarities"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_9",
  "x": "We consider three metrics: 6 (1) Variation of Information (VI, Meila, 2007) . (2) One-to-One Overlap (1-1,<cite> Elsner and Charniak, 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_10",
  "x": "There are three regions of performance. First, the baseline has consistently low scores since it forms a single conversation containing all messages. Second, <cite>Elsner and Charniak (2008)</cite> and Lowe et al. (2017) per-form similarly, with one doing better on VI and the other on 1-1, though <cite>Elsner and Charniak (2008)</cite> do consistently better across the exact conversation extraction metrics. Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better. Dataset Variations: Table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_11",
  "x": "Decreasing the data size to match <cite>Elsner and Charniak (2008)</cite> 's training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled. ---------------------------------- **CHANNEL TWO RESULTS** For channel Two, we consider two annotations of the same underlying text: ours and <cite>Elsner and Charniak (2008)</cite>'s.",
  "y": "differences uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_12",
  "x": "For channel Two, we consider two annotations of the same underlying text: ours and <cite>Elsner and Charniak (2008)</cite>'s. To compare with prior work, we use the metrics defined by Shen et al. (2006, Shen) and Elsner and Charniak (2008, Loc) . 8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc). We do not evaluate on graphs because <cite>Elsner and Charniak (2008)</cite> 's annotations do not include them. This also prevents us from training our method on their data.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_13",
  "x": "8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc). We do not evaluate on graphs because <cite>Elsner and Charniak (2008)</cite> 's annotations do not include them. This also prevents us from training our method on their data. Model Comparison: For Elsner's annotations (top section of Table 6 ), their approach remains the most effective with just Channel Two data. However, training on our Ubuntu data, treating Channel Two as an out-of-domain sample, yields substantially higher performance on two metrics and comparable performance on the third.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_14",
  "x": "The difficulty of samples varies considerably, with the F-score of our model varying from 11 to 40 and annotator agreement scores before adjudication varying from 0.65 to 0.78. The model performance and agreement levels are also strongly correlated, with a Spearman's rank correlation of 0.77. This demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance. How far apart consecutive messages in a conversation are: <cite>Elsner and Charniak (2008)</cite> and Mehri and Carenini (2017) use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages. Figure 4 shows the distribution of time differences in our conversations.",
  "y": "differences"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_0",
  "x": "As a result, while these efforts have enabled processing of codemixed texts, they still suffer from data scarcity and poor representation learning, and the small individual dataset sizes usually limiting the model performance. Curriculum Learning, as introduced by [Bengio et al., 2009 ] is \"to start small, learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level\". They also draw parallels with human learning curriculum and education system, where different concepts are introduced in an order at different times, and has led to advancement in research towards animal training [Krueger and Dayan, 2009] . Previous experiments with tasks like language modelling<cite> [Bengio et al., 2009]</cite> , Dependency Parsing, and entailment <cite>[Hashimoto et al., 2016]</cite> have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks. [Weinshall and Cohen, 2018 ] also find theoretical and experimental evidence for curriculum learning by pretraining on another task leading to faster convergence.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_1",
  "x": "[Singh et al., 2018b] developed a dataset for HiEn codemixed Part of Speech tagging, and proposed a CRF based approach. [Singh et al., 2018b] developed a dataset for Hindi English Codemixed Language Identification and NER, and propose a CRF based approach with handcrafted features for Named Entity Recognition. Bengio et. al. [2009] introduced curriculum learning approaches towards both vision and language related task, and show significant convergence and performance gains for language modelling task. <cite>[Hashimoto et al., 2016]</cite> propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_2",
  "x": "<cite>[Hashimoto et al., 2016]</cite> propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions. [Swayamdipta et al., 2018] also propose a syntactico semantic curriculum with chunking, semantic role labelling and coreference resolution, and show performance gains over strong baselines. Like <cite>[Hashimoto et al., 2016]</cite> , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same. 3 Datasets [Prabhu et al., 2016] released a Hi-En codemixed dataset for Sentiment Analysis, comprising 3879 Facebook comments on public pages of Salman Khan and Narendra Modi. Comments are annotated as positive, negative and neutral based on their sentiment polarity, and are distributed across the 3 classes as 15% negative, 50% neutral and 35% positive comments.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_3",
  "x": "While our proposed model enables efficient transfer learning by progressive abstraction of representations for more complicated tasks, the highlight of the approach lies in the training regimen followed. Curriculum learning can be seen as a sequence of training criteria<cite> [Bengio et al., 2009]</cite> , with increasing task or sample difficulty as the training progresses. It is also closely related with transfer learning by pretraining, especially in the case when the tasks form a logical hierarchy and contribute to the downstream tasks. With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks. Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis.",
  "y": "background motivation"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_4",
  "x": "Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis. Since tokens in codemixed texts have distinct semantic spaces based on their source language, Language Identification can incorporate this disparity among the learnt trigram representations. Following this, the Part of Speech Tagging groups the words based on their logical semantic categories, and encodes simpler word category information in a sequence. Also, as in [Singh et al., 2018a; Sharma et al., 2016] , Language Tag and Part of Speech Tag have previously been provided as manual handcrafted features for a range of downstream syntactic and semantic tasks. In addition to the above tasks, Language Model pretraining has shown significant performance gains as reported by<cite> [Howard and Ruder, 2018]</cite> .",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_5",
  "x": "**TRANSFER LEARNING** As noted in earlier efforts<cite> [Howard and Ruder, 2018</cite> ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage. On the other hand, too cautious finetuning can cause slow convergence and overfitting. To this end, we experiment with different strategies which can be broadly categorized as: Discriminative Finetuning : As also noted by [Yosinski et al., 2014] , different layers capture different types of information, and thus need to be optimised differently.",
  "y": "background motivation"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_6",
  "x": "Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones. Gradual Unfreezing: Similar to<cite> [Howard and Ruder, 2018]</cite> , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it. Thus for Sentiment Analysis finetuning, for the first epoch, only \u03b8 sentiment receives the gradient updates, after which we unfreeze the \u03b8 lstm2 , and subsequently unfreeze the lower layers in a similar manner.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_7",
  "x": "In the context of our model, the embedding layer captures the individual character trigram information, the LSTM layer 1 is trained towards capturing the token level information such as Part of Speech and Language Tag, and the final LSTM Layer 2 is trained to capture the overall textual representation to perform Language Modelling and Sentiment Analysis. With this purview, similar to<cite> [Howard and Ruder, 2018]</cite> , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task. We thus split the parameters as {\u03b8 1 , ..., \u03b8 l } , where \u03b8 i corresponds to the parameters of layer i, and optimize them with separate learning rates {\u03b7 1 , ...., \u03b7 l } . Also, when finetuning a pretrained layer for a downstream task, we keep \u03b7 i < \u03b7 j ; \u2200i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_8",
  "x": "We add a dropout layer with the dropout rate set to 0.2 between the LSTM layers to prevent overfitting. We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to<cite> [Howard and Ruder, 2018]</cite> , and observe increase in model accuracy by 2.2% on sentiment analysis. To evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task (Sentiment Analysis) for 25 epochs. We approach the evaluation of our curriculum by training the model sequentially for four subtasks -Language Identification, POS Tagging, Language Modelling and Sentiment Analysis. We evaluate the strategy of pretraining with only POS Tagging and Language Identification, and observe similar performance as no curriculum training.",
  "y": "background similarities"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_9",
  "x": "We hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the Source Tasks (POS + Lang Id) and Target Task(Sentiment Analysis). This experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation (LSTM Layer 2 output). We experiment with only Language Modelling as pretraining task, and observe significant gains over no curriculum strategy. We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009;<cite> Howard and Ruder, 2018]</cite> . This is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i.e. Sentiment Analysis in this case.",
  "y": "background similarities"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_10",
  "x": "We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009;<cite> Howard and Ruder, 2018]</cite> . This is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i.e. Sentiment Analysis in this case. As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by<cite> [Howard and Ruder, 2018]</cite> . We segregate our model parameters in the following 4 groups: \u2022 Emb Layer",
  "y": "background similarities"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_0",
  "x": "---------------------------------- **INTRODUCTION** Attention has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems (Das et al., 2017; Rockt\u00e4schel et al., 2015; Santos et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Yin et al., 2016; Zhu et al., 2016; Xu et al., 2015; Chorowski et al., 2015) such as VQA, reading comprehension, textual entailment, image captioning, speech recognition and so forth. It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014;<cite> Vaswani et al., 2017)</cite> . This work proposes an end-to-end co-attentional neural structure, named Crossed Co-Attention Networks (CCNs) to address machine translation, a typical sequence-to-sequence NLP task.",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_1",
  "x": "Attention has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems (Das et al., 2017; Rockt\u00e4schel et al., 2015; Santos et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Yin et al., 2016; Zhu et al., 2016; Xu et al., 2015; Chorowski et al., 2015) such as VQA, reading comprehension, textual entailment, image captioning, speech recognition and so forth. It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014;<cite> Vaswani et al., 2017)</cite> . This work proposes an end-to-end co-attentional neural structure, named Crossed Co-Attention Networks (CCNs) to address machine translation, a typical sequence-to-sequence NLP task. We customize the transformer<cite> (Vaswani et al., 2017)</cite> featured by non-local operations (Wang et al., 2018) with two * The work was done when Yaoyiran was working at Living Analytics Research Centre, Singapore Management University who is now a PhD student at University of Cambridge. input branches and tailor the transformer's multihead attention mechanism to the needs of information exchange between these two parallel branches.",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_2",
  "x": "Then a generic non-local operation is formulated as follows: We basically follow the definition of no-local operation in (Wang et al., 2018) where f : is a unary function and C : R d \u00d7 R n\u00d7d \u2192 R calculates a normalizer, but dispense with the assumption that then the non-local operation degrades to the multihead self-attention as is described in<cite> (Vaswani et al., 2017)</cite> (formula 2 describes only one attention head): Considering two input channels, denoted as 'left' and 'right', we present the following non-local operation as a definition of co-attention where \u03b1(\u00b7), \u03b2(\u00b7) \u2208 { lef t , right }.",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_3",
  "x": "Note that when \u03b1(\u00b7) = lef t , \u03b2(\u00b7) = right the co-attention degrades to two self-attention modules. ---------------------------------- **CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively.",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_4",
  "x": "Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right . In our encoder, the left branch takes input from X Lef t as Value (V) and Key (K) and takes the input X right as Query (Q).",
  "y": "uses extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_5",
  "x": "**CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections.",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_6",
  "x": "---------------------------------- **CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query.",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_7",
  "x": "**RELATED WORK** Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models<cite> (Vaswani et al., 2017)</cite> , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) . While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) . It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019) . Yang et al. propose to model localness for self-attention which would be conducive to capturing local information by learning a Gaussian bias predicting the region of local attention (Yang et al., 2018a) .",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_8",
  "x": "While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) . It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019) . Yang et al. propose to model localness for self-attention which would be conducive to capturing local information by learning a Gaussian bias predicting the region of local attention (Yang et al., 2018a) . Other work indicates that adding convolution layers would ameliorate the aforementioned issue (Yang et al., 2018b (Yang et al., , 2019b . Multi-head attention can also be used in multi-modal scenarios when V, K and Q gates take in data from different domains.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_0",
  "x": "**INTRODUCTION** Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016;<cite> Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) . This task is challenging because it has a well-defined structured output and the input structure and output structure are in different forms. A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_1",
  "x": "A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_2",
  "x": "Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data. In machine translation (MT) problems Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018) , hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words.",
  "y": "background motivation"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_3",
  "x": "We also propose a dynamic programming-based sentence-tosentence alignment method that can be applied to similar sentences to perform word substitution and enable retrieval of imperfect matches. These contributions allow us to improve on previous stateof-the-art results. ---------------------------------- **SYNTACTIC CODE GENERATION** Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a. In this work, we start with the syntactic code gen-eration model by<cite> Yin and Neubig (2017)</cite> , which uses sequences of actions to generate the AST before converting it to surface code.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_4",
  "x": "APPLYRULE(r) expands the current node in the tree by applying production rule r from the abstract syntax grammar 2 to the current node. GENTOKEN(v) populates terminal nodes with the variable v which can be generated from vocabulary or by COPYing variable names or values from the NL description. The generation process follows a preorder traversal starting with the root node. Figure 1 shows an action tree for the example code: the nodes correspond to actions per time step in the construction of the AST. Interested readers can reference<cite> Yin and Neubig (2017)</cite> for more detail of the neural model, which consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder with action embeddings, context vectors, parent feeding, and a copy mechanism using pointer networks.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_5",
  "x": "Thus, in the example in Figure 1 , the GENTOKEN(LST) action in t 5 will not be executed. ---------------------------------- **RETRIEVAL-GUIDED CODE GENERATION** N -gram subtrees from all retrieved sentences are assigned a score, based on the best similarity score<cite> Yin and Neubig (2017)</cite> of all instances where they appeared. We normalize the scores for each input sentence by subtracting the average over the training dataset.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_6",
  "x": "We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by<cite> Yin and Neubig (2017)</cite> . HS consists of Python classes that implement Hearthstone card descriptions while Django contains pairs of Python source code and English pseudo-code from Django web framework. Table  1 summarizes dataset statistics. For evaluation metrics, we use accuracy of exact match and the BLEU score following<cite> Yin and Neubig (2017)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_7",
  "x": "For evaluation metrics, we use accuracy of exact match and the BLEU score following<cite> Yin and Neubig (2017)</cite> . ---------------------------------- **EXPERIMENTS** For the neural code generation model, we use the settings explained in<cite> Yin and Neubig (2017)</cite> . For the retrieval method, we tuned hyperparameters and achieved best result when we set n max = 4 and \u03bb = 3 for both datasets 3 .",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_8",
  "x": "**EXPERIMENTS** For the neural code generation model, we use the settings explained in<cite> Yin and Neubig (2017)</cite> . For the retrieval method, we tuned hyperparameters and achieved best result when we set n max = 4 and \u03bb = 3 for both datasets 3 . For HS, we set M = 3 and M = 10 for Django. We compare our model with<cite> Yin and Neubig (2017)</cite>'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_9",
  "x": "For the retrieval method, we tuned hyperparameters and achieved best result when we set n max = 4 and \u03bb = 3 for both datasets 3 . For HS, we set M = 3 and M = 10 for Django. We compare our model with<cite> Yin and Neubig (2017)</cite>'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented. SEQ2SEQ is an attentionenabled encoder-decoder model (Bahdanau et al., 2015) . The encoder is a bidirectional LSTM and the decoder is an LSTM.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_10",
  "x": "From our observation and as mentioned in Rabinovich et al. (2017) , HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values. However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails. This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_11",
  "x": "**DISCUSSION AND ANALYSIS** From our observation and as mentioned in Rabinovich et al. (2017) , HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values. However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails.",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_12",
  "x": "In the second example, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared Example 1 \"if offset is lesser than integer 0, sign is set to '-', otherwise sign is '+' \" Input sign = offset < 0 or '-' YN17 sign = '-' if offset < 0 else '+' RECODE sign = '-' if offset < 0 else '+' to the output of YN17 because our model can predict part of the code (timesince(d, now, reversed)) correctly. The third example shows where our method fails to apply the correct action as it cannot cast s to str type while YN17 can at least cast s into a type (bool). Another common type of error that we found RECODE's generated outputs is incorrect variable copying, similarly to what is discussed in<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) . Table 4 presents a result on the HS dataset 4 . We can see that our retrieval model can handle complex code more effectively.",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_13",
  "x": "Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017) . Ling et al. (2016) ; Jia and Liang (2016) ; Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) which represent code as an AST. Another close work is Dong and Lapata (2018) , which uses a two-staged structure-aware neural architecture. They initially generate a lowlevel sketch and then fill in the missing information using the NL and the sketch.",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_0",
  "x": "A variety of NLP tasks, such as question answering (Harabagiu et al., 2003) , document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) . These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; .",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_1",
  "x": "On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) . These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; . On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008) . The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies.",
  "y": "extends motivation"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_2",
  "x": "Fig 1 il lustrates this via an example. The parent category seafish and its two child categories shark and ray are closely related as: (1) there is a hypernym-hyponym (is-a) relation between the words \"seafish\" and \"shark\"/\"ray\" through text descriptions like \"...seafish, such as shark and ray...\", \"...shark and ray are a group of seafish...\"; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a subset of images of seafish. To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features<cite> Bansal et al., 2014)</cite> , we extract features by leveraging the distributed representations that embed images (Simonyan and Zisserman, 2014) and words as compact vectors, based on which the semantic closeness is directly measured in vector space. Further, we develop a probabilistic framework that integrates the rich multi-modal features to induce \"is-a\" relations between categories, encouraging local semantic consistency that each category should be visually and textually close to its parent and siblings. In summary, this paper has the following contributions: (1) We propose a novel probabilistic Bayesian model (Section 3) for taxonomy induction by jointly leveraging textual and visual data.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_3",
  "x": "Extensive comparisons demonstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3). ---------------------------------- **RELATED WORK** Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015) .",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_4",
  "x": "The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014) , syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained with multi-modal data. The works of Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> use similar language-based features as ours.",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_5",
  "x": "Our features are partially motivated by Fu et al. (2014) , but we jointly leverage both textual and visual information. In Kiela et al. (2015) , both textual and visual evidences are exploited to detect pairwise lexical entailments. Our work is significantly different as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features.",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_6",
  "x": "Our features are partially motivated by Fu et al. (2014) , but we jointly leverage both textual and visual information. In Kiela et al. (2015) , both textual and visual evidences are exploited to detect pairwise lexical entailments. Our work is significantly different as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_7",
  "x": "Output taxonomy selection. To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z * by finding the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;<cite> Bansal et al., 2014)</cite> . Training. We need to learn the model parameters w l of each layer l, which capture the relative importance of different features. The model is trained using the EM algorithm.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_8",
  "x": "In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009;<cite> Bansal et al., 2014)</cite> . ---------------------------------- **EXPERIMENTS** We first disclose our implementation details in section 5.1 and the supplementary material for better reproducibility.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_9",
  "x": "We then compare our model with previous state-of-the-art methods (Fu et al., 2014;<cite> Bansal et al., 2014)</cite> with two taxonomy induction tasks. Finally, we provide analysis on the weights and taxonomies induced. ---------------------------------- **IMPLEMENTATION DETAILS** Dataset.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_10",
  "x": "Specifically, we select two trees as the training set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree. We use Ancestor F 1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011;<cite> Bansal et al., 2014)</cite> . Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_11",
  "x": "Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks. The ancestor-F 1 scores are reported. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_12",
  "x": "Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks. The ancestor-F 1 scores are reported. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_13",
  "x": "We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by <cite>Bansal et al. (2014)</cite> retrained using our dataset; (5) Ours (LB): By excluding visual features, but including other language features from <cite>Bansal et al. (2014)</cite> ; (6) Ours (LVB): Our full model further enhanced with all semantic features from <cite>Bansal et al. (2014)</cite> ; (7) Ours (LVB -E): By excluding word embeddingbased language features from Ours (LVB). As shown, on the hierarchy construction task, our model with only language features still outperforms Fu2014 with a large gap (0.30 compared to 0.18 when h = 7), which uses similar embeddingbased features. The potential reasons are two-fold. First, we take into account not only parent-child relations but also siblings. Second, their method is designed to induce only pairwise relations.",
  "y": "differences uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_14",
  "x": "When comparing to Bansal2014, our model with only word embedding-based features underperforms theirs. However, when introducing visual features, our performance is comparable (pvalue = 0.058).Furthermore, if we discard visual features but add semantic features from <cite>Bansal et al. (2014)</cite> , we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity. Finally, if we enhance our full model with all semantic features from <cite>Bansal et al. (2014)</cite> , our model outperforms theirs by a gap of 0.04 (p-value < 0.01), which justifies our intuition that perceptual semantics underneath visual contents are quite helpful. ---------------------------------- **QUALITATIVE ANALYSIS**",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_15",
  "x": "Relevance of textual and visual features v.s. depth of tree. Compared to <cite>Bansal et al. (2014)</cite> , a major difference of our model is that different layers of the taxonomy correspond to different weights w l , while in<cite> (Bansal et al., 2014)</cite> all layers share the same weights. Intuitively, introducing layer-wise w not only extends the model capacity, but also differentiates the importance of each feature at different layers. For example, the images of two specific categories, such as shark and ray, are very likely to be visually similar. However, when the taxonomy goes from bottom to up (specific to general), the visual similarity is gradually undermined -images of fish and terrestrial animal are not necessarily similar any more.",
  "y": "differences"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_0",
  "x": "---------------------------------- **PREVIOUS RESEARCH** Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015</cite>) , humor recognition was modeled as a binary classification task. In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_1",
  "x": "In a recent work (<cite>Yang et al., 2015</cite>) , a new corpus was constructed from the Pun of the Day website. <cite>Yang et al. (2015)</cite> explained and computed stylistic features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building. Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b) . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous based on canned laughter.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_2",
  "x": "1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations. ---------------------------------- **TED TALK DATA** TED Talks 2 are recordings from TED conferences and other special TED programs.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_3",
  "x": "Beyond treating humor detection as a binary classification task, Bertero and Fung (2016a) formulated the recognition to be a sequential labeling task and utilized Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997) on top of CNN models (serving as feature extractors) to utilize context information among utterances. From the brief review, it is clear that there is a great need for an open corpus that can support investigating humor in presentations. 1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_4",
  "x": "We collected 1,192 TED Talk transcripts 3 . An example transcription is given in Figure 1 . The collected transcripts were split into sentences using the Stanford CoreNLP tool (Manning et al., 2014) . In this study, sentences containing or immediately followed by '(Laughter)' were used as 'Laughter' sentences, as shown in Figure 1 ; all other sentences were defined as 'No-Laughter' sentences. Following Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> , we selected the same numbers (n = 4726) of 'Laughter' and 'NoLaughter' sentences.",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_5",
  "x": "**CONVENTIONAL MODEL** Following <cite>Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to perform humor recognition by using the following two groups of features. The first group are humor-specific stylistic features covering the following 4 categories 4 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to the sentence being evaluated (found by using a k-Nearest Neighbors (kNN) ----------------------------------",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_6",
  "x": "The Pun data allows us to verify that our implementation of the conventional model is consistent with the work reported in <cite>Yang et al. (2015)</cite> . In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold crossvalidation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences.",
  "y": "similarities"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_7",
  "x": "When implementing CNN, Bergstra et al. (2013) . After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), n f is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (Kingma and Ba, 2014) . When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid over-fitting. On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in <cite>Yang et al. (2015)</cite> . In particular, precision has been greatly increased from 0.762 to 0.864.",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_1",
  "x": "The 12K rolesets in PB describe mostly single word predicates, to a great part leaving aside multiword expressions (MWEs). Complex predicates (CPs), 'predicates which are multi-headed: they are composed of more than one grammatical element' (Ramisch, 2012) , are most relevant in the context of SRL. Light verb constructions (LVCs), e.g. take care, and verb particle constructions (VPCs), e.g. watch out, are the most frequently occurring types of CPs. <cite>As Bonial et al. (2014)</cite> stated 'PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages'. For example the predicates in the CPs take a hard line, take time and many others are all annotated with a sense of take, meaning acquire, come to have, chose, bring with you from somewhere.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_2",
  "x": "This results in a loss of semantic information in the PB annotations. This is especially critical because CPs are a frequent phenomenon. The Wiki50 corpus (Vincze et al., 2011) , which provides a full coverage MWE annotation, counts 814 occurrences of LVCs and VPCs in 4350 sentences. This makes for one CP in every fifth sentence. Recently, <cite>Bonial et al. (2014)</cite> have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_4",
  "x": "This allows us to compute the most suitable roleset for a given CP fully automatically. Our contributions are as follows: To the best of our knowledge, this work is the first to address the handling of CPs for SRL in an automatic way. We are thus able to scale up previous work that relies on manual intervention. In addition, we set up an annotation effort to gather a frequency-balanced, data-driven evaluation set that is larger and more diverse than the annotated set provided by <cite>Bonial et al. (2014)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_5",
  "x": "Similarly to Hwang et al. (2010) they argue that CPs should be treated as single predicates, not only for LVCs but for all CPs. They automatically extract CP candidates from a corpus and represent, if possible, the meaning of the CPs with one or more single-verb paraphrases. Atkins et al. (2003) describe a way in which LVCs can be annotated in FrameNet (Baker et al., 1998) , a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_6",
  "x": "They automatically extract CP candidates from a corpus and represent, if possible, the meaning of the CPs with one or more single-verb paraphrases. Atkins et al. (2003) describe a way in which LVCs can be annotated in FrameNet (Baker et al., 1998) , a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum. <cite>Bonial et al. (2014)</cite> conducted a pilot study re-annotating 138 CPs involving the verb take.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_7",
  "x": "2 According to this process, take care could be aliased to the existing PB roleset care.01 whose entry is shown in Figure 3 . This alias replaces (take+care).01 shown in Figure 2 and thus avoids the creation of a new roleset. Roleset id: care.01, to be concerned Arg0: carer, agent Arg1: thing cared for/about Encouraged by the high proportion of CPs that could successfully be aliased in the pilot study by <cite>Bonial et al. (2014)</cite> , we created a method to automatically find aliases for CPs in order to decrease the amount of human intervention, thereby scaling up the coverage of CPs in PB. ---------------------------------- **METHOD**",
  "y": "motivation"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_8",
  "x": "**EVALUATION FRAMEWORK** Human Annotation. In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by <cite>Bonial et al. (2014)</cite> . We selected 50 LVCs and 50 VPCs from the Wiki50 corpus (Vincze et al., 2011) divided equally over two frequency groups: Half of the expressions occur only once in the Wiki50 corpus (low-frequency subgroup) and the other half occur at least twice (high-frequency subgroup). All occurrences of these 100 CP types in the corpus were selected to account for the polysemy of CPs.",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_9",
  "x": "First, they were asked to decide if there is already an appropriate PB roleset for the CP and then provide it. The annotators were requested to divide these cases into semantically compositional CPs (e.g. obtain permission with the roleset obtain.01) and uncompositional CPs for which PB already provides a multi-word predicate (e.g. open.03 for open up). For the remaining CPs, they were asked to suggest PB rolesets (aliases) that share the same semantics and argument structure as the CP. The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in <cite>Bonial et al. (2014)</cite> , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements. Annotator A discussed the annotations with the other annotators and they were able to reach a consensus that resulted in a final agreed-upon test set.",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_10",
  "x": "For the remaining CPs, they were asked to suggest PB rolesets (aliases) that share the same semantics and argument structure as the CP. The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in <cite>Bonial et al. (2014)</cite> , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements. Annotator A discussed the annotations with the other annotators and they were able to reach a consensus that resulted in a final agreed-upon test set. Table 1 shows the final decisions with respect to the complete set of 197 expressions. In line with the results from <cite>Bonial et al. (2014)</cite> who aliased 100 out of 138 uncompositional take MWEs, we were also able to alias most of the CPs in our annotation set.",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_11",
  "x": "All CPs were labeled with one to four appropriate PB alias rolesets. In addition, we evaluated our system on the dataset from <cite>Bonial et al. (2014)</cite> , restricted to the type of CP our system handles (LVCs and VPCs) and verb aliases (as opposed to aliases being a noun or adjective roleset). We used 70 of the 100 MWEs from their annotations. Evaluation Measures and Baseline. We report the accuracy of our system's predictions as compared to the gold standard.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_12",
  "x": "We evaluated our approach on the 160 CPs annotated in the course of this work (Wiki50 set), as well as on the 70 take CPs from <cite>Bonial et al. (2014)</cite> (take set) and compare our results to the baseline. Table 2 shows percentage coverage, accuracy and the harmonic mean of coverage and accuracy for our system and the baseline. We report results on the two evaluation sets in the strict and lenient evaluation. The first five rows of Table 2 show the results for the Wiki50 set and its subsets. We see that our system scores 44.1 accuracy on the whole test set in the strict evaluation and 69.0 in the lenient evaluation.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_13",
  "x": "Lastly, our system adheres to the most frequent sense baseline due to lack of word sense disambiguation of the CPs and assigns the alias that fits the most dominant sense of the CP in the corpus. ---------------------------------- **CONCLUSIONS** We have presented an approach to handle CPs in SRL that extends on work from <cite>Bonial et al. (2014)</cite> . We automatically link VPCs and LVCs to the PB roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods.",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_14",
  "x": "We have presented an approach to handle CPs in SRL that extends on work from <cite>Bonial et al. (2014)</cite> . We automatically link VPCs and LVCs to the PB roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods. We set up an annotation effort to gather a frequency-balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by <cite>Bonial et al. (2014)</cite> . Our method can be used to alleviate the manual annotation effort by providing a correct alias in 44% of the cases (up to 72% for the more frequent test items when taking synonymous rolesets into account). These results are not too far from the upper bounds we calculate from human annotations.",
  "y": "extends"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_0",
  "x": "Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016) , speech recognition (Graves et al., 2013) , and various natural language processing tasks Kim, 2014; Xiong et al., 2016) . Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer. An example task is given in Figure 1 .",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_1",
  "x": "---------------------------------- **INTRODUCTION** Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016) , speech recognition (Graves et al., 2013) , and various natural language processing tasks Kim, 2014; Xiong et al., 2016) . Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks .",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_2",
  "x": "While N2Ns generally work well with either weight tying approach, as reported in<cite> Sukhbaatar et al. (2015)</cite> , the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed. In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task. This is realised through the use of a gating vector, inspired by Liu and Perez (2017) . Our method achieves the best performance for a memory network-based model on the bAbI dataset, superior to both adjacent and layer-wise weight tying, and competitive results on Dialog bAbI.",
  "y": "motivation"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_3",
  "x": "Section 4 and 5 present the experimental results on the bAbI and Dialog bAbI datasets with analyses in Section 6. Lastly, Section 7 concludes the paper. ---------------------------------- **RELATED WORK** End-to-End Memory Networks: Building on top of memory networks ,<cite> Sukhbaatar et al. (2015)</cite> ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_4",
  "x": "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights: where softmax(a i ) = e a i P j e a j . Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory,<cite> Sukhbaatar et al. (2015)</cite> further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop: Lastly, N2N predicts the answer to question q using a softmax function: where\u0177 is the predicted answer distribution, W 2 R |V |\u21e5d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_5",
  "x": "**CURRENT ISSUES AND MOTIVATION:** In<cite> Sukhbaatar et al. (2015)</cite> , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\"). With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs. With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 . While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_6",
  "x": "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task. Table 1 : Accuracy (%) reported in<cite> (Sukhbaatar et al., 2015)</cite> on a selected subset of the 20 bAbI 10k tasks. Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop. Related reasoning models: Gated End-to-End Memory Networks (GN2Ns) (Liu and Perez, 2017) are a variant of N2N with a simple yet effective gating mechanism on the connections between hops, allowing the model to dynamically regulate the information flow between the controller and the memory. Dynamic Memory Networks (DMNs) and its improved version (DMN+) employ RNNs to sequentially process contextual information stored in the memory.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_7",
  "x": "Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs . To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K . Moreover, following<cite> Sukhbaatar et al. (2015)</cite> , we add a linear mapping H 2 R d\u21e5d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in: Regularisation: In order to prevent the input and output embedding matrices A k and C k from being dominated by the unconstrained embedding matrices, it is necessary to restrain the magnitude of the values in\u00c3 1 andC k . Therefore, in addition to the cross entropy loss over N training instances:",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_8",
  "x": "either 1k or 10k training instances per task. In this work, we focus exclusively on the 10k version. 3 Training Details: Following<cite> Sukhbaatar et al. (2015)</cite>, we hold out 10% of the bAbI training set to form a development set. Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_9",
  "x": "Following<cite> Sukhbaatar et al. (2015)</cite> , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_10",
  "x": "Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to ---------------------------------- **0.001.** Consistent with other published results over bAbI<cite> (Sukhbaatar et al., 2015</cite>; Seo et al., 2017) , we repeat training 30 times for each task, and select the model which performs best on the development set. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_11",
  "x": "**0.001.** Consistent with other published results over bAbI<cite> (Sukhbaatar et al., 2015</cite>; Seo et al., 2017) , we repeat training 30 times for each task, and select the model which performs best on the development set. ---------------------------------- **RESULTS** The results on the 20 bAbI QA tasks are presented in Table 3 .",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_12",
  "x": "---------------------------------- **RESULTS** The results on the 20 bAbI QA tasks are presented in Table 3 . We benchmark against other memory network based models: (1) N2N with ADJ and LW<cite> (Sukhbaatar et al., 2015)</cite> ; (2) DMN (Kumar et al., 2016) and its improved version DMN+ (Xiong et al., 2016) ; and (3) GN2N (Liu and Perez, 2017) . Major improvements on the difficult tasks.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_13",
  "x": "Each of these 7 features indicates whether there are any exact matches between words in the candidate and those in the question or memory. We refer to these 7 features as the match features. In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1. As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following<cite> (Sukhbaatar et al., 2015)</cite> and (Liu and Perez, 2017) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks. N2N: .",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_14",
  "x": "The results on the Dialog bAbI tasks are shown in Table 4 . In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N<cite> (Sukhbaatar et al., 2015)</cite> ; and (2) GN2N<cite> (Sukhbaatar et al., 2015)</cite> . While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in . Improvements on task 5. It can be observed that UN2N offers consistent performance boost on task 5 across all experiments settings, especially in the non-OOV group.",
  "y": "uses"
 },
 {
  "id": "7b4a976ba6a43b5ba42cc350b4d132_0",
  "x": "In LRP, the classification decision of a deep neural network is decomposed backward across the network layers and evidence about the contribution to the final decision brought by individual input fragments (i.e., pixels of the input image) is gathered. We propose here to extend the LRP application to a linguistically motivated network architecture, known as <cite>Kernel-Based Deep Architecture</cite> (<cite>KDA</cite>) <cite>[5]</cite> , which frames semantic information captured by linguistic Tree Kernel [2] methods within the neural-based learning paradigm. The result is a mechanism that, for each system's prediction such as in question classification, generates an argument-by-analogy explanation based on real training examples, not necessarily similar to the input. We also propose here a novel approach to evaluate numerically the interpretability of any explanation-enriched model applied in semantic inference tasks. By defining a specific audit process, we derive a synthetic metric, i.e. Auditing Accuracy, that takes into account the properties of transparency, informativeness and effectiveness.",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_0",
  "x": "---------------------------------- **INTRODUCTION** Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008) . Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996) . We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006;<cite> Galley et al., 2006</cite>; May and Knight, 2007) , as opposed to formally syntactic systems such as Hiero (Chiang, 2007) .",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_1",
  "x": "The variables in a TTS template are further transformed using other TTS templates, and the recursive process continues until there are no variables left. There are two ways that TTS templates are commonly used in machine translation. The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_2",
  "x": "The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. ----------------------------------",
  "y": "background similarities"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_3",
  "x": "This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems <cite>(Galley et al., 2006</cite>; Liu et al., 2006; Huang et al., 2006) . The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems. In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents. If some noisy alignments happen to cross over the boundaries of two constituents, as shown in Figure 2 , a much bigger tree fragment will be extracted as a TTS template.",
  "y": "similarities"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_4",
  "x": "There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN)<cite> (Galley et al., 2006)</cite> , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_5",
  "x": "The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007) . LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences. But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string). In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_6",
  "x": "LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. 4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (Rose et al., 1992 ) is is used in our system to speed up the training process, similar to Goldwater et al. (2006) .",
  "y": "uses"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_0",
  "x": "This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation <cite>(Zhang and Clark, 2007)</cite> , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010) , joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013) , and joint segmentation, POS-tagging and parsing . In addition to the aforementioned tasks, the framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process.",
  "y": "background"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_1",
  "x": "---------------------------------- **TUTORIAL OVERVIEW** In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues <cite>(Zhang and Clark, 2007)</cite> , as well as beam-search and the early-update strategy (Collins and Roark, 2004) . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing.",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_0",
  "x": "Perhaps the most widely used representations in natural language processing are word embeddings (e.g. Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014) . Recently there has been a growing interest in models for sentence-level representations using neural networks. Sentence embeddings are distributed representations of natural language sentences with the intention to encode the meaning of the sentences in a neural network representation. Sentence embeddings have been generated using unsupervised learning approaches (e.g. Hill et al., 2016) , and supervised learning (e.g. Bowman et al., 2016;<cite> Conneau et al., 2017)</cite> . Sentence-level representations have shown promise in multiple different NLP tasks.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_1",
  "x": "There are two main approaches to NLI utilizing neural networks. Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. Bowman et al., 2015 Bowman et al., , 2016<cite> Conneau et al., 2017)</cite> . Other approaches do not treat the two sentences separately but utilize e.g. crosssentence attention (Tay et al., 2018; Chen et al., 2017a) . In this paper we focus on the sentence embedding approach. Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_2",
  "x": "Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) . We also test our model on a number of transfer learning tasks using the SentEval testing library<cite> (Conneau et al., 2017)</cite> , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <cite>Conneau et al. (2017)</cite> . Moreover, our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences (Conneau et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_3",
  "x": "We also test our model on a number of transfer learning tasks using the SentEval testing library<cite> (Conneau et al., 2017)</cite> , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <cite>Conneau et al. (2017)</cite> . Moreover, our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences (Conneau et al., 2018) . This highlights the generalization capability of our proposed model, confirming that the proposed architecture is able to produce sentence embeddings with strong performance across a wide variety of different NLP tasks. ---------------------------------- **RELATED WORK**",
  "y": "differences uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_4",
  "x": "**RELATED WORK** Sentence embeddings have been utilized in a wide variety of approaches to natural language inference. Bowman et al. (2015 Bowman et al. ( , 2016 explore RNN and LSTM architectures, Mou et al. (2016) convolutional neural networks and Vendrov et al. (2015) GRUs, to name a few. The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier. <cite>Conneau et al. (2017)</cite> explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_5",
  "x": "Given a sequence T of words (w 1 . . . , w T ), the output of the bi-directional LSTM is a set of vec- of a forward and backward LSTMs The max pooling layer produces a vector of the same dimensionality as h t , returning, for each dimension, its maximum value over the hidden units (h 1 , . . . , h T ). Motivated by the strong results of the BiLSTM max pooling network by <cite>Conneau et al. (2017)</cite> , we experimented with combining BiLSTM max pooling networks as a hierarchical structure. 1 To improve the BiLSTM layers' ability to remember the input words, we let each layer of the stack re-read the input sentence.",
  "y": "extends motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_6",
  "x": "**ERROR ANALYSIS** To understand better what kind of inferential relationships our model is able to identify, we conducted an error analysis for the three datasets. We report the results below. 3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <cite>Conneau et al. (2017)</cite> (our implementation). 4 3 For more detailed error statistics, see the appendix.",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_7",
  "x": "To understand better what kind of inferential relationships our model is able to identify, we conducted an error analysis for the three datasets. We report the results below. 3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <cite>Conneau et al. (2017)</cite> (our implementation). 4 3 For more detailed error statistics, see the appendix. 4 The scores for our implementation of InferSent are on par or slightly higher than the scores reported by <cite>Conneau et al. (2017)</cite> using their training setup.",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_8",
  "x": "We also conducted additional linguistic error analysis using the annotation test set provided for MultiNLI. The results are reported in the appendix and they are mostly inconclusive. ---------------------------------- **TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought .",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_9",
  "x": "---------------------------------- **TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought . For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total. This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data<cite> (Conneau et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_10",
  "x": "For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total. This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data<cite> (Conneau et al., 2017)</cite> . <cite>Conneau et al. (2017)</cite> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4. We used the SentEval sentence embedding evaluation library using the default settings 6 recommended on the SentEval website, with a logistic regression classifier, Adam optimizer with learning rate of 0.001, batch size of 64 and epoch size of 4.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_13",
  "x": "Our model also compares well against the other models, outperforming Decomposable Attention model (51.90%) (Parikh et al., 2016) and Residual Encoders (62.20%) (Nie and Bansal, 2017b) in the overall score. As these models are not based purely on sentence embeddings, the obtained result highlights that sentence embedding approaches can be competitive when handling inferences requiring lexical information. Our model is still outperformed by and ESIM (Chen et al., 2017a) and KIM, an ESIM model incorporating external knowledge (Chen et al., 2018) . The results of the comparison are summarized in Glockner et al. (2018) . InferSent results obtained with our implementation using the architecture and training set-up described in<cite> (Conneau et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_14",
  "x": "We evaluated the sentence embedding architecture with three different natural language inference datasets, including the Stanford Natural Language Inference (SNLI) corpus, the Multi-Genre Natural Language Inference (MultiNLI) corpus and the SciTail dataset. In all our experiments with the three datasets we used only the training data provided in the respective corpus. For the transfer learning tasks, described in Section 7, we used training data from both the SNLI and the MultiNLI datasets in order to compare to the results by <cite>Conneau et al. (2017)</cite> . ---------------------------------- **SNLI**",
  "y": "motivation"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_0",
  "x": "Finally, Cosine is affected by the hubness effect Schn-abel et al., 2015) , i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009 ), Vector Cosine is still by far the most popular one (Turney and Pantel, 2010) . However, in a recent paper of Santus et al. (2016b) , the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) and <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_1",
  "x": "The results are also discussed in relation to the state-of-the-art DSMs, as reported in <cite>Hill et al. (2015)</cite> . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_2",
  "x": "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) , we test the ability of the models to quantify genuine semantic similarity. ---------------------------------- **BACKGROUND** ---------------------------------- **DSMS, MEASURES OF ASSOCIATION AND DIMENSIONALITY REDUCTION**",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_3",
  "x": "**DATASETS** For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (Finkelstein et al., 2001 ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, <cite>Hill et al. (2015)</cite> claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_4",
  "x": "**DATASETS** For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (Finkelstein et al., 2001 ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, <cite>Hill et al. (2015)</cite> claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_5",
  "x": "Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above). The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5). Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs. Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), <cite>Hill et al. (2015)</cite> argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_6",
  "x": "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_7",
  "x": "According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. <cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_8",
  "x": "According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. <cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_9",
  "x": "**STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by <cite>Hill and colleagues</cite>, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008) , which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012) , which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al. (2013) , which was trained on 1000 million words of Wikipedia and on the RCV Vol. 1 Corpus (Lewis et al., 2004 Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_10",
  "x": "<cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach. ---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by <cite>Hill and colleagues</cite>, the results are perfectly comparable.",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_12",
  "x": "**DSMS** For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables. All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and <cite>SimLex-999</cite>) and the pos-tagged contexts having frequency above 100 in the two corpora. We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances. As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_13",
  "x": "For each corpus, the models differed according to the window size (i.e. 2 and 3), to the statistical association measure used as a weighting scheme (i.e. none, PPMI and LMI) and to the application of SVD to the previous combinations. ---------------------------------- **MEASURING WORD SIMILARITY AND RELATEDNESS** Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs. Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_14",
  "x": "---------------------------------- **MEASURING WORD SIMILARITY AND RELATEDNESS** Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs. Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_15",
  "x": "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on <cite>SimLex-999</cite>, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_16",
  "x": "For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in <cite>Hill et al. (2015)</cite> (see Section 2.5). ---------------------------------- **PERFORMANCE ANALYSIS** With a glance at the tables, it can be easily noticed that the measures perform particularly well in two models: i) APSyn, when applied on the PPMI-weighted DSM (henceforth, APSynPPMI); ii) Vector Cosine, when applied on the SVD-reduced PPMI-weighted matrix (henceforth, CosSVDPPMI). These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_17",
  "x": "These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353. Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_18",
  "x": "Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity).",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_19",
  "x": "The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_20",
  "x": "The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_21",
  "x": "It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors. APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (Radovanovic et al., 2010) . Further investigation is needed to see whether variations of APSyn can tackle this problem. Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) . Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009) .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_22",
  "x": "Interestingly, our best models achieve results that are comparable to -or even better than -those reported by <cite>Hill et al. (2015)</cite> for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in . On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features. It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on <cite>SimLex-999</cite> (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_23",
  "x": "In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in . On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features. It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on <cite>SimLex-999</cite> (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity. To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_0",
  "x": "Natural Language Generation systems require content planning and format for the selected subject domain as input and specifics about the natural language in order to generate text (Staykova, 2014) , of which the latter tend to be bootstrappable for related languages (de Oliveira and Sripada, 2014) . Our NLG system uses ontologies to represent domain knowledge. As for language, we are interested in Runyankore, a Bantu language indigenous to south western Uganda. The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore.",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_1",
  "x": "The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore. We present our implementation of these algorithms and required linguistic annotations as a Prot\u00e9g\u00e9 5.x plugin. also ensures no typographical errors are made in the XML file. These annotation fields are mandatory, and we allowed for the use of 0 as the NC for the POS which is not a noun.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_2",
  "x": "The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) . We used this tool for three main reasons: our grammar engine implementation was done in Java, so we wanted a Java tool as well; we wanted a small CFG implementation for reasonable performance; and their tool extended Purdom's algorithm to fulfill Context-Dependent Rule Coverage (CDRC), which generates more and simpler sentences. A sample of the generated text is presented below: \u2022 Buri rupapura rwamakuru n'ekihandiiko ekishohoziibwe, (generated from: Newspaper Publication) \u2022 Buri ntaama nerya ebinyaansi byoona, (gener-",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_3",
  "x": "We implemented the algorithms for verbalization and pluralization presented in (Byamugisha et al., 2016a; Byamugisha et al., 2016c ) as a Java application. The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) . We used this tool for three main reasons: our grammar engine implementation was done in Java, so we wanted a Java tool as well; we wanted a small CFG implementation for reasonable performance; and their tool extended Purdom's algorithm to fulfill Context-Dependent Rule Coverage (CDRC), which generates more and simpler sentences. A sample of the generated text is presented below: \u2022 Buri rupapura rwamakuru n'ekihandiiko ekishohoziibwe, (generated from: Newspaper Publication)",
  "y": "uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_0",
  "x": "Work in factoid question-answering (Q/A) has long leveraged answer type detection (ATD) systems in order to identify the semantic class of the entities, words, or phrases which are most likely to correspond to the exact answer to a natural language question. For example, given a question like Who was responsible for coordinating disaster relief for victims of Hurricane Katrina?, ATD components enable Q/A systems to retrieve the sets of candidate answers which include the INDIVIDUALs and/or ORGANIZATIONs who provided aid to the victims of the hurricane. Early work in ATD (Harabagiu et al., 2000; Harabagiu et al., 2001 ) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done by<cite> (Li and Roth, 2002)</cite> in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question. While learning-based approaches have dramatically increased the precision of open-domain ATD systems, most current ATD components have only been tasked with distinguishing amongst a limited set of EATs.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_1",
  "x": "Most modern Q/A systems, however, follow work done by<cite> (Li and Roth, 2002)</cite> in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question. While learning-based approaches have dramatically increased the precision of open-domain ATD systems, most current ATD components have only been tasked with distinguishing amongst a limited set of EATs. For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by<cite> (Li and Roth, 2002)</cite> , includes only a total of 50 unique expected answer types (generally referred to as \"fine\" answer types), organized into 6 different categories (referred to as \"coarse\" answer types). (The entire UIUC ATH is presented in Table 1 .) While ATD systems based on the the UIUC hierarchy has been employed by a number of participants in recent TREC 1 Question-Answering evaluations, we believe that the size (and coverage) of current answer type hierarchies represents a factor which significantly limits both the performance of open-domain factoid question-answering systems and the number of questions that a Q/A system can be used to answer. Without coverage for a specific answer type within its ATH, a Q/A system must retrieve - and extract -answers using other answer types from its ATH which are conceptually \"nearest\" to the expected answer type of the question.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_2",
  "x": "While the ATH in (Harabagiu et al., 2000) could easily be scaled to include a potentially very large number of types (e.g. see (Harabagiu et al., 2005) for an example of how this could be accomplished for a top-performing TREC Q/A system), it is constrained in its aapproach to WORDNET's hand-built hypernym relations, which does not coincide with common answer types in questions. in contrast, the<cite> (Li and Roth, 2002)</cite> UIUC ATH is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when Q/A systems are capable of handling more detailed answer types. (See Section 3 for a more detailed discussion of the UIUC hierarchy.) In order to create a new ATH capable of addressing the needs of today's open-domain factoid Q/A systems, we wanted to develop an ATH which was capable of scaling to any given set of named entity types. Our work in this paper sought to address the following requirements: Requirement 1. The ability to add \"abstract\" answer types that represent some subset of other answer types.",
  "y": "differences"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_3",
  "x": "We assume that the taxonomy of answer types included in an answer type hierarchy (ATH) can be represented as a tree structure of varying depth where branchings mark decision points between different semantic classes that all share a hypernym-like relationship with their parent. (A graphical representation of a portion of the LCC ATH is presented in Figure 1 .) The UIUC answer type hierarchy<cite> (Li and Roth, 2002)</cite> We feel that the time is right for work in ATD to move beyond the UIUC ATH and to begin to tackle problems of organizing and learning answer type hierarchies that encompass several hundreds of diverse expected answer types. We believe that this effort would be in line with recent work looking at a similar type of semantic categorization problem -named entity recognition -in which researchers have moved from using simple heuristics and classifiers to unsupervised or semi-supervised methods capable of inducing hundreds (if not thousands) of entity types from large collections of texts. In our work, we used output from LCC's own, widecoverage named entity recognition system, CICEROLITE in order to construct a novel ATH which included more than 200 different EATs. (A table listing Furthermore, answers need not necessarily be drawn from entities.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_4",
  "x": "---------------------------------- **ANNOTATING THE QUESTION CORPUS** In this section, we describe how we used the large ATH introduced in Section 3 in order to annotate a corpus drawn from more than 10,000 questions compiled from (1) existing annotated question corpora<cite> (Li and Roth, 2002)</cite> , (2) collections of questions mined from the web, and (3) questions submitted to LCC's FERRET question-answering system (Hickl et al., 2006a) . (A breakdown of the number of questions obtained from each of these three strategies is provided in Table 3 Table 3 : Distribution of 10,000 annotated questions by originating data set. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_5",
  "x": "A hierarchical structure (i.e., only the children of the current type are considered as outcomes) to make a classification at every branching point in the hierarchy. 3 classifiers are machine-learning based while the remainder are heuristic classifiers. In a departure from previous machine-learning based approaches<cite> (Li and Roth, 2002</cite>; Krishnan et al., 2005) , we used a maximum entropy classifier to learn our ATH. Our classification process currently uses three machinelearned classifiers. The first resolves all questions into one of 11 \"coarse\" types that are similar to the UIUC coarse types in Table 1 . If the first classifier's outcome is HU-MAN, then a machine classifier resolves between INDIVID-UAL, HUMAN-GROUP, ORGANIZATION, and HUMAN (not enough information).",
  "y": "similarities uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_6",
  "x": "Indeed, the ability to adapt the hierarchy described in Section 3 to include alternate domains such as biological and aeronautical questions means there is no functional limit on the number of answer types. ---------------------------------- **CONCLUSIONS** This paper described the creation of a new answer type detection system capable of recognizing more than 200 different expected answer types with greater 85% precision. In a departure from previous work in answer type detection (Krishnan et al., 2005;<cite> Li and Roth, 2002)</cite> , we have demonstrated how a large, multi-tiered answer type hierarchy can be created which incorporates many of the entity types included in LCC's wide coverage named entity recognition system, CICEROLITE; this hierarchy was then used in order to create a new corpus of more than 10,000 questions which could be used to train an ATD system.",
  "y": "similarities"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_0",
  "x": "**RELATED WORK** Unsupervised speech representation learning [2, 3, 4, 5, 6,<cite> 7,</cite> 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_1",
  "x": "SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_2",
  "x": "Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_3",
  "x": "Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
  "y": "background motivation"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_4",
  "x": "Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6,<cite> 7,</cite> 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks. We show that finetuning for 2 epochs easily acquires significant improvement. The proposed approach outperforms other representations and features. When compared to the commonly used log Mel-features, we outperformed it by 35.2% (absolute improvement) for phoneme classification accuracy, 28.0% (absolute improvement) for speaker recognition accuracy, and 6.4% (absolute improvement) for sentiment discrimination accuracy on a spoken content dataset unseen during pre-train. We also experiment in low resource settings to show that Mockingjay is capable of improving supervised training in real-life low-resource scenarios.",
  "y": "differences"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_5",
  "x": "We train with a batch size of 6 using a single 1080Ti GPU. We provide pre-trained models with our implementation, they are publicly available for reproducibility 2 . ---------------------------------- **EXPERIMENT** Following previous works [2, 3, 4, 5, 6,<cite> 7,</cite> 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content.",
  "y": "uses"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_6",
  "x": "---------------------------------- **COMPARING WITH OTHER REPRESENTATIONS** The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification. As reported in [6] , the APC approach outperformed CPC representations [5,<cite> 7,</cite> 9] in both two tasks, which makes APC suitable as a strong baseline. APC uses an unidirectional autoregressive model.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_0",
  "x": "The CHiME-5 data is heavily degraded by reverberation and overlapped speech. As much as 23 % of the time more than one speaker is active at the same time [12] . The challenge's baseline system poor performance (about 80 % WER) is an indication that ASR training did not work well. Recently, Guided Source Separation (GSS) enhancement on the test data was shown to significantly improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data <cite>[13]</cite> . GSS is a spatial mixture model based blind source separation approach which exploits the annotation given in the CHiME-5 database for initialization and, in this way, avoids the frequency permutation problem [14] .",
  "y": "background"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_1",
  "x": "Using a single acoustic model trained with 308 hrs of training data, which resulted after applying multi-array GSS data cleaning and a three-fold speed perturbation, we achieved a WER of 41.6 % on the development (DEV) and 43.2 % on the evaluation (EVAL) test set of CHiME-5, if the test data is also enhanced with multi-array GSS. This compares very favorably with the recently published topline in <cite>[13]</cite> , where the single-system best result, i.e., the WER without system combination, was 45.1 % and 47.3 % on DEV and EVAL, respectively, using an augmented training data set of 4500 hrs total. The rest of this paper is structured as follows. Section 2 describes the CHiME-5 corpus, Section 3 briefly presents the guided source separation enhancement method, Section 4 shows the ASR experiments and the results, followed by a discussion in Section 5. Finally, the paper is concluded in Section 6.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_2",
  "x": "It follows the approach presented in <cite>[13]</cite> , which was shown to outperform the baseline version. The system operates in the STFT domain and consists of two stages: (1) a dereverberation stage, and (2) a guided source separation stage. For the sake of simplicity, the overall system is referred to as GSS for the rest of the paper. Regarding the first stage, the multiple input multiple output version of the Weighted Prediction Error (WPE) method was used for dereverberation (M inputs and M outputs) [15, 16] five mixture components, one representing each speaker, and an additional component representing the noise class. The role of the MM is to support the source extraction component for estimating the target speech.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_3",
  "x": "Temporal context also plays an important role in the EM initialization. Simulations have shown that a large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for CHiME-5 [14] . However, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement <cite>[13]</cite> . Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance.",
  "y": "motivation"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_4",
  "x": "Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance. Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered <cite>[13]</cite> . Another avenue we have explored for further source separation improvement was to refine the baseline CHiME-5 annotations using ASR output (see Fig. 1 ). A first-pass decoding using an ASR system is used to predict silence intervals.",
  "y": "uses extends"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_5",
  "x": "Comparing the amount of training data used to train the acoustic models, we observe that it decreases drastically from no enhancement to the GSS6 enhancement. ---------------------------------- **STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 . As explained in Section 5.1, we opted for <cite>[13]</cite> instead of [14] as baseline because the former system is stronger.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_6",
  "x": "---------------------------------- **STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 . As explained in Section 5.1, we opted for <cite>[13]</cite> instead of [14] as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_7",
  "x": "To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ). We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table 4 ).",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_8",
  "x": "To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ). We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table 4 ).",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_9",
  "x": "We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ).",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_10",
  "x": "Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in <cite>[13]</cite> , as shown in Table 5 . This translates to significantly faster and cheaper training of acoustic models, which is a major advantage in practice. ---------------------------------- **DISCUSSION** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_11",
  "x": "Our experiments have shown that the temporal context of some GSS components has a significant effect on the WER. Two cases are investigated: (i) partially dropping the temporal context for the EM stage, and (ii) dropping the temporal context for beamforming. The evaluation was conducted with an acoustic model trained on unprocessed speech and the enhancement was applied during test only. Results are depicted in Table 6 . The first row corresponds to the GSS configuration in [14] while the second one corresponds to the GSS configuration in <cite>[13]</cite> .",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_13",
  "x": "Consequently, we have chosen system <cite>[13]</cite> as baseline in this study since is using the stronger GSS configuration. ---------------------------------- **ANALYSIS OF SPEAKER OVERLAP EFFECT ON WER ACCURACY** The results presented so far were overall accuracies on the test set of CHiME-5. However, since speaker overlap is a major issue for these data, it is of interest to investigate the methods' performance as a function of the amount of overlapped speech.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_0",
  "x": "1 1. Hereford United were seeing stars at Gillingham after letting in 2 early goals 2. Look into the night sky to see the stars MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010) , and must be able to make such distinctions. This is particularly important for applications such as machine translation (Sag et al., 2002) , where the appropriate meaning of word combinations in context must be preserved for accurate translation. In this paper, following prior work (e.g., <cite>Salton et al., 2016</cite> ), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal. We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013) , word embeddings tailored to representing sentences (Kenter et al., 2016) , and skip-thoughts sentence embeddings (Kiros et al., 2015) . We then train a support vector machine (SVM) on these representations to classify unseen VNC instances.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_1",
  "x": "This is particularly important for applications such as machine translation (Sag et al., 2002) , where the appropriate meaning of word combinations in context must be preserved for accurate translation. In this paper, following prior work (e.g., <cite>Salton et al., 2016</cite> ), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal. We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013) , word embeddings tailored to representing sentences (Kenter et al., 2016) , and skip-thoughts sentence embeddings (Kiros et al., 2015) . We then train a support vector machine (SVM) on these representations to classify unseen VNC instances. Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by <cite>Salton et al. (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_2",
  "x": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. <cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_3",
  "x": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. <cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_4",
  "x": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. <cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_5",
  "x": "During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. <cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against. Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural). They then determine the canonical form, C(v, n), for a given VNC as follows: 2",
  "y": "motivation"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_6",
  "x": "We use a publicly-available skip-thoughts model, that was pre-trained on a corpus of books. 6 We represent a given sentence containing a VNC instance using the skip-thoughts encoder. Note that this approach is our re-implementation of the skipthoughts based method of <cite>Salton et al. (2016)</cite> , and we use it as a strong baseline for comparison. ---------------------------------- **DATA AND EVALUATION**",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_7",
  "x": "**DATA AND EVALUATION** In this section, we discuss the dataset used in our experiments, and the evaluation of our models. ---------------------------------- **DATASET** We use the VNC-Tokens dataset (Cook et al., 2008) -the same dataset used by Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> -to train and evaluate our models.",
  "y": "similarities uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_8",
  "x": "VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic. Following <cite>Salton et al. (2016)</cite> , we use DEV and TEST, and ignore all token instances annotated as \"unknown\". Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV).",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_9",
  "x": "Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV). <cite>Salton et al.</cite>, on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data. We borrowed ideas from both of these approaches in structuring our experiments.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_10",
  "x": "We borrowed ideas from both of these approaches in structuring our experiments. We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as <cite>Salton et al. (2016)</cite> . This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST. ---------------------------------- **EVALUATION**",
  "y": "similarities uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_11",
  "x": "We randomly divide both DEV and TEST into training and testing portions ten times, following <cite>Salton et al. (2016)</cite> . For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions. We then report the average accuracy over the ten runs. ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_12",
  "x": "On both DEV and TEST, the accuracy achieved by each supervised model is higher than that of the unsupervised CForm approach, except for Siamese CBOW on TEST. The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively. The difference between the word2vec model and the next-best model, skip-thoughts, is significant using a bootstrap test (Berg-Kirkpatrick et al., 2012) with 10k repetitions for DEV (p = 0.006), but not for TEST (p = 0.051). Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by <cite>Salton et al. (2016)</cite> . 8 8 The word2vec and skip-thoughts models were trained on different corpora, which could contribute to the differences in results for these models.",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_13",
  "x": "**CONCLUSIONS** Determining whether a usage of a VNC is idiomatic or literal is important for applications such as machine translation, where it is vital to preserve the meanings of word combinations. In this paper we proposed two approaches to the task of classifying VNC token instances as idiomatic or literal based on word2vec embeddings and Siamese CBOW. We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (<cite>Salton et al., 2016</cite>) . Our experimental results show that a comparatively simple approach based on averaging word embeddings performs at least as well as, or better than, the approach based on skip-thoughts.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_0",
  "x": "Recently, however, <cite>Hirschberg and Litman (1993)</cite> have presented rules for classifying cue phrases in both text and speech. <cite>Hirschberg and Litman</cite> pre-classi ed a set of naturally occurring cue phrases, described each cue phrase in terms of prosodic and textual features, then manually examined the data to construct rules that best predicted the classi cations from the features. This paper examines the utility of machine learning for automating the construction of rules for classifying cue phrases. A set of experiments are conducted that use two machine learning programs, cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; , to induce classi cation rules from sets of preclassi ed cue phrases and their features. To support a quantitative and comparative evaluation of the automated and manual approaches, both the error rates and the content of the manually derived and learned rulesets are compared.",
  "y": "background motivation"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_1",
  "x": "A set of experiments are conducted that use two machine learning programs, cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; , to induce classi cation rules from sets of preclassi ed cue phrases and their features. To support a quantitative and comparative evaluation of the automated and manual approaches, both the error rates and the content of the manually derived and learned rulesets are compared. The experimental results show that machine learning is indeed an e ective technique for automating the generation of classi cation rules. The accuracy of the learned rulesets is often higher than the accuracy of the rules in <cite>(Hirschberg & Litman 1993)</cite> , while the linguistic implications are more precise. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_3",
  "x": "The accuracy of the learned rulesets is often higher than the accuracy of the rules in <cite>(Hirschberg & Litman 1993)</cite> , while the linguistic implications are more precise. ---------------------------------- **CUE PHRASE CLASSI CATION** This section summarizes <cite>Hirschberg and Litman's study</cite> of the classi cation of multiple cue phrases in text and speech <cite>(Hirschberg & Litman 1993)</cite> . The data from <cite>this study</cite> is used to create the input for the machine learning experiments, while the results are used as a benchmark for evaluating performance.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_4",
  "x": "<cite>Hirschberg and Litman</cite> each classi ed the 953 tokens (as discourse, sentential or ambiguous) while listening to a recording and reading a transcription. Each token was also described as a set of prosodic and textual features. Previous observations in the literature correlating discourse structure with prosodic information, and discourse usages of cue phrases with initial position in a clause, contributed to the choice of features. The prosody of the corpus was described using Pierrehumbert's theory of English intonation (Pierrehumbert 1980) . In Pierrehumbert's theory, intonational contours are described as sequences of low (L) and high (H) tones in the fundamental frequency (F0) contour (the physical correlate of pitch).",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_5",
  "x": "Prosody was manually determined by examining the fundamental frequency (F0) contour, and by listening to the recording. To produce the F0 contour, the recording of the corpus was digitized and pitchtracked using speech analysis software. This resulted in a display of the F0 where the x-axis represented time and the y-axis represented frequency in Hz. Various phrase nal characteristics (e.g., phrase accents, boundary tones, as well as pauses and syllable lengthening) helped to identify intermediate and intonational phrases, while peaks or valleys in the display of the F0 contour helped to identify pitch accents. In <cite>(Hirschberg & Litman 1993)</cite> , every cue phrase was described using the following prosodic features.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_6",
  "x": "Several characteristics of the token's immediate context were also noted, in particular, whether the token was immediately preceded or succeeded by orthography (punctuation or a paragraph boundary), and whether the token was immediately preceded or succeeded by a lexical item corresponding to a cue phrase. The set of classi ed and described tokens was used to evaluate the accuracy of the classi cation models shown in Figure 1 , developed in earlier studies. The prosodic model resulted from a study of 48 \\now\"s produced by multiple speakers in a radio callin show (Hirschberg & Litman 1987) . In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_7",
  "x": "The prosodic model resulted from a study of 48 \\now\"s produced by multiple speakers in a radio callin show (Hirschberg & Litman 1987) . In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens. (When later tested on 52 new examples of \\now\" from the radio corpus, the model also performed nearly perfectly). The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_8",
  "x": "The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse. When part of a larger intermediate phrase and either in initial position with a H* or complex accent, or in a non-initial position, it is sentential. The textual model was also manually developed, and was based on an examination of the rst 17 minutes of the single speaker technical address (Litman & Hirschberg 1990) ; the model correctly classi ed 89.4% of these 133 tokens. When a cue phrase is preceded by any type of orthography it is classi ed as discourse, otherwise as sentential. The models were evaluated by quantifying their performance in correctly classifying two subsets of the 953 tokens from the corpus.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_9",
  "x": "The textual model was also manually developed, and was based on an examination of the rst 17 minutes of the single speaker technical address (Litman & Hirschberg 1990) ; the model correctly classi ed 89.4% of these 133 tokens. When a cue phrase is preceded by any type of orthography it is classi ed as discourse, otherwise as sentential. The models were evaluated by quantifying their performance in correctly classifying two subsets of the 953 tokens from the corpus. The rst subset (878 examples) consisted of only the classi able tokens, i.e., the tokens that both <cite>Hirschberg and Litman</cite> classi ed as discourse or that both classi ed as sentential. The second subset, the classi able non-conjuncts (495 examples), was created from the classi able tokens by removing all examples of \\and\", \\or\" and \\but\".",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_10",
  "x": "The error rate of the textual model was 19.1% for the classi able tokens and 16.1% for the classi able non-conjuncts. In contrast, a model which just predicts the most frequent class in the corpus (sentential) has an error rate of 39% and 41% for the classi able tokens and the classi able nonconjuncts, respectively. ---------------------------------- **EXPERIMENTS USING MACHINE INDUCTION** This section describes experiments that use the machine learning programs C4.5 (Quinlan 1986; and cgrendel (Cohen 1992; 1993) to automatically induce cue phrase classi cation rules from both the data of <cite>(Hirschberg & Litman 1993)</cite> and an extension of this data.",
  "y": "uses extends"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_11",
  "x": "The output of each program is a set of classi cation rules, expressed in C4.5 as a decision tree and in cgrendel as an ordered set of if-then rules. Both cgrendel and C4.5 learn the classi cation rules using greedy search guided by an \\information gain\" metric. The rst set of experiments does not distinguish among the 34 cue phrases. In each experiment, a different subset of the features coded in <cite>(Hirschberg & Litman 1993 )</cite> is examined. The experiments consider every feature in isolation (to comparatively evaluate the utility of each individual knowledge source for classi cation), as well as linguistically motivated sets of features (to gain insight into the interactions between the knowledge sources).",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_12",
  "x": "This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments. The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> . These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_13",
  "x": "The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential. The features considered in the learning experiments are shown in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_14",
  "x": "These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_15",
  "x": "These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_16",
  "x": "Feature values can either be a numeric value or one of a xed set of user-de ned symbolic values. The feature representation shown here follows the representation of <cite>(Hirschberg & Litman 1993 )</cite> except as noted. Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values.",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_17",
  "x": "Feature values can either be a numeric value or one of a xed set of user-de ned symbolic values. The feature representation shown here follows the representation of <cite>(Hirschberg & Litman 1993 )</cite> except as noted. Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_18",
  "x": "This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_19",
  "x": "This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_20",
  "x": "Finally, the lexical feature token is new to this study, and represents the actual cue phrase being described. The second input to each learning program is training data, i.e., a set of examples for which the class and feature values are speci ed. Consider the following utterance, taken from the corpus of <cite>(Hirschberg & Litman 1993)</cite>: Example 1 (Now) (now that we have all been welcomed here)] it's time to get on with the business of the conference. This utterance contains two cue phrases, corresponding to the two instances of \\now\". The brackets and parentheses illustrate the intonational and intermediate phrases, respectively, that contain the tokens. Note that a single intonational phrase contains both tokens, but that each token is uttered in a di erent interme- diate phrase.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_21",
  "x": "Thus the token was not preceded by another cue phrase (C-P), but it was preceded by a form of orthography (O-P and O-P*). Since the token was immediately followed by another instance of \\now\" in the transcription, the token was succeeded by another cue phrase (C-S) but was not succeeded by orthography (O-S and O-P*). For each of the 56 feature sets (14 single feature, Table 2 : cgrendel and C4.5 error rates for the classi able tokens (N=878). 14 multiple feature, and 28 token sets), 2 actual sets of examples are created as input to the learning systems. These sets correspond to the two subsets of the corpus examined in <cite>(Hirschberg & Litman 1993 )</cite> { the classi able tokens, and the classi able non-conjuncts.",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_22",
  "x": "Cross-validation (Weiss & Kulikowski 1991 ) is used to estimate the error rates of the learned rulesets. Instead of running each learning program once on each of the 112 sets of examples, 10 runs are performed, each using a random 90% of the examples for training (i.e., for learning the ruleset) and the remaining 10% for testing. An estimated error rate is obtained by averaging the error rate on the testing portion of the data from each of the 10 runs. Note that for each run, the training and testing examples are disjoint subsets of the same set of examples, and the training set is much larger than the test set. In contrast (as discussed above), the \\training\" and test sets for the intonational model of <cite>(Hirschberg & Litman 1993)</cite> were taken from di erent corpora, while for the textual model of <cite>(Hirschberg & Litman 1993 )</cite> the test set was a superset of the training set.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_23",
  "x": "Each numeric cell shows the result (as a percentage) for one of the 56 feature sets. The standard error for each cell ranged from .6 to 2.7. The left half of the table considers the single feature and single feature plus token sets, while the right half considers the multiple features with and without token. The top of the table considers prosodic features, the bottom textual features, and the bottom right prosodic/textual combinations. The error rates in italics indicate that the performance of the learned ruleset exceeds the performance reported in <cite>(Hirschberg & Litman 1993)</cite> , where the rules of Figure 1 were tested using 100% of the 878 classi able tokens.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_24",
  "x": "However, the last 2 cgrendel rules also correlate L* and no accent with sentential status when the phrase is of a certain length, while rules (4) and (5) in Figure 1 provide a di erent interpretation and do not take length into account. (Recall that length was coded by <cite>Hirschberg and Litman</cite> only in <cite>their</cite> test data. Length was thus never used to generate or revise their prosodic model.) Both of the learned rulesets perform similarly to each other, and outperform the prosodic model of Figure 1 . Examination of the learned textual rulesets yields similar ndings. Consider the rulesets learned from O-P* (preceding orthography, where the particular type of orthography is not noted), shown towards the bottom of Figure 4 .",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_26",
  "x": "Without the feature token, the single feature sets based on position and preceding orthography are again the best performers, and along with many multiple feature non-token sets, perform nearly as well as the models in Figure 1 . When the feature token is taken into account, however, the learned rulesets outperform the models of <cite>(Hirschberg & Litman 1993</cite> sider this feature), and also provide new insights into cue phrase classi cation. Figure 5 shows the cgrendel ruleset learned from A+, which reduces the 30% error rate of A to 12%. The rst rule corresponds to line (5) of Figure 1 . In contrast to line (4), however, cgrendel uses deaccenting to predict discourse for only the tokens \\say\" and \\so.\" If the token is \\now\", \\ nally\", \\however\", or \\ok\", discourse is assigned (for all accents).",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_27",
  "x": "Siegel (in press) was the rst to apply machine learning to cue phrases. He developed a genetic learning algorithm to induce decision trees using the non-ambiguous examples of <cite>(Hirschberg & Litman 1993 )</cite> (using the classi cations of only one judge) as well as additional examples. Each example was described using a feature corresponding to token, as well as textual features containing the lexical or orthographic item immediately to the left of and in the 4 positions to the right of the example. Thus, new textual features were examined. Prosodic features were not investigated.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_28",
  "x": "Prosodic features were not investigated. Siegel reported a 21% estimated error rate, with half of the corpus used for training and half for testing. An examination of Table 2 shows that the error of the best C4.5 and cgrendel rulesets was often lower than 21% (even for theories which did not consider the token), as was the 19.1% error of the textual model of <cite>(Hirschberg & Litman 1993)</cite> . Siegel and McKeown (1994) have also proposed a method for developing linguistically viable rulesets, based on the partitioning of the training data produced during induction. ----------------------------------",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_29",
  "x": "---------------------------------- **CONCLUSION** This paper has demonstrated the utility of machine learning techniques for cue phrase classi cation. A rst set of experiments were presented that used the programs cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; to induce classi cation rules from the preclassi ed cue phrases and their features that were used as test data in <cite>(Hirschberg & Litman 1993)</cite> . The results of these experiments suggest that machine learning is an e ective technique for not only automating the generation of linguistically plausible classi cation rules, but also for improving accuracy.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_30",
  "x": "A rst set of experiments were presented that used the programs cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; to induce classi cation rules from the preclassi ed cue phrases and their features that were used as test data in <cite>(Hirschberg & Litman 1993)</cite> . The results of these experiments suggest that machine learning is an e ective technique for not only automating the generation of linguistically plausible classi cation rules, but also for improving accuracy. In particular, a large number of learned rulesets (including P-P, an extremely simple one feature model) had signicantly lower error rates than the rulesets of <cite>(Hirschberg & Litman 1993)</cite> . One possible explanation is that the hand-built classi cation models were derived using very small \\training\" sets; as new data became available, this data was used for testing but not for updating the original models. In contrast, machine learning supported the building of rulesets using a much larger amount of the data for training.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_31",
  "x": "In particular, a large number of learned rulesets (including P-P, an extremely simple one feature model) had signicantly lower error rates than the rulesets of <cite>(Hirschberg & Litman 1993)</cite> . One possible explanation is that the hand-built classi cation models were derived using very small \\training\" sets; as new data became available, this data was used for testing but not for updating the original models. In contrast, machine learning supported the building of rulesets using a much larger amount of the data for training. Furthermore, if new data becomes available, it is trivial to regenerate the rulesets. For example, in a second set of experiments, new classi cation rules were induced using the feature token, which was not considered in <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_0",
  "x": "**INTRODUCTION** Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax<cite> (Bansal and Klein, 2011</cite>; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences. Longer n-grams are also noisier and sparser, limiting the range of potential features. In this paper we develop new features for the graph-based MSTParser (McDonald and Pereira, 2006) from the Google Syntactic Ngrams corpus (Goldberg and Orwant, 2013) , a collection of Stanford dependency subtree counts.",
  "y": "background motivation"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_1",
  "x": "Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax<cite> (Bansal and Klein, 2011</cite>; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences. Longer n-grams are also noisier and sparser, limiting the range of potential features. In this paper we develop new features for the graph-based MSTParser (McDonald and Pereira, 2006) from the Google Syntactic Ngrams corpus (Goldberg and Orwant, 2013) , a collection of Stanford dependency subtree counts. These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task. We compare the performance of our syntactic n-gram features against the surface n-gram features of<cite> Bansal and Klein (2011)</cite> in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies.",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_2",
  "x": "We also develop paraphrase-style features like those of<cite> Bansal and Klein (2011)</cite> based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2). Figure 1 depicts the potential context words available the hold \u2192 hearing dependency. We experiment with a number of second-order features, mirroring those extracted for surface ngrams in Section 3.3. We extract all triple and sibling word and POS structures considered by the parser in the training and test corpora (following the factorization depicted in Figure 2 ), and counted their frequency in the Syntactic Ngrams corpus. Importantly, we require that matching subtrees in the Syntactic Ngrams corpus maintain the position of the parent relative to its children.",
  "y": "similarities uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_3",
  "x": "Similar to the surface n-gram features (Section 3), counts for our syntactic n-gram features are precomputed to improve the run-time efficiency of the parser. Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_4",
  "x": "Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain. We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
  "y": "motivation"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_5",
  "x": "We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ---------------------------------- **SURFACE N-GRAM CORPORA**",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_6",
  "x": "A uniform cutoff of 40 applies to all n-grams in this corpus. This corpus is affected by the accuracy of OCR and digitization tools; the changing typography of books across time is one issue that may create spurious cooccurrences and counts (Lin et al., 2012) . ---------------------------------- **FIRST-ORDER SURFACE N-GRAM FEATURES** Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005;<cite> Bansal and Klein, 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_7",
  "x": "For example, they give the example of sentence (1) below, paraphrase it into sentence (2), and examine how frequent the paraphrase is. If it should happen sufficiently often, this serves as evidence for the nominal attachment to demands in sentence (1) rather than the verbal attachment to meet. ---------------------------------- **MEET DEMANDS FROM CUSTOMERS 2. MEET THE CUSTOMERS DEMANDS** In<cite> Bansal and Klein (2011)</cite> , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus. For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_8",
  "x": "We encode the POS tags of the parent and children (or just the children for sibling features), along with the bucketed count, directionality, and the binned distance between the two children. We also extract paraphrase-style features for siblings in the same way as for first-order n-grams, and cumulative variants up to the maximum bucket size. ---------------------------------- **EXPERIMENTAL SETUP** As with<cite> Bansal and Klein (2011) and</cite> Pitler (2012) , we convert the Penn Treebank to dependencies using pennconverter 3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MX-POST (Ratnaparkhi, 1996) .",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_9",
  "x": "**RELATED WORK** Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from<cite> Bansal and Klein (2011)</cite> , other feature-based approaches to improving dependency parsing include Pitler (2012) , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors. Chen et al. (2013) describe a novel way of generating meta-features that work to emphasise important feature types used by the parser. Chen et al. (2009) generate subtree-based features that are similar to ours. However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_10",
  "x": "Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain. We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_11",
  "x": "We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ---------------------------------- **SURFACE N-GRAM CORPORA**",
  "y": "extends"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_0",
  "x": "Similarly, recent work on neural TTS has focused on adapting to new voices with limited reference data [13] [14] [15] [16] . Initial approaches to end-to-end speech-to-text translation (ST) [17, 18] performed worse than a cascade of an ASR model and an MT model. [19, 20] achieved better end-to-end performance by leveraging weakly supervised data with multitask learning. <cite>[21]</cite> further showed that use of synthetic training data can work better than multitask training. In this work we take advantage of both synthetic training targets and multitask training.",
  "y": "background"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_1",
  "x": "Two optional auxiliary decoders, each with their own attention components, predict source and target phoneme sequences. Following recent speech translation <cite>[21]</cite> and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers. As shown in Fig. 1 , the final layer output is passed to the primary decoder, whereas intermediate activations are passed to auxiliary decoders predicting phoneme sequences. We hypothesize that early layers of the encoder are more likely to represent the source content well, while deeper layers might learn to encode more information about the target content. The spectrogram decoder uses an architecture similar to Tacotron 2 TTS model [26] , including pre-net, autoregressive LSTM stack, and post-net components.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_2",
  "x": "We study two Spanish-to-English translation datasets: the large scale \"conversational\" corpus of parallel text and read speech pairs from <cite>[21]</cite> , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style. In Sections 3.1 and 3.2, we synthesize target speech from the target transcript using a single (female) speaker English TTS system; In Section 3.4, we use real human target speech for voice transfer experiments on the conversational dataset. Models were implemented using the Lingvo framework [39] . See Table 1 for dataset-specific hyperparameters. To evaluate speech-to-speech translation performance we compute BLEU scores [40] as an objective measure of speech intelligibility and translation quality, by using a pretrained ASR system to recognize the generated speech, and comparing the resulting transcripts to ground truth reference translations.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_3",
  "x": "---------------------------------- **CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> . The resulting dataset contains 979k parallel utterance pairs, containing 1.4k hours of source speech and 619 hours of synthesized target speech.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_4",
  "x": "**CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> . The resulting dataset contains 979k parallel utterance pairs, containing 1.4k hours of source speech and 619 hours of synthesized target speech. The total target speech duration is much smaller because the TTS output is better endpointed, and contains fewer pauses.",
  "y": "extends differences"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_5",
  "x": "The total target speech duration is much smaller because the TTS output is better endpointed, and contains fewer pauses. 9.6k pairs are held out for testing. Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in <cite>[21]</cite> . The speaker encoder was not used in these experiments since the target speech always came from the same speaker. Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST \u2192 TTS cascade model using a speech-to-text translation model <cite>[21]</cite> trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets.",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_6",
  "x": "Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in <cite>[21]</cite> . The speaker encoder was not used in these experiments since the target speech always came from the same speaker. Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST \u2192 TTS cascade model using a speech-to-text translation model <cite>[21]</cite> trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets. Note that the ground truth BLEU score is below 100 due to ASR errors during evaluation, or TTS failure when synthesizing the ground truth. Training without auxiliary losses leads to extremely poor performance.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_7",
  "x": "We found that 4-head attention works better than one head, unlike the conversational task, where both attention mechanisms had similar performance. Finally, as in <cite>[21]</cite> , we find that pretraining the bottom 6 encoder layers on an ST task improves BLEU scores by over 5 points. This is the best performing direct S2ST model, obtaining 76% of the baseline performance. ---------------------------------- **SUBJECTIVE EVALUATION OF SPEECH NATURALNESS**",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_8",
  "x": "The model achieves high translation quality on two Spanish-to-English datasets, although performance is not as good as a baseline cascade of ST and TTS models. In addition, we demonstrate a variant which simultaneously transfers the source speaker's voice to the translated speech. The voice transfer does not work as well as in a similar TTS context [15] , reflecting the difficulty of the cross-language voice transfer task, as well as evaluation [44] . Potential strategies to improve voice transfer performance include improving the speaker encoder by adding a language adversarial loss, or by incorporating a cycle-consistency term [13] into the S2ST loss. Other future work includes utilizing weakly supervision to scale up training with synthetic data <cite>[21]</cite> or multitask learning [19, 20] , and transferring prosody and other acoustic factors from the source speech to the translated speech following [45] [46] [47] .",
  "y": "future_work"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_0",
  "x": "AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors. There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13,<cite> 14,</cite> 15, 16, 17] . In this paper we explore new embedding models based on recurrent neural networks (RNNs), applied to a word discrimination task related to query-by-example search. RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences.",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_1",
  "x": "Levin et al. [12] developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of reference word segments. This representation was subsequently used in several applications: a segmental approach for query-by-example search [13] , lexical clustering [19] , and unsupervised speech recognition [20] . Voinea et al. [16] developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification. Kamper et al.<cite> [14]</cite> compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data [12, 21, 22, 23] .",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_2",
  "x": "We use two main training approaches, inspired by prior work but with some differences in the details. As in <cite>[14,</cite> 11] , our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final layer of g(X) is a log-softmax layer. Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see<cite> [14]</cite> for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_4",
  "x": "The second training approach, based on earlier work of Kamper et al.<cite> [14]</cite> , is to train \"Siamese\" networks [31] . In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different. The base model remains the same as before-an RNN followed by a set of fully connected layers-but the final layer is no longer a softmax but rather a linear activation layer of arbitrary size. In order to learn the parameters, we simultaneously feed three word segments through three copies of our model (i.e. three networks with shared weights). One input segment is an \"anchor\", x a , the second is another segment with the same word label, x s , and the third is a segment corresponding to a different word label, x d .",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_5",
  "x": "By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP. The data used for this task is drawn from the Switchboard conversational English corpus [32] . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work <cite>[14,</cite> 12] , and the same acoustic features as in<cite> [14]</cite> , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP).",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_6",
  "x": "The data used for this task is drawn from the Switchboard conversational English corpus [32] . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work <cite>[14,</cite> 12] , and the same acoustic features as in<cite> [14]</cite> , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in<cite> [14]</cite> , when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_7",
  "x": "We use a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training minibatch consists of 2B triplets. B triplets are of the form (x a , x s , x d ) where x a and x s are examples of the same class (a pair from the 100k same-word pair set) and x d is a randomly sampled example from a different class. Then, for each of these B triplets (x a , x s , x d ) , an additional triplet (x s , x a , x d ) is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work<cite> [14]</cite> , which we found to improve stability in training and performance on the development set.",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_10",
  "x": "In the first few lines of Table 2, we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding fixed the number of fully-connected layers at F = 1. There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of<cite> [14]</cite> until we begin adding fully connected layers. ---------------------------------- **EFFECT OF MODEL STRUCTURE** After exploring a variety of stacked RNNs, we fixed the stack to 3 layers and varied the number of fully connected layers.",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_12",
  "x": "---------------------------------- **EFFECT OF EMBEDDING DIMENSIONALITY** For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. 2 . This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings<cite> [14]</cite> for all dimensionalities \u2265 16. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_0",
  "x": "**ABSTRACT** We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the <cite>Visual Madlibs task</cite>. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_1",
  "x": "Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>. ---------------------------------- **INTRODUCTION** Question answering about real-world images is a relatively new research thread [2, 5, 14, 15] that requires a chain of machine visual perception, natural language understanding, and deductive capabilities to successfully come up with an answer on a question about visual content. Although similar in nature to image description [3, 8, 27] it requires a more focused attention to details in the visual content, yet it is easier to evaluate different architectures on the task.",
  "y": "motivation differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_2",
  "x": "Although similar in nature to image description [3, 8, 27] it requires a more focused attention to details in the visual content, yet it is easier to evaluate different architectures on the task. Moreover, in contrast to many classical Computer Vision problems such as recognition or detection, the task does not evaluate any internal representation of methods, yet it requires a holistic understanding of the image. Arguably, it is also less prone to over-interpretations compared with the classical Turing Test [16, 25] . To foster progress on this task, a few metrics and datasets have been proposed [2, 4, 14, 20] . The recently introduced <cite>Visual Madlibs task</cite> <cite>[32]</cite> removes ambiguities in question or scene interpretations by introducing a multiple choice \"filling the blank\" task, where a c 2016.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_4",
  "x": "Due to its unambiguous evaluation, this work focuses on this task. Contributions. We present two main contributions. Mean Box Pooling: We argue for a rich image representation in the form of pooled representations of the objects. Although related ideas have been explored for visual question answering [22] , and even have been used in <cite>Visual Madlibs</cite> <cite>[32]</cite> , we are first to show a significant improvement of such representation by using object proposals.",
  "y": "motivation differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_5",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> .",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_6",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> .",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_7",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> .",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_8",
  "x": "This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> . ---------------------------------- **RELATED WORK**",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_9",
  "x": "The very first work [14] on this topic has considered real world indoor scenes with associated natural language questions and answers. Since then different variants and larger datasets have been proposed: FM-IQA [4] , COCO-QA [20] , and VQA [2] . Although answering questions on images is, arguably, more susceptible to automatic evaluation than the image description task [3, 8, 27] , ambiguities in the output space still remain. While such ambiguities can be handled using appropriate metrics [14, 15, 17, 26] , <cite>Visual Madlibs</cite> <cite>[32]</cite> has taken another direction, and handles them directly within the task. <cite>It</cite> asks machines to fill the blank prompted with a natural language description with a phrase chosen from four candidate completions (Figure 4 ).",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_10",
  "x": "While a majority of the most recent work on visual question answering combine LSTM [7] with CNN [11, 23, 24] by concatenation or summation or piece-wise multiplication, Canonical Correlation Analysis (CCA and nCCA) [6] have also been shown to be a very effective multimodal embedding technique <cite>[32]</cite> . Our work further investigates this embedding method as well as brings ideas from CCA over to an CNN+LSTM formulation. ---------------------------------- **METHOD** We use normalized CCA (nCCA) [6] to embed the textual embedding of answers and the visual representation of the image into a joint space, where candidate sentence completions are compared to the image.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_11",
  "x": "In Section 3.3, we describe nCCA approach to encode two modalities into a joint space in greater details. In Section 3.4, we also investigate a CNN+LSTM architecture. Instead of generating a prompt completion that is next compared against candidate completions in a post-hoc process, we propose to choose a candidate completion by directly comparing candidates in the embedding space. This puts CNN+LSTM approach closer to nCCA with a tighter integration with the multi-choice <cite>Visual Madlibs task</cite>. This approach is depicted in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_13",
  "x": "---------------------------------- **MULTIMODAL EMBEDDING** We use the Normalized Canonical Correlation Analysis (nCCA) to learn a mapping from two modalities: image and textual answers, into a joint embedding space. This embedding method has shown outstanding performance on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . At the test time, given the encoded image, we choose an answer (encoded by the mean pooling over word2vec words representations) from the set of four candidate answers that is the most similar to the encoded image in the multimodal embedding space.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_14",
  "x": "where tr is the matrix trace,X := XW 1 ,\u0176 := YW 2 , and X,Y are two views (encoded images, and textual answers in our case). Normalized Canonical Correlation Analysis (nCCA) [6] has been reported to work significantly better than the plain CCA. Here, columns of the projection matrices W 1 and W 2 are scaled by the p-th power (p=4) of the corresponding eigen values. The improvement is consistent with the findings of <cite>[32]</cite> , where nCCA performs better than CCA by about five percentage points in average on the hard task. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_15",
  "x": "Similarly to prior work, we encode an image with a CNN encoder that is next concatenated with (learnable) word embeddings of the prompt sentence, and fed to a recurrent neural network. We use a special '<BLANK>' token to denote the empty blank space in the image description. On the other side, for each completion candidate s we compute its representation by averaging over word2vec [18] representations of the words contributing to s. However, in contrast to the prior work <cite>[32]</cite> , instead of comparing the discrete output of the network with the representation of s, we directly optimize an objective in the embedding space. During training we maximize the similarity measure between the output embedding and the representation of \u03c3 by optimizing the following objective: which is a cosine similarity between the representation of the available during the training correct completion\u015d i , and an output embedding vector of the i-th image-prompt training Table 4 : BLEU-1 and BLEU-2 computed on <cite>Madlibs testing dataset</cite> for different approaches.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_17",
  "x": "Results of the methods on this subset are shown in Table 2 bottom set of rows. These results show the same pattern as on the unfiltered set, with slightly higher accuracy. Table 4 shows BLEU-1 and BLEU-2 scores for targeted generation. Although the CNN+LSTM models we trained on <cite>Madlibs</cite> were not quite as accurate as nCCA for selecting the correct multiple-choice answer, they did result in better, sometimes much better, accuracy (as measured by BLEU scores) for targeted generation. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_19",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS** We evaluate our method on the multiple choice task of the <cite>Visual Madlibs dataset</cite>. <cite>The dataset</cite> consists of about 360k descriptions, spanning 12 different categories specified by different types of templates, of about 10k images. The selected images from the MS COCO dataset comes with rich annotations.",
  "y": "background uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_20",
  "x": "Since each category has fixed prompt, there is no need to include the prompt in the modeling given the training is done per each category. Finally, <cite>Visual Madlibs</cite> considers an easy and difficult tasks that differ in how the negative 3 candidate completions (distractors) are chosen. In the easy task, the distractors are randomly chosen from three descriptions of the same question type from other images. In the hard task, 3 distractors are chosen only from these images that contain the same objects as the given question image, and hence it requires a more careful and detailed image understanding. We use ADAM gradient descent method [10] with default hyper-parameters.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_22",
  "x": "Comparison to the state-of-the-art. Guided by the results of the previous experiments, we compare nCCA that uses Edge Boxes object proposals (nCCA (ours)) with the state-ofthe-arts on <cite>Visual Madlibs</cite> (nCCA <cite>[32]</cite> ). Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words. The models are trained per category (a model trained over all the categories performs inferior on the hard task <cite>[32]</cite> ). As Table 3 shows using a large number of object proposals Table 4 : Accuracies computed for different approaches on the easy and hard task.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_23",
  "x": "Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words. The models are trained per category (a model trained over all the categories performs inferior on the hard task <cite>[32]</cite> ). As Table 3 shows using a large number of object proposals Table 4 : Accuracies computed for different approaches on the easy and hard task. nCCA (ours) uses the representation with object proposals (NMS 0.75, and 100 proposals with mean-pooling). nCCA(bbox) mean-pools over the representations computed on the available ground-truth bounding boxes both at train and test time.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_24",
  "x": "This naturally leads to the following question if better localization helps. To see the limits, we compare nCCA (ours) against nCCA (bbox) <cite>[32]</cite> that crops over ground truth bounding boxes from MS COCO segmentations and next averages over theirs representations (Table 3 in <cite>[32]</cite> shows that ground truth bounding boxes outperforms automatically detected bounding boxes, and hence they can be seen as an upper bound for a detection method trained to detect objects on MS COCO). Surprisingly, nCCA (ours) outperforms nCCA (bbox) by a large margin as Table 4 shows. Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space.",
  "y": "differences uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_25",
  "x": "Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_26",
  "x": "Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case. Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details).",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_27",
  "x": "Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_28",
  "x": "Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case. Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details).",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_29",
  "x": "Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ). Note that, here we feed the sentence prompt to LSTM even though it is fixed per category.",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_30",
  "x": "This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ). Note that, here we feed the sentence prompt to LSTM even though it is fixed per category. Table 5 shows the performance of different methods. Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM <cite>[32]</cite> (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM).",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_31",
  "x": "Table 5 shows the performance of different methods. Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM <cite>[32]</cite> (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM). ---------------------------------- **CONCLUSION** We study an image representation formed by averaging over representations of object proposals, and show its effectiveness through experimental evaluation on the <cite>Visual Madlibs dataset</cite> <cite>[32]</cite> .",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_0",
  "x": "This paper describes an expanded convolution parse tree kernel to incorporate entity information into syntactic structure of relation examples. Similar to<cite> Zhang et al. (2006)</cite> , we employ a convolution parse tree kernel in order to model syntactic structures. Different from their method, we use the convolution parse tree kernel expanded with entity information other than a composite kernel. One of our motivations is to capture syntactic and semantic information in a single parse tree for further graceful refinement, the other is that we can avoid the difficulty with tuning parameters in composite kernels. Evaluation on the ACE2004 corpus shows that our method slightly outperforms the previous feature-base and kernel-based methods.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_1",
  "x": "Figure 1: Different representations of a relation instance in the example sentence \"in many cities, angry crowds roam the streets.\", which is excerpted from the ACE2004 corpus, where a relation \"PHSY.Located\" holds between the first entity \"crowds\"(PER) and the second entity \"streets\" (FAC). We employ the same convolution tree kernel used by Collins and Duffy (2001) , Moschitti (2004) and<cite> Zhang et al. (2006)</cite> . This convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees. However, the kernel value will depend greatly on the size of the trees, so we should normalize the kernel. From ACE definition on relation types and subtypes, we know that entity features impose a strong constraint on relation types.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_2",
  "x": "(1) Compressed Path-enclosed Tree (CPT, T1 in Fig.1 ): Originated from PT in<cite> Zhang et al. (2006)</cite> , we further make two kinds of compression. One is to prune out the children nodes right before the second entity under the same parent node of NP. The other is to compress the sub-structure like \"X-->Y-->Z\" into \"X-->Z\" in the parse trees. (2) Bottom-attached CPT (B-CPT, T2 in Fig.1 ): the entity type information is attached to the bottom of the entity node, i.e., two more nodes whose tags are \"TP\" are added under the first and the second entity nodes respectively. (3) Entity-attached CPT (E-CPT, T3 in Fig.1 ): the entity type name is combined with entity order name, e.g. \"E1-PER\" denotes the first entity whose type is \"PER\".",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_3",
  "x": "(3) Entity-attached CPT (E-CPT, T3 in Fig.1 ): the entity type name is combined with entity order name, e.g. \"E1-PER\" denotes the first entity whose type is \"PER\". This case is also explored by<cite> Zhang et al. (2006)</cite> , and we include it here just for the purpose of comparison. (4) Top-attached CPT (T-CPT, T4 in Fig.1 ): the entity type information is attached to the top node of the parse tree. In order to distinguish between two entities, we use tags \"TP1\" and \"TP2\" to represent the first entity type and the second entity type respectively. From the above four cases, we want to evaluate whether and how the entity information will be useful for relation extraction and in what way we can embed the entity information (especially the location where we attach) in the parse tree in order to achieve the best performance.",
  "y": "similarities"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_4",
  "x": "This may be that the syntactic structure in this condition is diverse and it contains too much noise in this kind of parse tree. It also suggests that much more noise needs to be pruned out from the parse tree while the key relation structure should remain in this condition. (4) Comparison with recent work Table 4 compares our system with recent work on the ACE2004 corpus. It shows that our system slightly outperforms recently best-reported systems. Compared with the composite kernel<cite> (Zhang et al, 2006)</cite> , our system further prunes the parse tree and incorporates entity features into the convolution parse tree kernel.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_0",
  "x": "In these approaches, a symbolic sequence of words (or characters) in the source language is used as an intermediary representation during the speech translation process. However, recent works have attempted to build end-to-end speech-to-text translation without using source language transcription during learning or decoding. One attempt to translate directly a source speech signal into target language text is that of [1] . However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is <cite>our own work</cite> <cite>[2]</cite> but it was applied to a synthetic (TTS) speech corpus.",
  "y": "background"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_1",
  "x": "However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is <cite>our own work</cite> <cite>[2]</cite> but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3] . This paper is a follow-up of <cite>our previous work</cite> <cite>[2]</cite> . We now investigate end-to-end speech-to-text translation on a corpus of audiobooks -LibriSpeech [4] -specifically augmented to perform end-to-end speech translation [5] .",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_2",
  "x": "This paper is a follow-up of <cite>our previous work</cite> <cite>[2]</cite> . We now investigate end-to-end speech-to-text translation on a corpus of audiobooks -LibriSpeech [4] -specifically augmented to perform end-to-end speech translation [5] . While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3.",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_3",
  "x": "While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in <cite>[2]</cite> and the audiobook dataset described in section 2. Finally, section 5 concludes this work.",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_4",
  "x": "Furthermore, we double the training size by concatenating the aligned references with the Google Translate references. We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to <cite>[2]</cite> . ---------------------------------- **END-TO-END MODELS** For the three tasks, we use encoder-decoder models with attention [9, 10, 11, <cite>2,</cite> 3] .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_5",
  "x": "We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to <cite>[2]</cite> . ---------------------------------- **END-TO-END MODELS** For the three tasks, we use encoder-decoder models with attention [9, 10, 11, <cite>2,</cite> 3] . Because we want to share some parts of the model between tasks (multi-task training), the ASR and AST models use the same encoder architecture, and the AST and MT models use the same decoder architecture.",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_6",
  "x": "Because we want to share some parts of the model between tasks (multi-task training), the ASR and AST models use the same encoder architecture, and the AST and MT models use the same decoder architecture. ---------------------------------- **SPEECH ENCODER** The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder <cite>[2]</cite> . It takes as input a sequence of audio features: x = (x 1 , . . . , x Tx ) \u2208 R Tx\u00d7n .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_7",
  "x": "Like <cite>[2]</cite> , these features are given as input to two non-linear (tanh) layers, which output new features of size n . Like [3] , this new set of features is then passed to a stack of two convolutional layers. Each layer applies 16 convolution filters of shape (3, 3, depth) with a stride of (2, 2) w.r.t. time and feature dimensions; depth is 1 for the first layer, and 16 for the second layer. We get features of shape (T x /2, n /2, 16) after the 1 st layer, and (T x /4, n /4, 16) after the 2 nd layer.",
  "y": "similarities"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_8",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **MODEL SETTINGS** Speech files were preprocessed using Yaafe [13] , to extract 40 MFCC features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [14, <cite>2]</cite> .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_9",
  "x": "The pre-trained model is identical to end-to-end, but its encoder and decoder are initialized with our ASR and MT Table 3 : Results of the AST task on BTEC test. \u2020 was obtained with an ensemble of 5 models, while we use ensembles of 2 models. The non-cascaded ensemble combines the pre-trained and multi-task models. Contrary to <cite>[2]</cite> , we only present mono-reference results. ASR mono ASR multi MT mono MT multi Fig. 1 : Augmented LibriSpeech Dev BLEU scores for the MT task, and WER scores for the ASR task, with the initial (mono-task) models, and when multi-task training picks up.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_10",
  "x": "(3) contrary to [3] , in both BTEC and LibriSpeech settings, best AST performance is observed when a symbolic sequence of symbols in the source language is used as an intermediary representation during the speech translation process (cascaded system); (4) finally, the AST results presented on Lib-riSpeech demonstrate that our augmented corpus is useful, although challenging, to benchmark end-to-end AST systems on real speech at a large scale. We hope that our baseline on Augmented LibriSpeech will be challenged in the future. The large improvements on MT and AST on the BTEC corpus, compared to <cite>[2]</cite> are mostly due to our use of a better decoder, which outputs characters instead of words. 6 BLEU End-to-End Pre-train Multi-task Fig. 2 : Dev BLEU scores on 3 models for end-to-end AST of audiobooks. Best scores on the dev set for the end-to-end (mono-task), pre-train and multi-task models were achieved at steps 369k, 129k and 95k.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_11",
  "x": "---------------------------------- **ANALYSIS** ---------------------------------- **CONCLUSION** We present baseline results on End-to-End Automatic Speech Translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from BTEC (follow-up to <cite>[2]</cite> ).",
  "y": "extends"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_0",
  "x": "More recently, there have been efforts in applying the intuition to larger semantic units, such as sentences, or documents. However, approaches based on distributional semantics are limited by the grounding problem [7] , which calls for techniques to ground certain conceptual knowledge in perceptual information. Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_1",
  "x": "However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_2",
  "x": "Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output. In the case of visually grounded multi-modal framework, applying such attention mechanism could help the encoder to identify visually significant words or phrases.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_3",
  "x": "However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_4",
  "x": "Joint Learning of Language and Vision. Convergence between computer vision and NLP researches have increasingly become common. Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_5",
  "x": "Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation. Similar intuitions have been applied to various NLP [5, 22, 23] and vision tasks [11] . [11] applied attention mechanism to images to bind specific visual features to language.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_6",
  "x": "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_7",
  "x": "**PROPOSED METHOD** Given a data sample (X, Y, h I ) \u2208 D, where X is the source caption, Y is the target caption, and h I is the hidden representation of the image, our goal is to predict Y and h I with X, and the hidden representation in the middle serves as the general sentence representation. ---------------------------------- **VISUALLY GROUNDED ENCODER-DECODER FRAMEWORK** We base our model on the <cite>encoder-decoder framework</cite> introduced in <cite>[8]</cite> .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_8",
  "x": "---------------------------------- **VISUAL GROUNDING** Given the source caption representation h S and the relevant image representation h I , we associate the two representations by projecting h S into image feature space. We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions. Although margin ranking loss has been the dominant choice for training cross-modal feature matching <cite>[8</cite>, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency.",
  "y": "differences"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_9",
  "x": "Attended representation h A and encoder-decoder representation h S are concatenated into the final self-attentive sentence representation h. This hybrid representation replaces h S and is used to predict image features (Section 3.2) and target caption (Section 3.1). ---------------------------------- **LEARNING OBJECTIVES** Following the experimental design of <cite>[8]</cite> , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG. Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_10",
  "x": "We evaluate sentence representation quality using SentEval 2 <cite>[8</cite>, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch. ---------------------------------- **EVALUATION** Adhering to the experimental settings of <cite>[8]</cite> , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_11",
  "x": "We evaluate sentence representation quality using SentEval 2 <cite>[8</cite>, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch. ---------------------------------- **EVALUATION** Adhering to the experimental settings of <cite>[8]</cite> , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_12",
  "x": "**ATTENTION MECHANISM AT WORK** In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ). For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\". Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\". These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations <cite>[8]</cite> .",
  "y": "extends differences"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_0",
  "x": "Semantic parsing is the task of mapping a natural language (NL) sentence into a complete, formal meaning representation (MR) which a computer program can execute to perform some task, like answering database queries or controlling a robot. These MRs are expressed in domain-specific unambiguous formal meaning representation languages (MRLs). Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs. Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> . These systems use supervised learning methods which only utilize annotated NL sentences.",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_1",
  "x": "Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> . These systems use supervised learning methods which only utilize annotated NL sentences. In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.",
  "y": "background differences"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_2",
  "x": "We modify KRISP, a supervised learning system for semantic parsing presented in<cite> (Kate and Mooney, 2006)</cite> , to make a semi-supervised system we call SEMISUP-KRISP. Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences. ---------------------------------- **BACKGROUND** This section briefly provides background needed for describing our approach to semi-supervised semantic parsing.",
  "y": "extends"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_3",
  "x": "**BACKGROUND** This section briefly provides background needed for describing our approach to semi-supervised semantic parsing. ---------------------------------- **KRISP: THE SUPERVISED SEMANTIC PARSING** Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) <cite>(Kate and Mooney, 2006</cite> ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_4",
  "x": "Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data<cite> (Kate and Mooney, 2006)</cite> . ---------------------------------- **TRANSDUCTIVE SVMS** SVMs (Cristianini and Shawe-Taylor, 2000) are state-of-the-art machine learning methods for classification. Given positive and negative training examples in some vector space, an SVM finds the maximum-margin hyperplane which separates them.",
  "y": "background"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_0",
  "x": "However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input [2] . While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling <cite>[5]</cite> have shown improvements, it does not solve the underlying problem. In creative text generation, the objective is not strongly bound to the ground truth-instead the objective is to generate diverse, unique or original samples. We attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens. The contributions of this paper are in the usage of a GAN framework to generate creative pieces of writing.",
  "y": "motivation"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_1",
  "x": "We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of argmax on the log-likelihood probabilities, as sampling has shown to produce better output quality <cite>[5]</cite> . Please refer to Supplementary Section Table 3 for training parameters of each dataset and Table 2 for hyperparameters of each encoder. We pick these values after experimentation with our validation set. Training and output generation code can be found online 2 . ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_0",
  "x": "In the case of online social networks, the great variety of users, including very different language registers, spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have models robust enough to be applied in all cases. (Hosseini et al., 2017) have then shown that it is very easy to bypass automatic toxic comment detection systems by making the abusive content difficult to detect (intentional spelling mistakes, uncommon negatives...). Because the reactions of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only on the targeted message itself. For instance, (Yin et al., 2009 ) use features derived from the sentences neighboring a given message to detect harassment on the Web. (Balci and Salah, 2015) take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work<cite> (Papegnies et al., 2019)</cite> , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation.",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_1",
  "x": "For instance, (Yin et al., 2009 ) use features derived from the sentences neighboring a given message to detect harassment on the Web. (Balci and Salah, 2015) take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work<cite> (Papegnies et al., 2019)</cite> , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection. Our conversational network extraction process is inspired from other works leveraging such graphs for other purposes: chat logs (Mutton, 2004) or online forums (Forestier et al., 2011) interaction modeling, user group detection (Camtepe et al., 2004) . Additional references on abusive message detection and conversational network modeling can be found in<cite> (Papegnies et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_2",
  "x": "In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources. For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game. We then perform a feature study, finding the most informative ones and discussing their role. Our contribution is twofold: the exploration of fusion methods, and more importantly the identification of discriminative features for this problem.",
  "y": "extends"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_3",
  "x": "For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game. We then perform a feature study, finding the most informative ones and discussing their role. Our contribution is twofold: the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. The rest of this article is organized as follows.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_4",
  "x": "**METHODS** In this section, we summarize the content-based method from (Papegnies et al., 2017b ) (Section 2.1) and the graph-based method from<cite> (Papegnies et al., 2019</cite> ) (Section 2.2). We then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information (Section 2.3). Figure 1 shows the whole process, and is discussed through this section. Figure 1 .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_5",
  "x": "Figure 1 . Representation of our processing pipeline. Existing methods refers to our previous work described in (Papegnies et al., 2017b ) (content-based method) and<cite> (Papegnies et al., 2019)</cite> (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). ---------------------------------- **CONTENT-BASED METHOD**",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_6",
  "x": "**GRAPH-BASED METHOD** This method corresponds to the top-left part of Figure 1 (in red). It completely ignores the content of the messages, and only focuses on the dynamics of the conversation, based on the interactions between its participants<cite> (Papegnies et al., 2019)</cite> . It is three-stepped: 1) extracting a conversational graph based on the considered message as well as the messages preceding and/or following it; 2) computing the topological measures of this graph to characterize its structure; and 3) using these values as features to train an SVM to distinguish between abusive and non-abusive messages. The vertices of the graph model the participants of the conversation, whereas its weighted edges represent how intensely they communicate.",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_7",
  "x": "All these measures are computed for each graph, and allow describing the conversation surrounding the message of interest. The SVM is then trained using these values as features. In this work, we use exactly the same measures as in<cite> (Papegnies et al., 2019)</cite> . ---------------------------------- **FUSION**",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_8",
  "x": "The dataset is the same as in our previous publications (Papegnies et al., 2017b <cite>(Papegnies et al., , 2019</cite> . It is a proprietary database containing 4,029,343 messages in French, exchanged on the in-game chat of SpaceOrigin 1 , a Massively Multiplayer Online Role-Playing Game (MMORPG). Among them, 779 have been flagged as being abusive by at least one user in the game, and confirmed as such by a human moderator. They constitute what we call the Abuse class. Some inconsistencies in the database prevent us from retrieving the context of certain messages, which we remove from the set.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_9",
  "x": "The graph extraction method used to produce the graph-based features requires to set certain parameters. We use the values matching the best performance, obtained during the greedy search of the parameter space performed in<cite> (Papegnies et al., 2019)</cite> . In particular, regarding the two most important parameters (see Section 2.2), we fix the context period size to 1,350 messages and the sliding window length to 10 messages. Implementation-wise, we use the iGraph library (Csardi and Nepusz, 2006) to extract the conversational networks and process the corresponding features. We use the Sklearn toolkit (Pedregosa et al., 2011) to get the text-based features.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_10",
  "x": "Table 1 presents the Precision, Recall and F -measure scores obtained on the Abuse class, for both baselines (Content-based (Papegnies et al., 2017b) and Graph-based<cite> (Papegnies et al., 2019)</cite> ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion). It also shows the number of features used to perform the classification, the time required to compute the features and perform the cross validation (Total Runtime) and to compute one message in average (Average Runtime). Note that Late Fusion has only 2 direct inputs (content-and graph-based SVMs), but these in turn have their own inputs, which explains the values displayed in the table. ---------------------------------- **CLASSIFICATION PERFORMANCE**",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_11",
  "x": "There are three Content-Based TF. The first is the Naive Bayes prediction, which is not surprising as it comes from a fully fledged classifier processing BoWs. The second is the tf -idf score computed over the Abuse class, which shows that considering term frequencies indeed improve the classification performance. The third is the Capital Ratio (proportion of capital letters in the comment), which is likely to be caused by abusive message tending to be shouted, and therefore written in capitals. The Graph-Based TF are discussed in depth in our previous article<cite> (Papegnies et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_12",
  "x": "For instance, with the Late Fusion TF, we need only 3% of the total Late Fusion runtime. ---------------------------------- **CONCLUSION AND PERSPECTIVES** In this article, we tackle the problem of automatic abuse detection in online communities. We take advantage of the methods that we previously developed to leverage message content (Papegnies et al., 2017a) and interactions between users<cite> (Papegnies et al., 2019)</cite> , and create a new method using both types of information simultaneously.",
  "y": "extends"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_0",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather ---------------------------------- ****",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_1",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec.",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_2",
  "x": "Bansal et al. [8] and <cite>Melamud et al. [11]</cite> show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees. There are three main steps in our rescoring approach. The first step is to have the parser to produce n-best parse trees with their structural scores.",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_0",
  "x": "Many studies have analyzed gender and racial biases in song lyrics [14, 18] . However, such an approach of manual analysis cannot scale to millions of songs. Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . Their test quantifies biases by computing similarity scores between various sets of words. To compute similarity, the WEAT test represents words using a distributed word representation method such as fastText or word2vec [4, 16] .",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_1",
  "x": "However, such an approach of manual analysis cannot scale to millions of songs. Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . Their test quantifies biases by computing similarity scores between various sets of words. To compute similarity, the WEAT test represents words using a distributed word representation method such as fastText or word2vec [4, 16] . We apply the WEAT test on song lyrics and discuss its implications.",
  "y": "background uses"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_2",
  "x": "A positive value of the IAT test indicates that people are biased to associate first attribute word set to the first target word set (Bias: flowers are pleasant) and second attribute word set with second target word set (Bias: insects are unpleasant). A negative value of the effect size indicates the bias in the other direction, that is flowers are unpleasant, and insects are pleasant. Larger magnitude of effect size indicates a stronger bias. If the value of effect size is closer to zero, then it indicates slight or no bias. Caliskan et al. designed the Word Embedding Association Test (WEAT) by tweaking the IAT test <cite>[5]</cite> .",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_3",
  "x": "They have shown that their results correlate with the IAT tests conducted with human subjects. We applied the WEAT test on our song lyrics dataset. Due to the small size of popular songs dataset, we cannot apply the WEAT test separately on popular songs lyrics. Please refer to Table 1 . Corresponding to eight rows of the table, we have measured eight biases. We borrowed these attribute and target word sets from Caliskan et al. <cite>[5]</cite> . First two columns (w2v and FT) correspond to measurements on the song lyrics dataset with word2vec and Out of all tests, we can see that the effect size of both FT and CA column is highest for test 4.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_0",
  "x": "More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (Collins- Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; <cite>Feng et al., 2010)</cite> ; such works are described in further detail in the next Section 2. In recent work (Ma et al., 2012) , we approached the problem of fine-grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain. A further finding was that visually-oriented features that consider the visual layout of the page (e.g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc.) play an important role in predicting the reading levels of children's books in which pictures and textual layout dominate the book content over text. However, the data preparation process in our previous study involves human intervention-we ask human annotators to draw rectangle markups around text region over pages. Moreover, we only use a very shallow surface level text-based feature set to compare with the visually-oriented features.",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_1",
  "x": "However, the data preparation process in our previous study involves human intervention-we ask human annotators to draw rectangle markups around text region over pages. Moreover, we only use a very shallow surface level text-based feature set to compare with the visually-oriented features. Hence in this paper, we assess the effect of using completely automated annotation processing within the same framework. We are interested in exploring how much performance will change by completely eliminating manual intervention. At the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by<cite> Feng et al. (2010)</cite> , which capture deeper syntactic complexities of the text.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_2",
  "x": "They argued that grammar-based features are more pertinent for second language learners than for the first language readers. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) both used a support vector machine to classify texts based on the reading level. They combined traditional methods of readability assessment and the features from language models and parsers. Aluisio et al. (2010) have developed a tool for text simplification for the authoring process which addresses lexical and syntactic phenomena to make text readable but their assessment takes place at more coarse levels of literacy instead of finer-grained levels used for children's books. A detailed analysis of various features for automatic readability assessment has been done by<cite> Feng et al. (2010)</cite> .",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_3",
  "x": "We utilize the Stanford Parser (Klein and Manning, 2003) to extract the following features from the XML files based on those used in<cite> (Feng et al., 2010)</cite> : ---------------------------------- **EXPERIMENTS** In the experiments, we look at how much the performance dropped by switching to zero human inputs. We also investigate the impact of using a richer set of text-based features.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_4",
  "x": "---------------------------------- **INCORPORATING STRUCTURAL FEATURES** Our previous study demonstrated that combining surface features with visual features produces promising results. As mentioned above, the second aim of this study is to see how much benefit we can get from incorporating high-level structural features, such as those used in<cite> (Feng et al., 2010)</cite> (described in Section 4.2), with the features in our previous study. annotation under the \u00b11 accuracy metric, the visual features and the structural features have the same performance, whose accuracy are both slightly lower than that of surface level features.",
  "y": "uses"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_0",
  "x": "However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012) , argument summarization (Misra et al., 2015) , sarcasm detection (Justo et al., 2014) and classification of propositions and arguments <cite>(Park and Cardie, 2014</cite>; Park et al., 2015; Oraby et al., 2015) . Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_1",
  "x": "Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012) , argument summarization (Misra et al., 2015) , sarcasm detection (Justo et al., 2014) and classification of propositions and arguments <cite>(Park and Cardie, 2014</cite>; Park et al., 2015; Oraby et al., 2015) . Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies<cite> (Park and Cardie, 2014)</cite> compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_2",
  "x": "In this paper, we use the term \"claim\" loosely to refer to an individual proposition (a sentence or independent clause) in an argument, or to a short argumentative text containing one or more propositions. In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_3",
  "x": "In this paper, we use the term \"claim\" loosely to refer to an individual proposition (a sentence or independent clause) in an argument, or to a short argumentative text containing one or more propositions. In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features.",
  "y": "differences motivation"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_4",
  "x": "Park and Cardie (2014) extracted clause-specific features using the Stanford syntactic parser and the Penn Treebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context-based embeddings can inherently capture these linguistic characteristics and can replace these features. Dependency context based embeddings capture functional similarities across the words using different contexts (Levy and Goldberg, 2014) . Komninos and Manandhar (2016) have shown that dependency-based models produce word embeddings that better capture functional properties of words for question type classification and relation detection.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_5",
  "x": "Park and Cardie (2014) extracted clause-specific features using the Stanford syntactic parser and the Penn Treebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context-based embeddings can inherently capture these linguistic characteristics and can replace these features. Dependency context based embeddings capture functional similarities across the words using different contexts (Levy and Goldberg, 2014) . Komninos and Manandhar (2016) have shown that dependency-based models produce word embeddings that better capture functional properties of words for question type classification and relation detection.",
  "y": "extends"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_6",
  "x": "In online argumentative discourse, claims often serve as implicit arguments with inappropriate or missing justification<cite> (Park and Cardie, 2014)</cite> . The certainty and factuality signals present in such claims may be appropriate for determining its factuality or verifiability. As the claims in our data set are objective, subjective and factual types, predicates, adverbs and other modals (related to certainty and factuality) present in FactBank 1.0 may help in better distinguishing various types of claims. As an example, consider the sentence in Figure 2 , a complex claim of type \"verifiable non-experiential\". The predicate \"seems\" and the modal verb \"must\" can be viewed as certainty and factuality information related to the speaker's commitment to their utterance.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_7",
  "x": "2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments. The propositions are manually annotated with three classes-\"verifiable experiential\", \"verifiable non-experiential\", and \"unverifiable\"-where the support types are evidence, optional evidence, and reason, respectively. The annotation distribution and our train/test splits are shown in Table 2 . ---------------------------------- **VERIFIABLE AND UNVERIFIABLE USER COMMENTS**",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_8",
  "x": "The annotation distribution for these splits is shown in Table 1 . We also use a development set to tune the hyper-parameters of the model. (Park and Cardie, 2014) . This corpus consists of 9476 manually annotated sentences and independent clauses from 1047 user comments extracted from the Regulation Room website. 2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments.",
  "y": "similarities"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_0",
  "x": "EmoContext@SemEval2019 shared task (Chatterjee et al., 2019) goal is to encourage more research in the field of contextual emotion detection in textual conversations. The shared task focuses on identifying emotions namely Angry, Happy, Sad and Others from conversation with three turns. Since, emotion detection is a classification problem, research works have been carried out by using machine learning with lexical features (Sharma et al., 2017) and deep learning with deep neural network (Phan et al., 2016) and convolutional neural network<cite> (Zahiri and Choi, 2018)</cite> to detect the emotions from text. However, we have adopted Seq2Seq deep neural network for detecting the emotions from textual conversations which include sequence of phrases. This paper elaborates our Seq2Seq approach for identifying emotions from text sequences.",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_1",
  "x": "This section reviews the research work reported for emotion detection from text / tweets (Perikos and Hatzilygeroudis, 2013; Rao, 2016; AbdulMageed and Ungar, 2017; Samy et al., 2018; AlBalooshi et al., 2018; Gaind et al., 2019 ) and text conversations (Phan et al., 2016; Sharma et al., 2017;<cite> Zahiri and Choi, 2018)</cite> . Sharma et al. (2017) proposed a methodology to create a lexicon -a vocabulary consisting of positive and negative expressions. This lexicon is used to assign an emotional value which is derived from a fuzzy set function. Gaind et al. (2019) classified twitter text into emotion by using textual and syntactic features with SMO and decision tree classifiers. The tweets are annotated manually by Liew and Turtle (2016) with 28 fine-grained emotion categories and experimented with different machine learning algorithms.",
  "y": "uses"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_2",
  "x": "The topic vectors resembling an image are fed to CNN to learn the contextual information. Abdul-Mageed and Ungar (2017) built a very large dataset with 24 fine-grained types of emotions and classified the emotions using gated RNN. Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> . They proposed several sequence-based convolution neural network (SCNN) models with attention to facilitate sequential dependencies among utterances. All the models discussed above show that the emotion prediction can be handled using variants of deep neural network such as C-GRU, G-RNN and Sequential-CNN.",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_3",
  "x": "Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> . All the models discussed above show that the emotion prediction can be handled using variants of deep neural network such as C-GRU, G-RNN and Sequential-CNN. The commonality between the above models are the variations of RNN or LSTM. This motivated us to use the Sequenceto-Sequence (Seq2Seq) model which consists of stacked LSTMs to predic the emotion labels conditioned on the given utterance sequences.",
  "y": "background motivation"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_0",
  "x": "This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches. Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from <cite>Godard et al. (2018)</cite> . There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units. We experiment with the Mboshi-French parallel corpus, translating the French text into four other well-resourced languages in order to investigate language impact in this CLD approach. Our results hint that this language impact exists, and that models based on different languages will output different word-like units.",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_1",
  "x": "2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from <cite>Godard et al. (2018)</cite> to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence). Due to the attention mechanism present in these networks (Bahdanau et al., 2014) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences. These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs.",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_2",
  "x": "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs. Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from <cite>Godard et al. (2018)</cite> . The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold T over the output boundaries.",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_0",
  "x": "Linear methods include the use of a projection layer for meta-embedding (known as 1TON) Yin and Sch\u00fctze (2015) , which is simply trained using an 2 -based loss. Similarly, Bollegala et al. (Bollegala et al., 2017) has focused on finding a linear transformation between count-based and prediction-based embeddings, showing that linearly transformed count-based embeddings can be used for predictions in the localized neighborhoods in the target space. Most recent work<cite> (Bao and Bollegala, 2018</cite> ) has focused on the use of an autoencoder (AE) to encode a set of N pretrained embeddings using 3 different variants: (1) Decoupled Autoencoded Meta Embeddings (DAEME) that keep activations separated for each respective embedding input during encoding and uses a reconstruction loss for both predicted embeddings while minimizing the loss for each respective decoded output, (2) Coupled Autoencoded Meta Embeddings (CAEME) which instead learn to predict from a shared encoding and (3) Averaged Autoencoded Meta-Embedding (AAME) is simply an averaging of the embedding set as input instead of using a concatenation. This is the most relevant work to our paper, hence, we include these 3 autoencoding schemes along with aforementioned methods for experiments, described in Section 3. We also include two subtle variations of the aforementioned arXiv:1808.04334v1 [cs.CL] 13 Aug 2018",
  "y": "background"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_1",
  "x": "---------------------------------- **METHODOLOGY** Before describing the loss functions used, we explain the aforementioned variation on the autoencoding method and how it slightly differs from 1TON/1TON + (Yin and Sch\u00fctze, 2015) and standard AEs<cite> (Bao and Bollegala, 2018)</cite> presented in previous work. Target Autoencoders (TAE) are defined as learning an ensemble of nonlinear transformations between sets of bases X s in sets of vector spaces X S = {X 1 , .., X s , .., X N } s.t X s \u2208 R |vs|\u00d7ds to a target space X t \u2208 R |vt|\u00d7dt , where f \u2192 X t \u2200i is the nonlinear transformation function used to make the mapping.",
  "y": "differences uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_2",
  "x": "This means thats all vector spaces have been mapped to a target space and there hidden meta-word representations have been averaged, as illustrated in Figure 1 . Figure 2 shows a comparison of the previous autoencoder approaches<cite> (Bao and Bollegala, 2018)</cite> (left) and the alternative AE (right), where dashed lines indicate connections during training and bold lines indicate prediction. The ConcatAutoEncoder (CAEME) simply concatenates the embedding set into a single vector and trains the autoencoder so to produce a lower-dimensional representation (shown in red), while the decoupled autoencoder (DAEME) keeps the embedding vectors separate in the encoding. In contrast the target encoder (TAE) is similar to that of CAEME only the label is a single embedding from the embedding set and the input are remaining embeddings from the set. After training, TAE then concatenates the hidden layer encoding with the original target vector.",
  "y": "background uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_3",
  "x": "As stated, we compare against previous methods (Yin and Sch\u00fctze, 2015;<cite> Bao and Bollegala, 2018</cite> ) that use 2 distance, as shown in Equation 1). Similarly, the Mean Absolute Error ( 1 norm of difference) loss 1 N N i=1 |y \u2212\u0177| is tested. We also compare against a KL divergence objective, as shown in Equation 2,\u0177 is the last activation output from the log-softmax that represents q(x) and the KL-divergence is given as Since tanh functions are used and input vectors are 2 normalized we propose a Squared Cosine ----------------------------------",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_4",
  "x": "**EXPERIMENTS** The following word association and word similarity datasets are used throughout experimentation: Simlex (Hill et al., 2015) , WordSim-353 (Finkelstein et al., 2001) , RG (Rubenstein and Goodenough, 1965) , MTurk (MechanicalTurk-771) (Halawi et al., 2012) , RareWord (RW) (Luong et al., 2014) and MEN (Bruni et al., 2012) . The word vectors considered in the embeddings set are skipgram and cbow (Mikolov et al., 2013) , FastText (Bojanowski et al., 2016) , LexVec (Salle et al., 2016) , Hellinger PCA (HPCA) (Lebret and Collobert, 2013) and Hierarchical Document Context (HDC) (Sun et al., 2015) . We now report results on the performance of meta-embedding autoencodings with various loss functions, while also presenting target autoencoders for combinations of word embeddings and compare against existing current SoTA meta-embeddings. Table 1 shows the scaled Spearman correlation test scores, where (1) shows the original single embeddings, (2) results for standard metaembedding approaches that either apply a single mathematical operation or employ a linear projection as an encoding, (3) presents the results using autoencoder schemes by<cite> (Bao and Bollegala, 2018</cite> ) that we have used to test the various losses, (4) introduces TAE without concatenating the target Y embedding post-training with MSE loss and (5) shows the results of concatenating Y with the lower-dimensional (200-dimensions) vector that encodes all embeddings apart from the target vector.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_0",
  "x": "Automatic methods of conversation summarization have a potential to increase the capacity of the call-centers to analyze and assess their work. Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and<cite> (Oya et al., 2014)</cite> , abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; <cite>Oya et al., 2014)</cite> .",
  "y": "motivation"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_1",
  "x": "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; <cite>Oya et al., 2014)</cite> . The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models. The graph paths are ranked to yield abstract sentences -a template. And these templates are selected for population with entities extracted from a conversation. Thus the abstractive summarization systems are limited to these templates generated by supervised data sources.",
  "y": "background"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_2",
  "x": "The template selection strategy in these systems leverages on the manual links between summary and conversation sentences. Unfortunately, such manual links are rarely available. In this paper we evaluate a set of heuristics for automatic linking of summary and conversations sentences, i.e. 'community' creation. The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings. The heuristics are evaluated within the template-based abstractive summarization system of<cite> Oya et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_3",
  "x": "---------------------------------- **TEMPLATE GENERATION** Template Generation follows the approach of<cite> (Oya et al., 2014)</cite> and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps. The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing. For English, we use Illinois Chunker (Punyakanok and Roth, 2001) to identify noun phrases and extract part-of-speech tags; and the the tool of (De Marneffe et al., 2006) for generating dependency parses.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_4",
  "x": "The template graphs, created using this similarity, are then clustered using the Normalized Cuts method (Shi and Malik, 2000) . The clustered templates are further generalized using a word graph algorithm extended to templates in<cite> (Oya et al., 2014)</cite> . The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster. ---------------------------------- **COMMUNITY CREATION**",
  "y": "extends"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_5",
  "x": "**SENTENCE RANKING** Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models. In this paper, different from<cite> (Oya et al., 2014)</cite> , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries. ---------------------------------- **EXPERIMENTS AND RESULTS**",
  "y": "differences"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_6",
  "x": "The AMI meeting corpus (Carletta et al., 2006 ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following<cite> (Oya et al., 2014)</cite> , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation. The LUNA Human-Human corpus (Dinarelli et al., 2009 ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone. The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog). For the Call Centre Conversation Summarization (CCCS) shared task a set of 100 dialogs was manually translated to English.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_7",
  "x": "The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries. For AMI corpus, following<cite> (Oya et al., 2014)</cite> , we report ROUGE-2 F-measures on 3-fold cross-validation. For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%. Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall. For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004) .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_8",
  "x": "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2. Following the Call-Center Conversation Summarization Shared Task at MultiLing 2015 , for LUNA Corpus (Dinarelli et al., 2009) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (Trione, 2014) , and (3) Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998 ) with \u03bb = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in<cite> (Oya et al., 2014)</cite> . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities -<cite> (Oya et al., 2014)</cite> and (Mehdad et al., 2013) ; and our run of the system of<cite> (Oya et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_9",
  "x": "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in<cite> (Oya et al., 2014)</cite> . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities -<cite> (Oya et al., 2014)</cite> and (Mehdad et al., 2013) ; and our run of the system of<cite> (Oya et al., 2014)</cite> . With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus. obtained average F-measure of 0.072.",
  "y": "uses"
 },
 {
  "id": "8f0aab7fd30ffc56cc477b25e6bb16_0",
  "x": "\u2022 Introduction ---------------------------------- **** \u2022 Introduction Michael Ellsworth (International Computer Science Institute, infinity@icsi.berkeley.edu) has been involved with FrameNet for well over a decade. His chief focus is on semantic relations in FrameNet (Ruppenhofer et al. 2006) , how they can be used for paraphrase <cite>(Ellsworth & Janin 2007)</cite> , and mapping to other resources (Sche\u21b5czyk & Ellsworth 2006; Ferr\u00e1ndez et al. 2010b) .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_0",
  "x": "Comparison of our approach against VCR baseline. Top row: baseline approach by Zellers et al. <cite>[18]</cite> . Bottom row: our approach. Q:Question, Ac:Correct Answer, Ap: Predicted Answer, R: Predicted Rationale, Q\u2212 > AR: Both answer and rationale prediction given question. well beyond such trivial recognition tasks. By just looking at an image, we are able to deduce many things -contexts, situations, mental states of actors and many more things.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_1",
  "x": "Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 . The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2). This way, the task can be essentially seen as a Visual Question Answering as in both cases the model is trying to predict an answer given the image and query.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_2",
  "x": "The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2). This way, the task can be essentially seen as a Visual Question Answering as in both cases the model is trying to predict an answer given the image and query. Since the rationale prediction module is conditioned on the correct answer while training, the inherent assumption is that the answer prediction network can predict correct answer with 100% accuracy. This assumption is clearly far fetched as even the state-of-the-art Visual Question Answering (VQA) model can barely reach 75% accuracy (Kim et al. [11] ). Moreover, as rationale task is carried out independently of the answer prediction task, it is apparent that the model fails to capture causal reasoning and the \"cognition\" ability.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_3",
  "x": "Additionally, being able to solve this challenging task will help the vision community as a whole to move to the next generation of vision systems which goes beyond normal recognition. The main goal of Visual Commonsense Reasoning (VCR) is to solve this cognition task. Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_4",
  "x": "We use this sampled answer appended to the question as \"question\" to the rationale module. We make the end-to-end network differentiable using expectation loss. \u2022 Direct Cross Entropy -We train the model to directly predict the correct rationale and answers, given the question and all sixteen pairs of answers and rationales. By adopting these methods, we can get rid of the assumption that answer prediction network has to be 100% accurate. Concurrently, it makes our approaches incomparable to the baselines provided by Zellers et al. <cite>[18]</cite> .",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_5",
  "x": "---------------------------------- **RELATED WORK** The task in Zellers et al. <cite>[18]</cite> is essentially posed as a question-answering task. Although they have enforced reasoning for the network, the reasoning is still in the questionanswer format. As such, it makes sense to explore current work in visual question answer domain.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_6",
  "x": "The method, while provided good baseline performance, was naive in its effort to jointly learn the combined meaning of question and image representation. Maaten et al. [9] improved upon the results of [2] , by concatenating the question features, image features and response features together, followed by a MLP and softmax. They posed the task as a \"yes\" or \"no\" answer by training on question, image and response triplet. This method again naively approached the task of combining question and image features by only concatenating them. Anderson et al. [1] propose an orthogonal work to <cite>[18]</cite> , in which Faster-RCNN [15] is used to predict the image regions the model should attend to.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_7",
  "x": "They posed the task as a \"yes\" or \"no\" answer by training on question, image and response triplet. This method again naively approached the task of combining question and image features by only concatenating them. Anderson et al. [1] propose an orthogonal work to <cite>[18]</cite> , in which Faster-RCNN [15] is used to predict the image regions the model should attend to. We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset <cite>[18]</cite> in form of bounding box and segmentation maps. Akira et al. [6] propose a more sophisticated method to combine the feature vectors from questions and images.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_8",
  "x": "Zellers et al. <cite>[18]</cite> propose to jointly learn language and image representation using Bi-LSTM by feeding in image features from CNN for all annotated words. They call this step grounding. Further, query and responses is contextualized using attention mechanism. Finally, the attended query, attended image and response is passed through a Bi-LSTM to make final predictions. One major drawback of the work is that separate networks are trained to predict answers and to reason.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_9",
  "x": "One major drawback of the work is that separate networks are trained to predict answers and to reason. We seek to build on <cite>[18]</cite> by proposing a method to jointly train prediction and reasoning networks. We first choose the correct answer based on the predicted probability distribution using image and question. Then, a combined representation of image, question and chosen answer is fed to reasoning network to select the correct reason. It is to be noted that the step involving choosing an answer to feed to reasoning network is non-differentiable.",
  "y": "extends"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_10",
  "x": "We use the same backbone architecture to predict answers and rationales as in <cite>[18]</cite> . The questions and response (answers and rationales) are provided as a combination of natural language words and tags for annotated objects in the image. Word embedding for question q and response r are calculated using BERT [5] . Image features for annotated objects are calculated using ResNet50 [7] . The word embedding and image features for tagged objects in the question and response are fed to a bi-directional LSTM [8] .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_11",
  "x": "Image features for annotated objects are calculated using ResNet50 [7] . The word embedding and image features for tagged objects in the question and response are fed to a bi-directional LSTM [8] . This learns a joint language-visual representation vector. <cite>[18]</cite> call this step Grounding. Next, the response vector is contextualized against the question vector using attention mechanism.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_12",
  "x": "The dataset used in all experiments in this work is VCR 1.0 <cite>[18]</cite> . VCR dataset contains 290k multiple choice questions which has been collected from 110k movie scenes. The dataset provides object annotations, labels and classes for all objects in the image. The questions, answers and rationales are quite open ended. A lot of questions seek to ask 'Why?' making the task non-trivial.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_13",
  "x": "We train the model using the whole VCR dataset for 20 epochs as was done in <cite>[18]</cite> to align with the baselines. We also use gradient clipping while training. We use two losses for all our methods -answer prediction loss and rationale prediction loss, corresponding to each module. All our results have been reported on the validation set of the dataset as test set labels are not available since it's an ongoing challenge. We report test set results only for our best model which was submitted to the leaderboard.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_14",
  "x": "We define certain terms which we are going to use henceforth: Q->A -answer prediction network, given question and image, QA->R -rationale prediction network, given question, image and answer, Q->AR -rnswer and rationale both prediction network, given question and image. ---------------------------------- **BASELINES** As mentioned earlier, our approach is not directly comparable to the baseline provided by Zellers et al. <cite>[18]</cite> since they feed the correct answer to the rationale module while we feed in the predicted answer. As such, we generate new baseline model.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_15",
  "x": "Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows: \u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_16",
  "x": "For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows: \u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'. Basically, it takes a query, response, and image features as inputs and trains by passing the result through MLP layer.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_17",
  "x": "We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows:",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_18",
  "x": "These baselines are as follows: \u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'. Basically, it takes a query, response, and image features as inputs and trains by passing the result through MLP layer. \u2022 Bottom-up and Top-down attention (BottomUpTop-Down) [1] : <cite>[18]</cite> adopted this model as another baseline by passing object regions referenced by the query and response. The main model attends over region proposals given by an object detector.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_19",
  "x": "It is to be noted that for the QA->R task, we don't provide the correct answer as input to the model. Rather, a weighted average of answers (according to probabilities predicted by Q->A module) is provided to the rationale prediction module, unlike the baseline <cite>[18]</cite> , which gives the correct answer as input. Gumbel-Softmax: For gumbel-softmax, we anneal the temperature \u03c4 from 5 to 1 for 10 epochs and then keep it constant at 1. As can be seen from 1, this model gives the best result among all the approaches we used. Again, we provided the gumbel-softmax weighted average of answer representation to the rationale prediction module rather than the correct answer as was used in the baseline.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_20",
  "x": "As can be seen, our best model (gumbel-softmax) performs strongly over the new baseline. It's better in all the three tasks -1% better in Q->A task, 3.5% better in QA->R task and 2% better in Q->AR task. We conclude from this that gradient flow between the two Q->A and QA->R modules enabled by our end-to-end joint learning scheme, helps the network learn better answers for better/correct reasons. Comparison with State-of-the-art: We provide comparison of our best model against state-of-the-art for visual common sense reasoning task. We also summarize results of other baselines reported in <cite>[18]</cite> .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_21",
  "x": "It's better in all the three tasks -1% better in Q->A task, 3.5% better in QA->R task and 2% better in Q->AR task. We conclude from this that gradient flow between the two Q->A and QA->R modules enabled by our end-to-end joint learning scheme, helps the network learn better answers for better/correct reasons. Comparison with State-of-the-art: We provide comparison of our best model against state-of-the-art for visual common sense reasoning task. We also summarize results of other baselines reported in <cite>[18]</cite> . As can be seen from table 3, our Gumbel-softmax method performs better than the baseline <cite>[18]</cite> in Q->A task.",
  "y": "differences"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_0",
  "x": "We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach<cite> (Chollampatt and Ng, 2018)</cite>, we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%. ---------------------------------- **INTRODUCTION AND PREVIOUS WORK** In the past decade, there has been a great deal of research on grammatical error correction for English including a series of shared tasks, Helping Our Own in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012) and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013 (Ng et al., , 2014 , which have contributed to the development of larger English GEC corpora.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_1",
  "x": "In the past decade, there has been a great deal of research on grammatical error correction for English including a series of shared tasks, Helping Our Own in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012) and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013 (Ng et al., , 2014 , which have contributed to the development of larger English GEC corpora. On the basis of these resources along with advances in machine translation, the current state-of-the-art English GEC systems use ensembles of neural MT models<cite> (Chollampatt and Ng, 2018)</cite> and hybrid systems with both statistical and neural MT models (Grundkiewicz and Junczys-Dowmunt, 2018) . In addition to using gold GEC corpora, which are typically fairly small in the context of MTbased approaches, research in GEC has taken a number of alternate data sources into consideration such as artificially generated errors (e.g., Wagner et al., 2007; Foster and Andersen, 2009; Yuan and Felice, 2013) , crowd-sourced corrections (e.g., Mizumoto et al., 2012) , or errors from native language resources (e.g., Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014) . For English, Grundkiewicz and JunczysDowmunt (2014) extracted pairs of edited sentences from the Wikipedia revision history and filtered them based on a profile of gold GEC data in order to extend the training data for a statistical MT GEC system and found that the addition of filtered edits improved the system's F 0.5 score b\u1ef9 2%. For languages with more limited resources, native language resources such as Wikipedia offer an easily accessible source of additional data.",
  "y": "background"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_2",
  "x": "Using a similar approach that extends existing gold GEC data with Wikipedia edits, we develop a neural machine translation grammatical error correction system for a new language, in this instance German, for which there are only small gold GEC corpora but plentiful native language resources. ---------------------------------- **DATA AND RESOURCES** The following sections describe the data and resources used in our experiments on GEC for German. We create a new GEC corpus for German along with the models needed for the neural GEC approach presented in <cite>Chollampatt and Ng (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_3",
  "x": "**BPE MODEL AND SUBWORD EMBEDDINGS** We learn a byte pair encoding (BPE) (Sennrich et al., 2016) with 30K symbols using the corrections from the Falko-MERLIN training data plus the complete plain Wikipedia article text. As suggested by <cite>Chollampatt and Ng (2018)</cite> , we encode the Wikipedia article text using the BPE model and learn fastText embeddings (Bojanowski et al., 2017) with 500 dimensions. ---------------------------------- **LANGUAGE MODEL**",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_4",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We evaluate the effect of extending the Falko-MERLIN GEC Corpus with Wikipedia edits for a German GEC system using the multilayer convolutional encoder-decoder neural network approach from <cite>Chollampatt and Ng (2018)</cite> , using the same parameters as for English. 8 We train a single model for each condition and evaluate on the Falko-MERLIN test set using M 2 scorer (Dahlmeier and Ng, 2012 The results, presented in Table 3 , show that the addition of both unfiltered and filtered Wikipedia edits to the Falko-MERLIN GEC training data lead to improvements in performance, however larger numbers of unfiltered edits (>250K) do not consistently lead to improvements, similar to the results for English in Grundkiewicz and JunczysDowmunt (2014) . However for filtered edits, increasing the number of additional edits from 100K to 1M continues to lead to improvements, with an overall improvement of 5.2 F 0.5 for 1M edits over the baseline without additional reranking.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_6",
  "x": "---------------------------------- **CONCLUSION** We provide initial results for grammatical error correction for German using data from the Falko and MERLIN corpora augmented with Wikipedia edits that have been filtered using a new German extension of the automatic error annotation tool ERRANT (Bryant et al., 2017) . Wikipedia edits are extracted using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014) , profiled with ER-RANT, and filtered with reference to the gold GEC data. We evaluate our method using the multilayer convolutional encoder-decoder neural network GEC approach from <cite>Chollampatt and Ng (2018)</cite> and find that augmenting a small gold German GEC corpus with one million filtered Wikipedia edits improves the performance from 39.22 to 44.47 F 0.5 and additional language model reranking increases performance to 45.22.",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_0",
  "x": "Testing data is TREC-10. In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet. <cite>Li and Roth (2002)</cite> use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type. The taxonomy consists of 6 coarse and 50 fine semantic classes. The training corpus used consists of 5,500 questions.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_1",
  "x": "The input to the classifiers is a list of features. The features used were words, part-ofspeech tags, chunks, named entities, head chunks (e.g. the first noun chunk in a sentence), and semantically related words (words that often occur with a specific question class). Apart from these primitive features, a set of operators were used to compose more complex features. Zhang and Lee (2003) used the same taxonomy as <cite>Li and Roth (2002)</cite> , as well as the same training and testing data. In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Na\u00efve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_2",
  "x": "This splits the multi-class data into m binary class data. Therefore, m SVM classifiers can be designed and their output combined. The SVM:s also used linear kernels. The same taxonomy, training and testing data was used as in <cite>Li and Roth (2002)</cite> ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_3",
  "x": "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions.",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_4",
  "x": "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_5",
  "x": "The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions. The AnswerBus corpus consists of 25,000 questions.",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_6",
  "x": "---------------------------------- **EXPERIMENT 1** The first experiment is intended to be a straightforward re-examination of previous work to establish what differences in performance there really are between machine learners. This experiment has been done under two different settings. First, we have used the corpus originally developed by<cite> (Li and Roth, 2002)</cite> , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data.",
  "y": "extends differences"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_7",
  "x": "In terms of significant differences between classifiers, the results from the AnswerBus corpus deviates from what could have been expected given the results on the TREC corpora. It seems than Na\u00efve Bayes, Decision Trees and Support Vector Machines are on par with each other, while k Nearest Neighbours and Sparse Network of Winnows are sigificantly worse in terms of performance. ---------------------------------- **CONCLUSIONS** The results in this paper indicate that some of the results found in previous work<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) on question classification might be incorrect due to an unbiased training and test corpus.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_0",
  "x": "We propose a system for generating fluent and natural questions from a KB, which significantly reduces the human effort by leveraging massive web resources. In more detail, a seed question set is first generated by applying a small number of hand-crafted templates on the input KB, then more questions are retrieved by iteratively forming already obtained questions as search queries into a standard search engine, before finally questions are selected by estimating their fluency and domain relevance. Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of <cite>Serban et al. (2016)</cite> by human graders. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_1",
  "x": "Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work (Seyler et al., 2015; <cite>Serban et al., 2016</cite> ) relies on massive human-labeled data. Treating question generation as a machine translation problem, <cite>Serban et al. (2016)</cite> train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs. At test time, input triples are \"translated\" into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_2",
  "x": "There has been much literature so far (Chen et al., 2009; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Labutov et al., 2015) studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) , are freely available, and entities and their relations are already present in KBs but not for texts. Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work (Seyler et al., 2015; <cite>Serban et al., 2016</cite> ) relies on massive human-labeled data. Treating question generation as a machine translation problem, <cite>Serban et al. (2016)</cite> train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_3",
  "x": "The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) . Evaluated by 3 human graders, questions generated by our system are significantly better then <cite>Serban et al. (2016)</cite> on grammaticality and naturalness.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_4",
  "x": "Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) . Evaluated by 3 human graders, questions generated by our system are significantly better then <cite>Serban et al. (2016)</cite> on grammaticality and naturalness. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_5",
  "x": "In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to E if E does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with I max (Line 4). The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model (Mikolov and Dean, 2013 ) and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For System grammatical naturalness <cite>Serban et al. (2016)</cite> 3.36 3.14 Ours 3.53 3.31 Table 1 : Comparing generated questions domain relevance, we take the seed question set as the in-domain data D in , the domain relevance of expanded question q is defined as:",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_6",
  "x": "**EXPERIMENTS** We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method <cite>(Serban et al., 2016)</cite> on Freebase (Bollacker et al., 2008) , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our endto-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_7",
  "x": "In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our endto-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs. ---------------------------------- **EVALUATION ON FREEBASE** We first compare our system with <cite>Serban et al. (2016)</cite> on 500 randomly selected triples from Freebase (Bollacker et al., 2008) 2 .",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_8",
  "x": "To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> . We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_9",
  "x": "We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_10",
  "x": "Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from <cite>Serban et al. (2016)</cite> Ma et al. (2015) 85.48 Ours 85.65 Table 3 : Precision on the web snippet dataset was someone who was involved in the leukemia ?\" and \"whats the title of a book of the subject of the bible ?\"), unnatural (\"what 's one of the mountain where can you found in argentina in netflix ?\") or confusing (\"who was someone who was involved in the leukemia ?\"). ---------------------------------- **DOMAIN RELEVANCE**",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_11",
  "x": "The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from <cite>Serban et al. (2016)</cite> Ma et al. (2015) 85.48 Ours 85.65 Table 3 : Precision on the web snippet dataset was someone who was involved in the leukemia ?\" and \"whats the title of a book of the subject of the bible ?\"), unnatural (\"what 's one of the mountain where can you found in argentina in netflix ?\") or confusing (\"who was someone who was involved in the leukemia ?\").",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_12",
  "x": "**CONCLUSION** We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from <cite>Serban et al. (2016)</cite> on 500 random-selected triples from Freebase. We also demonstrated generated questions from our in-house KB of power tool domain, which are fluent and domain-relevant in general.",
  "y": "differences"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_0",
  "x": "**LEXICAL DISTRIBUTION IN SPOKEN MANDARIN** Results presented by <cite>Tseng (2001)</cite> show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers. All subjects used these words more than three times. The occurrences of these 30 core words make up about 80% of the overall tokens in conversation. Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric<cite> (Tseng 2001)</cite> .",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_1",
  "x": "The occurrences of these 30 core words make up about 80% of the overall tokens in conversation. Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric<cite> (Tseng 2001)</cite> . For instance, verbs \"is located\", \"is\", \"that is\", \"say\", \"want\" and \"have\" were frequently used, so were pronouns \"s/he\", \"you\" and \"I\". The negation \"don't have\" was a high-frequency word, so were words \"right\", \"this/these\" and \"that/those\". Grammatical particles as well as discourse particles were also among the core words.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_2",
  "x": "In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993) . Discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers. In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_3",
  "x": "In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993) . Discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers. In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_4",
  "x": "In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation.",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_5",
  "x": "This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation. Namely, there exist discourse particles appearing preferably in turn-beginning position and some other discourse particles may exclusively mark the location of repairs. Regarding the small size of data used in <cite>Tseng (2001)</cite> , it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_6",
  "x": "---------------------------------- **1.3** Taking Turns in Dialogues In spontaneous conversation, turn-taking usually takes place arbitrarily to the extent that every individual interacts differently with the others under different circumstances. Thus, how to annotate overlapping sequences is one of the essential tasks in developing annotation systems. In Mandarin conversation, there are words preferably used in turn-initial position<cite> (Tseng 2001</cite> , Chui 2000 .",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_7",
  "x": "---------------------------------- **1.4** Prosody in Spoken Mandarin Lexical tones are typically characteristic of spoken Mandarin. The interaction of lexical tones and the other prosodic means such as stress and intonation are related to a number of research issues, particularly in conversation. Falling tones may not show falling tendency anymore, when the associated words are used for specific discourse functions such as for indicating hesitation or the beginning of a turn<cite> (Tseng 2001)</cite> .",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_0",
  "x": "The production of knowledge bases and the need to answer questions over such resources received researchers attentions to propose different models to find the answer of questions from the knowledge bases, known as KBQA 1 . Answering factoid questions with one relation, also known as simple question answering, has been widely studied in recent years (Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> . A common approach that has been used in most of the researches is utilizing a two-component system, including an entity linker and a relation extractor. In this paper, we focus on the relation extraction component, which is also treated as a classification problem. This topic demands certain tools to capture relation that is mentioned in the questions, as a part of the QA systems. In this paper, we aim to view this kind of relation prediction.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_1",
  "x": "For instance, in the question \"Which artist recorded georgia?\", \"artist\" conveys the topic and \"georgia\" stands for the first entity. In this context, extracting relation from single-relation questions obtains higher accuracy compared to multi-relational ones. Having a large number of relations in a knowledge base, however, this simple question relation extraction is not a solved problem yet. Classifying questions to predefined set of relations is one of the main approaches for this task (Mohammed et al., 2018) . Moreover, matching question content with relations has also been proposed and shown promising results (Yin et al., 2016;<cite> Yu et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_2",
  "x": "**QUESTION ANSWERING OVER KNOWLEDGE BASE** One paradigm in proposed approaches for relation extraction in KBQA is based on semantic parsing in which questions were parsed and turned into logical forms in order to query the knowledge base (Berant et al., 2013; Berant and Liang, 2014) . However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_3",
  "x": "From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention. To this aim, they used ---------------------------------- **NEURAL TEXT MATCHING**",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_4",
  "x": "The architecture of our model is depicted in Figure 1 . As can be seen, represents the similarity of two questions (Q' with Q). Additionally, following <cite>Yu et al. (2017)</cite> , we add another neural network (Q'-R), the right part of the architecture, to compute the matching score of Q' with the relation of Q (R). By doing so, we are enhancing the matching signals between Q' and Q to estimate the overall score. In the first step, our proposed model projects the input question as well as the available questions and relations of training data into an embedding space.",
  "y": "extends differences"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_5",
  "x": "Following the previous works by Yin et al. (2016) and <cite>Yu et al. (2017)</cite> , we use the common benchmark dataset of the simple question answering, namely SimpleQuestions, which was originally introduced by Bordes et al. (2015) . This dataset contains 108442 questions gathered with the help of English-speaking annotators. Yin et al. (2016) proposed a new benchmark for evaluating relation extraction task on SimpleQuestion. In this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels. We use the same dataset which contains 72239, 10310 and 20610 question samples as train, validation, and test sets respectively.",
  "y": "similarities uses"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_7",
  "x": "In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task. Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM. BiCNN (Yih et al., 2015) uses convolutional neural networks for matching a question with relations. The model is reimplemented for SimpleQuestions by <cite>Yu et al. (2017)</cite> . As it is shown, our proposed model outperforms the state-of-the-art models in relation extraction for SimpleQuestions dataset by a margin of 0.11 percentage.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_8",
  "x": "In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task. Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM. BiCNN (Yih et al., 2015) uses convolutional neural networks for matching a question with relations. The model is reimplemented for SimpleQuestions by <cite>Yu et al. (2017)</cite> . As it is shown, our proposed model outperforms the state-of-the-art models in relation extraction for SimpleQuestions dataset by a margin of 0.11 percentage.",
  "y": "similarities uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_0",
  "x": "However, neither of these are suitable for fact checking a claim made in natural language against a database. Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims<cite> (Vlachos and Riedel, 2015)</cite> . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work. We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from <cite>Vlachos and Riedel (2015)</cite> .",
  "y": "background"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_1",
  "x": "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims<cite> (Vlachos and Riedel, 2015)</cite> . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work. We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from <cite>Vlachos and Riedel (2015)</cite> . We make the source code publicly available to the community.",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_2",
  "x": "**DESIGN CONSIDERATIONS** We developed our fact-checking approach in the context of the HeroX challenge 2 -a competition organised by the fact checking organization FullFact 3 . The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 . To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of <cite>Vlachos and Riedel (2015)</cite> who used distant supervision (Mintz et al., 2009 ) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015.",
  "y": "extends"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_3",
  "x": "Thus we ensured that the system can readily incorporate new tables taken from encyclopedic sources such as Wikipedia and the World Bank. In our system, this step is achieved by simply importing a CSV file and running a script to generate the new instances to train the relation matching classifier. Analysis of our entry to this competition showed that two errors were caused by incorrect initial source data and one partial error caused by recalling a correct property but making an incorrect deduction. Of numerical claims that we did not attempt, we observed that many required looking up multiple entries and performing a more complex deduction step which was beyond the scope of this project. We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by<cite> (Vlachos and Riedel, 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_4",
  "x": "---------------------------------- **CONCLUSIONS AND FUTURE WORK** The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB. Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set<cite> (Vlachos and Riedel, 2015)</cite> and on real-world claims presented as part of the HeroX fact checking challenge. In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types.",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_0",
  "x": "The lexicon can be considered the most dynamic part of all linguistic knowledge sources over time. There are two innovative change strategies typical for lexical systems: the creation of entirely new lexical items, commonly reflecting the emergence of novel ideas, technologies or artifacts, on the one hand, and, on the other hand, shifts in the meaning of already existing lexical items, a process which usually takes place over larger periods of time. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . This work is based on the assumption that the measurement of semantic change patterns can be reduced to the measurement of lexical similarity between lexical items.",
  "y": "background"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_1",
  "x": "Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) , are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015) . Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item \"gay\" with the meaning dimension of \"homosexuality\"<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . We here investigate the accuracy and reliability of such similarity judgments derived from different training protocols dependent on word frequency, word ambiguity and the number of training epochs (i.e., iterations over all training material).",
  "y": "background motivation"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_2",
  "x": "**RELATED WORK** Neural language models for tracking semantic changes over time typically distinguish between two different training protocols-continuous training of models<cite> (Kim et al., 2014)</cite> where the model for each time span is initialized with the embeddings of its predecessor, and, alternatively, independent training with a mapping between models for different points in time (Kulkarni et al., 2015) . A comparison between these two protocols, such as the one proposed in this paper, has not been carried out before. Also, the application of such protocols to non-English corpora is lacking, with the exception of our own work relating to German data (Hellrich and Hahn, 2016b; Hellrich and Hahn, 2016a) . The word2vec algorithm is a heavily trimmed version of an artificial neural network used to generate low-dimensional vector space representations of a lexicon.",
  "y": "background motivation"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_3",
  "x": "Approaches to modelling changes of lexical semantics not using neural language models, e.g., Wijaya and Yeniterzi (2011), Gulordava and Baroni (2011), Mihalcea and Nastase (2012) , Riedl et al. (2014) or Jatowt and Duh (2014) are, intentionally, out of the scope of this paper. In the same way, we here refrain from comparison with computational studies dealing with literary discussions related to the Romantic period (e.g., Aggarwal et al. (2014) ). ---------------------------------- **EXPERIMENTAL SET-UP** For comparability with earlier studies<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) , we use the fiction part of the GOOGLE BOOKS NGRAM corpus (Michel et al., 2011; Lin et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_4",
  "x": "We thus concentrate on two experimental protocols-the one described by <cite>Kim et al. (2014)</cite> (referred to as Kim protocol) and the one from Kulkarni et al. (2015) (referred to as Kulkarni protocol), including close variations thereof. Kulkarni's protocol operates on all 5-grams occurring during five consecutive years (e.g., [1900] [1901] [1902] [1903] [1904] and trains models independently of each other. Kim's protocol operates on uniformly sized samples of 10M 5-grams for each year from 1850 onwards in a continuous fashion (years before 1900 are used for initialization only). Its constant sampling sizes result in both oversampling and undersampling as is evident from Figure 1 . We use the PYTHON-based GENSIM 1 implementation of word2vec for our experiments; the relevant code is made available via GITHUB.",
  "y": "background uses"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_0",
  "x": "**SYSTEM DESCRIPTION** We trained Neural Machine Transaltion systems using the encoder-decoder architecture with attention (Bahdanau et al., 2014) for English-Hindi as well Hindi-English translation. We compared convolutional neural network (ConvS2S)<cite> (Gehring et al., 2017)</cite> and recurrent neural network (RNNS2S) (Bahdanau et al., 2014) based sequence to sequence learning architectures. While RNN based architectures have proved to be successful and produce state-of-the-art results for machine translation, they take a long time to train. The temporal dependencies between the elements in the sequence due to the RNN state vector requires sequential processing.",
  "y": "uses"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_1",
  "x": "On the other hand, different parts of the sequence can be processed in parallel using a ConvS2S. Hence, it is appealing to explore ConvS2S as the basis of an architecture to speed up training and decoding. Recent work<cite> (Gehring et al., 2017)</cite> has shown that a purely CNN based encoder-decoder network is competitive with a RNN based network. ---------------------------------- **RECURRENT SEQUENCE TO SEQUENCE MODEL (RNNS2S)**",
  "y": "background motivation"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_2",
  "x": "---------------------------------- **CONVOLUTIONAL SEQUENCE TO SEQUENCE MODEL (CONVS2S)** In convolutional sequence to sequence model<cite> (Gehring et al., 2017)</cite> , the input sequence is encoded into distributional vector space using a CNN and decoded back to output sequence again using CNN instead of RNN (Sutskever et al., 2014) . Each input element embedding is combined with its positional embedding (signifies the position of the input element). Positional embeddings help the network to realize what part of input it is dealing with, currently.",
  "y": "background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_3",
  "x": "The baseline model with 4 encoder layers and 3 decoder layers was trained using nag optimizer<cite> (Gehring et al., 2017</cite> ) with a learning rate of 0.25 with 0.2 as its dropout value and gradient clipping was also applied. Table 6 : Hindi to English Translation Systems at WAT2017 The inferencing was done using beam search with a beam size of 10 for both Hindi-English and English-Hindi translation task. The model was also trained with more number of layers in the encoder and the decoder. The resulting BLEU scores for different number of encoder and decoder layers are shown in Table 4 .",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_0",
  "x": "Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).",
  "y": "differences"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_1",
  "x": "**SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_2",
  "x": "We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_3",
  "x": "It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments.",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_4",
  "x": "For each token, we convert the distance to the two arguments of the example to two position embeddings. We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by Zeng et al. (2014) . The entity embedding <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i \u2208 [0, T ), T is the length of the sentence.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_5",
  "x": "The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden ----------------------------------",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_6",
  "x": "It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden ---------------------------------- **LEARNING UNIFIED REPRESENTATION**",
  "y": "similarities uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_8",
  "x": "The ACE05 dataset provides a cross-domain evaluation setting . It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_9",
  "x": "We follow<cite> Nguyen and Grishman (2016)</cite> to set the position and entity type embedding size to be 50. We use 150 dimensions for the GRU state, 100 dimensions for the word context vector and use 300 dimensions for the hidden layer in the decoders. We train the model using Adam (Kingma and Ba, 2014) optimizer with learning rate 0.001. We tune \u03bb linearly from 0 to 1, and \u03b2 logarithmically from 5 \u00b7 10 \u22121 to 10 \u22124 For all scores, we run experiments 10 times and take the average. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_10",
  "x": "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data. A common way to do so is pre-training.",
  "y": "future_work"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_11",
  "x": "**AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data.",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_12",
  "x": "Since ACE05 has been studied for a long time, numerous features have been found to be effective on this dataset. (Nguyen and Grishman, 2016) incorporated some of those features into the neural net and beat the state-of-art on the dataset. Although representation learning from scratch could be more general across multiple datasets, we compare the effect of multi-task learning with extra features on this specific dataset. We add chunk embedding and on dep path embedding<cite> (Nguyen and Grishman, 2016)</cite> . Similar to entity type embedding, chunk embedding is created according to each token's chunk type, we set the embedding size to 50.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_0",
  "x": "As Gruner (1985) said, humor in public speakings will \"produce a more favorable reaction toward a speaker\" and \"enhance speaker image.\" However, there is no guarantee that the expected reactions would occur in an actual talk. If an automatic system can provide audience reactions which are likely to occur in actual talks, it will be helpful in the process of preparing a talk. In this study, we investigated the feasibility of current NLP technologies in building a system which provides expected audience reactions to public speaking. Studies on automatic humor recognition (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006 ) have defined the recognition task as a binary classification task. So, their classification models categorized a given sentence as a humorous or non-humorous sentence.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_1",
  "x": "So, their classification models categorized a given sentence as a humorous or non-humorous sentence. Among the studies on humor classification, Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> reported high performance on the task. Considering the performance of their systems, it is reasonable to test the applicability of their models to a real application. In this study, we specifically applied a state-of-the-art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters. In our application of the state-of-art system to talks, we could not achieve a comparable performance to the reported performance of the system.",
  "y": "background motivation"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_2",
  "x": "In our application of the state-of-art system to talks, we could not achieve a comparable performance to the reported performance of the system. We investigated the potential reasons for the performance difference through further analysis. Some humor classification studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Barbieri and Saggion, 2014 ) have used negative instances from different domains or topics, because non-humorous sentences could not be found or are very challenging to collect in target domains or topics. Their studies showed that it was possible to achieve promising performance using data from heterogeneous domains. However, our study showed that humorous sentences which were semantically close to non-humorous sentences were very challenging to distinguish.",
  "y": "background motivation differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_3",
  "x": "Previous studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006; Bertero and Fung, 2016) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous. These studies collected textual data which consisted of humorous texts and non-humorous texts and built a classification model using textual features. Humorous and non-humorous texts were from different domains across the studies. Pun websites, daily joke websites, or tweets were used as sources of humorous texts. Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_4",
  "x": "Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts. <cite>Yang et al. (2015)</cite> tried to minimize genre differences between humorous and non-humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences. Barbieri and Saggion (2014) examined cross-domain application of humor detection systems using Twitter data. For example, they trained a model using tweets with '#humor' and '#education' hashtags and evaluated the performance of the model on evaluation data containing tweets with '#humor' and '#politics' hashtags. They also reported promising performance in the cross-domain application.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_5",
  "x": "Their study was the first study where a deep learning algorithm was used for humor classification. In this study, our target task was to categorize sentences in talk data into humorous and non-humorous sentences. We only examined textual features. Compared to previous studies, one innovation of this study was that a trained model was evaluated using humorous and non-humorous sentences from the same genre and same topic. Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites or proverbs.",
  "y": "differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_6",
  "x": "<cite>Yang et al. (2015)</cite> collected a corpus of Pun of Day data 1 . The data consisted of 2,423 humorous (positive) texts and 2,403 non-humorous (negative) texts. The humorous texts were from the Pun of the Day website, and the negative texts from AP News2, New York Times, Yahoo! Answers and Proverb websites. Examples of humorous and non-humorous sentences are given below. Humorous The one who invented the door knocker got a No-bell prize. Non-Humorous The one who discovered/invented it had the last name of fahrenheit.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_7",
  "x": "Humorous The one who invented the door knocker got a No-bell prize. Non-Humorous The one who discovered/invented it had the last name of fahrenheit. In order to reduce the differences between positive and negative instances in the data, <cite>Yang et al. (2015)</cite> used two constraints when collecting negative instances. Non-humorous texts were required to have lengths between the minimum and maximum lengths of positive instances, in order to be selected as negative instances. In addition, only non-humorous texts which consisted of words found in positive instances were collected.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_8",
  "x": "The average length of 'Pun of the Day' data was 14 words, with a standard deviation of 5. The number of humorous sentences left after removing sentences with fewer than seven words was 4,726. Utilizing the same experimental setup as Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> (50% positive and 50% negative instances), we selected 4,726 sentences from among all collected nonhumorous sentences as negative instances. During selection, we minimized differences between positive and negative instances. A negative instance was selected from among sentences located close to a positive instance in a talk.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_9",
  "x": "**IMPLEMENTATION OF FEATURES** Features from <cite>Yang et al. (2015)</cite> , which we implemented, consisted of (1) two incongruity features, (2) six ambiguity features, (3) four interpersonal effect features, (4) four phonetic features, (5) five k-Nearest Neighbor features, and (6) 300 Word2Vec features. The total number of features used in this study was 321. We describe our implementation of the features in this section. The justifications for the features can be found in the original paper.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_10",
  "x": "**APPLICATION OF STATE-OF-ART TECHNOLOGY TO TALK DATA** In this section, we present expeirments that we ran to determine 1) how effective a model trained using 'Pun of Day' data (Pun) is when applied to TED Talk data (Talk), and 2) whether the performance of a model trained using Talk data would be similar to the performance reported in <cite>Yang et al. (2015)</cite> . We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data. Considering the different characteristics of Talk data versus Pun data, we sought to investigate whether Yang's model could achieve the reported performance (over 85% accuracy) on our Talk data. The differences were 1) humorous sentences in Talk data were sentences which induced audience laughters, compared to Pun data which used canned textual humor, 2) all non-humorous sentences in Talk data were also from TED talks, and 3) each pair of humorous and non-humorous sentences were semantically close because they were closely placed.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_11",
  "x": "In this section, we present expeirments that we ran to determine 1) how effective a model trained using 'Pun of Day' data (Pun) is when applied to TED Talk data (Talk), and 2) whether the performance of a model trained using Talk data would be similar to the performance reported in <cite>Yang et al. (2015)</cite> . We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data. Considering the different characteristics of Talk data versus Pun data, we sought to investigate whether Yang's model could achieve the reported performance (over 85% accuracy) on our Talk data. The differences were 1) humorous sentences in Talk data were sentences which induced audience laughters, compared to Pun data which used canned textual humor, 2) all non-humorous sentences in Talk data were also from TED talks, and 3) each pair of humorous and non-humorous sentences were semantically close because they were closely placed. These differences made the humor classification task more challenging.",
  "y": "extends differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_12",
  "x": "We followed the experimental setup of <cite>Yang et al. (2015)</cite> in order to see if the performance of our duplicated features was comparable to their reported performance. Their best performance was 85.4% accuracy (Yang in Table 1 ) when they used Random Forest as a classifier and 10-fold cross validation (CV) as an evaluation method. Replicating this experiment setup, we were able to achieve 86.0% accuracy (Pun-to-Pun in Table 1 ), which is slightly better than the performance reported in their paper. The performance difference could be due to the difference in partitions in CV. After verifying the feature implementation, we built a humor recognition model using the entirety of the Pun data.",
  "y": "differences uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_13",
  "x": "The results of our experiments raised questions about why two different results were observed for two different data sets. A major difference in the two data sets was the source of negative instances. <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites and proverbs. But, in Talk-to-Talk, both positive and negative instances were from the same genre. Furthermore, each humorous instance had a corresponding non-humorous instance from the same talk. In this section, we investigate the impact of genre differences in the humor classification task, using Pun and Talk data.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_0",
  "x": "To be an effective teammate, an AI must overcome the challenges involved with adapting to humans; however, progress in AI is routinely measured in isolation, without a human in the loop. In this work, we focus specifically on the evaluation of visual conversational agents and develop a human computation game to benchmark their performance as members of human-AI teams. Visual conversational agents (Das et al. 2017a;<cite> Das et al. 2017b</cite>; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game. At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human. Both ALICE and the human are then provided a brief description of the image.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_1",
  "x": "trained to understand and communicate about the contents of a scene in natural language. For example, in Fig. 1 , the visual conversational agent (shown on the right) replies to answers questions about a scene while inferring context from the dialog history -Human: \"What is he doing?\" Agent: \"Playing frisbee\". These agents are typically trained to mimic large corpora of human-human dialogs and are evaluated automatically on how well they retrieve actual human responses (ground truth) in novel dialogs. Recent work has evaluated these models more pragmatically by evaluating how well pairs of visual conversational agents perform on goal-based conversational tasks rather than response retrieval from fixed dialogs. Specifically, <cite>(Das et al. 2017b</cite> ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_2",
  "x": "<cite>(Das et al. 2017b</cite> ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task. They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining. While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans. GuessWhich. In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_3",
  "x": "Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents. Mirroring the setting of<cite> (Das et al. 2017b)</cite> , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer. At the start of the game, the answerer is provided an image that is unknown to the questioner and both questioner and answerer are given a brief description of the image content. The questioner interacts with the answerer for a fixed number of rounds of question-answer (dialog) to identify the secret image from a fixed pool of images (see Fig. 1 ). We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE).",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_4",
  "x": "We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE). Specifically, we evaluate two versions of ALICE for GuessWhich: 1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (Das et al. 2017a ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2. ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in<cite> (Das et al. 2017b)</cite> . It is important to appreciate the difficulty and sensitivity of the GuessWhich game as an evaluation tool -agents have to understand human questions and respond with accurate, consistent, fluent and informative answers for the human-AI team to do well.",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_5",
  "x": "Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work<cite> (Das et al. 2017b)</cite> , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1). This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter. This is an important finding to guide future research. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_6",
  "x": "**RELATED WORK** Given that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below. Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (Das et al. 2017a;<cite> Das et al. 2017b</cite>; . (Das et al. 2017a ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer). <cite>(Das et al. 2017b</cite> ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_7",
  "x": "<cite>(Das et al. 2017b</cite> ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other. However, (Aras et al. 2010; Chamberlain, Poesio, and Kruschwitz 2008) , movies (Michelucci 2013) etc. While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams. Rather than collecting labeled data, our game is designed to measure the effectiveness of the AI in the context of human-AI teams. Evaluating Conversational Agents.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_8",
  "x": "Rather than collecting labeled data, our game is designed to measure the effectiveness of the AI in the context of human-AI teams. Evaluating Conversational Agents. Goal-driven (nonvisual) conversational models have typically been evaluated on task-completion rate or time-to-task-completion (Paek 2001) , so shorter conversations are better. At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (Liu et al. 2016) . Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in <cite>(Das et al. 2017b</cite> ) and (Li et al. 2016) .",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_10",
  "x": "<cite>(Das et al. 2017b</cite> ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog. At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT). The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool. The goal of the QBOT-ABOT team is two-fold, QBOT should: 1) build a mental model of the unseen image purely from the dialog and 2) be able to retrieve that image from a line-up of images. Both QBOT and ABOT are modeled as Hierarchical Recurrent Encoder-Decoder neural networks (Das et al. 2017a; ) which encode each round of dialog independently via a recurrent neural network (RNN) before accumulating this information through time with an additional RNN (resulting in hierarchical encoding).",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_12",
  "x": "**ALICE SL VS. ALICE RL** We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game. These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog. <cite>(Das et al. 2017b</cite> ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of \u223c10k images (rank \u223c1000 for SL, rank \u223c500 for RL). In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_13",
  "x": "On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better). Error bars are 95% CIs from 1000 bootstrap samples. Unlike<cite> (Das et al., 2017b)</cite> , we find no significant difference between ALICE SL and ALICE RL . cally significant. As we collected additional data, the error margins became smaller but the means also became closer.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_14",
  "x": "cally significant. As we collected additional data, the error margins became smaller but the means also became closer. This interesting finding stands in stark contrast to the results reported by<cite> (Das et al. 2017b)</cite> , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team. Our results suggest that the improvements of RL over SL (in AI-AI teams) do not seem to translate to when the agents are paired with a human in a similar setting. MR with varying number of games.",
  "y": "differences"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_0",
  "x": "Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English. ---------------------------------- **INTRODUCTION** Neural Machine Translation <cite>(Luong et al., 2015</cite>; Bahdanau et al., 2014; Johnson et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts. NMT often outperforms Statistical Machine Translation (SMT) techniques but it still struggles if the parallel data is insufficient like in the case of Indian languages.",
  "y": "background"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_1",
  "x": "Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in<cite> (Luong et al., 2015)</cite> . The model directly estimates the posterior distribution P \u03b8 (y|x) of translating a source sentence x = (x 1 , .., x n ) to a target sentence y = (y 1 , .., y m ) as: Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t : where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as:",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_2",
  "x": "Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in<cite> (Luong et al., 2015)</cite> . The model directly estimates the posterior distribution P \u03b8 (y|x) of translating a source sentence x = (x 1 , .., x n ) to a target sentence y = (y 1 , .., y m ) as: Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t : where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as:",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_3",
  "x": "An NMT model is typically trained under the maximum log-likelihood objective: where D is the training set. Our NMT model uses a bi-directional RNN as an encoder and a unidirectional RNN as a decoder with global attention<cite> (Luong et al., 2015)</cite> . ---------------------------------- **MULTILINGUAL NEURAL MACHINE TRANSLATION**",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_4",
  "x": "We used script conversion only with our additional Multilingual NMT experiments based on Transformer architecture. ---------------------------------- **TRAINING DETAILS** The structure of our NMT model is same as in<cite> Luong et al. (2015)</cite> , an RNN based encoder-decoder model with Global Attention mechanism. We used an LSTM based Bi-directional encoder and a unidirectional decoder.",
  "y": "uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_0",
  "x": "These all start with ===, but not all messages starting with === are system messages, as shown by the second message in Figure 1 . 3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel (Elsner and Charniak, 2008; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by<cite> Mehri and Carenini (2017)</cite> with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009;<cite> Mehri and Carenini, 2017</cite>; Jiang et al., 2018) . We are aware of three other IRC disentanglement datasets. First, Adams and Martell (2008) studied disentanglement and topic identification, but did not release their data.",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_1",
  "x": "Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data. Recent work has applied neural networks <cite>(Mehri and Carenini, 2017</cite>; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison. Our data is much larger than prior work, one of the only released sets, and the only one with context and adjudication. '+a' indicates there was an adjudication step to resolve disagreements. '?' indicates the value is not in the paper and the authors no longer have access to the data. ----------------------------------",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_2",
  "x": "Almost all prior work with annotated graph structures has been for threaded web forums (Schuth et al., 2007; Kim et al., 2010; Wang et al., 2011b) , which do not exhibit the disentanglement problem we explore. Studies that do consider graphs for disentanglement have used small datasets (Dulceanu, 2016;<cite> Mehri and Carenini, 2017)</cite> that are not always released (Wang et al., 2008; Guo et al., 2017) . ---------------------------------- **DATA** We introduce a manually annotated dataset of 77,563 messages: 74,963 from the #Ubuntu IRC channel, 3 and 2,600 messages from the #Linux IRC channel.",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_3",
  "x": "Values are in the good agreement range proposed by Altman (1990) , and slightly higher than for<cite> Mehri and Carenini (2017)</cite>'s annotations. Results are not shown for Elsner and Charniak (2008) because they did not annotate graphs. ---------------------------------- **CONVERSATIONS:** We consider three metrics: 6 (1) Variation of Information (VI, Meila, 2007) .",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_4",
  "x": "(2) One-to-One Overlap (1-1, Elsner and Charniak, 2008) . Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm. We follow<cite> Mehri and Carenini (2017)</cite> and keep system messages. (3) Exact Match F 1 . Calculated using the number of perfectly matching conversations, excluding conversations with only one message (mostly system messages).",
  "y": "similarities uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_5",
  "x": "Interestingly, while \u03ba was higher for us than<cite> Mehri and Carenini (2017)</cite> , our scores for conversations are lower. This is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations. This may reflect the fact that our annotation guide was developed for the Ubuntu channel, which differs in conversation style from the Channel Two data. Manually comparing the annotations, there was no clear differences in the types of disagreements. Agreement is lower on the Channel Two data, particularly on its test set.",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_6",
  "x": "Vote: Run 10 FF models and combine output by keeping the edges they all agree on. Link messages with no agreed antecedent to themselves. Intersect: Conversations that 10 FF models agree on, and other messages as singleton conversations. For Channel Two we also compare to Wang and Oard (2009) and<cite> Mehri and Carenini (2017)</cite> , but their code was unavailable, preventing evaluation on our data. We exclude Jiang et al. (2018) as they substantially modified the dataset.",
  "y": "similarities"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_7",
  "x": "The difficulty of samples varies considerably, with the F-score of our model varying from 11 to 40 and annotator agreement scores before adjudication varying from 0.65 to 0.78. The model performance and agreement levels are also strongly correlated, with a Spearman's rank correlation of 0.77. This demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance. How far apart consecutive messages in a conversation are: Elsner and Charniak (2008) and<cite> Mehri and Carenini (2017)</cite> use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages. Figure 4 shows the distribution of time differences in our conversations.",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_0",
  "x": "Geolocation models are usually trained on the small set of users whose location is known (e.g. through GPS-based geotagging), and other users are geolocated using the resulting model. These models broadly fall into two categories: text-based and network-based methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare.",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_1",
  "x": "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013;<cite> Rahimi et al., 2015)</cite> ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets.",
  "y": "motivation"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_2",
  "x": "Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013;<cite> Rahimi et al., 2015)</cite> ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets. ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_3",
  "x": "As shown by<cite> Rahimi et al. (2015)</cite> , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation. ---------------------------------- **DATA** We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US <cite>(Roller et al., 2012)</cite> , and (3) TWITTER-WORLD (Han et al., 2012) . In each dataset, users are represented by a single meta-document, generated by concatenating their tweets.",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_4",
  "x": "**DATA** We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US <cite>(Roller et al., 2012)</cite> , and (3) TWITTER-WORLD (Han et al., 2012) . In each dataset, users are represented by a single meta-document, generated by concatenating their tweets. The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information. The first two datasets were constructed to contain mostly English messages.",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_5",
  "x": "In the case of the collapsed network of TWITTER-WORLD, k is decreased by a factor of 4000 after setting the celebrity threshold T to 5. We apply celebrity removal over both binary (\"MADCEL-B\") and weighted (\"MADCEL-W\") networks (using the respective T for each dataset). The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error. A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (Zhu and Ghahramani, 2002; Goldberg and Zhu, 2006) . The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of<cite> Rahimi et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_6",
  "x": "Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound. To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region. Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets. The results are also compared with prior work on network-based geolocation using label propagation (LP)<cite> (Rahimi et al., 2015)</cite> , text-based classification models (Han et al., 2012; Wing and Baldridge, 2011;<cite> Rahimi et al., 2015</cite>; Cha et al., 2015) , textbased graphical models (Ahmed et al., 2013) , and network-text hybrid models (LP-LR)<cite> (Rahimi et al., 2015)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_7",
  "x": "**RESULTS** Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification<cite> (Rahimi et al., 2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_8",
  "x": "**RESULTS** Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification<cite> (Rahimi et al., 2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_0",
  "x": "Word2vec <cite>[23]</cite> is a recently proposed family of algorithms for training such vector representations from unstructured text data via shal- * Work done while with Yahoo, Inc. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_1",
  "x": "The geometry of the resulting vectors was shown in <cite>[23]</cite> to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\"Madrid\") -vec(\"Spain\") + vec(\"France\") \u2248 vec(\"Paris\"). More recently, novel applications of word2vec involving unconventional generalized \"words\" and training corpuses have been proposed. These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32] , general text-based attributes [17] , descriptive text of images [18] , nodes in graph structure of networks [27] , and queries [15] , to name a few. While most NLP applications of word2vec do not require training of large vocabularies, many of the above mentioned real-world applications do. For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_2",
  "x": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32] , general text-based attributes [17] , descriptive text of images [18] , nodes in graph structure of networks [27] , and queries [15] , to name a few. While most NLP applications of word2vec do not require training of large vocabularies, many of the above mentioned real-world applications do. For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations. The training of vectors for such large vocabularies presents several challenges. In word2vec, each vocabulary word has two associated d-dimensional vectors which must be trained, respectively referred to as input and output vectors, each of which is represented as an array of d single precision floating point numbers <cite>[23]</cite> .",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_3",
  "x": "---------------------------------- **THE WORD2VEC TRAINING PROBLEM** In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> . Given a corpus consisting of a sequence of sentences s1, s2, . . . , sn each comprising a sequence of words si = wi,1, wi,2, . .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_4",
  "x": "---------------------------------- **THE WORD2VEC TRAINING PROBLEM** In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> . Given a corpus consisting of a sequence of sentences s1, s2, . . . , sn each comprising a sequence of words si = wi,1, wi,2, . .",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_5",
  "x": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_6",
  "x": "over input and output word row vectors u(w) and v(w) with w ranging over the words in the vocabulary V, where: \u2022 \u03c3(\u00b7) denotes the sigmoid function \u03c3(x) = 1/(1 + e \u2212x ); \u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_7",
  "x": "\u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_8",
  "x": "We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}). This is a massive computational task when carried out for multiple iterations over data sets with tens of billions of words, as encountered in applications described in the previous section.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_9",
  "x": "The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}). This is a massive computational task when carried out for multiple iterations over data sets with tens of billions of words, as encountered in applications described in the previous section. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_10",
  "x": "We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}).",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_11",
  "x": "**SINGLE MACHINE** Several existing word2vec training systems are limited to running on a single machine, though with multiple parallel threads of execution operating on different segments of training data. These include the original open source implementation of word2vec <cite>[23]</cite> , as well as those of Medallia [22] , and Rehurek [28] . As mentioned in the introduction, these systems would require far larger memory configurations than available on typical commodity-scale servers. ----------------------------------",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_12",
  "x": "There are on average w \u00b7 n of these per minibatch word. For w = 10, n = 10, d = 500, values within the ranges recommended in <cite>[23]</cite> , this works out to r(10, 10, 500) \u2248 200, 000 bytes transferred per word with each get and put. For 10 iterations of training on a data set of roughly 50 billion words, which is in the middle of the relevant range for the sponsored search application described in Section 2, attaining a total training latency of one week using the above system would require an aggregate bandwidth of at least 1300Gbits/sec to and from the parameter servers 4 . This is impractically large for a single application on a commodity-hardware shared compute cluster. Moreover, one week training latency is already at the boundary of usefulness for our applications.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_13",
  "x": "In the sequel, we shall equate each word w with , its index in this ordering, so that u(w ) \u2261 u( ), and so on. For S shards, the vocabulary size can thus be scaled up by as much as a factor of S relative to a single machine. The vectors are initialized in the parameter server shards as in <cite>[23]</cite> . Multiple clients running on cluster nodes then read in different portions of the corpus and interact with the parameter server shards to carry out minibatch stochastic gradient descent (SGD) optimization of (1) over the word vectors, following the algorithm in Figure 7 (in the appendix). Specifically, the corpus is partitioned into disjoint minibatches with index sets B1, B2, . . . , BN wherein each B h is a subset of (sentence index, word index) pairs.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_14",
  "x": "where \u03b1 is a (suitably small) learning rate. Once a client has assembled the indices (indexing according to the order O above) of positive output examples and input words corresponding to a minibatch B h , it interacts with the parameter server shards to compute (4) and (5) using two remote procedure calls (RPCs), dotprod and adjust, which are broadcasted to all PS shards, along with an intervening computation to aggregate results from the dotprod RPC returned by each shard. The RPC calls are detailed in Figures 5 and 6 (in the Appendix), and, at a higher level, entail the following server/shard side operations: \u2022 dotprod: Select negative examplesw in (4) according to a probability distribution derived from the vocabulary histogram proposed in <cite>[23]</cite> , but with the client thread supplied seed initializing the random number generation, and then return all partial dot products required to evaluate the gradient (4) for all positive output, negative output, and input word vectors associated with the minibatch, wherein the partial dot products involve those vector components stored on the designated shard: usv T s . \u2022 adjust: Regenerate negative examples used in preceding dotprod call using the same seed that is again supplied by the client thread.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_15",
  "x": "The retransmission simplifies the server in that state need not be maintained between corresponding dotprod and adjust calls. Note that the same seeds are sent to all shards in both calls so that each shard generates the same set of negative example indices. The shards are multithreaded and each thread handles the stream of RPC's coming from all client threads running on a single node. In a typical at scale run of the algorithm, the above process is carried out by multiple client threads running on each of a few hundred nodes, all interacting with the PS shards in parallel. The data set is iterated over multiple times and after each iteration, the learning rate \u03b1 is reduced in a manner similar to the open source implementation of <cite>[23]</cite> .",
  "y": "similarities"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_16",
  "x": "To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of <cite>[23]</cite> . This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases. We then randomly shuffled the order of sentences (delimited by new line) in the data set, retaining order of words within each sentence. The resulting data set has about 8 billion words and yields a vocabulary of about 7 million words and phrases (based on a cut off of 5 occurrences in the data set). We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_17",
  "x": "We also show bucket test results on live web search traffic that compare query-ad matching performance of our large-vocabulary model to the one trained using single-machine implementation, which led to the decision to deploy the proposed system in production in late 2015. ---------------------------------- **BENCHMARK DATA SET** To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of <cite>[23]</cite> . This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_18",
  "x": "The resulting data set has about 8 billion words and yields a vocabulary of about 7 million words and phrases (based on a cut off of 5 occurrences in the data set). We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_19",
  "x": "We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set. It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_20",
  "x": "It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches. In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test. We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> .",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_21",
  "x": "Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches. In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test. We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_22",
  "x": "We also compared the cosine similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open-source implementation of <cite>[23]</cite> , again on a large search session data set. The former was trained using a vocabulary of 200 million generalized words while the latter was trained using about 90 million words which is the most that could fit onto a specialized large memory machine. For a set of 7,560 generalized word pairs with words common to the vocabularies trained by the respective systems we found very good agreement in cosine similarities between the corresponding vectors from the two systems, with over 50% of word pairs having cosine similarity differences less than 0.06, and 91% of word pairs having differences less than 0.1. ---------------------------------- **ONLINE A/B TESTS**",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_23",
  "x": "---------------------------------- **ONLINE A/B TESTS** Following successful offline evaluation of the proposed distributed system, in the following set of experiments we conducted tests on live web search traffic. We ran two bucket tests, each on 5% of search traffic, where we compared queryad matches produced by training query and ad vectors using search session data set spanning 9 months of search data. One model was trained using implementation from <cite>[23]</cite> and the other was trained using the proposed distributed system.",
  "y": "uses"
 },
 {
  "id": "99aebc86f34ace0133c9f0922373fe_0",
  "x": "Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; <cite>Gehring et al., 2017)</cite> , CNN (Kalchbrenner et al., 2014; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced. The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding. In particular, the memory network Chien and Lin, 2018) , neural variational learning (Serban et al., 2017; Chung et al., 2015) , neural discrete representation (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017) , recurrent ladder network (Rasmus et al., 2015; Pr\u00e9mont-Schwarz et al., 2017; S\u00f8nderby et al., 2016) , stochastic neural network (Fraccaro et al., 2016; Goyal et al., 2017; Shabanian et al., 2017) , Markov recurrent neural network (Venkatraman et al., 2017; Kuo and Chien, 2018) , sequence GAN (Yu et al., 2017) and reinforcement learning (Tegho et al., 2017) are introduced in various deep models which open a window to more practical tasks, e.g. reading comprehension, sentence generation, dialogue system, question answering and machine translation.",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_0",
  "x": "The proposed model can be viewed as a speech version of Word2Vec <cite>[1]</cite>. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on<cite> 1</cite>3 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_1",
  "x": "The learned word embeddings are evaluated and analyzed on<cite> 1</cite>3 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions. ---------------------------------- **INTRODUCTION** Natural language processing (NLP) techniques such as Word2Vec<cite> [1,</cite> 2] and GloVe [3] transform words into fixed dimensional vectors, or word embeddings. The embeddings are obtained via unsupervised learning from co-occurrence information in text, and contain semantic information about the word which are useful for many NLP tasks [4, 5, 6, 7, 8] .",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_2",
  "x": "The embeddings are obtained via unsupervised learning from co-occurrence information in text, and contain semantic information about the word which are useful for many NLP tasks [4, 5, 6, 7, 8] . Researchers have also explored the concept of learning vector representations from speech [9,<cite> 1</cite>0,<cite> 1</cite>1,<cite> 1</cite>2,<cite> 1</cite>3,<cite> 1</cite>4] . These approaches are based on notions of acoustic-phonetic (rather than semantic) similarity, so that different instances of the same underlying word would map to the same point in a latent embedding space. Our work, highly inspired by Word2Vec <cite>[1]</cite> , uses a skipgrams or continuous bag-of-words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself. We show that the resulting acoustic embedding space is more semantic in nature.",
  "y": "motivation"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_9",
  "x": "The intrinsic method directly tests for semantic or syntactic relationships between words, and includes the tasks of word similarity and word analogy <cite>[1]</cite> . In this paper, we focus on the intrinsic method, especially the word similarity task, for evaluating and analyzing the Speech2Vec word embeddings. We used<cite> 1</cite>3 benchmarks [30] [40] , and SimVerb-3500 [41] . These<cite> 1</cite>3 benchmarks contain different numbers of pairs of English words that have been assigned similarity ratings by humans, and each of them evaluates the word embeddings in terms of different aspects. For example, RG-65 and MC-30 focus on nouns, YC-130 and SimVerb-3500 focus on verbs, and Rare-Word focuses on rare-words.",
  "y": "uses"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_21",
  "x": "From Table<cite> 1</cite> we see that skipgrams Speech2Vec achieves the highest \u03c1 in 8 out of<cite> 1</cite>3 benchmarks, outperforming cbow and skipgrams Word2Vec in combination. We believe a possible reason for such results is due to skipgrams Speech2Vec's ability to capture semantic information present in speech such as prosody that is not in text. Comparing skipgrams to cbow Speech2Vec. From Table<cite> 1</cite> we observe that skipgrams Speech2Vec consistently outperforms cbow Speech2Vec on all benchmarks for all embedding sizes. This result aligns with the empirical fact that skipgrams Word2Vec is likely to work better than cbow Word2Vec with small training corpus size <cite>[1]</cite> .",
  "y": "similarities"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_26",
  "x": "---------------------------------- **VISUALIZATIONS** ---------------------------------- **CONCLUSIONS AND FUTURE WORK** Speech2Vec, which integrates a RNN Encoder-Decoder framework with skipgrams or cbow for training, extends the textbased Word2Vec <cite>[1]</cite> model to learn word embeddings directly from speech.",
  "y": "extends"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_1",
  "x": "Language models (LMs) by themselves are valuable because well trained LMs improve the underlying metrics of downstream tasks such as word error rate for speech recognition, BLEU score for translation. In addition, LMs compactly extract knowledge encoded in training data [7] . The current state of the art on modeling both PTB and WikiText 2 [8] datasets as reported in <cite>[4]</cite> shows little sensitivity to hyper parameters; sharing almost all hyper parameters values between both datasets. In [9] , its is also shown that deep learning model can jointly learn a number of large-scale tasks from multiple domains by designing a multi-modal architecture in which as many parameters as possible are shared. Training and evaluating a neural network involves mapping the hyper parameter configuration (set of values for each hyper parameter) used in training the network to the validation error obtained at the end.",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_2",
  "x": "In [13] , the authors propose initializing Bayesian hyper parameters using meta-learning. The idea being initializing the configuration space for a novel dataset based on configurations that are known to perform well on similar, previously evaluated, datasets. Following a meta-learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in <cite>[4]</cite> to search the space around optimal hyper parameters for the AWD-LSTM model. Twitter tweets collected using a geolocation filter for Nigeria and Kenya with the goal of acquiring a code-mixed text corpus serve as our evaluation datasets. We report the test perplexity distributions of the various evaluated configurations and draw inferences on the sensitivity of each hyper parameter to our unique dataset.",
  "y": "uses"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_3",
  "x": "---------------------------------- **III. METHODOLOGY** We begin our work by establishing what the baseline and current state of the art model is for a language modeling task <cite>[4]</cite> . Applying the AWD-LSTM model, based on the open sourced code and trained on code-mixed Twitter data, we sample 84 different hyper parameter configurations for each dataset, and evaluate the resulting test perplexity distributions while varying individual hyperparameter values to understand the effect of the set of hyper parameter values selected on the model perplexity. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_4",
  "x": "This sequence of processes define how configurations from a current generation are used to derive the next generation. ---------------------------------- **E. META-LEARNING INITIALIZATION** Both the population based and sequential search space were manually initialized with four (4) values of each hyperparameter in the neighbourhood of the best values reported in <cite>[4]</cite> as shown in Table I . It is important to note that the sampled configuration space is very small compared to the overall space which is of size 4",
  "y": "similarities"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_6",
  "x": "In this work we assess the performance of the AWD-LSTM model <cite>[4]</cite> for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance. Our results show that as a whole, the set of hyperparameters considered the best <cite>[4]</cite> are reasonably good, however ther are better sets hyperparamers for the code-mixed corpora. Moreover, even with the best set of hyper parameters, the perplexity observed for our data are significantly higher (i.e. performance is worse at the task of language modeling) than the performance demonstrated in the literature. Finally, our implemented approach is one that not only enables confirmation of the goodness of the hyper parameters values, but we can also develop inferences about which hyper parameter values would yield better results. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_0",
  "x": "Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , [11] ) and maximum entropy <cite>[10]</cite> . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ). It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] .",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_1",
  "x": "An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_2",
  "x": "An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_3",
  "x": "Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features. These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES**",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_4",
  "x": "To avoid this problem, we combine the contextual features in the target context with the pre-prepared collocations list to build our classifier. As stated early, an important issue is what features will be used to construct the classifier in WSD. Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features. These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information.",
  "y": "differences"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_5",
  "x": "In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = \u00b1 5).",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_6",
  "x": "The sources of the collocations will be explained in Section 4.1. In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_0",
  "x": "By adding the generated sentences and incorporating syntactic information to the training data, we achieve better performance by 10% compared to an LSTM baseline <cite>[10]</cite> and 5% to the equivalent constraint. ---------------------------------- **RELATED WORK** The synthetic code-switching generation approach was introduced by adapting equivalence constraint on monolingual sentence pairs during the decoding step on an automatic speech recognition (ASR) model [6] . [11] explored Functional Head Constraint, which was found to be more restrictive than the Equivalence Constraint, but complex to be implemented, by using a lattice parser with a weighted finitestate transducer.",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_1",
  "x": "A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator <cite>[10]</cite> . A copy mechanism was proposed to copy words directly from the input to the output using an attention mechanism [16] . This mechanism has proven to be effective in several NLP tasks including text summarization [8] , and dialog systems [17] . The common characteristic of these tasks is parts of the output are exactly the same as the input source. For example, in dialog systems the responses most of the time have appeared in the previous dialog steps.",
  "y": "background"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_2",
  "x": "The word and syntax unit are represented as a vector x w and x p respectively. Next, we concatenate both vectors and use it as an input [x w |x p ] to an LSTM layer similar to <cite>[10]</cite> . 4. EXPERIMENT ---------------------------------- **CORPUS**",
  "y": "similarities"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_3",
  "x": "In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals [22] . As the data preprocessing, words are tokenized using Stanford NLP toolkit [23] and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to <cite>[10]</cite> and it is showed in Table 1 . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_0",
  "x": "**INTRODUCTION** Machine Transliteration is defined as phonetic transformation of names across languages Karimi et al., 2011) . Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010) . Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>) . In neural machine translation, a single neural network is responsible for reading a source sentence and generates its translation.",
  "y": "background"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_1",
  "x": "**EXPERIMENTS** We conducted a set of experiments to show the effectiveness of <cite>RNN Encoder-Decoder model</cite> (Cho et al., 2014b; <cite>Sutskever et al., 2014</cite>) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task . Table 1 shows different datasets in our experiments. Each dataset covers different levels of difficulty and training set size. The proposed model has been applied on .",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_2",
  "x": "For some datasets (such as 'En-Ch'), it takes more time to fit the model to the training data while for some others (such as 'En-He'), the model fit to the training data after a few iterations. ---------------------------------- **CONCLUSION** In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (<cite>Sutskever et al., 2014</cite>) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a) . Neural Machine Transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. Different parts of the proposed model jointly trained using stochastic gradient descent to minimize the log-likelihood.",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_3",
  "x": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015) . Based on successful studies on Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>; Hirschberg and Manning, 2015) , in this paper, we proposed a character-based encoderdecoder model which learn to transliterate endto-end. In the opposite side of classical models which contains different components, the proposed model is trained end-to-end, so it able to apply to any language pairs without tuning for a spacific one. ---------------------------------- **PROPOSED MODEL**",
  "y": "background uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_4",
  "x": "Here, we describe briefly the underlying framework, called <cite>RNN Encoder-Decoder</cite>, proposed by (Cho et al., 2014b) and <cite>(Sutskever et al., 2014)</cite> upon which we build a machine transliteration model that learns to transliterate end-to-end. The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence. This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , \u00b7 \u00b7 \u00b7 , h T ): Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character. The representation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h 1 , h 2 , ..., h T } (Dong et al., 2015; Chung et al., 2016) .",
  "y": "background uses"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_0",
  "x": "In section ??, we present the technical details of SINNET and describe our web demo. ---------------------------------- **RESEARCH** The SINNET system is the result of several years of research<cite> Agarwal et al., 2012</cite>; Agarwal et al., 2013) . In , we introduced the notion of social events.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_1",
  "x": "Examples of INR are a meeting or a dinner. OBS is a one-directional event in which only one party is aware of the other. Examples of OBS are thinking about someone, or missing someone. In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles. In<cite> Agarwal et al. (2012)</cite> , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_2",
  "x": "The direction of the event is from the observer to the one being observed. In the second figure there are two entity mentions: Alice and Mouse. There is a bidirectional interaction link between the Alice and Mouse triggered by the word asked. Figure 2 shows the network extracted from an abridged version of Alice in Wonderland <cite>(Agarwal et al., 2012)</cite> . Figure 3 shows the output of running the Hubs and Authority algorithm (Kleinberg, 1998 ) on the network.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_3",
  "x": "This network may be used for other In<cite> Agarwal et al. (2012)</cite> , we argued that a static network does not bring out the true nature of a network. For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 -The drying ceremony). Figure 4 shows the the network at the end of chapter 1 and chapter 3. ---------------------------------- **SYSTEM DETAILS AND WEB DEMO**",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_0",
  "x": "The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models.",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_1",
  "x": "Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true. This one misstep, however, does not detract from the quality of the authors' system, nor from the interesting presentation of too-often-ignored aspects of spoken language systems engineering. The book and the toolkit it describes can serve a very useful pedagogical purpose by providing a foundation upon which students and researchers",
  "y": "motivation"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_0",
  "x": "The idea of tree-based approaches <cite>[26]</cite> , [27] , [28] , [15] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2 . One is called expression tree that is used in <cite>[26]</cite> , [28] , [15] and the other is called equation tree in [27] . These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x. The overall algorithmic framework among the tree-based approaches consists of two processing stages.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_1",
  "x": "One is called expression tree that is used in <cite>[26]</cite> , [28] , [15] and the other is called equation tree in [27] . These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x. The overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated. In the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_2",
  "x": "In the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree. Roy et al. <cite>[26]</cite> proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_3",
  "x": "In the ideal case, all the irrelevant quantities are correctly predicted with high confidence, resulting in a large value for the sum of \u03c6(q). The other term, denoted by \u03c6(op), is the likelihood of selecting op as the operator for an internal tree node. With these two factors, Score(E) is formally defined as where I (E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes. To further reduce the tree enumeration space, beam search is applied in <cite>[26]</cite> .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_4",
  "x": "The solution in [27] , named ALGES, differs from <cite>[26]</cite> in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More specifically, ALGES does not discard irrevalent quantities, but enumerates all the syntactically valid trees. Integer Linear Programming (ILP) is applied as it can help enforce the constraints such as syntactic validity, type consistence and domain specific simplicity considerations. Consequently, its computation cost is dozens of times higher than that in [27] , according to an efficiency evaluation in [15] .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_5",
  "x": "With customized feature selection and SVM based classifier, the relevant quantities and variables are extracted and used as the leaf nodes of the equation tree. The tree is built in a bottom-up manner. It is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node. UnitDep [28] can be viewed as an extension work of <cite>[26]</cite> by the same authors. An important concept, named Unit Dependency Graph (UDG), is proposed to enhance the scoring function.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_7",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . 2) IL <cite>[26]</cite> .",
  "y": "background uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_9",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . 3) CC <cite>[26]</cite> .",
  "y": "background uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_10",
  "x": "For instance, all quantities and the main-goal are first identified by rules in LogicForm [24] , [25] and explicitly associated with their roletags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid [12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation. Second, the results of tree-based methods in AI2, IL and CC are collected from [15] where the same experimental setting of 3fold cross validation is applied. It is interesting to observe that ALGES [27] , ExpressionTree <cite>[26]</cite> and UNITDEP [28] cannot perform equally well on the three datasets.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_11",
  "x": "For example, if the i th character in the output sequence is an operator in {+, \u2212, \u00d7, \u00f7}, then the model cannot result in c \u2208 {+, \u2212, \u00d7, \u00f7, ), =} for the (i + 1) th character. To further improve the accuracy, DNS enhances the model in two ways. First, it builds a LSTM-based binary classification model to determine whether a number is relevant. This is similar to the relevance model trained in ExpressionTree <cite>[26]</cite> and UNITDEP [28] . The difference is that DNS uses LSTM as the classifier with unsupervised word-embedding features whereas ExpressionTree and UNITDEP use SVM with handcrafted features.",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_12",
  "x": "UnitDep [28] automatically generates features from a given math problem by analyzing its derived parser tree using the Compositional Vector Grammar parser [67] . Additionally, the Stanford Dependencies representation [68] has been applied in multitple solvers. We observed its occurrence in Formula [23] and ARIS [21] to extract attributes of entities (the subject, verb, object, preposition and temporal information), in KAZB [39] to generate part-of-speech tags, lematizations, and dependency parses to compute features, and in ALGES [27] to obtain syntactic information used for grounding and feature computation. ExpressionTree <cite>[26]</cite> is an exceptional case without using Stanford Parser. Instead, it uses the easy-fist parsing algorithm [69] to detect the verb associated with each quantity.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_13",
  "x": "As shown in Table 5 , a binary indicator to determine whether a quantity refers to a rate is adopted in many solvers <cite>[26]</cite> [28] [15] [40] [45] . It signals a strong connection between the quantity and operators of {\u00d7, \u00f7}. The value of the quantity is also useful for operator classifier or quantity relevance classifier. For instance, a quantity whose value is a real number between [0, 1] is likely to be associated with multiplication or division operators [40] , [45] . It is also observed that quantities in the text format of \"one\" or \"two\" are unlikely to be relevant with the solution [39] [40], [45] , [13] . Examples include \"if one airplane averages 400 miles per hour,...\" and \"the difference between two numbers is 36\".",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_14",
  "x": "The information embedded in the text window centered at a particular quantity can also provide important clues for solving math word problems. To differentiate two quantities both in the numeric format, we can leverage the word lemmas, part of speech (POS) tags and dependence types within the window as the features. In this manner, quantities associated with the same operators would to likely to share similar context information. A trivial trick used in <cite>[26]</cite> [28] [15] is to examine whether there exists comparative adverbs. For example, terms \"more\", \"less\" and \"than\" indicate operators of {+, \u2212}.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_15",
  "x": "---------------------------------- **QUANTITY-PAIR FEATURES** The relationship between two quantities is helpful to determine their associated operator. A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] . If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_16",
  "x": "A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] . If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] . Numeric relation and context similarity are two types of quantity-pair features proposed in [40] [45] . The former obtains two sets of nouns located within the same sentence as the two quantities and sorts them by the distance in the dependency tree. Then, a scoring function is defined to measure the similarity between these two sorted noun lists.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_17",
  "x": "Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator. ---------------------------------- **QUESTION-RELATED FEATURES** Distinguishing features can also be derived from questions. It is straightforward to figure out that the unknown variable can be inferred from the question and if a quantity whose unit appears in the question, this quantity is likely to be relevant.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_18",
  "x": "Two types of quantity-pair features were both adopted in the template-based solutions to equation set problems [39] [40] . Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator.",
  "y": "background uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_19",
  "x": "---------------------------------- **QUESTION-RELATED FEATURES** Distinguishing features can also be derived from questions. It is straightforward to figure out that the unknown variable can be inferred from the question and if a quantity whose unit appears in the question, this quantity is likely to be relevant. The remain question-related features presented in Table 5 were proposed by Roy et al. <cite>[26]</cite> , [28] and followed by MathDQN [15] .",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_20",
  "x": "<cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features. Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] . The features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention. As we can see from the examples in Table 5 , the difference between these two types of features is the occurrence number of the dependent verb in the sentence.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_21",
  "x": "**VERB-RELATED FEATURES** Verbs are important indicators for correct operator determination. For example, \"lose\" is a verb indicating quantity loss for an entity and related to the subtraction operator. Given a quantity, we call the verb closest to it in the dependency tree as its dependent verb. <cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features.",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_22",
  "x": "Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] . The features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention. As we can see from the examples in Table 5 , the difference between these two types of features is the occurrence number of the dependent verb in the sentence. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_23",
  "x": "There are certain types of global features in the document-level proposed by existing solvers. <cite>[26]</cite> , [28] , [15] use the number of quantities in the problem text as part of feature space. Unigrams and bigrams are also applied in [20] [39] . They may play certain effect in determining the quantities and their order. Note that the unigrams and bigrams are defined in the word level rather than the character level.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_24",
  "x": "\", the word lemmas around the quantity \"41.0\" are {Connie, have, red, marker}. POS tags [28] [39] [40] [45] [20] For \"A chef needs to cook 16.0 potatoes.\", the POS tags within a window of size 2 centered at the quantity \"16.0\" are {TO, VB, NNS}. Dependence type [39] [40] [45] For \"Ned bought 14.0 boxes of chocolates candy.\", we can detect multiple dependencies within the window of size 2 around the \"14.0\": (boxes, 14.0) \u2192 (num), (boxes, of)\u2192 (prep), (bought, Ned) \u2192 (nsubj). The dependence root is \"bought\". Comparative adverbs <cite>[26]</cite> [28] [15] For \"If she drank 25 of them and then bought 30 more.\", \"more\" is a comparative term in the window of quantity \"30\". ---------------------------------- **QUANTITY-PAIR FEATURES**",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_25",
  "x": "---------------------------------- **QUANTITY-PAIR FEATURES** Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\". Numeric relation of two quantities [40] [45] For each quantity, the nouns around it are extracted and sorted by the distance in the dependency tree.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_26",
  "x": "Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\". Numeric relation of two quantities [40] [45] For each quantity, the nouns around it are extracted and sorted by the distance in the dependency tree. Then, a scoring function is defined on the two sorted lists to measure the numeric relation. Context similarity between two quantities [40] [45] The context is represented by the set of words around the quantity.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_27",
  "x": "Number of quantities which happen to have the maximum number of matching tokens with the question <cite>[26]</cite> [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends. How many apples dose each friend get?\", the number of matching tokens for quantities 9, 12 and 3 is 1, 0 and 1. Hence, there are two quantities with the maximum matching token number. Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_28",
  "x": "Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ---------------------------------- **VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_29",
  "x": "How many apples dose each friend get?\", the number of matching tokens for quantities 9, 12 and 3 is 1, 0 and 1. Hence, there are two quantities with the maximum matching token number. Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_30",
  "x": "---------------------------------- **VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_31",
  "x": "Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence. ---------------------------------- **GLOBAL FEATURES** Number of quantities mentioned in text <cite>[26]</cite> [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_32",
  "x": "Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence. ---------------------------------- **GLOBAL FEATURES** Number of quantities mentioned in text <cite>[26]</cite> [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph. In [78] , a structure mapping engine named GeoRep was proposed to generate qualitative spatial descriptions from line diagrams.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_0",
  "x": "In Natural Language Processing (NLP), studies have built sophisticated concordancers to support second language writing and translators in searching bilingual sentence-aligned corpus (Wu et al., 2004; Jian et al., 2004; Lux-Pogodalla et al., 2010) . However, the information that conventional concordancers can provide for analyses of each usage is limited to the frequency of surrounding context patterns, parts of speech, and so on. The words that second language learners can search to learn their usages tend to be frequent. Therefore, a more sophisticated method to summarize many word usages in a large corpus for concordancers is desirable. Recently, contextualized word embeddings such as (<cite>Devlin et al., 2019</cite>) were proposed in NLP to capture the context of each word usage in vectors and to model the semantic distances between the usages using contexts as a clue.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_1",
  "x": "2 System Overview and Use Cases Fig. 1 shows our system layout. Once a user provides a word to the system, it automatically searches the word in the corpus in a similar way to typical concordancers. Unlike concordancers, our system has a database that stores contextualized word embeddings for each usage or occurrence of each word in the corpus. We used half a million sentences from the British National Corpus (BNC Consortium, 2007) as the raw corpus. We built the database by applying the bert-base-uncased model of the PyTorch Pretrained the BERT project 1 (<cite>Devlin et al., 2019</cite>) to the corpus.",
  "y": "uses"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_2",
  "x": "Users can 1 <cite>https://github.com/huggingface/ pytorch-pretrained-BERT</cite> 2 Fig. 2 and Fig. 3 shows use cases on a 10, 000-sentence experpt of the BNC corpus to avoid having too many hits hinder the reading of the paper. freely and interactively drag and move the probe point to change the list of usages below the visualization. Each line of the list shows the surrounding words of the usage, followed by the distance between the vectors of the usage and probe point in the two-dimensional visualization. In Fig. 2 , the probe point is on the left part of the visualized figure. In the first several lines of the list, the system successfully shows the usages of the word book about reading.",
  "y": "uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_0",
  "x": "Whilst gender bias in the form of concepts of masculinity and femininity has been found inscribed in implicit ways in AI systems more broadly (Adam, 2006) , this paper focuses on gender bias on word embeddings. Word embeddings are one of the most common techniques for giving semantic meaning to words in text and are used as input in virtually every neural NLP system (Goldberg, 2017) . It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016;<cite> Caliskan et al., 2017</cite>; Garg et al., 2018) . For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender. Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_1",
  "x": "It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016;<cite> Caliskan et al., 2017</cite>; Garg et al., 2018) . For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender. Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> . WEAT is a statistical test that detects bias in word embeddings using cosine similarity and averaging methods, paired with hypothesis testing. WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language \"Common Crawl\" corpus (Pennington et al., 2014) as well as the Skip-Gram (word2vec) embeddings trained on the Google News corpus (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_2",
  "x": "However, there is a diverse range of publicly-available word embeddings trained on corpora of different domains. To address this, we applied the WEAT test on four sets of word embeddings trained on corpora from four domains: social media (Twit-ter), a Wikipedia-based gender-balanced corpus (GAP) and a biomedical corpus (PubMed) and news (Google News, in order to reproduce and validate our results against those of<cite> Caliskan et al. (2017)</cite> ) (see Section 3). Caliskan et al. (2017) confirmed the presence of gender bias using three categories of words wellknown to be prone to exhibit gender bias: (B1) career vs. family activities, (B2) Maths vs. Arts and (B3) Science vs. Arts. Garg et al. (2018) expanded on this work and tested additional gender bias word categories: (B4) differences on personal descriptions based on intelligence vs. appearance and on (B5) physical or emotional strength vs. weakness. In this paper, we use these five categories to test for the presence of gender bias in the aforementioned domain corpora.",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_3",
  "x": "English-language word embeddings were selected with the intention of giving an insight into gender bias over a range of domains and with the expectation that some word embeddings would demonstrate much more bias than others. The word embeddings selected were: (a) Skip-Gram embeddings trained on the Google News corpus 2 , with a vocabulary of 3M word types (Mikolov et al., 2013) ; (b) Skip-Gram embeddings trained on 400 million Twitter micro-posts 3 , with a vocabulary of slightly more than 3M word types (Godin et al., 2015) ; (c) Skip-Gram embeddings trained on the PubMed Central Open Access subset (PMC) and PubMed 4 , with a vocabulary of about 2.2M word types (Chiu et al., 2016) and trained using two different sliding window sizes: 2 and 30 words; (d) FastText embeddings trained on the GAP corpus (Webster et al., 2018) by us 5 , with a vocabulary of 7,400 word types. ---------------------------------- **WEAT HYPOTHESIS TESTING 4.1 EXPERIMENTAL PROTOCOL** We largely follow the WEAT Hypothesis testing protocol introduced by<cite> Caliskan et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_4",
  "x": "In<cite> Caliskan et al. (2017)</cite> H o is tested through a permutation test, in which X \u222a Y is partitioned into alternative target listsX and\u0176 exhaustively and computing the one-sided p-value p[s(X,\u0176 , M, F ) > s(X, Y, M, F )], i.e. the proportion of partition permutationsX,\u0176 in which the test statistic s(X,\u0176 , M, F ) is greater than the observed test statistic s(X, Y, M, F ). This p-value is the probability that H o is true. In other words, it is the probability that there is no difference between X and Y (in relation to M and F ) and therefore that the word category is not biased. The ----------------------------------",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_5",
  "x": "**WEAT RESULTS** Before experimentation we expected to find a great deal of gender bias across the Google News and Twitter embedding sets and far less in the PubMed and GAP sets. However, results in Table 2 are somewhat different to our expectations: Google News We detect statistically significant (p-values in bold) gender bias in all 5 categories (B1-B5) on this corpus. Although one would hope to find little gender bias in a news corpus, given that its authors are professional journalists, bias had already been detected by<cite> Caliskan et al. (2017)</cite> and Garg et al. (2018) using methods similar to ours.",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_0",
  "x": "Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data? Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, <cite>13]</cite> . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans. In other words, it is easier for us to inspect and manipulate the data.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_1",
  "x": "In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed. By altering the latent representations of the segments from a labeled out-of-domain utterance properly according to the computed statistics, one can synthesize an in-domain utterance without changing the linguistic content using the trained VAE decoder.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_2",
  "x": "In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed. By altering the latent representations of the segments from a labeled out-of-domain utterance properly according to the computed statistics, one can synthesize an in-domain utterance without changing the linguistic content using the trained VAE decoder.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_3",
  "x": "Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. In this paper, we extend VAE-DA and address the issue by learning interpretable and disentangled representations using a variant of VAEs that is designed for sequential data, named factorized hierarchical variational autoencoders (FHVAEs) [15] .",
  "y": "extends"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_4",
  "x": "**ESTIMATING LATENT NUISANCE VECTORS** A key observation made in<cite> [13]</cite> is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment. In other words, latent nuisance vectors zn are relatively consistent within an utterance, while the distribution of z conditioned on an utterance can be assumed to have the same distribution as the prior. Therefore, suppose the prior is a diagonal Gaussian with zero mean. Given an utterance",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_5",
  "x": "**FHVAE-BASED DATA AUGMENTATION** With a trained FHVAE, we are able to infer disentangled latent representations that capture linguistic factors z1 and nuisance factors z2. To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2. Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is \u00b52, we can regard \u00b52 as the representation of nuisance factors of an utterance. We now derive two data augmentation methods similar to those proposed in<cite> [13]</cite> , named nuisance factor replacement and nuisance factor perturbation.",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_6",
  "x": "Therefore, we adopt a similar soft perturbation scheme as in<cite> [13]</cite> . First, {\u00b52} M i=1 for all M utterances are estimated with the approximated MAP. Principle component analysis is performed to obtain D pairs of eigenvalue \u03c3 d and eigenvectors e d , where D is the dimension of \u00b52. Lastly, one random perturbation vector p is drawn for each utterance to perturb as follows: where \u03b3 is used to control the perturbation scale.",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_8",
  "x": "The first two rows of results show that the WER gap between the unadapted model and the model trained on in-domain data is 24%. The third row reports the results of training with domain invariant feature, z1, extracted with a FHVAE as is done in [10] . It improves over the baseline by 6% absolute. VAE-DA<cite> [13]</cite> results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows. We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 .",
  "y": "uses"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_9",
  "x": "We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 . We observe about 12% WER reduction on the in-domain development set for both nuisance factor perturbation (p) and nuisance factor replacement (repl), with little degradation on the out-of-domain development set. Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content. To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to<cite> [13]</cite> , with the same expected squared Euclidean norm as the proposed method.",
  "y": "similarities"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_0",
  "x": "In string-based statistical machine translation, the alignment space is typically restricted by the n-grams considered in the underlying language model, but in syntax-based machine translation the alignment space is restricted by very different and less transparent structural contraints. While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003;<cite> Wellington et al., 2006</cite>; Macken, 2007; S\u00f8gaard and Kuhn, 2009) . The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism <cite>(Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009 ). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism.",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_1",
  "x": "The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism <cite>(Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009 ). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) . As noted by S\u00f8gaard (2009) , this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004) , since simultaneity is transitive.",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_2",
  "x": "The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) . As noted by S\u00f8gaard (2009) , this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004) , since simultaneity is transitive. While previous work (S\u00f8gaard and Kuhn, 2009 ) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004) , defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal form ITGs can induce the same class of translations as the full class of ITGs, they do not induce the same class of alignments.",
  "y": "background motivation"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_3",
  "x": "**NON-INDUCED CONFIGURATIONS** Inside-out alignments were first described by Wu (1997) , and their frequency has been a matter of some debate (Lepage and Denoual, 2005;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) . Cross-serial DTUs are made of two DTUs noncontiguous to the same side such that both have material in the gap of each other. Bonbons are similar, except the DTUs are non-contiguous to different sides, i.e. D has a gap in the source side that contains at least one token in E, and E has a gap in the target side that contains at least one token in D. Here's an example of a bonbon configuration from Simard et al. (2005) Multigap DTUs with mixed transfer are, as already mentioned multigap DTUs with crossing alignments from material in two distinct gaps. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a7559a8775941622d269433937633a_0",
  "x": "The details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high-utility context-free grammars for speech recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; <cite>Charniak 2001</cite>; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in Johnson (1998) , where the beneficial impact of various tree transformations for probabilistic grammars is presented.",
  "y": "background similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_0",
  "x": "Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles<cite> McClosky and Manning, 2012</cite>; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March. [11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads. al., 2012). When people are instead communicating informally about their lives, they refer to time more informally and frequently from their personal frame of reference rather than from an impersonal third person frame of reference. For example, they may use their own birthday as a time reference.",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_1",
  "x": "Most of the TEs are non-standard and need to be resolved to calendar dates (year and month). Once the full set of event mention sentences has been extracted for a user, all the temporal expressions (TEs) that appear in the same sentence with an event mention are resolved to a set of candidate dates. Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier. Previous work only retrieves time-rich sentences that include both the query and some TEs<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) . However, sentences that contain only the event mention but no explicit TE can also be informative.",
  "y": "background motivation"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_2",
  "x": "Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task<cite> (McClosky and Manning, 2012)</cite> . Their goal was to extract the temporal bounds of event relations. Our task has two key differences. First, they used newswire, Wikipedia and blogs as data sources from which they extract temporal bounds of facts found in Wikipedia infoboxes. Second, in the KBP task, the set of gold event relations are provided as input, so that the task is only to identify a date for an event that is guaranteed to have been mentioned.",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_3",
  "x": "We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (UzZaman and Allen, 2010). Features for the classifier include many of those in <cite>(McClosky and Manning, 2012</cite>; Yoshikawa et al., 2009 ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features. In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ). So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag.",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_4",
  "x": "We represent a MaxEnt classifier by P relation (R|t, ds) for a candidate date t in date sentence ds and possible relation R = {overlap, no-overlap}. We map the distribution over relations to a distribution over dates by defining P DateSentence (t|DS u ): We refer to this model as the Date Classifier. ---------------------------------- **CONSTRAINT-BASED CLASSIFIER** Previous work only retrieves time-rich sentences (i.e., date sentences) (Ling and Weld, 2010;<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) .",
  "y": "background motivation"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_5",
  "x": "In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes. ---------------------------------- **BASELINES AND ORACLE** Based on our temporal tagger, we provide two baselines to describe heuristic methods of aggregating the hard decisions from the classifier To set an upper bound on performance given our TE retrieval system, we calculate the oracle score by considering an extraction as correct if the gold date is one of the retrieved candidate dates.",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_6",
  "x": "For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice). Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes. ----------------------------------",
  "y": "background differences"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_0",
  "x": "Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion websites vary a great deal. One important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts, which varies in our data from 34% to 80%. The ability to explicitly rebut a previous post gives these debates both monologic and dialogic properties (Biber, 1991; Crystal, 2001; Fox Tree, 2010) ; Compare Figure 1 to Figure 2 . We believe that discussions containing many rebuttal links require a different type of analysis than other types of debates or discussions.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_1",
  "x": "The discussion topic was \"Death Penalty\", and the argument was framed as yes we should keep it vs. no we should not. Our long term goal is to understand the discourse and dialogic structure of such conversations. This could be useful for: (1) creating automatic summaries of each position on an issue (SparckJones, 1999); (2) gaining a deeper understanding of what makes an argument persuasive (Marwell and Schmitt, 1967) ; and (3) identifying the linguistic reflexes of perlocutionary acts such as persuasion and disagreement (Walker, 1996; Greene and Resnik, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Marcu, 2000) . As a first step, in this paper we aim to automatically identify rebuttals, and identify the speaker's stance towards a particular topic. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_2",
  "x": "Cat owners HAVE to handle sh*t, they MUST clean out a litter box...so suck on that! The most similar work to our own is that of Somasundaran & Wiebe (2009 , 2010 who also focus on automatically determining the stance of a debate participant with respect to a particular issue. Their data does not provide explicit indicators of dialogue structure such as are provided by the rebuttal links in Convinceme. Thus, this work treats each post as a monologic text to be classified in terms of stance, for a particular topic. They show that discourse relations such as concessions and the identification of argumentation triggers improves performance over sentiment features alone (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . This work, along with others, indicates that for such tasks it is difficult to beat a unigram baseline (Pang and Lee, 2008) .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_3",
  "x": "Human Topline. The best performance for siding ideological debates in previous work is approximately 64% accuracy over all topics, for a collection of 2nd Amendment, Abortion, Evolution, and Gay Rights debate posts<cite> (Somasundaran and Wiebe, 2010)</cite> . Their best performance is 70% for the 2nd amendment topic. The website that these posts were collected from apparently did not support dialogic threading, and thus there are no explicitly linked rebuttals in this data set. Given the dialogic nature of our data, as indicated by the high percentage of rebuttals in the ideological debates, we first aim to determine how difficult it is for humans to side an individual post from a debate without context.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_4",
  "x": "Given the dialogic nature of our data, as indicated by the high percentage of rebuttals in the ideological debates, we first aim to determine how difficult it is for humans to side an individual post from a debate without context. To our knowledge, none of the previous work on debate side classification has attempted to establish a human topline. We set up a Mechanical Turk task by randomly selected a subset of our data excluding the first post on each side of a debate and debates with fewer than 6 posts on either side. Each of our 12 topics consists of more than one debate: each debate was mapped by hand to the topic and topic-siding (as in<cite> (Somasundaran and Wiebe, 2010)</cite>). We selected equal numbers of posts for each topic for each side, and created 132 tasks (Mechanical Turk HITs).",
  "y": "similarities"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_5",
  "x": "We describe and motivate these feature sets below. Counts, Unigrams, Bigrams. Previous work suggests that the unigram baseline can be difficult to beat for certain types of debates <cite>(Somasundaran and Wiebe, 2010</cite> ). Thus we derived both unigrams and bigrams as features. We also include basic counts such as post length.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_6",
  "x": "Some LIWC features that we expect to be important are words per sentence (WPS), pronominal forms (Pro), and positive and negative emotion words (PosE) and (NegE). See Table 1 . Syntactic Dependency. Previous research in this area suggests the utility of dependency structure to determine the TARGET of an opinion word (Joshi and Penstein-Ros\u00e9, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . The dependency parse for a given sentence is a set of triples, composed of a grammatical relation and the pair of words for which the grammatical relation holds (rel i , w j , w k ), where rel i is the dependency relation among words w j and w k .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_7",
  "x": "It is interesting to note that in general the unigram accuracies are significantly below what Somasundaran and Wiebe achieve (who report overall unigram of 62.5%). This suggests a difference between the debate posts in their corpus and the Convinceme data we used which may be related to the proportion of rebuttals. The overal lack of impact for either the POS generalized dependency features (GDepP) or the Opinion generalized dependency features (GDep0) is surprising given that they improve accuracy for other similar tasks (Joshi and Penstein-Ros\u00e9, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set. The LIWC feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show improvement over a good range of topics.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_8",
  "x": "While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set. The LIWC feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show improvement over a good range of topics. We believe that further analysis is needed; we do not want to handpick topics for which particular feature sets perform well. Our results also showed that context did not seem to help uniformly over all topics. The mean performance over all topics for contextual features using the combination of all features and the Naive Bayes learner was 53.0% for context and 54.62% for no context (p = .15%, not significant).",
  "y": "extends differences"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_9",
  "x": "Interesting, the use of contextual features provided surprisingly greater performance for particular topics. For example for 2nd Amendment, unigrams with context yield a performance of 69.23% as opposed to the best performing without context features using LIWC of 64.10%. The best performance of<cite> (Somasundaran and Wiebe, 2010)</cite> below the majority class baseline for all of the features without context. Should we conclude anything from the fact that 6 of the topics are idealogical, out of the 7 topics where contextual features provide the best performance? We believe that the significantly greater percentage of rebuttals for these topics should give a greater weight to contextual features, so it would be useful to examine stance classification performance on the subset of the posts that are rebuttals.",
  "y": "background"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_0",
  "x": "3 Experiments and Results ---------------------------------- **DATASETS AND CORPUS** We experimented with four datasets widely used in literature: BLESS (Baroni and Lenci, 2011) , EVALution (Santus et al., 2015) , Lenci/Benotto (Benotto, 2015) , and Weeds (Weeds et al., 2014) taken from the repository provided by <cite>(Shwartz et al., 2017)</cite> . The corpus of articles we use is a complete xml dump of the English Wikipedia dated 3 Nov 2017.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_1",
  "x": "For BLESS, as seen in (Santus et al., 2014) , the performance of SLQS is 87%. Similar to SLQS, our depth measure is motivated by distributional informativeness hypothesis <cite>(Shwartz et al., 2017)</cite> . However, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance. As can be seen by physically examining Wikipedia articles, many of them tend to have a Star topology. This is also indicative that the topology used plays a major role in this feature.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_2",
  "x": "We compared our numbers with those given in <cite>(Shwartz et al., 2017)</cite> . In that paper, multiple measures are used, and the best performing measure for every row of the table is presented. We conducted the experiments for both, Star as well as the Linear topology. However, the results for Star topology were slightly better, hence we present these in Table ( 2). For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_3",
  "x": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_4",
  "x": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_5",
  "x": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_6",
  "x": "For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS. Our systems performs worse than the best measure whenever an Informativeness Measure <cite>(Shwartz et al., 2017)</cite> , like SLQS and its variants perform well. It performs better, or at least competitive, when the best performing measure is an Inclusion Measure or Similarity Measure (except for hypernym-vs-event in BLESS). A possible explanation of this is that the heading features that we use do not capture how informative a phrase is.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_7",
  "x": "A possible explanation of this is that the heading features that we use do not capture how informative a phrase is. However, having common headings is an indication of shared features, implying similarity, which is also indicated by inclusion measures. However, it should be noted, that we are comparing our single system against the best performing one in each case. For finding the best measure, <cite>(Shwartz et al., 2017)</cite> finds the best by varying the measures as well as the features, whereas we have a fixed system. Our system took a day to set up (including coding effort), and a few mins to run.",
  "y": "extends differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_0",
  "x": "**RELATED WORKS** There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach. The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method [8,<cite> 9,</cite> 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method. [8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_1",
  "x": "Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. <cite>[9]</cite> followed the method of [8] , but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_2",
  "x": "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus. Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of <cite>[9]</cite> . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus. However, the evaluation was conducted on a small part of senses of the target words like [8] .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_3",
  "x": "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated. Like [8] , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of <cite>[9]</cite> using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_4",
  "x": "They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> . ---------------------------------- **WORD SENSE DISAMBIGUATION BY RELATIVE SELECTION** Our method disambiguates senses of a target word in a sentence by selecting only a relative among the relatives of the target word that most probably occurs in the sentence. A flowchart of our method is presented in Figure 1 with an example 3 : 1) Given a new sentence including a target word, a set of relatives of the target word is created by looking up in WordNet.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_5",
  "x": "Our method does not depend on the training data, but on co-occurrence frequency matrix. Hence in our method, it is not necessary to build the training data, which requires too much time and space. 3) Finally, a sense of the target word is determined as the sense that is related to the selected relative. In this example, the relative stork is selected with the highest probability and the proper sense is determined as crane#1, which is related to the selected relative stork. Our method makes use of ambiguous relatives as well as unambiguous relatives unlike <cite>[9]</cite> and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in [8] by handling relatives separately instead of putting example sentences of the relatives together into a pool.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_6",
  "x": "**EXPERIMENTAL RESULTS** Comparison with Other Relative Based Methods. We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_7",
  "x": "However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data. When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data. Instead, we implemented the previous methods to work with our CFM. WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which <cite>[9]</cite> adopted though [8] utilized International Roget's Thesaurus and other classifier similar to decision lists.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_8",
  "x": "WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which <cite>[9]</cite> adopted though [8] utilized International Roget's Thesaurus and other classifier similar to decision lists. Also the bias of word senses, which is presented at WordNet, is reflected on the implementation in order to be in a same condition with our method. Hence, the reimplemented methods in this paper are not exactly same with the previous methods, but the main ideas of the methods are not corrupted. A correct sense of a target word tw i in a sentence C is determined as follows: where Sense(tw i , C) is a sense of tw i in C, s ij is the j-th sense of tw i .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_9",
  "x": "f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs. The main difference between [8] and <cite>[9]</cite> is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of <cite>[9]</cite> to exclude the ambiguous relatives. Table 3 . Comparison results with top 3 systems at SENSEVAL S2 LS S2 ALL S3 ALL [15] 40.2% 56.9% .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_10",
  "x": "Since the training data are built by collecting example sentences of relatives in the previous works, the frequencies in Eq. 12 and 13 are calculated with our matrix as follows: where r l is a relative related to the sense s ij . f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs. The main difference between [8] and <cite>[9]</cite> is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of <cite>[9]</cite> to exclude the ambiguous relatives.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_11",
  "x": "[5] 24.4% 32.8% . [17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> .",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_12",
  "x": "7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> . Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data. From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_13",
  "x": "7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> . Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data. From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_14",
  "x": "From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives. [10] evaluated their method on nouns of lexical sample task of SENSEVAL-2. Their method achieved 49.8% recall. When evaluated on the same nouns of the lexical sample task, our proposed method achieved 47.26%, and the method of [8] 45.61%, and the method of <cite>[9]</cite> 38.03%. Compared with our implementations, [10] utilized a web as a raw corpus that is much larger than our raw corpus, and employed various kinds of features such as bigram, trigram, part-of-speeches, etc.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_15",
  "x": "Compared with our implementations, [10] utilized a web as a raw corpus that is much larger than our raw corpus, and employed various kinds of features such as bigram, trigram, part-of-speeches, etc. 8 Therefore, it can be conjectured that a size of a raw corpus and features play an important role in the performance. We can observe that in our implementation of the method of <cite>[9]</cite> , the data sparseness problem is very serious since unambiguous relatives are usually not frequent in the raw corpus. In the web, the problem seems to be alleviated. Further studies are required for the effects of various features.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_16",
  "x": "The experimental results show that the proposed method effectively disambiguates many ambiguous words in SemCor and in test data for SENSEVAL all words task, as well as a small number of ambiguous words in test data for SENSEVAL lexical sample task. Also our method more correctly disambiguates senses than [8] and <cite>[9]</cite> . Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3. In consequence, our method has two advantages over the previous methods ( [8] and <cite>[9]</cite> ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words. However, our method did not achieve good performances.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_17",
  "x": "In consequence, our method has two advantages over the previous methods ( [8] and <cite>[9]</cite> ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words. However, our method did not achieve good performances. One reason of the low performance is on the relatives irrelevant to the target words. That is, investigation of several instances which assign to incorrect senses shows that relatives irrelevant to the target words are often selected as the most probable relatives. Hence, we will try to devise a filtering method that filters out the useless relatives before the relative selection phase.",
  "y": "differences"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_1",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_2",
  "x": "In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees. There are three main steps in our rescoring approach. The first step is to have the parser to produce n-best parse trees with their structural scores. For each parsed tree including words, part-of-speech (PoS) and semantic role labels. Second, we extract word-to-word associations (or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective) from large amounts of auto-parsed data and adopt word2vecf <cite>[13]</cite> to train dependency-based word embeddings.",
  "y": "uses"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_0",
  "x": "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these<cite> (Rahimi et al., 2015b</cite>,a) . The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Wing and Baldridge, 2014) or dialectology Eisenstein, 2015) . In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015) . Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.",
  "y": "background"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_1",
  "x": "4 The results reported in <cite>Rahimi et al. (2015b</cite>; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a) . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets). To test the applicability of the model's embeddings in dialectology, we created DAREDS. The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.",
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_2",
  "x": [
   "Most of the errors are the result of geolocating users from Eastern U.S. in Western U.S. particularly in Los Angeles and San Francisco. ---------------------------------- **DIALECTOLOGY** We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 . Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space."
  ],
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_3",
  "x": "The results are also compared with state-of-the-art text-based methods based on a flat<cite> (Rahimi et al., 2015b</cite>; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation. Our method outperforms both the flat and hierarchical text-based models by a large margin. Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work. We analysed the Table 2 : Nearest neighbours of place names.",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_0",
  "x": "Until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. In this paper, we propose to scale up discriminative training of<cite> (He and Deng, 2012)</cite> to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline. ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_1",
  "x": "State-of-the-art statistical machine translation systems based on a log-linear framework are parameterized by {\u03bb, \u03a6}, where the feature weights \u03bb are discriminatively trained (Och and Ney, 2002; Chiang et al., 2008b; Simianer et al., 2012) by directly optimizing them against a translation-oriented metric such as BLEU. The feature parameters \u03a6 can be roughly divided into two categories: dense feature that measures the plausibility of each translation rule from a particular aspect, e.g., the rule translation probabilities p(f |e) and p(e|f ); and sparse feature that fires when certain phenomena is observed, e.g., when a frequent word pair co-occured in a rule. In contrast to \u03bb, feature parameters in \u03a6 are usually modeled by generative models for dense features, or by indicator functions for sparse ones. It is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion. The maximum expected BLEU training of<cite> (He and Deng, 2012</cite> ) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically.",
  "y": "background extends"
 },
 {
  "id": "af39041414dec545df878404328aab_2",
  "x": "This shows that discriminative training makes it possible to achieve smaller models that perform comparably or even better than the baseline model. Our contributions in this paper are two-folded: First of all, we scale up the maximum expected BLEU training proposed in<cite> (He and Deng, 2012)</cite> in a number of ways including using 1) a hierarchical phrase-based model, 2) a richer feature set, and 3) a larger training set with a much larger parameter set, resulting in more than 150 million parameters in the model being updated, which is one order magnitude higher than the phrase-based model reported in<cite> (He and Deng, 2012)</cite> . We are able to show a reasonable improvement over this strong baseline. Secondly, we combine discriminative training with pruning techniques to reestimate parameters of pruned grammar. Our approach is shown to alleviate the loss due to pruning, and sometimes can even outperform the baseline unpruned grammar.",
  "y": "differences extends"
 },
 {
  "id": "af39041414dec545df878404328aab_3",
  "x": "Our goal is to update \u03a6 towards \u03a6 that maximizes the expected BLEU scores of the entire training data given the current \u03bb: where B(\u00ca 1 ...\u00ca N ) is the BLEU score of the concatenated hypothesis of the entire training data, following<cite> (He and Deng, 2012)</cite> . Eq. 1 summarizes over all possible combinations of\u00ca 1 ...\u00ca N , which is intractable. Hence we make two simplifying approximations as follows. First, let the k-best hypotheses of the n-th sentence,\u00ca n = \u00ca 1 n , ...,\u00ca K n , approximate all its possible translation.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_4",
  "x": "To further simplify the problem and relate it with model pruning, we consider to update a subset of \u03b8 \u2282 \u03a6 while keeping other parameterization of \u03a6 unchanged, where \u03b8 = {\u03b8 ij = p(e j |f i )} denotes our parameter set that satisfies j \u03b8 ij = 1 and \u03b8 ij \u2265 0. In experiments, we also consider {\u03b8 ji = p(f i |e j )}. To alleviate overfitting, we introduce KL-distance based reguralization as in<cite> (He and Deng, 2012)</cite> . We thus arrive at the following objective function: where \u03c4 controls the regularization term's contribution, and \u03b8 0 represents a prior parameter set, e.g., from the conventional maximum likelihood training.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_5",
  "x": "The optimization algorithm is based on the Extended Baum Welch (EBW) (Gopalakrishnan et al., 1991) as derived by<cite> (He and Deng, 2012)</cite> . The final update rule is as follow: where \u03b8 ij is the updated parameter, and \u03bb is the current feature's weight. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_6",
  "x": "Following<cite> (He and Deng, 2012)</cite> , we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title. For the tuning and development sets, we set aside 1275 and 1239 sentences respectively from LDC2010E30 corpus. The tune set is used by PRO for tuning \u03bb while the dev set is used to decide the best DT model. As for the blind test set, we report the performance on the NIST MT06 evaluation set, which consists of 1644 sentences from news and web-blog domains. Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_7",
  "x": "Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community. Table 1 compares the key components of our baseline system with that of<cite> (He and Deng, 2012)</cite> . As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|. He&Deng (2012) ----------------------------------",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_8",
  "x": "Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community. Table 1 compares the key components of our baseline system with that of<cite> (He and Deng, 2012)</cite> . As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|. He&Deng (2012) ----------------------------------",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_9",
  "x": "As shown, the utility function log(U (\u03b8)) increases monotonically but is countered by the KL term, resulting in a smaller but consistent increase of the objective function O(\u03b8). This monotonicallyincreasing trend of the objective function confirms the correctness of our implementation since EBW algorithm is a bound-based technique that ensures growth transformations between updates. We then explore the optimal setting for \u03c4 which controls the contribution of the regularization term. Specifically, we perform grid search, exploring values of \u03c4 from 0.1 to 0.75. For each \u03c4 , we run several iterations of discriminative training where each iteration involves one simultaneous update of p(f |e) and p(e|f ) according to Eq. 4, followed by one update of \u03bb via PRO (as in<cite> (He and Deng, 2012)</cite> ).",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_10",
  "x": "We report the best score of each \u03c4 in Fig. 1 and at which iteration that score is produced. As shown in Fig. 1 , all settings of \u03c4 improve over the baseline and \u03c4 = 0.10 gives the highest gain of 0.45 BLEU score. This improvement is in the same ballpark as in<cite> (He and Deng, 2012</cite> ) though on a scaledup task. We next decode the MT06 using the best model (i.e. \u03c4 = 0.10 at 6-th iteration) observed on the dev set, and obtained 40.33 BLEU with an improvement of around 0.4 BLEU point. We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by<cite> (He and Deng, 2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_11",
  "x": "This improvement is in the same ballpark as in<cite> (He and Deng, 2012</cite> ) though on a scaledup task. We next decode the MT06 using the best model (i.e. \u03c4 = 0.10 at 6-th iteration) observed on the dev set, and obtained 40.33 BLEU with an improvement of around 0.4 BLEU point. We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by<cite> (He and Deng, 2012)</cite> . ---------------------------------- **DT FOR SIGNIFICANCE PRUNING**",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_12",
  "x": "**CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million. Our experiments show that discriminative training these two features (out of 50) gives around 0.40 BLEU point improvement, which is consistent with the conclusion of<cite> (He and Deng, 2012)</cite> but in a much larger-scale system.",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_13",
  "x": "This empirical result shows that leaving probability mass unassigned after pruning is suboptimal and that discriminative training provides a principled way to redistribute the mass. ---------------------------------- **CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_14",
  "x": "**CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million. Our experiments show that discriminative training these two features (out of 50) gives around 0.40 BLEU point improvement, which is consistent with the conclusion of<cite> (He and Deng, 2012)</cite> but in a much larger-scale system.",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_15",
  "x": "**APPENDIX** We describe the process to simplify Eq. 1 to Eq. 2, which is omitted in<cite> (He and Deng, 2012)</cite> . For conciseness, we drop the conditions and write P (\u00ca i |F i ) as P (\u00ca i ). We write Eq. 1 again below as Eq. 5 . We first focus on the first sentence E 1 /F 1 and expand the related terms from the equation as follow:",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_0",
  "x": "Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; <cite>Fader et al., 2011</cite> ) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> . Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_1",
  "x": "Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; <cite>Fader et al., 2011</cite> ) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> . Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_2",
  "x": "The main contributions of the paper are: \u2022 We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term. \u2022 We establish the effectiveness of our approach through human-based evaluation. \u2022 We conduct a comparative study with the verb-based relation extraction system ReVerb<cite> (Fader et al., 2011)</cite> and show that our approach accurately extracts more verb-based relations. \u2022 We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations.",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_3",
  "x": "---------------------------------- **RELATED WORK** Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008) . Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance<cite> (Fader et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_4",
  "x": "However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb<cite> (Fader et al., 2011)</cite> surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (Banko et al., 2007; Banko, 2009 ) define relation to be any verb-prep, adj-noun construction. While such systems are great at learning general relations, they are not guided but simply gather in an undifferentiated way whatever happens to be contained in their input. In order to be able to extract all verb relations associated with a given term, such systems need to part-of-speech tag and parse a large document collection, then they have to extract all verb constructions and all arguments matching specific sets of patterns which were written by humans (or experts). Finally, they must filter out the information and retrieve only those verb relations that are associated with the specific term.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_5",
  "x": "For our comparative study with existing systems, we used ReVerb 4<cite> (Fader et al., 2011)</cite> , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to<cite> (Fader et al., 2011)</cite> ReVerb outperforms TextRunner (Banko et al., 2007) and the open Wikipedia extractor WOE (Wu and Weld, 2010) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.",
  "y": "similarities"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_6",
  "x": "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to<cite> (Fader et al., 2011)</cite> ReVerb outperforms TextRunner (Banko et al., 2007) and the open Wikipedia extractor WOE (Wu and Weld, 2010) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_7",
  "x": "**CONCLUSION** Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations. We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers. We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb<cite> (Fader et al., 2011)</cite> system and existing knowledge bases like NELL (Carlson et al., 2009) , Yago (Suchanek et al., 2007) and ConceptNet (Liu and Singh, 2004) . Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.",
  "y": "similarities"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_0",
  "x": "This scenario is known as zero-shot NMT. Zero-shot NMT also requires a pivot language but it is only used during training without the need to generate pseudoparallel corpora. Training. Zero-shot NMT was first demonstrated by<cite> Johnson et al. (2017)</cite> . However, this zero-shot translation method is inferior to pivoting.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_1",
  "x": "The dominant NMT approach is the Embed -Encode -Attend -Decode paradigm. Recurrent neu- * equal contribution ral network (RNN) (Bahdanau et al., 2015) , convolutional neural network (CNN) (Gehring et al., 2017) and self-attention (Vaswani et al., 2017) architectures are popular approaches based on this paradigm. For a more detailed exposition of NMT, we refer readers to some prominent tutorials (Neubig, 2017; Koehn, 2017) . While initial research on NMT started with building translation systems between two languages, researchers discovered that the NMT framework can naturally incorporate multiple languages. Hence, there has been a massive increase in work on MT systems that involve more than two languages (Dong et al., 2015; Firat et al., 2016a; Cheng et al., 2017; <cite>Johnson et al., 2017</cite>;<cite> Chen et al., 2017</cite> Neubig and Hu, 2018) etc.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_2",
  "x": "In addition, MNMT systems will be compact, because a single model handles translations for multiple languages<cite> (Johnson et al., 2017)</cite> . This can reduce the deployment footprint, which is crucial for con- There are multiple MNMT scenarios based on available resources and studies have been conducted for the following scenarios ( Figure 1 1 ) : Multiway Translation. The goal is constructing a single NMT system for one-to-many (Dong et al., 2015) , many-to-one (Lee et al., 2017) or many-tomany (Firat et al., 2016a ) translation using parallel corpora for more than one language pair. Low or Zero-Resource Translation. For most of the language pairs in the world, there are small or no parallel corpora, and three main directions have been studied for this scenario.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_3",
  "x": "Pivot translation: Using a high-resource language (usually English) as a pivot to translate between a language pair (Firat et al., 2016a) . Zeroshot translation: Translating between language pairs without parallel corpora<cite> (Johnson et al., 2017)</cite> . Multi-Source Translation. Documents that have been translated into more than one language might, in the future, be required to be translated 1 Please see the supplementary material for papers related to each category. into another language.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_4",
  "x": "In fact, the number of parameters is only a small multiple of the compact model (the multiplication factor accounts for the language embedding size)<cite> (Johnson et al., 2017)</cite> , but the language embeddings can directly impact the model parameters instead of the weak influence that language tags have. Universal Encoder Representation. Ideally, multiway systems should generate encoder representations that are language agnostic. However, the attention mechanism sees a variable number of encoder representations depending on the sentence length (this could vary for translations of the same sentence). To overcome this, an attention bridge network generates a fixed number of contextual representations that are input to the attention network (Lu et al., 2018; V\u00e1zquez et al., 2018) .",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_5",
  "x": "This is a challenging scenario because parameter sharing has to be balanced with the capability to generate sentences in each target language. Blackwood et al. (2018) added the language tag to the beginning as well as end of sequence to avoid its attenuation in a leftto-right encoder. explored multiple methods for supporting target languages: (a) target language tag at beginning of the decoder, (b) target language dependent positional embeddings, and (c) divide hidden units of each decoder layer into shared and language-dependent ones. Each of these methods provide gains over<cite> Johnson et al. (2017)</cite> , and combining all gave the best results. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_6",
  "x": "To avoid this, sentence pairs from different language pairs are sampled to maintain a healthy balance. Mini-batches can be comprised of a mix of samples from different language pairs<cite> (Johnson et al., 2017)</cite> or the training schedule can cycle through mini-batches consisting of a language pair only (Firat et al., 2016a) . For architectures with language specific layers, the latter approach is convenient to implement. Knowledge Distillation. In this approach suggested by Tan et al. (2019) , bilingual models are first trained for all language pairs involved.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_7",
  "x": "There are also many differences between MNMT and domain adaptation for NMT. While pivoting is a popular approach for MNMT (Cheng et al., 2017) , it is unsuitable for domain adaptation. As there are always vocabulary overlaps between different domains, there are no zero-shot translation<cite> (Johnson et al., 2017)</cite> settings in domain adaptation. In addition, it not uncommon to write domain specific sentences in different styles and so multi-source approaches are not applicable either. On the other hand, data selection approaches in domain adaptation that select out-of-domain sentences which are similar to in-domain sentences (2017a) have not been applied to MNMT.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_8",
  "x": "Addressing intrasentence multilingualism i.e. code mixed input and output, creoles and pidgins is an interesting research direction. The compact MNMT models can handle code-mixed input, but code-mixed output remains an open problem<cite> (Johnson et al., 2017)</cite> . Multilingual and Multi-Domain NMT. Jointly tackling multilingual and multi-domain translation is an interesting direction with many practical use cases. When extending an NMT system to a new language, the parallel corpus in the domain of interest may not be available.",
  "y": "future_work"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_0",
  "x": "Most image captioning research has focused on single-sentence captions, but the descriptive capacity of this form is limited; a single sentence can only describe in detail a small aspect of an image. Recent work has argued instead for image paragraph captioning with the aim of generating a (usually 5-8 sentence) paragraph describing an image. Compared with single-sentence captioning, paragraph captioning is a relatively new task. The main paragraph captioning dataset is the Visual Genome corpus, introduced by <cite>Krause et al. (2016)</cite> . When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_1",
  "x": "Single-sentence and paragraph captioning models are evaluated with a number of metrics, including some designed specifically for captioning (CIDEr) and some adopted from machine translation (BLEU, METEOR). CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches. We discuss these metrics in greater detail when analyzing our experiments. <cite>Krause et al. (2016)</cite> introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning. Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_2",
  "x": "---------------------------------- **RELATED MODELS** The paragraph captioning models proposed by <cite>Krause et al. (2016)</cite> included template-based (nonneural) approaches and two encoder-decoder models. In both neural models, the encoder is an object detector pre-trained for dense captioning. In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_3",
  "x": "In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators. To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data). For our experiments, we use the top-down single-sentence captioning model in Anderson et al. (2017) . This model is similar to the \"flat\" model in <cite>Krause et al. (2016)</cite> , except that it incorporates attention with a top-down mechanism. ----------------------------------",
  "y": "background similarities"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_4",
  "x": "---------------------------------- **METHODS AND RESULTS** For our paragraph captioning model we use the top-down model from Anderson et al. (2017) . Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in <cite>Krause et al. (2016)</cite> and Liang et al. (2017) ). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.",
  "y": "differences"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_5",
  "x": "METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by <cite>Krause et al. (2016)</cite> . We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 \u00b7 10 \u22124 . We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 \u00b7 10 \u22125 .",
  "y": "uses"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_0",
  "x": "In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm, Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access. The aim of this tutorial is to familiarize the attendees with the subjectivity and sentiment research carried out on languages other than English in order to enable and promote crossfertilization. Specifically, we will review work along three main directions.",
  "y": "background"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_1",
  "x": "While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm, Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access.",
  "y": "motivation"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_2",
  "x": "While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm, Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access.",
  "y": "motivation"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_0",
  "x": "---------------------------------- **INTRODUCTION** Word embeddings are nowadays pervasive on a wide spectrum of Natural Language Processing (NLP) and Natural Language Understanding (NLU) applications. These word representations improved downstream tasks in many domains such as machine translation, syntactic parsing, text classification, and machine comprehension, among others [6] . Ranging from count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3] , Word2Vec [28] , GloVe [32] , and more recently ELMo <cite>[33]</cite> , to name a few.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_1",
  "x": "Recently, bi-directional LSTM models were also employed by InferSent [10] on a supervised training scheme using the Stanford Natural Language Inference (SNLI) dataset [5] to predict entailment/contradiction. InferSent [10] proved to yield much better results on a variety of downstream tasks when compared to many strong baselines or self-supervised methods such as Skip-Thought [23] , by leveraging strong supervision. Lately, the Universal Sentence Encoder (USE) [8] mixed an unsupervised task using a large corpus together with the supervised SNLI task and showed a significant improvement by leveraging the Transformer architecture [37] , which is solely based on attention mechanisms, although without providing an evaluation with other baselines and previous works such as InferSent [10] . Neural Language Models can be tracked back to [3] , and more recently deep bi-directional language models (biLM) <cite>[33]</cite> have successfully been applied to word embeddings in order to incorporate contextual information. Very recently, [34] used unsupervised generative pre-training of language models followed by discriminative fine-tunning to achieve state-of-the-art results in several NLP downstream tasks (improving 9 out of 12 tasks).",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_2",
  "x": "Following the pioneering work by [3] on the neural language model for distributed word representations, the seminal Word2Vec [28] is one of the first popular approaches of word embeddings based on neural networks. This type of representation is able to preserve semantic relationships between words and their context, where context is modeled by nearby words. In [28] , they presented two different methods to compute Two main challenges exist when learning high-quality representations: they should capture semantic and syntax and the different meanings the word can represent in different contexts (polysemy). To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_3",
  "x": "This type of representation is able to preserve semantic relationships between words and their context, where context is modeled by nearby words. In [28] , they presented two different methods to compute Two main challenges exist when learning high-quality representations: they should capture semantic and syntax and the different meanings the word can represent in different contexts (polysemy). To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_4",
  "x": "ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_5",
  "x": "To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_6",
  "x": "Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning. Nowadays, there is still no consensus on how to represent sentences and many studies were proposed towards that research direction. Skip-Thought Vectors [23] are based on a sentence encoder that, instead of predicting the context of a word as Word2Vec, it predicts the surrounding sentences of a given sentence.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_7",
  "x": "Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_8",
  "x": "**EXPERIMENTAL SETUP** In this section, we describe where the pre-trained models were obtained as well as the procedures employed to evaluate each method. ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_9",
  "x": "ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_10",
  "x": "ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_11",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_12",
  "x": "We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011. To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_13",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_14",
  "x": "An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.",
  "y": "extends"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_15",
  "x": "We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_16",
  "x": "We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . [7] To measure the semantic similarity between two sentences from 0 (not similar Knowledge Semantic Relatedness (SICK-R) [27] To measure the degree of semantic relatedness between sentences from 0 (not related) to 5 (related) A man is singing a song and playing the guitar A man is opening a package that contains headphones",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_17",
  "x": "As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks. Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment). We hypothesize that these results were due to the similarity of these tasks to the tasks were InferSent [10] was trained on (SNLI and MultiNLI).",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_18",
  "x": "---------------------------------- **DOWNSTREAM CLASSIFICATION TASKS** In Table 6 we show the tabular results for the downstream classification tasks, and in Figure 1 we show a graphical comparison between the different methods. As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks. Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_19",
  "x": "Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment). We hypothesize that these results were due to the similarity of these tasks to the tasks were InferSent [10] was trained on (SNLI and MultiNLI). As described in [10] , the SICK-E can be seen as an out-domain version of the SNLI dataset. The Universal Sentence Encoder (USE) [8] model, with the Transformer encoder, also achieved good results on the product review (CR) and on the question-type (TREC) tasks.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_20",
  "x": "As we can see, sentence embedding methods are still far away from the idea of a universal sentence encoder that can have a broad transfer quality. Given that ELMo <cite>[33]</cite> demonstrated excellent results on a broad set of tasks, it is clear that a proper integration of deep representation from language models can potentially improve sentence embedding methods by a significant margin and it is a promising research line. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  11 of the appendix. ---------------------------------- **SEMANTIC RELATEDNESS AND TEXTUAL SIMILARITY TASKS**",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_21",
  "x": "**LINGUISTIC PROBING TASKS** In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_22",
  "x": "In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_23",
  "x": "As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_24",
  "x": "Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks. However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo <cite>[33]</cite> bag-of-words or InferSent [10] and USE in the downstream classification tasks.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_25",
  "x": "However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_26",
  "x": "However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_27",
  "x": "However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo <cite>[33]</cite> bag-of-words or InferSent [10] and USE in the downstream classification tasks. We believe that the high performance of the p-mean [35] in the WC task is due to the concatenative approach employed to aggregate the different power means. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  10 of the appendix. ---------------------------------- **INFORMATION RETRIEVAL TASKS**",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_28",
  "x": "We provided a comprehensive evaluation of the inductive transfer as well as an exploration of the linguistic properties of multiple sentence embedding techniques that included bag-of-word baselines, as well as encoder architectures trained with supervised or self-supervised approaches. We showed that a bag-of-words approach using a recently introduced context-dependent word embedding technique was able to achieve excellent performance on many downstream tasks as well as capturing important linguistic properties. We demonstrated the importance of the linguistic probing tasks as a means for exploration of sentence embeddings. Especially for evaluating different levels of word representations, where it can be a very useful tool to provide insights on what kind of relationships and linguistic properties each representation level (in the case of deep representations such as ELMo <cite>[33]</cite> ) is capturing. We also showed that no method had a consistent performance across all tasks, with performance being linked mostly with the downstream task similarity to the trained task of these techniques.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_29",
  "x": "We also showed that no method had a consistent performance across all tasks, with performance being linked mostly with the downstream task similarity to the trained task of these techniques. Given that we are still far from a universal sentence encoder, we believe that this evaluation can provide an important basis for choosing which technique can potentially perform well in particular tasks. Finally, we believe that new embedding training techniques that include language models as a way to capture context and meaning, such as ELMo <cite>[33]</cite> , combined with clever techniques of encoding sentences such as in InferSent [10] , can improve the performance of these encoders by a significant margin. However, as we saw in the experiments, the performance of these encoders trained on particular datasets such as entailment did not perform well on a broad set of downstream tasks. Therefore, one hypothesis is that these encoders are too narrow at modeling what these embeddings can carry.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_30",
  "x": "However, as we saw in the experiments, the performance of these encoders trained on particular datasets such as entailment did not perform well on a broad set of downstream tasks. Therefore, one hypothesis is that these encoders are too narrow at modeling what these embeddings can carry. We believe that the research direction of incorporating language models and multiple levels of representations can help to provide a wide set of rich features that can capture context-dependent semantics as well as linguistic features, such as seen on ELMo <cite>[33]</cite> downstream and linguistic probing task experiments, but for sentence embeddings. ---------------------------------- **A.1 SUPPLEMENTAL RESULTS**",
  "y": "differences future_work"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_0",
  "x": "**INTRODUCTION** Readers fixate more and longer on open syntactic categories (verbs, nouns, adjectives) than on closed class items like prepositions and conjunctions (Rayner and Duffy, 1988; Nilsson and Nivre, 2009) . Recently,<cite> Barrett and S\u00f8gaard (2015)</cite> presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS) . Their study uses all the coarse-grained POS labels proposed by Petrov et al. (2011) . This paper investigates to what extent gaze data can also be used to predict grammatical functions such as subjects and objects.",
  "y": "background"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_1",
  "x": "**EYE TRACKING DATA** The data comes from <cite>(Barrett and S\u00f8gaard, 2015)</cite> and is publicly available 1 . In this experiment 10 native English speakers read 250 syntactically annotated sentences in English (min. 3 tokens, max. 120 characters).",
  "y": "uses"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_2",
  "x": "The features predictive of grammatical functions are similar to the features that were found to be predictive of POS <cite>(Barrett and S\u00f8gaard, 2015)</cite> , however, the probability that a word gets first and second fixation were not important features for POS classification, whereas they are contributing to dependency classification. This could suggest that words with certain grammatical functions are consistently more likely or less likely to get first and second fixation, but could also be due to a frequent syntactic order in the sample. Binary discrimination Error reduction over the baseline can be seen in Figure 2 . The mean accuracy using logistic regression on all binary classification problems between grammatical functions is 0.722. The frequency-position-word length baseline is 0.706.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_0",
  "x": "We also compare our model with an end-toend tree-based LSTM model (SPTree) by <cite>Miwa and Bansal (2016)</cite> and show that our model performs within 1% on entity mentions and 2% on relations. Our finegrained analysis also shows that our model performs significantly better on AGENT-ARTIFACT relations, while SPTree performs better on PHYSICAL and PART-WHOLE relations. ---------------------------------- **INTRODUCTION** Extraction of entities and their relations from text belongs to a very well-studied family of structured prediction tasks in NLP.",
  "y": "similarities uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_1",
  "x": "Joint models have been argued to perform better than the pipeline models as knowledge of the typed relation can increase the confidence of the model on entity extraction and vice versa. Recurrent networks (RNNs) (Elman, 1990 ) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens. However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored. For these tasks, RNNs that make use of tree structures have been deemed more suitable. <cite>Miwa and Bansal (2016)</cite> , for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_2",
  "x": "We also add an additional layer to our network to encode the output sequence from right-to-left and find significant improvement on the performance of relation identification using bi-directional encoding. Our model significantly outperforms the feature-based structured perceptron model of Li and Ji (2014) , showing improvements on both entity and relation extraction on the ACE05 dataset. In comparison to the dependency treebased LSTM model of <cite>Miwa and Bansal (2016)</cite> , our model performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their tree-based model on the AGENT-ARTIFACT relation, while their tree-based model performs better on PHYSICAL and PART-WHOLE relations; the two models perform comparably on all other relation types. The very competitive performance of our non-tree-based model bodes well for relation extraction of non-adjacent entities in low-resource languages that lack good parsers.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_3",
  "x": "For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014) , joint inference integer linear programming models (Yih and Roth, 2007; Yang and Cardie, 2013) , card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency parse tree features.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_4",
  "x": "Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency parse tree features. We also present the first neural network based joint model that can extract entity mentions and relations along with the relation type. In our previous work (Katiyar and Cardie, 2016) , as explained earlier, we proposed a LSTM-based model for joint extraction of opinion entities and relations, but no relation types.",
  "y": "motivation"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_5",
  "x": "In this paper, we only use the hidden state from the last layer L for output layer and compute the top hidden layer representation as below: \u2190 \u2212 V are weight matrices for combining hidden representations from the two directions. ---------------------------------- **ENTITY DETECTION** We formulate entity detection as a sequence labeling task using BILOU scheme similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_6",
  "x": "During training, we pass the gold label embedding to the next time step which enables better training of our model. However, at test time when the gold label is not available we use the predicted label at previous time step as input to the current step. At inference time, we can greedily decode the sequence to find the most likely entity E and relation R tag sequences: Since, we add another top layer to encode tag sequences in the reverse order as explained in Section 3.5, there may be conflicts in the output. We select the positive and more confident label similar to <cite>Miwa and Bansal (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_7",
  "x": "We select the positive and more confident label similar to <cite>Miwa and Bansal (2016)</cite> . Multiple Relations Our approach to relation extraction is different from <cite>Miwa and Bansal (2016)</cite> . <cite>Miwa and Bansal (2016)</cite> present each pair of entities to their model for relation classification. In our approach, we use pointer networks to identify the related entities. Thus, for our approach described so far if we only compute the argmax on our objective then we limit our model to output only one relation label per token.",
  "y": "differences"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_8",
  "x": "We evaluate our proposed model on the two datasets from the Automatic Content Extraction (ACE) program -ACE05 and ACE04. There are 7 main entity types namely Person (PER), Organization (ORG), Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Vehicle (VEH). For each entity, both entity mentions and its head phrase are annotated. For the scope of this paper, we only use the entity head phrase similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . Also, there are relation types namely Physical (PHYS), Person-Social (PER-SOC), Organization-Affiliation (ORG-AFF), Agent-Artifact (ART), GPE-Affiliation (GPE-AFF).",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_9",
  "x": "ACE05 has a total of 6 relation types including PART-WHOLE. We use the same data splits as Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> such that there are 351 documents for training, 80 for development and the remaining 80 documents for the test set. ACE04 has 7 relation types with an additional Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_10",
  "x": "---------------------------------- **EVALUATION METRICS** In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . An entity is considered correct if we can identify its head and the entity type correctly. A relation is considered correct if we can identify the head of the argument entities and also the relation type.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_12",
  "x": "1 We ran the system made publicly available by <cite>Miwa and Bansal (2016)</cite> , on ACE05 dataset for filling in the missing values and comparing our system with theirs at fine-grained level. Table 2 : Performance of different encoding methods on ACE05 dataset. with 300-dimensional word2vec (Mikolov et al., 2013) word embeddings trained on Google News dataset. We have 3 hidden layers in our network and the dimensionality of the hidden units is 100. All the weights in the network are initialized from small random uniform noise.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_13",
  "x": "---------------------------------- **ERROR ANALYSIS** In this section, we perform a fine-grained comparison of our model with respect to the SPTree<cite> (Miwa and Bansal, 2016)</cite> model. We compare the performance of the two models with respect to entities, relation types and the distance between the relation arguments and provide examples from the test set in Table 6 . Table 3 : Performance on ACE04 test dataset.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_15",
  "x": "The dashed (\"-\") performance numbers were missing in the original paper<cite> (Miwa and Bansal, 2016)</cite> . ---------------------------------- **ENTITIES** We find that our model has lower recall on entity extraction than SPTree as shown in Table 1 . <cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_16",
  "x": "We find that our model has lower recall on entity extraction than SPTree as shown in Table 1 . <cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately. We leave it for future work. Li and Ji (2014) mentioned in their analysis of the dataset that there were many \"UNK\" tokens in the test set which were never seen during training.",
  "y": "future_work"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_17",
  "x": "Li and Ji (2014) mentioned in their analysis of the dataset that there were many \"UNK\" tokens in the test set which were never seen during training. We verified the same and we hypothesize that for this reason the performance on the entities depends largely on the pretrained word embeddings being used. We found considerable improvements on entity recall when using pretrained word embeddings, if available, for these \"UNK\" tokens. <cite>Miwa and Bansal (2016)</cite> also use additional features such as POS tags in addition to pretrained word embeddings at the input layer. Table 5 : Performance based on the distance between entity arguments in relations for ACE05 test dataset.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_18",
  "x": "---------------------------------- **CONCLUSION** In this paper, we propose a novel attention-based LSTM model for joint extraction of entity mentions and relations. Experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by Li and Ji (2014) . We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_19",
  "x": "In this paper, we propose a novel attention-based LSTM model for joint extraction of entity mentions and relations. Experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by Li and Ji (2014) . We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_20",
  "x": "We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types. In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by <cite>Miwa and Bansal (2016)</cite> . We introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions. We use heuristics in this paper to combine the predictions.",
  "y": "future_work"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_0",
  "x": "This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3] . We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> .",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_1",
  "x": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known.",
  "y": "extends"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_2",
  "x": "**POSTERIOR EVALUATION DISTRIBUTION OF SUBSETS** We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4] ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%).",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_3",
  "x": "We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4] ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II).",
  "y": "extends"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_0",
  "x": "(3) For entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the F 1 score between 7% and 12% over our baseline approach and outperform <cite>(Zhang and Iria, 2009 )</cite> on all learned concepts (subject, location, temporal). ---------------------------------- **RELATED WORK** We divide the related work in automatic gazetteer population into three groups: (1) Machine learning approaches (2) Pattern driven approaches Finally, like our own work, (3) knowledge driven approaches Knowledge Driven. In any case, machine learning and pattern driven approaches extract their terms from unstructured sources -despite the fact that large, general knowledge bases became available in the last years.",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_1",
  "x": "The approach presented in (Kazama and Torisawa, 2007; Kazama and Torisawa, 2008 ) relies solely on WIKIPEDIA, producing gazetteers without explicitly named concepts, arguing that consistent but anonymous labels are still useful. Most closely related to our own work, the authors of <cite>(Zhang and Iria, 2009 )</cite> build an approach solely on WIKIPEDIA which does not only exploit the article text but also analyzes the structural elements of WIKIPEDIA: 3 Automatically Extending Gazetteer Lists 3.1 Extraction Algorithm: Overview Algorithm 1 shows an outline of the gazetteer expansion algorithm used in EAGER. To extend an initial seed set S EAGER proceeds, roughly, in three steps: First, it identifies DBPEDIA articles for seed entities and extracts implicit category and synonym information from abstracts and redirect information (Lines 1-11). Second, it finds additional categories from the DBPEDIA category hierarchy .",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_2",
  "x": "This allows for more precise article matching and avoids complex URL encodings as necessary in previous, add all labels of a to G ; WIKIPEDIA-based approaches such as (Kazama and Torisawa, 2007) . As <cite>(Zhang and Iria, 2009 )</cite>, we reject redirection entries in this step as ambiguous. With the articles identified, we can proceed to extract category information from the abstracts and new entities from the redirect information. In the dependency analysis of article abstracts (Lines 6-9), we aim to extract category (or, more generally, hypernym) information from the abstracts of articles on the ssed list. We perform a standard dependency analysis on the sentences of the abstract and return all nouns that stand in nsubj relation to a seed entity or (directly or indirectly) in conj (correlative conjunction) relation to a noun that stands in nsubj relation to a seed entity.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_3",
  "x": "We perform a standard dependency analysis on the sentences of the abstract and return all nouns that stand in nsubj relation to a seed entity or (directly or indirectly) in conj (correlative conjunction) relation to a noun that stands in nsubj relation to a seed entity. This allows us to extract, e.g., both \"general\" and \"statesman\" as categories from a sentence such as \"Julius Caesar was a Roman general and statesman\". This analysis is inspired by <cite>(Zhang and Iria, 2009 )</cite>, but performed on the entire abstract which is clearly dis- <cite>(Zhang and Iria, 2009)</cite> , where this is applied only to the first sentence (as WIKIPEDIA does not directly provide a concept of \"abstract\"). All categories thus obtained are added to P and will be used in Section 3.4 to generate additional entities. Finally, we are interested in redirection information (Lines 10-11) about an article for a seed entity as that provides such with synonyms, plural forms, different spellings, etc.",
  "y": "differences motivation"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_4",
  "x": "In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact. However, EAGER uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph. Biased towards precision over recall, Section 4 shows that combined with the category extraction from abstracts it provides a significantly extended Gazetteer without introducing substantial noise.",
  "y": "background"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_5",
  "x": "In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact. However, EAGER uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph. Biased towards precision over recall, Section 4 shows that combined with the category extraction from abstracts it provides a significantly extended Gazetteer without introducing substantial noise.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_6",
  "x": "**EVALUATION** To evaluate the impact of EAGER on entity recognition, we performed a large set of experiments (on the archeology domain). The experiment domains and <cite>(Zhang and Iria, 2009 )</cite>, which we outperform for all entity types, in some cases up to 5% in F 1 score. ---------------------------------- **EVALUATION SETUP**",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_7",
  "x": "In this experiment, we consider entity recognition in the domain of archaeology. As part of this effort, (Jeffrey et al., 2009) identified three types of entities that are most useful for archaeological research; Subject(SUB), Temporal Terms(TEM), Location (LOC). In this evaluation, we use the same setup as in <cite>(Zhang and Iria, 2009 )</cite>: A corpus of 30 full length UK archaeological reports archived by the Arts and Humanities Data Service (AHDS). The length of the documents varies from 4 to 120 pages. The corpus is inter-annotated by three archaeologists.",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_8",
  "x": "**RESULT** For the evaluation, we perform a 5-fold validation on the above corpus. The evaluate the performance (in terms of precision, recall and F 1 score) for entity recognition of the baseline system as well as the baseline system extended with a gazetteer feature. For the latter, we consider full EAGER as described in Section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information. Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings).",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_9",
  "x": "Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings). Table 1 show the results of the comparison: EA-GER significantly improves precision and recall over the baseline system and outperforms <cite>(Zhang and Iria, 2009 )</cite> in all cases. Furthermore, the impact of all three types of information (dependencies from abstract, category, redirection) of EAGER individually is quite notable with a slight disadvantage for category information. However, in all cases the combination of all three types as proposed in EAGER shows a significant further increase in performance. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_0",
  "x": "Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the EmbodiedQA task [5, <cite>6]</cite> , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches.",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_1",
  "x": "This model consists of a hierarchical navigation module: a planner and a controller, and a question answering module that acts when the navigation module has given up control. In a later work, Das et al.<cite> [6]</cite> introduce Neural Modular Control (NMC) which is a hierarchical policy network that operates over expert sub-policy sketches. The master and sub-policies are initialized with Behavior Cloning (BC), and later fine-tuned with Asynchronous Advantage Actor-Critic (A3C) [19] . Dataset Biases and Trivial Baselines: Many recent studies in language and vision show how biases in a dataset allow models to perform well on a task without leveraging the meaning of the text or image in the underlying dataset. A simple CNN-BoW model was shown to achieve state-of-the-art results [12] on the Visual7W [2<cite>6]</cite> task while also performing surprisingly well compared to the most complex systems proposed for the VQA dataset [1] and other joint vision and language tasks [2, 10] .",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_2",
  "x": "This shows that there is room for building new models that leverage the context and embodiment in the environment. Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. We reproduce the settings for training the VQA model 2 . Specifically we train the VQA model described in<cite> [6]</cite> on the last 5 frames of oracle navigation for 50 epochs with ADAM and a learning rate of 3e \u2212 4 using batch size 20. We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in PACMAN reduces performance to below the text baselines.",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_3",
  "x": "**T 10** T 20 T 50 T any Navigation + VQA PACMAN (BC) [5] 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from<cite> [6]</cite> for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment. Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in [5] Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both.",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_4",
  "x": "Results Detailed results are reported in Table 4 . Following Das et al.<cite> [6]</cite> , we report the agent's top-1 accuracy on the test set when spawned 10, 20 and 50 steps away from the goal, denoted as T 10 , T 20 and T 50 respectively. Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 . We observe that the BoW model outperforms all existing methods except NMC(BC+A3C) in the case where agent is spawned very close to the target. The Nearest Neighbour method also does pretty well, and only falls behind to PACMAN (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case.",
  "y": "uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_0",
  "x": "Finally, we discuss our future plans using this corpus. ---------------------------------- **INTRODUCTION** The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus <cite>[1]</cite> 1 . It was constructed from the Ubuntu chat logs 2 -a collection of logs from Ubuntu-related chat rooms on the Freenode IRC network.",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_1",
  "x": "In this paper, we introduce our preliminary research and experiments with this corpus, and report state-of-the-art results. The rest of the paper continues as follows: 1. we introduce the setup -the data as well as the evaluation of the task; 2. we briefly describe the previously evaluated models; 3. we introduce three different models (one of them being the same as in the previous work); 4. we evaluate these models and experiment with different amount of training data; 5. we conclude and discuss our plans for future works ---------------------------------- **DATA** In this section we briefly describe the data and evaluation metrics used in <cite>[1]</cite> .",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_2",
  "x": "---------------------------------- **APPROACHES** This task can naturally be formulated as a ranking problem which is often tackled by three techniques [3] : (i) pointwise; (ii) pairwise and (iii) listwise ranking. While pairwise and listwise ranking approaches are empirically superior to the pointwise ranking approach, our preliminary experiments use pointwise ranking approach for its simplicity. Note that pointwise method was also used in the original baselines <cite>[1]</cite> .",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_3",
  "x": "---------------------------------- **PREVIOUS WORK** The pointwise architectures reported in <cite>[1]</cite> included (i) TF-IDF, (ii) RNN and (iii) LSTM. In this section, we briefly describe these models. A neural network is used to compute the embedding for the context and the response, denoted as c and r. These are fed through a sigmoid function to compute the pairwise probability.",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_4",
  "x": "**EXPERIMENTS** ---------------------------------- **METHOD** To match the original setup of <cite>[1]</cite> we use the same training data 3 . We use one million training examples and we use the same word vectors pre-trained by GloVe [9] .",
  "y": "similarities uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_5",
  "x": "Baselines from <cite>[1]</cite> Our Table 1 : Results of our experiments compared to the results reported in <cite>[1]</cite> . Meta-parameters of our architectures are the following: our CNN had 400 filters of length 1, 100 filters of length 2 and 100 filters of length 3; our LSTM had 200 hidden units and our bidirectional LSTM had 250 hidden units in each network. For CNNs and LSTMs, the best results were achieved with batch size 256. For Bi-LSTM, the best batch size was 128. Turn User Text 1 A: anyone know why \" aptitude update \" returns a non-successful status (255) ?",
  "y": "background uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_6",
  "x": "For instance, in the current dataset all named entities were replaced with generic tags, which could possibly harm the performance. ---------------------------------- **CONCLUSION** In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in <cite>[1]</cite> . The best performing system is an ensemble of multiple diverse neural networks.",
  "y": "extends differences"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_7",
  "x": "Thanks to the simplicity of this operation, the model does not over-fit the data and generalizes better when learned on small training datasets. On the other hand, the simplicity of the operation does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams occur in the text), thus recurrent models perform better given enough data; (ii) the recurrent models have not made its peak yet, suggesting that adding more training data would improve the model's accuracy. This agrees with Figure 3 of the previous evaluation <cite>[1]</cite> . Figure 3 : Training data size ranging from 100, 000 to the full 1, 000, 000 examples (X axis) and the resulting Recall@1 (Y axis). The CNN has 500, 100 and 100 filters of length 1, 2 and 3.",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_0",
  "x": "**TUTORIAL OVERVIEW** This tutorial will provide an overview of the growing number of multimodal tasks and datasets that combine textual and visual understanding. We will comprehensively review existing stateof-the-art approaches to selected tasks such as image captioning (Chen et al., 2015) , visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017) and visual dialog (Das et al., 2017a,b) , presenting the key architectural building blocks (such as co-attention (Lu et al., 2016) ) and novel algorithms (such as cooperative/adversarial games (Das et al., 2017b) ) used to train models for these tasks. We will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently-released interactive 3D simulation environments designed for this purpose (Anderson et al., 2018b; <cite>Das et al., 2018)</cite> . The goal of this tutorial is to provide a comprehensive yet accessible overview of existing work and to reduce the entry barrier for new researchers.",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_1",
  "x": "Finally, as a move away from static datasets, we will cover recent work on building active RL environments for language-vision tasks. Although models that link vision, language and actions have a rich history (Tellex et al., 2011; Paul et al., 2016; Misra et al., 2017) , we will focus primarily on embodied 3D environments (Anderson et al., 2018b; , considering tasks such as visual navigation from natural language instructions (Anderson et al., 2018b) , and question answering<cite> (Das et al., 2018</cite>; Gordon et al., 2018) . We will position this work in context of related simulators that also offer significant potential for grounded language learning (Beattie et al., 2016; Zhu et al., 2017) . To finish, we will discuss some of the challenges in developing agents for these tasks, as they need to be able to combine active perception, language grounding, commonsense reasoning and appropriate long-term credit assignment to succeed. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_0",
  "x": "Sparsification of the RNN is usually performed either at the level of individual weights (unstructured sparsification) [13, 11,<cite> 1]</cite> or at the level of neurons [14] (structured sparsification -removing weights by groups corresponding to neurons). The latter additionally accelerates the testing stage. However, most of the modern recurrent architectures (e. g. LSTM [3] or GRU [2] ) have a gated structure. We propose to add an intermediate level of sparsification between individual weights <cite>[1]</cite> and neurons [14] -gates (see fig. 1 , left). Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_1",
  "x": "Sparsification of the RNN is usually performed either at the level of individual weights (unstructured sparsification) [13, 11,<cite> 1]</cite> or at the level of neurons [14] (structured sparsification -removing weights by groups corresponding to neurons). The latter additionally accelerates the testing stage. However, most of the modern recurrent architectures (e. g. LSTM [3] or GRU [2] ) have a gated structure. We propose to add an intermediate level of sparsification between individual weights <cite>[1]</cite> and neurons [14] -gates (see fig. 1 , left). Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias.",
  "y": "uses extends"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_2",
  "x": "The described idea can be implemented for any gated architecture in any sparsification framework. We implement the idea for LSTM in two frameworks: pruning [14] and Bayesian sparsification <cite>[1]</cite> and observe that resulting gate structures (which gates are constant and which are not) vary for different NLP tasks. We analyze these gate structures and connect them to the specifics of the particular tasks. The proposed method also improves neuron-wise compression of the RNN in most cases. In this section, we describe the three-level sparsification approach for LSTM.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_3",
  "x": "In contrast to our approach, they do not sparsify gates, and remove a neuron if all its ingoing and outgoing connections are set to zero. Bayesian sparsification. We rely on Sparse Variational Dropout [10,<cite> 1]</cite> to sparsify individual weights. Following [5] , for each neuron, we introduce a group weight which is multiplied by the output of this neuron in the computational graph (setting to zero this group weight entails removing the neuron). To sparsify gates, for each gate we introduce a separate group weight which is multiplied by the preactivation of the gate before adding a bias (setting to zero this group weight makes the gate constant).",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_5",
  "x": "We use a standard model of Zaremba et al. [16] of two sizes (small and large) with an embedding layer, two LSTM layers, and a fully-connected output layer (Emb + 2 LSTM + FC). Here regularization is applied only to LSTM layers following [14] , and its strength is selected using grid search so that qualities of ISS and our model are approximately equal. In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following <cite>[1]</cite> . The architecture for the character-level LM is LSTM + FC, for the text classification is Emb + LSTM + FC on the last hidden state, for the word level LM is the same as in pruning. Here we regularize and sparsify all layers following <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_6",
  "x": "We use a standard model of Zaremba et al. [16] of two sizes (small and large) with an embedding layer, two LSTM layers, and a fully-connected output layer (Emb + 2 LSTM + FC). Here regularization is applied only to LSTM layers following [14] , and its strength is selected using grid search so that qualities of ISS and our model are approximately equal. In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following <cite>[1]</cite> . The architecture for the character-level LM is LSTM + FC, for the text classification is Emb + LSTM + FC on the last hidden state, for the word level LM is the same as in pruning. Here we regularize and sparsify all layers following <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_7",
  "x": "In <cite>[1]</cite> , SparseVD is adapted to the RNNs. Training our model. We work with the group weights z in the same way as with the weights W : we approximate the posterior with the fully factorized normal distribution given the fully factorized log-uniform prior distribution. To estimate the expectation in (1), we sample weights from the approximate posterior distribution in the same way as in <cite>[1]</cite> . With the integral estimated with one Monte-Carlo sample, the first term in (1) becomes the usual loss function (for example, cross-entropy in language modeling).",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_8",
  "x": "To estimate the expectation in (1), we sample weights from the approximate posterior distribution in the same way as in <cite>[1]</cite> . With the integral estimated with one Monte-Carlo sample, the first term in (1) becomes the usual loss function (for example, cross-entropy in language modeling). The second term is a regularizer depending on the parameters \u00b5 and \u03c3 (for the exact formula, see [10] ). After learning, we zero out all the weights and the group weights with the signal-to-noise ratio less than 0.05. At the testing stage, we use the mean values of all the weights and the group weights.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_9",
  "x": "In all the Bayesian models, we sparsify the weight matrices of all layers. Since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following Chirkova et al. <cite>[1]</cite> . For the networks with the embedding layer, in configurations W+N and W+G+N, we also sparsify the embedding components (by introducing group weights z x multiplied by x t .) We train our networks using Adam [4] . Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_10",
  "x": "Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. Models for the text classification and the character-level LM are trained in the same setting as in <cite>[1]</cite> (we used the code provided by the authors). For the text classification tasks, we use a learning rate equal to 0.0005 and train Bayesian models for 800 / 150 epochs on IMDb / AGNews. The embedding layer for IMDb / AGNews is initialized with word2vec [9] / GloVe [12] . For the language modeling tasks, we train Bayesian models for 250 / 50 epochs on character-level / word-level tasks using a learning rate of 0.002.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_12",
  "x": "**D EXPERIMENTS WITH UNSTRUCTURED BAYESIAN SPARSIFICATION** In this section, we present experimental results for the unstructured Bayesian sparsification (configuration Bayes W). This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> . Table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the IMDB classification task and the second LSTM layer of the word-level language modeling task. Since Bayes W model does not comprise any group weights, the overall compression of the RNN is lower than for Bayes W+G+N (tab. 1), so there are more non-constant gates.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_13",
  "x": "In the last columns, the numbers of the remaining hidden neurons and non-constant gates in the LSTM layers are reported. ---------------------------------- **D EXPERIMENTS WITH UNSTRUCTURED BAYESIAN SPARSIFICATION** In this section, we present experimental results for the unstructured Bayesian sparsification (configuration Bayes W). This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> .",
  "y": "background uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_0",
  "x": "To achieve the goal of \"strong AI\", we need to change our learning goal to really understand the sentiment of words. Which means that the algorithm should know how each word influences the sentiment of a document in different tasks. If we can achieve this learning goal, the algorithms are able to solve new tasks without teaching. Zhiyuan Chen and etc. <cite>[2]</cite> ever proposed a approach to close the goal.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_1",
  "x": "Zhiyuan Chen and etc. <cite>[2]</cite> ever proposed a approach to close the goal. They made a big progress but the supervised learning still is needed. Guangyi Lv and etc. [4] extend the work of <cite>[2]</cite> with a neural network based approach.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_2",
  "x": "Efficient Lifelong Machine Learning (ELLA) [6] raised by Ruvolo and Eaton. Comparing with the multi-task learning [1] , ELLA is much more efficient. Zhiyuan and etc. <cite>[2]</cite> improved the sentiment classification by involving knowledge. The object function was modified with two penalty terms which corresponding with previous tasks.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_3",
  "x": "The knowledge system contains the following components: ---------------------------------- **COMPONENTS OF LML** \u2022 Knowledge Base (KB): The knowledge Base <cite>[2]</cite> mainly used to maintain the previous knowledge. Based on the type of knowledge, it could be divided as Past Information Store (PIS), Meta-Knowledge Miner (MKM) and Meta-Knowledge Store (MKS).",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_4",
  "x": "Based on the type of knowledge, it could be divided as Past Information Store (PIS), Meta-Knowledge Miner (MKM) and Meta-Knowledge Store (MKS). \u2022 Knowledge Reasoner (KR): The knowledge reasoner is designed to generate new knowledge upon the archived knowledge by logic inference. A strict logic design is required so the most of the LML algorithms lack of the component. \u2022 Knowledge-Base Learner (KBL): The Knowledge-Based Learner <cite>[2]</cite> aims to retrieve and transfer previous knowledge to the current task. Hence, it contains two parts: task knowledge miner and leaner.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_5",
  "x": "Hong and etc. [3] had discussed that the NLP field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human. Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks. This requires the algorithms could know when the knowledge can be used and when can not due to the distribution of each sub-tasks is different. Known these, an algorithm can be called as \"lifelong\" because it is able to transfer previous knowledge to new tasks to improve performance.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_6",
  "x": "**SENTIMENT CLASSIFICATION** Hong and etc. [3] had discussed that the NLP field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human. Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks. This requires the algorithms could know when the knowledge can be used and when can not due to the distribution of each sub-tasks is different.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_7",
  "x": "This because the complexity of neural network limits the researches to define and extract knowledge from the data. As the previous work <cite>[2]</cite> , this paper also uses Na\u00efve Bayes as the knowledge can be presented by the probability. In this way, we need to know the probability of each word that shows in the positive or negative content. We also need to know well that some words may only have sentiment polarity in some specific domains(equal to tasks in this paper). \"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation.",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_8",
  "x": "This because the complexity of neural network limits the researches to define and extract knowledge from the data. As the previous work <cite>[2]</cite> , this paper also uses Na\u00efve Bayes as the knowledge can be presented by the probability. In this way, we need to know the probability of each word that shows in the positive or negative content. We also need to know well that some words may only have sentiment polarity in some specific domains(equal to tasks in this paper). \"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_9",
  "x": "---------------------------------- **CONTRIBUTION OF THIS PAPER** Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. It still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions:",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_10",
  "x": "Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions: \u2022 A improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge. \u2022 We introduce a novel approach to discover and store the words with sentiment polarity for reuse.",
  "y": "extends"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_11",
  "x": "Hence, to determine the words with polarity is the key to predict the sentiment. Na\u00efve Bayesian (NB) classifier [5] calculates the probability of each word w in a document d and then to predict the sentiment polarity (positive or negative). We use the same formula below as in the LSC <cite>[2]</cite> . P(w |c j ) is the probability of a word appears in a class: Where c j is either positive (+) or negative (-) sentiment polarity.",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_12",
  "x": [
   "As we known, not all words have sentimental polarity like \"a\", \"one\" and etc. while some words always have polarity like \"good\", \"hate\", \"excellent\" and so on. Hence, in order to achieve the goal of the lifelong learning. We need to find the words always have sentiment polarity and be careful for those words only shows polarity in specific domains."
  ],
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_13",
  "x": "**LIFELONG SEMI-SUPERVISED LEARNING FOR SENTIMENT CLASSIFICATION** Although LSC <cite>[2]</cite> considered the difference among domains, it still is a typical supervised learning approach. In this paper, we proposed to learn as two stages: (1) Initial Learning Stage: to explore a basic set of sentiment words. After that, the model should be able to basically classify a new domain with a good performance.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_14",
  "x": "In the experiment, we use the same datasets as LSC <cite>[2]</cite> used. It contains the reviews from 20 domains crawled from the Amazon.com and each domain has 1,000 reviews (the distribution of positive and negative reviews is imbalanced). ---------------------------------- **WORD POLARITY ANALYSIS** To answer the first question for the initial learning stage, we need to know which words exactly influence the sentiment classification.",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_0",
  "x": "**INTRODUCTION** Lexical simplification is the task of automatically rewriting a text by substituting words or phrases with simpler variants, while retaining its meaning and grammaticality. The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\u0161 and\u0160tajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) .",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_1",
  "x": "In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) . Moreover, deep architectures are not explored in these models.",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_2",
  "x": "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) . Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences.",
  "y": "motivation"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_3",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS AND TRAINING PROCEDURE OF THE DSSM** Following previous works that used supervised machine learning for ranking in lexical simplification<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , we train the DSSM using the LexMTurk dataset<cite> (Horn et al., 2014)</cite> , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (Paetzold and Specia, 2017) . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (Rumelhart et al., 1988) . The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005) .",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_4",
  "x": "---------------------------------- **DATASETS AND EVALUATION METRICS** To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (Paetzold and Specia, 2016b) , which contains 929 instances, and NNSEval (Paetzold and Specia, 2016a) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (Paetzold and Specia, 2017) . Since both datasets contain instances from the LexMturk dataset<cite> (Horn et al., 2014)</cite> , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_5",
  "x": "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (Paetzold and Specia, 2017) . Since both datasets contain instances from the LexMturk dataset<cite> (Horn et al., 2014)</cite> , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glava\u0161 and\u0160tajner (2015) and<cite> Horn et al. (2014)</cite> : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_6",
  "x": "with default parameters) for ranking substitution candidates, similar to the method described in<cite> (Horn et al., 2014)</cite> . All the three models employ the n-gram probability features extracted from the SubIMDB corpus (Paetzold and Specia, 2015) , as described in (Paetzold and Specia, 2017) , and are trained using the LexMTurk dataset. ---------------------------------- **RESULTS** The top part of table 1 (Substitution Candidates Ranking) summarizes the results of all three systems.",
  "y": "similarities"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_0",
  "x": "While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstructionbased objective of<cite> Zhang et al. (2017)</cite> with our sentence content probe objective in a semisupervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.",
  "y": "motivation"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_1",
  "x": "**INTRODUCTION** Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification<cite> (Zhang et al., 2017)</cite> , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) . However, downstream performance provides little insight into the kinds of linguistic properties that are encoded by these embeddings. Inspired by the growing body of work on sentence-level linguistic probe tasks (Adi et al., 2017; Conneau et al., 2018) , we set out to evaluate a state-of-the-art paragraph embedding method using a probe task to measure how well it encodes the identity of the sentences within a paragraph. We discover that the method falls short of capturing this basic property, and that implementing a simple objective to fix this issue improves classification performance, training speed, and generalization ability.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_2",
  "x": "Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification<cite> (Zhang et al., 2017)</cite> , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) . However, downstream performance provides little insight into the kinds of linguistic properties that are encoded by these embeddings. Inspired by the growing body of work on sentence-level linguistic probe tasks (Adi et al., 2017; Conneau et al., 2018) , we set out to evaluate a state-of-the-art paragraph embedding method using a probe task to measure how well it encodes the identity of the sentences within a paragraph. We discover that the method falls short of capturing this basic property, and that implementing a simple objective to fix this issue improves classification performance, training speed, and generalization ability. We specifically investigate the paragraph embedding method of<cite> Zhang et al. (2017)</cite> , which consists of a CNN-based encoder-decoder model paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs.",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_4",
  "x": "Our results indicate that incorporating probe objectives into downstream models might help improve both accuracy and efficiency, which we hope will spur more linguistically-informed research into paragraph embedding methods. ---------------------------------- **PROBING PARAGRAPH EMBEDDINGS FOR SENTENCE CONTENT** In this section, we first fully specify our probe task before comparing the model of<cite> Zhang et al. (2017)</cite> to a simple bag-of-words model. Somewhat surprisingly, the latter substantially outperforms the former despite its relative simplicity.",
  "y": "differences"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_5",
  "x": "Paragraphs to train our classifiers are extracted from the Hotel Reviews corpus (Li et al., 2015) , which has previously been used for evaluating the quality of paragraph embeddings (Li et al., 2015; <cite>Zhang et al., 2017)</cite> . We only consider paragraphs that have at least two sentences. Our dataset has 346,033 training paragraphs, 19,368 for validation, and 19,350 for testing. The average numbers of sentences per paragraph, tokens per paragraph, and tokens per sentence are 8.0, 123.9, and 15.6, respectively. The vocabulary contains 25,000 tokens.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_6",
  "x": "This result indicates that early stopping based on BLEU is not feasible with reconstruction-based pre-training objectives. With fine-tuning, CNN-SC substantially boosts accuracy and generalization We switch gears Table 4 : CNN-SC outperforms other baseline models that do not use external data, including CNN-R. All baseline models are taken from<cite> Zhang et al. (2017)</cite> . now to our fine-tuning experiments. Specifically, we take the CNN encoder pre-trained using our sentence content objective and then fine-tune it on downstream classification tasks with supervised labels. While our previous version of CNN-SC created just a single positive/negative pair of examples from a single paragraph, for our finetuning experiments we create a pair of examples from every sentence in the paragraph to maximize the training data.",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_8",
  "x": "**RELATED WORK** Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; <cite>Zhang et al., 2017)</cite> To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) . In this paper, we extend the notion of probe tasks to the paragraph level. Transfer learning Another line of related work is transfer learning, which has been the driver of recent successes in NLP. Recently-proposed objectives for transfer learning include surrounding sentence prediction (Kiros et al., 2015) , paraphrasing (Wieting and Gimpel, 2017) , entailment (Conneau et al., 2017) , machine translation (McCann et al., 2017) , discourse (Jernite et al., 2017; Nie et al., 2017) , and language modeling (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) .",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_9",
  "x": "In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstructionbased objective of<cite> Zhang et al. (2017)</cite> with our sentence content probe objective in a semisupervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_0",
  "x": "We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser <cite>(Cross and Huang, 2016)</cite> ; it employs the strong generalization power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information. We make the following contributions:",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_1",
  "x": "2. We propose a novel joint parser that parses at both constituency and discourse levels. Our parser performs discourse parsing in an endto-end manner, which greatly reduces the efforts required in preprocessing the text for segmentation and feature extraction, and, to our best knowledge, is the first end-to-end discourse parser in literature (Section 3). 3. Even though it simultaneously performs constituency parsing, our parser does not use any explicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the powerful span-based framework of<cite> Cross and Huang (2016)</cite> (Section 3). 4. Empirically, our end-to-end parser outperforms the existing pipelined discourse parsing efforts. When the gold EDUs are provided, our parser is also competitive to other existing approaches with sophisticated features (Section 4).",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_2",
  "x": "**JOINT SYNTACTO-DISCOURSE PARSING** Given the combined syntacto-discourse treebank, we now propose a joint parser that can perform end-to-end discourse segmentation and parsing. ---------------------------------- **EXTENDING SPAN-BASED PARSING** As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of<cite> Cross and Huang (2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_3",
  "x": "As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of<cite> Cross and Huang (2016)</cite> . As in span-based parsing, at each step, we maintain a a stack of spans. Notice that in conventional incremental parsing, the stack stores the subtrees Similar to span-based constituency parsing, we alternate between structural (either shift or combine) and label (label X or nolabel) actions in an odd-even fashion. But different from<cite> Cross and Huang (2016)</cite> , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift). This is because in our parsing mechanism, the discourse relation between two EDUs is actually determined after the previous combine action.",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_4",
  "x": "But different from<cite> Cross and Huang (2016)</cite> , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift). This is because in our parsing mechanism, the discourse relation between two EDUs is actually determined after the previous combine action. We need to keep the splitting point to clearly find the spans of the two EDUs to determine their relations. This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in<cite> Cross and Huang (2016)</cite> . The nolabel action makes the binarization of the discourse/constituency tree unnecessary, because nolabel actually combines the top two spans on the stack \u03c3 into one span, but without annotating the new span a label.",
  "y": "differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_5",
  "x": "Table 1 : Accuracies on PTB-RST at constituency and discourse levels. ---------------------------------- **RECURRENT NEURAL MODELS AND TRAINING** The scoring functions in the deductive system (Figure 4 ) are calculated by an underlying neural model, which is similar to the bi-directional LSTM model in<cite> Cross and Huang (2016)</cite> that evaluates based on span boundary features. Again, it is important to note that no discourse or syntactic tree structures are represented in the features.",
  "y": "similarities"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_6",
  "x": "We use the \"training with exploration\" strategy (Goldberg and Nivre, 2013) and the dynamic oracle mechanism described in<cite> Cross and Huang (2016)</cite> to make sure the model can handle unseen parsing configurations properly. ---------------------------------- **EMPIRICAL RESULTS** We use the treebank described in Section 2 for empirical evaluation. We randomly choose 30 documents from the training set as the development set.",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_7",
  "x": "We tune the hyperparameters of the neural model on the development set. For most of the hyperparameters we settle with the same values suggested by<cite> Cross and Huang (2016)</cite> . To alleviate the overfitting problem for training on the relative small RST Treebank, we use a dropout of 0.5. One particular hyperparameter is that we use a value \u03b2 to balance the chances between training following the exploration (i.e., the best action chosen by the neural model) and following the correct path provided by the dynamic oracle. We find that \u03b2 = 0.8, i.e., following the dynamic oracle with asyntactic feats.",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_8",
  "x": "The result is shown in Table 1 . Note that in constituency level, the accuracy is not directly comparable with the accuracy reported in<cite> Cross and Huang (2016)</cite> , since: a) our parser is trained on a much smaller dataset (RST Treebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-level accuracy. Table 2 shows that, in the perspective of endto-end discourse parsing, our parser first outperforms the state-of-the-art segmentator of Bach et al. (2012) , and furthermore, in end-to-end parsing, the superiority of our parser is more pronounced comparing to the previously best parser of Hernault et al. (2010) . On the other hand, the majority of the conventional discourse parsers are not end-to-end: they rely on gold EDU segmentations and pre-trained tools like Stanford parsers to generate features. We perform an experiment to compare the performance of our parser with them given the gold EDU segments ( Table 3 ). Note that most of these parsers do not handle multi-branching discourse nodes and are trained and evaluated on binarized discourse trees (Feng and Hirst, 2014; Li et al., 2014a,b; Ji and Eisenstein, 2014; Heilman and Sagae, 2015) , so their performances are actually not directly comparable to the results we reported.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_1",
  "x": "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection <cite>(Bolukbasi et al., 2016)</cite> is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA).",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_2",
  "x": "In addition to gender-appropriate analogies such as king:queen::man:woman, stereotypical analogies such as doctor:nurse::man:woman also hold in SGNS embedding spaces <cite>(Bolukbasi et al., 2016)</cite> . Caliskan et al. (2017) created an association test for word vectors called WEAT, which uses cosine similarity to measure how associated words are with respect to two sets of attribute words (e.g., 'male' vs. 'female'). For example, they claimed that science-related words were significantly more associated with male attributes and art-related words with female ones. Since these associations are socially undesirable, they were described as gender bias. Despite these remarkable findings, such undesirable word associations remain poorly understood.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_3",
  "x": "We begin by proving that for any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), debiasing vectors post hoc via subspace projection is, under certain conditions, equivalent to training on an unbiased corpus without reconstruction error. We find that contrary to what<cite> Bolukbasi et al. (2016)</cite> suggested, word embeddings should not be normalized before debiasing, as vector length can contain important information (Ethayarajh et al., 2018) . To guarantee unbiasedness, the bias subspace should also be the span -rather than a principal component -of the vectors used to define it. If applied this way, the subspace projection method can be used to provably debias SGNS and GloVe embeddings with respect to the word pairs that define the bias subspace. Using this notion of a \"bias subspace\", we then prove that WEAT, the most common association test for word embeddings, has theoretical flaws that cause it to systematically overestimate bias.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_4",
  "x": "2. To use the subspace projection method, one must have prior knowledge of which words are gender-specific by definition, so that they are not also debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space. In contrast to the supervised method proposed by<cite> Bolukbasi et al. (2016)</cite> for identifying these gender-specific words, we introduce an unsupervised method. Ours is much more effective at preserving gender-appropriate analogies and precluding gender-biased ones. To allow a fair comparison with prior work, our experiments in this paper focus on gender association.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_5",
  "x": "Each debiased word vector was thus orthogonal to the gender bias subspace and its projection on the subspace was zero. While this subspace projection method precluded gender-biased analogies from holding in the embedding space,<cite> Bolukbasi et al. (2016)</cite> did not provide any theoretical guarantee that the vectors were unbiased (i.e., equivalent to vectors that would be obtained from training on a gender-agnostic corpus with no reconstruction error). Other work has tried to learn gender-neutral embeddings from scratch (Zhao et al., 2018) , despite this approach requiring custom changes to the objective of each embedding model. ---------------------------------- **MEASURING ASSOCIATIONS**",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_6",
  "x": "**PROVABLY DEBIASING EMBEDDINGS** Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorization (e.g., GloVe, SGNS), debiasing embeddings post hoc using the subspace projection method is, under certain conditions, equivalent to training on a perfectly unbiased corpus without reconstruction error. Definition 1 Let M denote the symmetric wordcontext matrix for a given training corpus that is implicitly or explicitly factorized by the embedding model.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_7",
  "x": "---------------------------------- **PROVABLY DEBIASING EMBEDDINGS** Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorization (e.g., GloVe, SGNS), debiasing embeddings post hoc using the subspace projection method is, under certain conditions, equivalent to training on a perfectly unbiased corpus without reconstruction error.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_8",
  "x": "This implies that for any word w and any (x, y) \u2208 S, Each debiased word vector w d is orthogonal to the bias subspace in the word embedding space, so \u2200 (x, y) \u2208 S, w d , x \u2212 y = 0. In conjunction with (2), this implies that This means that if a debiased word w is represented with vector w d instead of w, it is unbiased with respect to S by Definition 1. This implies that the co-occurrence matrix M d that is reconstructed using the debiased word matrix W d is also unbiased with respect to S. The subspace projection method is therefore far more powerful than initially stated in<cite> Bolukbasi et al. (2016)</cite> : not only can it be applied to any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), but debiasing word vectors in this way is equivalent to training on a perfectly unbiased corpus when there is no reconstruction error.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_9",
  "x": "Because of this, WEAT seems to be an intuitive measure. However, as shown in Propositions 1 and 2, there are two key theoretical flaws to WEAT that cause it to overestimate the degree of association and ultimately make it an inappropriate metric for word embeddings. The only other metric of note quantifies association as |cos( w, b)| c , where b is the bias subspace and c \u2208 R the \"strictness\" of the measurement <cite>(Bolukbasi et al., 2016)</cite> . For the same reason discussed in Proposition 1, this measure can also overestimate the degree of association. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_10",
  "x": "**DEFINITION 2** The relational inner product association \u03b2 ( w; b) of a word vector w \u2208 V with respect to a relation vector b \u2208 V is w, b . Where S is a non-empty set of ordered word pairs (x, y) that define the association, b is the first principal component of { x \u2212 y | (x, y) \u2208 S}. Our metric, the relational inner product association (RIPA), is simply the inner product of a relation vector describing the association and a given word vector in the same embedding space. To use the terminology in<cite> Bolukbasi et al. (2016)</cite> , RIPA is the scalar projection of a word vector onto a onedimensional bias subspace defined by the unit vector b. In their experiments,<cite> Bolukbasi et al. (2016)</cite> defined b as the first principal component for a set of gender difference vectors (e.g., man \u2212 woman).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_11",
  "x": "**SETUP** For our experiments, we use SGNS embeddings trained on Wikipedia, since RIPA is highly interpretable for SGNS (see section 5.1). This means that for any given word in the vocabulary, we can compare its gender association in the training corpus to its gender association in the embedding space, which should be equal under perfect reconstruction. Words are grouped into three categories with respect to gender: biased, appropriate, and neutral. We create lists of biased and appropriate words using the<cite> Bolukbasi et al. (2016)</cite> lists of gender-biased and gender-appropriate analogies.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_12",
  "x": "Let g(w; x, y) denote the RIPA of a word w with respect to the gender relation vector defined by word pair (x, y), let\u011d(w; x, y) denote what g(w; x, y) would be under perfect reconstruction for an SGNS embedding model, and let \u2206 g denote the change in absolute gender association from corpus to embedding space. Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus. \u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them. The<cite> Bolukbasi et al. (2016)</cite> method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_13",
  "x": "Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus. \u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them. The<cite> Bolukbasi et al. (2016)</cite> method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right). Our unsupervised method (dashed line) does much better in both respects.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_14",
  "x": "\u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them. The<cite> Bolukbasi et al. (2016)</cite> method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right). Our unsupervised method (dashed line) does much better in both respects. \u03bb \u2248 1 in practice (Ethayarajh et al., 2018; Mimno and Thompson, 2017) . Similarly, \u03b1 \u2190 \u22121 because it minimizes the difference between x \u2212 y and its information theoretic interpretation over the gender-defining word pairs in S, though this is an estimate and may differ from the true value of \u03b1.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_15",
  "x": "As a result, \u03bb king, man > (PMI('king', 'man') \u2212 log k) for SGNS, for example. What is often treated as a useful property of word embeddings can have, with respect to gender bias, a pernicious effect. ---------------------------------- **DEBIASING WITHOUT SUPERVISION** To use the subspace projection method <cite>(Bolukbasi et al., 2016)</cite> , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_16",
  "x": "To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary. This bootstrapped list of gender-appropriate words was then left out during debiasing. The way in which<cite> Bolukbasi et al. (2016)</cite> evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies. However, this does not capture whether gender-appropriate analogies are successfully preserved and gender-biased analogies successfully precluded. In Figure 1 , we show how the number of appropriate and biased analogies changes after debiasing.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_17",
  "x": "This bootstrapped list of gender-appropriate words was then left out during debiasing. The way in which<cite> Bolukbasi et al. (2016)</cite> evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies. However, this does not capture whether gender-appropriate analogies are successfully preserved and gender-biased analogies successfully precluded. In Figure 1 , we show how the number of appropriate and biased analogies changes after debiasing. The x-axis captures how strongly gendered the analogy is, using the absolute RIPA value |\u03b2 ( w; b)| but replacing w with the difference vector defined by the first word pair (e.g., king \u2212 queen).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_18",
  "x": "Using difference vectors from biased analogies, such as doctor \u2212 midwife, we then create a bias-defining relation vector b the same way. We then debias a word w using the subspace projection method iff it satisfies |\u03b2 ( w; b * )|< |\u03b2 ( w; b )|. As seen in Figure 1 , this simple condition is sufficient to preserve almost all gender-appropriate analogies while precluding most gender-biased ones. In our debiased embedding space, 94.9% of gender-appropriate analogies with a strength of at least 0.5 are preserved in the embedding space while only 36.7% of gender-biased analogies are. In contrast, the<cite> Bolukbasi et al. (2016)</cite> approach 2 Available at https://github.com/tolga-b/debiaswe preserves only 16.5% of appropriate analogies with a strength of at least 0.5 while preserving 80.0% of biased ones.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_19",
  "x": "Recall that we use the same debiasing method as<cite> Bolukbasi et al. (2016)</cite> ; the difference in performance can only be ascribed to how we choose the gender-appropriate words. Combining our heuristic with other methods may yield even better results, which we leave as future work. ---------------------------------- **CONCLUSION** In this paper, we answered several open questions about undesirable word associations in embedding spaces.",
  "y": "differences uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_0",
  "x": "****ASSESSING BERT'S SYNTACTIC ABILITIES**** **ABSTRACT** I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) <cite>\"coloreless green ideas\"</cite> subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_1",
  "x": "In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. <cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting. The BERT model is based on the \"Transformer\" architecture (Vaswani et al., 2017) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.",
  "y": "differences"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_2",
  "x": "Recent work examines the extent to which RNN-based models capture syntax-sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure. In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. <cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting. The BERT model is based on the \"Transformer\" architecture (Vaswani et al., 2017) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_3",
  "x": "I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks. ---------------------------------- **METHODOLOGY** I use the stimuli provided by (Linzen et al., 2016; <cite>Gulordava et al., 2018</cite>; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_4",
  "x": "Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks. ---------------------------------- **METHODOLOGY** I use the stimuli provided by (Linzen et al., 2016; <cite>Gulordava et al., 2018</cite>; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_5",
  "x": "2 The evaluation is then performed on sentences with \"agreement attractors\" in which at there is at least one noun between the verb and its subject, and all of the nouns between the verb and subject are of the opposite number from the subject. <cite>Gulordava et al. (2018)</cite> also start with existing sentences. However, in order to control for the possibillity of the model learning to rely on \"semantic\" selectional-preferences cues rather than syntactic ones, <cite>they</cite> replace each content word with random words from the same part-ofspeech and inflection. This results in \"<cite>coloreless green ideas</cite>\" nonce sentences. The evaluation is then performed similarly to the LM setup of Linzen et al. (2016) : the sentence is fed into a pretraiend LSTM LM up to the focus verb, and the model is considered correct if the probability assigned to the correct inflection of the original verb form given the prefix is larger than that assigned to the incorrect inflection.",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_6",
  "x": "This differs from Linzen et al. (2016) and <cite>Gulordava et al. (2018)</cite> by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. 4 I experiment with the bert-large-uncased and bert-base-uncased models. Discarded Material The bi-directional setup precludes using using the NPI stimuli of Marvin and Linzen (2018) , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in <cite>Gulordava et al. (2018)</cite> , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",
  "y": "extends motivation"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_7",
  "x": "4 I experiment with the bert-large-uncased and bert-base-uncased models. Discarded Material The bi-directional setup precludes using using the NPI stimuli of Marvin and Linzen (2018) , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in <cite>Gulordava et al. (2018)</cite> , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb. 5 This is not an issue in the manually constructed (Marvin and Linzen, 2018 ) stimuli due to the patterns they chose. Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model).",
  "y": "differences extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_8",
  "x": "This include discarding Marvin and Linzen (2018) stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300). I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from <cite>(Gulordava et al., 2018)</cite> . Limitations The BERT results are not directly comparable to the numbers reported in previous work. Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books). 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_10",
  "x": "While not strictly comparable, the numbers reported by <cite>Gulordava et al. (2018)</cite> for the LSTM in this condition (on All) is 74.1 \u00b1 1.6. ---------------------------------- **REPRODUCABILITY** Code is available at https://github.com/yoavg/bert-syntax. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_11",
  "x": "---------------------------------- **DISCUSSION** The BERT models perform remarkably well on all the syntactic test cases. I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",
  "y": "background uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_0",
  "x": "Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of<cite> Sagae and Lavie (2006)</cite> . These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23.",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_1",
  "x": "We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. ---------------------------------- **INTRODUCTION** Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman an\u010f Zabokrtsk\u1ef3, 2005;<cite> Sagae and Lavie, 2006</cite>; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015) . These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F 1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser.",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_2",
  "x": "All of these have at least one of the caveats that (1) overall computation is increased and runtime is determined by the slowest parser and (2) using multiple parsers increases the system complexity, making it more difficult to deploy in practice. In this paper, we describe a simple hybridization extension (\"fusion\") which obtains much of hybridization's benefits while using only a single n-best parser and minimal extra computation. Our method treats each parse in a single parser's n-best list as a parse from n separate parsers. We then adapt parse combination methods by Henderson and Brill (1999) ,<cite> Sagae and Lavie (2006)</cite> , and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results.",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_3",
  "x": "Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows<cite> Sagae and Lavie (2006)</cite> . Parses are weighted by the estimated probabilities from the parser. Given n trees and their weights, the model computes a constituent's weight by summing weights of all trees containing that constituent.",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_4",
  "x": "For BLLIP, the Table 2 : F 1 of a baseline parser, fusion, and baselines on development sections of corpora (WSJ section 24 and Brown tune). smoothing exponent (\u03b2) is best set around 1.0, with accuracy falling off if the value deviates too much. Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by Henderson and Brill (1999) ). Since score values are normalized, this means that constituents need roughly half the \"score mass\" in order to be included in the chart. Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart <cite>(Sagae and Lavie, 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_5",
  "x": "We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ). Model combination Table 3 : Evaluation of the constituency fusion method on six parsers across six domains. x/y indicates the F 1 from the baseline parser (x) and the baseline parser with fusion (y) respectively.",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_6",
  "x": "We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ). Model combination Table 3 : Evaluation of the constituency fusion method on six parsers across six domains. x/y indicates the F 1 from the baseline parser (x) and the baseline parser with fusion (y) respectively.",
  "y": "similarities"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_7",
  "x": "We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in<cite> Sagae and Lavie (2006)</cite> . Approach (1) does not provide any benefit (LAS drops between 0.5% and 2.4%). This may result from fusion's artifacts including unusual unary chains or nodes with a large number of children -it is possible that adjusting unary handling and the precision/recall tradeoff may reduce these issues. Approach (2) provided only modest benefits compared to those from constituency parsing fusion. The largest LAS increase for (2) is 0.6% for the Stanford Parser, though for Berkeley and Self-trained BLLIP, dependency fusion results in small losses (-0.1% LAS).",
  "y": "similarities uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_0",
  "x": "In our experiments, we only use the English texts and corresponding DRSs. We use PMB release 2.2.0, which contains gold standard (fully manually annotated) data of which we use 4,597 as train, 682 as dev and 650 as test instances. It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Most sentences are between 5 and 15 tokens in length. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_1",
  "x": "In our experiments, we only use the English texts and corresponding DRSs. We use PMB release 2.2.0, which contains gold standard (fully manually annotated) data of which we use 4,597 as train, 682 as dev and 650 as test instances. It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_2",
  "x": "Figure 1: DRS in box format (a), gold clause representation (b) and example system output (c) for I am not working for Tom, with precision of 5/8 and recall of 5/9, resulting in an F-score of 58.8. ---------------------------------- **REPRESENTING INPUT AND OUTPUT** We represent the source and target data in the same way as<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters. The target DRS is also represented as a sequence of characters, with the exception of DRS operators, thematic roles and DRS variables, which are represented as super characters (Van Noord and Bos, 2017b), i.e. individual tokens.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_3",
  "x": "---------------------------------- **NEURAL ARCHITECTURE** We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> . However, their model was trained with OpenNMT (Klein et al., 2017) , which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018).",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_4",
  "x": "Produced DRSs are compared with the gold standard representations by using COUNTER (Van Noord, Abzianidze, Haagsma, and Bos, 2018) . This is a tool that calculates micro precision, recall and F-score over matching clauses, similar to the SMATCH (Cai and Knight, 2013 ) evaluation tool for AMR parsing. All clauses have the same weight in matching, except for REF clauses, which are ignored. An example of the matching procedure is shown in Figure 1 . The produced DRSs go through a strict syntactic and semantic validation process, as described in<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_5",
  "x": "Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008 (Bos, , 2015 ) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on \u03bb-calculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance 5 , while SIM-SPAR outputs the DRS of the most similar sentence in the training set, based on a simple word embedding metric.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_6",
  "x": "We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008 (Bos, , 2015 ) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on \u03bb-calculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance 5 , while SIM-SPAR outputs the DRS of the most similar sentence in the training set, based on a simple word embedding metric. 6 The results are shown in Table 5 .",
  "y": "similarities uses differences"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_7",
  "x": "When compared to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , retrained with the same data used in our systems, the largest improvement (3.6 and 3.5 for dev and test) comes from switching framework and changing certain parameters such as the optimizer and learning rate. However, the linguistic features are clearly still beneficial when using only gold data (increase of 2.7 and 1.9 for dev and test), and also still help when employing additional silver data (1.1 and 0.3 increase for dev and test, both significant). <cite>Abzianidze, Toral, and Bos (2018)</cite> ---------------------------------- **CONCLUSIONS**",
  "y": "differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_0",
  "x": "dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and<cite> Liu et al. (2018)</cite> , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking.",
  "y": "extends differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_1",
  "x": "Chiu and Nichols (2016) search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; <cite>Liu et al., 2018)</cite> . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly.",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_2",
  "x": "Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; <cite>Liu et al., 2018)</cite> , GRU (Yang et al., 2016) , and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_3",
  "x": "---------------------------------- **WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; <cite>Liu et al., 2018)</cite> . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011; dos Santos et al., 2015; Strubell et al., 2017) .",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_4",
  "x": "We investigate the main influencing factors to system accuracy, including the character sequence representations, word sequence representations, inference algorithm, pretrained embeddings, tag scheme, running environment and optimizer; analyzing system performances from the perspective of decoding speed and accuracies on in-vocabulary (IV) and out-of-vocabulary (OOV) entities/chunks/words. ---------------------------------- **SETTINGS** Data. The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (Toutanova et al., 2003; Santos and Zadrozny, 2014; Ma and Hovy, 2016; <cite>Liu et al., 2018)</cite> , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set.",
  "y": "differences"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_0",
  "x": "Garg et al. (2018) show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and Zhao et al. (2017) suggest that biases might in fact be amplified by embedding models. Several researchers have also investigated ways to counter stereotypes and biases in learned language models. While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral. Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_1",
  "x": "While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral. Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text. Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\". Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_2",
  "x": "Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text. Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\". Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words. Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_3",
  "x": "Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words. Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish. We are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed. Our experiments are therefore tightly tied to a real-world use case where gender bias would have potentially serious ramifications. We also provide further evidence of the inability of the debiasing method proposed by<cite> Bolukbasi et al. (2016b)</cite> to handle the type of bias we are concerned with.",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_4",
  "x": "We apply the debiasing methodology in<cite> (Bolukbasi et al., 2016b)</cite> to the pretrained embedddings. Debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific. The definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e.g., he -she, and motherfather. In our setting, there are 10 such pairs. The gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_5",
  "x": "Debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific. The definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e.g., he -she, and motherfather. In our setting, there are 10 such pairs. The gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space. We use the same methodology for growing a seed set of gender specific words into a larger set as described in<cite> (Bolukbasi et al., 2016b)</cite> , and end up with 486 manually curated gender specific words, including e.g., farfar (paternal grandfather), tvillingsystrar (twin sisters), and matriark (matriarch).",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_6",
  "x": "The approach described by<cite> (Bolukbasi et al., 2016b)</cite> includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs. The equality set is application specific, and since the current investigation of Swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case. We apply the method described above to all pretrained embeddings in Table 3 , as well as to the token vectors generated by ELMo and BERT. Although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_7",
  "x": "We now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of<cite> Bolukbasi et al. (2016b)</cite> . In this setting, a number of analogy pairs are extracted for the original and debiased embeddings, and human evaluators are used to asses the number of appropriate and stereotypical pairs in the respective representations. Bolukbasi et al. (2016b) used 10 crowdworkers to classify the analogy pairs as being appropriate or stereotypical. Their results indicated that 19% of the top 150 analogies generated using the original embedding model were deemed gender stereotypical, while the corresponding figure for the hard debiased model was 6%. We carry out a similar, but smaller, evaluation exercise using the analogy pairs generated by the original Swedish word2vec and fastText models, as well as their debiased counterparts.",
  "y": "uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_8",
  "x": "Second, there is a considerable amount of annotator uncertainty involved, either regarding the plausibility of a given analogy pair, or regarding its appropriateness. This is manifested by an increase of the number of uncertain analogy pairs that the annotators agree on between the original and debiased models (both for word2vec and fastText). However, the most interesting findings have to do with the number of stereotypical analogy pairs. The number of stereotypical analogy pairs output by the Swedish models is small compared to the numbers reported by<cite> Bolukbasi et al. (2016b)</cite> . Further, the number of stereotypical pairs is larger in the debiased word2vec model than in the original model (we anticipated that it should be lower).",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_0",
  "x": "This paper contains three contributions: \u2022 We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) to the sentiment analysis domain, extending past supervised learning work by<cite> Pang and Lee (2005)</cite> ; \u2022 We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3; \u2022 We show the benefit of semi-supervised learning for rating inference with extensive experimental results in section 4. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_1",
  "x": "A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in <cite>(Pang and Lee, 2005)</cite> , and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_2",
  "x": "A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in <cite>(Pang and Lee, 2005)</cite> , and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_3",
  "x": "Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is required.)",
  "y": "motivation"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_4",
  "x": "Before moving on to experiments, we note an interesting connection to the supervised learning method in <cite>(Pang and Lee, 2005)</cite> , which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002) . Consider a special case of our loss function (1) when b = 0 and M \u2192 \u221e. It is easy to show for labeled nodes j \u2208 L, the optimal value is the given label: f (x j ) = y j . Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while <cite>(Pang and Lee, 2005)</cite> used absolute difference.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_5",
  "x": "Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while <cite>(Pang and Lee, 2005)</cite> used absolute difference. Indeed in experiments comparing the two (not reported here), their differences are not statistically significant. From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_6",
  "x": "From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data. ---------------------------------- **EXPERIMENTS** We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the \"scale dataset v1.0\" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in <cite>(Pang and Lee, 2005)</cite> . We chose 4-class instead of 3-class labeling because it is harder.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_8",
  "x": "All reported results are average test set accuracy. We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in <cite>(Pang and Lee, 2005)</cite> . ---------------------------------- **REGRESSION** We ran linear -insensitive support vector regression using Joachims' SVM light package (1999) with all default parameters.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_9",
  "x": "We ran Pang and Lee's method based on metric labeling, using SVM regression as the initial label preference function. The method requires an itemsimilarity function, which is equivalent to our similarity measure w ij . Among others, we experimented with PSP-based similarity. For consistency with <cite>(Pang and Lee, 2005)</cite> , supervised metric labeling results with this measure are reported under 'reg+PSP. ' Note this method does not use unlabeled data for training either.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_10",
  "x": "Among others, we experimented with PSP-based similarity. For consistency with <cite>(Pang and Lee, 2005)</cite> , supervised metric labeling results with this measure are reported under 'reg+PSP. ' Note this method does not use unlabeled data for training either. PSP i is defined in <cite>(Pang and Lee, 2005)</cite> as the percentage of positive sentences in review x i . The similarity between reviews x i , x j is the cosine angle Figure 2 : PSP for reviews expressing each fine-grain rating.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_11",
  "x": "We identified positive sentences using SVM instead of Na\u00efve Bayes, but the trend is qualitatively the same as in <cite>(Pang and Lee, 2005)</cite> . between the vectors (PSP i , 1\u2212PSP i ) and (PSP j , 1\u2212 PSP j ). Positive sentences are identified using a binary classifier trained on a separate \"snippet data set\" located at the same URL as above. The snippet data set contains 10662 short quotations taken from movie reviews appearing on the rottentomatoes.com Web site. Each snippet is labeled positive or negative based on the rating of the originating review.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_12",
  "x": "In an attempt to reproduce the findings in <cite>(Pang and Lee, 2005)</cite> , we tuned c, \u03b1 with cross validation. Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas <cite>(Pang and Lee, 2005)</cite> tuned k and \u03b1 on a per-author basis. To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters. Table 1 shows the accuracy obtained over 20 trials with |L| = 0.9n for each author, using SVM regression, reg+PSP using shared c, \u03b1 parameters, and reg+PSP using authorspecific c, \u03b1 parameters (listed in parentheses).",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_13",
  "x": "Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas <cite>(Pang and Lee, 2005)</cite> tuned k and \u03b1 on a per-author basis. To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters. Table 1 shows the accuracy obtained over 20 trials with |L| = 0.9n for each author, using SVM regression, reg+PSP using shared c, \u03b1 parameters, and reg+PSP using authorspecific c, \u03b1 parameters (listed in parentheses). The best result in each row of the table is highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_14",
  "x": "We also show in bold any results that cannot be distinguished from the best result using a paired t-test at the 0.05 level. <cite>(Pang and Lee, 2005)</cite> found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d). Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results. Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning. The optimal shared parameters produced almost the same results as the optimal author-specific parameters, and were used in subsequent experiments.",
  "y": "background"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_15",
  "x": "**DISCUSSION** We have demonstrated the benefit of using unlabeled data for rating inference. There are several directions to improve the work: 1. We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews' sentiment patterns. For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work <cite>(Pang and Lee, 2005)</cite> . 2.",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_0",
  "x": "In this paper, we devise novel feature sets especially tailored to cope with the peculiarities of proverbs, which are generally short and figuratively rich. To the best of our knowledge, this is the first attempt to design a word-level metaphor recognizer specifically tailored to such metaphorically rich data. Even though some of the resources that we use (e.g., imageability and concreteness) have been used for this task before, we propose new ways of encoding this information, especially with respect to the density of the feature space and the way that the context of each word is modeled. On the proverb data, the novel features result in compact models that significantly outperform existing features designed for word-level metaphor detection in other genres <cite>(Klebanov et al., 2014)</cite> , such as news and essays. By also testing the new features on these other genres, we show that their generalization power is not limited to proverbs.",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_1",
  "x": "Similarly to<cite> Klebanov et al. (2014)</cite> , we classify each content word (i.e., adjective, noun, verb or adverb) appearing in a proverb as being used metaphorically or not. As a baseline, we use a set of features very similar to the one proposed by<cite> Klebanov et al. (2014)</cite> . To obtain results more easily comparable with<cite> Klebanov et al. (2014)</cite>, we use the same classifier, i.e., logistic regression, in the implementation bundled with the scikit-learn package (Pedregosa et al., 2011) .",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_2",
  "x": "Unigrams (u B ):<cite> Klebanov et al. (2014)</cite> use all content word forms as features without stemming or lemmatization. To reduce sparsity, we consider lemmas along with their POS tag. Part-of-speech (p B ): The coarse-grained part-ofspeech (i.e., noun, adjective, verb or adverb) of content words 2 . Concreteness (c B ): We extract the concreteness features from the resource compiled by Brysbaert et al. (2014) . Similarly to<cite> Klebanov et al. (2014)</cite> , the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments.",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_3",
  "x": "Part-of-speech (p B ): The coarse-grained part-ofspeech (i.e., noun, adjective, verb or adverb) of content words 2 . Concreteness (c B ): We extract the concreteness features from the resource compiled by Brysbaert et al. (2014) . Similarly to<cite> Klebanov et al. (2014)</cite> , the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments. We also add a binary feature which encodes the information about whether the lemma is found in the resource. Topic models (t B ): We use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) using Gibbs sampling for parameter estimation and inference (Griffiths, 2002) .",
  "y": "similarities"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_4",
  "x": "Finally, Table 3 shows the effect of the different feature sets on VUAMC used by<cite> Klebanov et al. (2014)</cite> . We use the same 12-fold data split as<cite> Klebanov et al. (2014)</cite> , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark.",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_5",
  "x": "We use the same 12-fold data split as<cite> Klebanov et al. (2014)</cite> , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in <cite>(Klebanov et al., 2014)</cite> are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization).",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_6",
  "x": "The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in <cite>(Klebanov et al., 2014)</cite> are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization). In particular, our implementation of the B features performs better than reported by<cite> Klebanov et al. (2014)</cite> on all four genres, namely: 0.52 vs. 0.51 for \"news\", 0.51 vs. 0.28 for \"academic\", 0.39 vs. 0.28 for \"conversation\" and 0.42 vs. 0.33 for \"fiction\".",
  "y": "extends differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_0",
  "x": "Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012) . Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007) . Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008) , dual decomposition , or multi-commodity flows (Martins et al., 2009<cite> (Martins et al., , 2011</cite> .",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_1",
  "x": "\u2022 We apply the third-order feature models of to non-projective parsing. \u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> . While AD 3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_2",
  "x": "\u2022 We apply the third-order feature models of to non-projective parsing. \u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> . While AD 3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method .",
  "y": "differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_3",
  "x": "**3** Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial Figure 1 : Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005) , and second-order models include also consecutive siblings and grandparents (Carreras, 2007) . Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in<cite> Martins et al. (2011)</cite> , in addition to third-order features for grand-and tri-siblings . problems in a modular and extensible manner (Komodakis et al., 2007; .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_4",
  "x": "In this paper, we employ alternating directions dual decomposition (AD 3 ;<cite> Martins et al., 2011)</cite> . Like the subgradient algorithm of , AD 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD 3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012 ) has shown that: (i) AD 3 converges at a faster rate, 2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach <cite>(Martins et al., 2011)</cite> , while still enjoying faster convergence.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_5",
  "x": "The difference is that the AD 3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012 ) has shown that: (i) AD 3 converges at a faster rate, 2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach <cite>(Martins et al., 2011)</cite> , while still enjoying faster convergence. ---------------------------------- **OUR SETUP**",
  "y": "background differences motivation"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_6",
  "x": "formed trees. Let {A s } S s=1 be a cover of A, where each A s \u2286 A. We assume that the score of a parse tree u \u2208 Y decomposes as f (u) := S s=1 f s (z s ), where each z s := z s,a a\u2208As is a \"partial view\" of u, and each local score function f s comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few \"large\" components, such as trees and head automata (Smith and Eisner, 2008; , or (ii) many \"small\" components, coming from a multi-commodity flow formulation (Martins et al., 2009<cite> (Martins et al., , 2011</cite> ). Let Y s \u2286 R |As| denote the set of feasible realizations of z s , i.e., those that are partial views of an actual parse tree. A tuple of views z 1 , . . . , z S \u2208 S s=1 Y s is said to be globally consistent if z s,a = z s ,a holds for every a, s and s such that a \u2208 A s \u2229A s .",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_7",
  "x": "We assume each parse u \u2208 Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following<cite> Martins et al. (2011)</cite> , the problem of obtaining the best-scored tree can be written as follows: where the equality constraint ensures that the partial views \"glue\" together to form a coherent parse tree. 3 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_8",
  "x": "4 Let \u2206 |Ys| := {\u03b1 \u2208 R |Ys| | \u03b1 \u2265 0, y s \u2208Ys \u03b1y s = 1} be the probability simplex. The convex hull of Ys is the set conv(Ys) := { y s \u2208Ys \u03b1y s y s | \u03b1 \u2208 \u2206 |Ys| }. Its members represent marginal probabilities over the arcs in As. The AD 3 algorithm <cite>(Martins et al., 2011)</cite> alternates among the following iterative updates: \u2022 z-updates, which decouple over s = 1, . . . , S, and solve a penalized version of Eq. 2:",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_9",
  "x": "\u2022 \u03bb-updates, where the Lagrange multipliers are adjusted to penalize disagreements: In sum, the only difference between AD 3 and the subgradient method is in the z-updates, which in AD 3 require solving a quadratic problem. While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm. However, the following result, proved in Martins et al. (2012) , allows to expand the scope of AD 3 to any problem which satisfies Assumption 1. Proposition 2. The problem in Eq. 3 admits a solution z * s which is spanned by a sparse basis W \u2286 Y s with cardinality at most |W| \u2264 O(|A s |).",
  "y": "background motivation"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_10",
  "x": "\u2022 u-updates, a simple averaging operation: \u2022 \u03bb-updates, where the Lagrange multipliers are adjusted to penalize disagreements: In sum, the only difference between AD 3 and the subgradient method is in the z-updates, which in AD 3 require solving a quadratic problem. While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm. However, the following result, proved in Martins et al. (2012) , allows to expand the scope of AD 3 to any problem which satisfies Assumption 1. Proposition 2.",
  "y": "extends"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_11",
  "x": "Head bigrams can be captured with a simple sequence model: Each score \u03c3 HB (m, h, h ) is obtained via features that look at the heads of consecutive words (as in<cite> Martins et al. (2011)</cite> ). This function can be maximized in time O(L 3 ) with the Viterbi algorithm. Arbitrary siblings. We handle arbitrary siblings as in<cite> Martins et al. (2011)</cite> , defining O(L 3 ) component functions of the form f ASIB h,m,s (z h,m , z h,s ) = \u03c3 ASIB (h, m, s).",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_12",
  "x": "Head bigrams can be captured with a simple sequence model: Each score \u03c3 HB (m, h, h ) is obtained via features that look at the heads of consecutive words (as in<cite> Martins et al. (2011)</cite> ). This function can be maximized in time O(L 3 ) with the Viterbi algorithm. Arbitrary siblings. We handle arbitrary siblings as in<cite> Martins et al. (2011)</cite> , defining O(L 3 ) component functions of the form f ASIB h,m,s (z h,m , z h,s ) = \u03c3 ASIB (h, m, s).",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_13",
  "x": "In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model. By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of Rush and Petrov, 2012 Martins et al. (2010<cite> Martins et al. ( , 2011</cite> , , Rush and Petrov (2012) , Zhang and McDonald (2012) . The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008) . Our third-order model achieved the best reported scores for English, Czech, German, and Dutchwhich includes the three largest datasets and the ones with the most non-projective dependenciesand is on par with the state of the art for the remaining languages.",
  "y": "similarities"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_0",
  "x": "Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response<cite> (Bordes and Weston, 2016)</cite> . While reasoning over memory is appealing, these models simply choose among a set of utterances rather than generating text and also must have temporal dialogue features explicitly encoded. However, the present literature lacks results for now standard sequence-to-sequence architectures, and we aim to fill this gap by building increasingly complex models of text generation, starting with a vanilla sequence-to-sequence recurrent architecture. The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of<cite> Bordes and Weston (2016)</cite> by 6.9%. Our contributions are as follows: 1) We perform a systematic, empirical analysis of increasingly complex sequence-to-sequence models for task-oriented dialogue, and 2) we develop a recurrent neural dialogue architecture augmented with an attention-based copy mechanism that is able to significantly outperform more complex models on a variety of metrics on realistic data.",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_1",
  "x": "Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response<cite> (Bordes and Weston, 2016)</cite> . While reasoning over memory is appealing, these models simply choose among a set of utterances rather than generating text and also must have temporal dialogue features explicitly encoded. However, the present literature lacks results for now standard sequence-to-sequence architectures, and we aim to fill this gap by building increasingly complex models of text generation, starting with a vanilla sequence-to-sequence recurrent architecture. The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of<cite> Bordes and Weston (2016)</cite> by 6.9%. Our contributions are as follows: 1) We perform a systematic, empirical analysis of increasingly complex sequence-to-sequence models for task-oriented dialogue, and 2) we develop a recurrent neural dialogue architecture augmented with an attention-based copy mechanism that is able to significantly outperform more complex models on a variety of metrics on realistic data.",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_2",
  "x": "These type features improve generalization to novel entities by allowing the model to hone in on positions with particularly relevant bits of dialogue context during its soft attention and copying. Other cited work using the DSTC2 dataset <cite>(Bordes and Weston, 2016</cite>; Liu and Perez, 2016; Seo et al., 2016) implement similar mechanisms whereby they expand the feature representations of candidate system responses based on whether there is lexical entity class matching with provided dialogue context. In these works, such features are referred to as match features. All of our architectures use an LSTM cell as the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Pham et al., 2014) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_3",
  "x": "**EXPERIMENTS** ---------------------------------- **DATA** For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) , a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from<cite> Bordes and Weston (2016)</cite> , which ignores the dialogue state annotations, using only the raw text of the dialogues.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_4",
  "x": "The attention, output parameters, word embeddings, and LSTM weights were randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015) . ---------------------------------- **METRICS** Evaluation of dialogue systems is known to be difficult . We employ several metrics for assessing specific aspects of our model, drawn from previous work: \u2022 Per-Response Accuracy:<cite> Bordes and Weston (2016)</cite> report a per-turn response accuracy, which tests their model's ability to select the system response at a certain timestep.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_5",
  "x": "Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of<cite> Bordes and Weston (2016)</cite> . \u2022 Per-Dialogue Accuracy:<cite> Bordes and Weston (2016)</cite> also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly. We calculate a similar value of dialogue accuracy, though again our model generates every token of every response. \u2022 BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002) , which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; . We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model's ability to accurately generate the language patterns seen in DSTC2.",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_6",
  "x": "\u2022 Per-Dialogue Accuracy:<cite> Bordes and Weston (2016)</cite> also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly. We calculate a similar value of dialogue accuracy, though again our model generates every token of every response. \u2022 BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002) , which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; . We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model's ability to accurately generate the language patterns seen in DSTC2. \u2022 Entity F 1 : Each system response in the test data defines a gold set of entities.",
  "y": "similarities differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_7",
  "x": "This issue, which recurs with evaluation of dialogue or other generative systems, could be alleviated through more forgiving evaluation procedures based on beam search decoding. ---------------------------------- **RESULTS** In Table 2 , we present the results of our models compared to the reported performance of the best performing model of<cite> (Bordes and Weston, 2016)</cite> , which is a variant of an end-to-end memory network (Sukhbaatar et al., 2015) . Their model is referred to as MemNN.",
  "y": "motivation"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_0",
  "x": "If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, <cite>(Gardner et al., 2013)</cite> augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_1",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference <cite>(Gardner et al., 2013)</cite> , (Gardner et al., 2014) . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_2",
  "x": "We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_3",
  "x": "This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) .",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_4",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_5",
  "x": "As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_6",
  "x": "Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in <cite>(Gardner et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_7",
  "x": "Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added. Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in <cite>(Gardner et al., 2013)</cite> . As we shall see in Section 4, this results in a more effective algorithm with faster runtime. We first present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) . The PRA uses paths as features for a logistic regression classifier which predicts if the given relation exists between a pair of entities.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_9",
  "x": "<cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION**",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_10",
  "x": "**<cite>PRA-SVO</cite> AND PRA-VS** <cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ----------------------------------",
  "y": "motivation differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_12",
  "x": "<cite>PRA-SVO</cite>, PRA-VS are the systems proposed in (<cite>Gardner et al., 2013</cite>; Gardner et al., 2014) . PRA-ODA is the approach proposed in this paper. Improvements in PRA-ODA over <cite>PRA-SVO</cite> is statistically significant with p < 0.007, with <cite>PRA-SVO</cite> as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_14",
  "x": "Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (<cite>Gardner et al., 2013</cite>; Gardner et al., 2014) , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented.",
  "y": "similarities uses"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_16",
  "x": "As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of <cite>PRA-SVO</cite> and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_17",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (<cite>PRA-SVO</cite>) (<cite>Gardner et al., 2013</cite>) and vector space random walk PRA (PRA-VS) (Gardner et al., 2014) . The run times, i.e, the time taken to perform an entire experiment for <cite>PRA-SVO</cite> and PRA-VS includes the time taken to augment NELL KB with SVO edges. The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_18",
  "x": "Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the <cite>PRA-SVO</cite> and PRA-VS. ---------------------------------- **CONCLUSION** In this paper, we investigated the usefulness of adding paths to a Knowledge Base for improving its connectivity by mining bridging entities from an external corpus.",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_0",
  "x": "First for Multi-NLI, we improve substantially over the CBOW and biL-STM Encoder baselines reported in the dataset paper . We also show that our final shortcut-based stacked encoder achieves around 3% improvement as compared to the 1layer biLSTM-Max Encoder in the second last row (using the exact same classifier and optimizer settings). Our shortcut-encoder was also the top singe-model (non-ensemble) result on the EMNLP RepEval Shared Task leaderboard. Next, for SNLI, we compare our shortcutstacked encoder with the current state-of-the-art encoders from the SNLI leaderboard (https:// nlp.stanford.edu/projects/snli/). We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point.",
  "y": "uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_1",
  "x": "Next, for SNLI, we compare our shortcutstacked encoder with the current state-of-the-art encoders from the SNLI leaderboard (https:// nlp.stanford.edu/projects/snli/). We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point. 1 The results indicate that 'Our Shortcut-Stacked Encoder' sur-passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on SNLI, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_2",
  "x": "Our encoder does not require any syntactic information of the sentence. It also does not contain any attention or memory structure. It is basically a stacked (multi-layered) bidirectional LSTM-RNN with shortcut connections (feeding all previous layers' outputs and word embeddings to each layer) and word embedding fine-tuning. The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural (similar to the classifier setup of Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> ). Our simple shortcut-stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties, on both matched and mis- matched evaluation settings for multi-domain natural language inference, as well as on the original SNLI dataset.",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_3",
  "x": "The model follows the 'encoding-based rule', i.e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence (e.g., cross-attention or memory comparing the two sentences). In order to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. This setting follows the idea of Siamese Networks in Bromley et al. (1994) . Figure 1 shows the overview of our encoding model (the standard classifier setup is not shown here; see Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> for that). ----------------------------------",
  "y": "background"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_4",
  "x": "Then, the input of ith biLSTM layer at time t is defined as: Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to <cite>Conneau et al. (2017)</cite> . The final layer is defined as: , d m is the dimension of the hidden state of the last forward and backward LSTM layers, and v is the final vector representation for the source sentence (which is later fed to the NLI classifier). The closest encoder architecture to ours is that of <cite>Conneau et al. (2017)</cite> , whose model consists of a single-layer biLSTM with a max-pooling layer, which we treat as our starting point.",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_5",
  "x": "Then, the input of ith biLSTM layer at time t is defined as: Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to <cite>Conneau et al. (2017)</cite> . The final layer is defined as: , d m is the dimension of the hidden state of the last forward and backward LSTM layers, and v is the final vector representation for the source sentence (which is later fed to the NLI classifier). The closest encoder architecture to ours is that of <cite>Conneau et al. (2017)</cite> , whose model consists of a single-layer biLSTM with a max-pooling layer, which we treat as our starting point.",
  "y": "extends similarities"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_7",
  "x": "These ablation results are shown in Tables 1, 2, 3 and 4, all based on the Multi-NLI development sets. Finally, Table 5 shows results for different encoders on SNLI and Multi-NLI test sets. First, Table 1 shows the performance changes for different number of biLSTM layers and their varying dimension size. The dimension size of a biLSTM layer is referring to the dimension of the hidden state for both the forward and backward LSTM-RNNs. As shown, each added layer model improves the accuracy and we achieve a substantial improvement in accuracy (around 2%) on both matched and mismatched settings, compared to the single-layer biLSTM in <cite>Conneau et al. (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "c78464cfe0fc44f5fd0da2e4f9d90e_0",
  "x": "Unsurprisingly, when NLP tools are applied directly to social media data, the results tend to be miserable when compared to data sets such as the Wall Street Journal component of the Penn Treebank. However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; <cite>Gimpel et al., 2011)</cite> . Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data (Lui and Baldwin, 2012; Han et al., to appear) . Furthermore, social media posts tend to be short and the content highly varied, meaning it is difficult to adapt a tool to the domain, or harness textual context to disambiguate the content. There is also the engineering challenge of real-time processing of the text stream, as much of NLP research is carried out offline with only secondary concern for throughput.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_0",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) .",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_1",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) .",
  "y": "background motivation"
 },
 {
  "id": "c7e304499654516cce43c550256eae_2",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) .",
  "y": "background motivation differences"
 },
 {
  "id": "c7e304499654516cce43c550256eae_3",
  "x": "Recently, neural network models have been developed for POS tagging and achieved good performance, such as RNN and bidirectional long short-term memory (BiLSTM) and CRF-BiLSTM models (Mikolov et al., 2010; Huang et al., 2015) . For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (Huang et al., 2015) . However, in low-resource languages, these models are seldom used because of limited labelled data. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; <cite>Fang and Cohn, 2016</cite>; Zhang et al., 2016) . Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_4",
  "x": "**MODEL** We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources. Our approach extends the work of<cite> Fang and Cohn (2016)</cite> , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations. There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer. 2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.",
  "y": "extends"
 },
 {
  "id": "c7e304499654516cce43c550256eae_5",
  "x": "As illustrated in Figure 1 , the distant components are generated directly as softmax outputs, y t \u223c Categorial(o t ), with parameters o t = Softmax(W h t + b) as a linear classifier over a sentence encoding, h t , which is the output of a bidirectional LSTM encoder over the words. Ground truth supervision The second component of the model is manually labelled text in the low-resource language. To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) . Formally,\u1ef9 t \u223c Categorial(\u00f5 t ) where\u00f5 t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. This component allows for a more expressive label mapping than<cite> Fang and Cohn (2016)</cite>'s linear matrix translation.",
  "y": "differences"
 },
 {
  "id": "c7e304499654516cce43c550256eae_6",
  "x": "For a more direct comparison, we include BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data. Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques. The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.",
  "y": "differences uses"
 },
 {
  "id": "c7e304499654516cce43c550256eae_7",
  "x": "Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques. The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision. The accuracies are higher overall for the European cf.",
  "y": "differences"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_0",
  "x": "For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion. The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., <cite>Choi 2000</cite>; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and Isahara 2001) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al. 2004; Morris and Hirst 1991) , or from collocations collected in large corpora (Bolshakov and Gelbukh 2001; Brants, Chen, and Tsochantaridis 2002; Choi et al. 2001; Ferret 2002; Kaufmann 1999; Ponte and Croft 1997) . According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity. Empirical arguments in favor of these methods have been provided recently by Choi et al. (2001) in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs. By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge.",
  "y": "background differences"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_1",
  "x": "This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated. The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words). The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, <cite>Choi's algorithm</cite> and the use of LSA within this framework are described. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_3",
  "x": "The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, <cite>Choi's algorithm</cite> and the use of LSA within this framework are described. ---------------------------------- **THE TWO VERSIONS OF <cite>CHOI'S ALGORITHM</cite>** The segmentation algorithm proposed by <cite>Choi (2000)</cite> is made up of the three steps usually found in any segmentation procedure based on lexical cohesion.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_4",
  "x": "The step of greatest interest here is the one that calculates the inter-sentence similarities. The procedure initially proposed by <cite>Choi (2000)</cite> , C99, rests exclusively on the information contained in the text to be segmented. According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_5",
  "x": "In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA). Briefly stated, LSA rests on the thesis that analyzing the contexts in which words occur permits an estimation of their similarity in meaning (Deerwester et al. 1990; Landauer and Dumais 1997) . The first step in the analysis is to construct a lexical table containing an information-theoretic weighting of the frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text) included in the corpus. This frequency table undergoes a Singular Value Decomposition that extracts the most important orthogonal dimensions, and, consequently, discards the small sources of variability in term usage.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_6",
  "x": "---------------------------------- **METHOD** This experiment was based on the procedure and test materials designed by <cite>Choi (2000)</cite> , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) . The task consists in finding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments.",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_7",
  "x": "This experiment was based on the procedure and test materials designed by <cite>Choi (2000)</cite> , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) . The task consists in finding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments. Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by <cite>Choi (2000)</cite> , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_8",
  "x": "The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus. Words were not stemmed, as in Choi et al. (2001) . To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (Berry 1992; Berry et al. 1993) , and the first 300 singular vectors were retained. Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine. An 11 \u00d7 11 rank mask was used for the ordinal transformation, as recommended by <cite>Choi (2000)</cite> .",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_9",
  "x": "**METHOD** The test materials were extracted from the 1997-1998 corpus following the guidelines given in <cite>Choi (2000)</cite> . It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences. Three types of LSA space were composed. The Within space is based on the whole 1997-1998 corpus.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_0",
  "x": "We show that TRANS-BLSTM models consistently lead to improvements in accuracy compared to BERT baselines in GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result. ---------------------------------- **INTRODUCTION** Learning representations (Mikolov et al., 2013) of natural language and language model pre-training <cite>(Devlin et al., 2018</cite>; Radford et al., 2019) has shown promising results recently.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_1",
  "x": "The innovation of BERT <cite>(Devlin et al., 2018)</cite> comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) . The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa (Liu et al., 2019b) investigated hyper-parameter design choices and suggested longer model training time. In addition, XLNet (Yang et al., 2019) has been proposed to address the BERT pre-training and fine-tuning discrepancy where masked tokens were found in the former but not in the latter. Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_2",
  "x": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa (Liu et al., 2019b) investigated hyper-parameter design choices and suggested longer model training time. In addition, XLNet (Yang et al., 2019) has been proposed to address the BERT pre-training and fine-tuning discrepancy where masked tokens were found in the former but not in the latter. Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance. For example, <cite>(Devlin et al., 2018)</cite> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_3",
  "x": "However, <cite>they</cite> stop at a hidden layer size of 1024. ALBERT (Lan et al., 2019) showed that it is not the case that simply increasing the model size would lead to better accuracy. In fact, they observed that simply increasing the hidden layer size of a model such as BERT-large can lead to significantly worse performance. On the other hand, model distillation (Hinton et al., 2015; Tang et al., 2019; Sun et al., 2019; Sanh et al., 2019) has been proposed to reduce the BERT model size while maintaining high performance. In this paper, we attempt to improve the performance of BERT via architecture enhancement.",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_4",
  "x": "In this paper, we attempt to improve the performance of BERT via architecture enhancement. BERT is based on the encoder of the transformer model (Vaswani et al., 2017) , which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications <cite>(Devlin et al., 2018)</cite> . Prior to BERT, bidirectional LSTM (BLSTM) has dominated sequential modeling for many tasks including machine translation (Chiu and Nichols, 2016) and speech recognition (Graves et al., 2013) . Given both models have demonstrated superior accuracy on various benchmarks, it is natural to raise the question whether a combination of the transformer and BLSTM can outperform each individual architecture. In this paper, we attempt to answer this question by proposing a transformer BLSTM joint modeling framework.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_5",
  "x": "Given both models have demonstrated superior accuracy on various benchmarks, it is natural to raise the question whether a combination of the transformer and BLSTM can outperform each individual architecture. In this paper, we attempt to answer this question by proposing a transformer BLSTM joint modeling framework. Our major contribution in this paper is two fold: 1) We propose the TRANS-BLSTM model architectures, which combine the transformer and BLSTM into one single modeling framework, leveraging the modeling capability from both the transformer and BLSTM. 2) We show that the TRANS-BLSTM models can effectively boost the accuracy of BERT baseline models on SQuAD 1.1 and GLUE NLP benchmark datasets. 2 Related work 2.1 BERT Our work focuses on improving the transformer architecture (Vaswani et al., 2017) , which motivated the recent breakthrough in language representation, BERT <cite>(Devlin et al., 2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_6",
  "x": "(Vaswani et al., 2017 ) employs a residual connection (He et al., 2016) around each of the two sub-layers, followed by layer normalization (Ba et al., 2016) . That is, the output of each sublayer is LayerN orm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension of 768 and 1024 for BERT-base and BERT-large, respectively. We used the same multi-head selfattention from the original paper (Vaswani et al., 2017) . We used the same input and output representations, i.e., the embedding and positional encoding, and the same loss objective, i.e., masked LM prediction and next sentence prediction, from the BERT paper <cite>(Devlin et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_7",
  "x": "The number of bidirectional LSTM layers is a hyper parameter to tune; we use 2 in this paper. While the bidirectional LSTM layers in encoder help the pre-training task for the masked language model and next sentence prediction task, the bidirectional LSTM in decoder may help in downstream sequential prediction tasks such as question answering. ---------------------------------- **OBJECTIVE FUNCTIONS** Following the BERT <cite>(Devlin et al., 2018)</cite> , we use masked language model loss and next sentence prediction (NSP) loss to train the models.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_8",
  "x": "We also use the next sentence prediction loss as introduced in <cite>(Devlin et al., 2018)</cite> to train our models. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus. We note that recent work (Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019; Raffel et al., 2019) has argued that the NSP loss may not be useful in improving model accuracy. Nevertheless, we used the NSP loss in our experiments to have a fair comparison between the proposed models and the original BERT models. Table 1 shows the model parameter size and training speedup for TRANS/BERT (TRANS and BERT are exchangeable in this paper), TRANS-BLSTM-SMALL, and TRANS-BLSTM respectively.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_9",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_10",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> . The two corpora consist of about 16GB of text. Following the original BERT setup <cite>(Devlin et al., 2018)</cite> , we format the inputs as \"[CLS] TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1 : Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_11",
  "x": "However, in the whole word masking setting, we are able to fairly compare the proposed TRANS-BLSTM models to the original BERT models. Similar to <cite>(Devlin et al., 2018)</cite> , the training data generator chooses 15% of the token positions at random for making. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. The model updates use a batch size of 256 and Adam optimizer with learning rate starting from 1e-4. Training was done on a cluster of nodes, where each node consists of 8 Nvidia Tesla V100 GPUs.",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_12",
  "x": "Training was done on a cluster of nodes, where each node consists of 8 Nvidia Tesla V100 GPUs. We vary the node size from 1 to 8 depending on the model size. Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 . 1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> . 2 https://github.com/huggingface/pytorch-transformers.",
  "y": "differences"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_13",
  "x": "1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> . 2 https://github.com/huggingface/pytorch-transformers. ---------------------------------- **DOWNSTREAM EVALUATION DATASETS** Following the previous work <cite>(Devlin et al., 2018</cite>; Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019) , we evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_14",
  "x": "Table 2 shows the BERT base models, including the original BERT-base model in <cite>(Devlin et al., 2018)</cite> and our implementation, and the bidirectional LSTM model accuracy over SQuAD 1.1 development dataset. Our implementation results in a higher F1 score (90.05%) compared to the original BERT-base one (88.50%). This may be due to the fact that we use the whole word masking while BERT-base used partial word masking (an easier task, which may prevent from learning a better model). We found that the BLSTM model has F1 score of 83.43%, which is significantly worse than our TRANS/BERT baseline (90.05%). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_16",
  "x": "**MODEL** EM F1 BERT-base <cite>(Devlin et al., 2018)</cite> ---------------------------------- **MODELS PRE-TRAINING** We run three pre-training experiments for base and large settings respectively.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_17",
  "x": "Compared to adding bidirectional LSTM layers to the encoder, the addition of bidirectional LSTMs to the decoder (see +BLSTM experiments in Table 4 ) offers additional improvements on five out of six cases. For example, it boosts the base TRANS/BERT model F1 score of 90.05% to 90.67%, and boosts the large TRANS-BLSTM model F1 score of 93.82% to 94.01%. ---------------------------------- **MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_18",
  "x": "For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_19",
  "x": "Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. Unlike the evaluation on SQuAD dataset, we do not apply the BLSTM layer to the decoder. This is because that the tasks on GLUE are classification tasks based on the [CLS] token, and are not sequential prediction tasks (for example the SQuAD dataset) which may benefit more from including a BLSTM layer.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_20",
  "x": "Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. Unlike the evaluation on SQuAD dataset, we do not apply the BLSTM layer to the decoder.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_0",
  "x": "**INTRODUCTION** Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5,<cite> 6,</cite> 20] . Meanwhile, pretrained contextualized language models (such as ELMo [1<cite>6</cite>] and BERT [4] ) have shown great promise on various natural language processing tasks [4, 1<cite>6</cite>] . These models work by pre-training LSTM-based or transformer-based [19] language models on a large corpus, and then by performing minimal task fine-tuning (akin to ImageNet [3, 23] ). Prior work has suggested that contextual information can be valuable when ranking.",
  "y": "background"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_4",
  "x": "Rather than building new models, in this work we use existing model architectures to test the effectiveness of various input representations. We evaluate our methods on three neural relevance matching methods: PACRR<cite> [6]</cite> , KNRM [20] , and DRMM [5] . Relevance matching models have generally shown to be more effective than semantic matching models, while not requiring massive amounts of behavioral data (e.g., query logs). For PACRR, we increase k max = 30 to allow for more term matches and better back-propagation to the language model. Contextualized language models.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_7",
  "x": "We re-rank to top k BM25 results for validation, and use P@20 on Robust and nDCG@20 on WebTrack to select the best-performing model. We different re-ranking functions and thresholds at test time for each dataset: BM25 with k = 150 for Robust04, and QL with k = 100 for WebTrack. The re-ranking setting is a better evaluation setting than ranking all qrels, as demonstrated by major search engines using a pipeline approach [18] . All models are trained using Adam [8] with a learning rate of 0.001 while BERT layers are trained at a rate of 2e-5. 5 Following prior work<cite> [6]</cite> , documents are truncated to 800 tokens.",
  "y": "uses"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_0",
  "x": "They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by<cite> Mohler et al. (2011)</cite> and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems.",
  "y": "background uses"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_1",
  "x": "Subsequently, we will zoom into the comparison of two of them, namely CoMiC-EN (Meurers et al., 2011a ) and the one which we call the Texas system <cite>(Mohler et al., 2011)</cite> and discuss the issues that arise with this endeavor. Returning to the bigger picture, we will explore how such systems could be compared in general, in the belief that meaningful comparison of approaches across research strands will be an important ingredient in advancing this relatively new research field. 2 The short answer assessment landscape ---------------------------------- **GENERAL ASPECTS** Researchers from all directions have settled in the landscape of short answer assessment, each of them with different backgrounds and different goals.",
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_2",
  "x": "Another recent approach is described by<cite> Mohler et al. (2011)</cite> , hereafter referred to as the Texas system. Student responses and target responses are annotated using a dependency parser. Thereupon, subgraphs of the dependency structures are constructed in order to map one response to the other. These alignments are generated using machine learning. Dealing with subgraphs allows for variation in word order between the two responses that are to be compared. In order to account for meaning, they combine lexical semantic similarity with the aforementioned alignment. They make use of several WordNet-based measures and two corpus-based measures, namely Latent Semantic Analysis and Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch 2007) .",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_3",
  "x": "For evaluating their system,<cite> Mohler et al. (2011)</cite> collected student responses from an online learning environment. 80 questions from ten introductory computer science assignments spread across two exams were gathered together with 2,273 student responses. These responses were graded by two human judges on a scale from zero to five. The judges fully agreed in 57% of all cases, their Pearson correlation computes to r = 0.586. The gold standard has been created by computing the arithmetic mean of the two judgments for each response. The Texas system achieves r = 0.518 and a Root Mean Square Error of 0.978 as its best result.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_4",
  "x": [
   "The gold standard has been created by computing the arithmetic mean of the two judgments for each response. The Texas system achieves r = 0.518 and a Root Mean Square Error of 0.978 as its best result. Mohler et al. (2011) mention that \"[t]he dataset is biased towards correct answers\". Data are publicly available. We used these in an evaluation experiment with the CoMiC-EN system, discussed in Section 3."
  ],
  "y": "background uses"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_5",
  "x": "---------------------------------- **A CONCRETE SYSTEM COMPARISON** After discussing the broad landscape of Short Answer Evaluation systems, the main characteristics and differences, we now turn to a comparison of two concrete systems, namely CoMiC-EN (Meurers et al., 2011a ) and the Texas system<cite> Mohler et al. (2011)</cite> , to explore what is involved in such a concrete comparison of two systems from different contexts. While CoMiC-EN was developed with meaning comparison in mind, the purpose of the Texas system is answer grading. We pick these two systems because they constitute recent and interesting instances of their respective fields and the corresponding data are freely available.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_6",
  "x": "In evaluating the Texas system,<cite> Mohler et al. (2011)</cite> used a corpus of ten assignments and two exams from an introductory computer science class. In total, the Texas corpus consists of 2,442 responses, which were collected using an online learning platform. Each response is rated by two annotators with a numerical grade on a 0-5 scale. Annotators were not given any specific instructions besides the scale itself, which resulted in an exact agreement of 57.7%. In order to arrive at a gold standard rating, the numerical average of the two ratings was computed. The data exist in raw, sentence-segmented and parsed versions and are freely available for research use. Table 2 presents a breakdown of the score counts and distribution statistics of the Texas corpus.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_7",
  "x": "Table 2 presents a breakdown of the score counts and distribution statistics of the Texas corpus. A bias towards correct answers can be observed, which is also mentioned by<cite> Mohler et al. (2011)</cite> . Table 2 : Details on the gold standard scores in the Texas corpus. Non-integer scores result from averaging between raters and normalization onto the 0-5 scale. ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_8",
  "x": [
   "Variety of Match Number of kinds of (0-5) token-level alignments dependency graph alignment in connection with two different machine learning approaches. Among the BOW features are WordNet-based similarity measures such as the one by Lesk (1986) and vector space measures such as tf * idf (Salton and McGill, 1983 ) and the more advanced LSA (Landauer et al., 1998) . The dependency graph alignment approach builds on a node-to-node matching stage which computes a score for each possible match between nodes of the student and target response. In the next stage, the optimal graph alignment is computed based on the node-to-node scores using the Hungarian algorithm. Mohler et al. (2011) also employ a technique they call \"question demoting\", which refers to the exclusion of words from the alignment process if they already appeared in the question string."
  ],
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_9",
  "x": "Therefore,<cite> Mohler et al. (2011)</cite> employ isotonic regression to map the ranking to the 0-5 scale. In terms of performance,<cite> Mohler et al. (2011)</cite> report that the SVMRank system produces a better correlation measure (r = 0.518) while the SVR system yields a better RMSE (0.978). ---------------------------------- **EVALUATION** We now turn to the evaluation of CoMiC-EN on the Texas corpus as it is a publicly available dataset.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_10",
  "x": "We discuss this point further in Section 4. For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison. This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy. We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by<cite> Mohler et al. (2011)</cite> . However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.",
  "y": "extends"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_11",
  "x": [
   "Mohler et al. (2011) also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric. Mohler et al. (2011) propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores. However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared. We can only suggest that in order to sufficiently describe a system's performance, several metrics need to be reported. Finally, an important point concerns the quality of gold standards."
  ],
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_12",
  "x": [
   "Mohler et al. (2011) also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric. Mohler et al. (2011) propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores. However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared. We can only suggest that in order to sufficiently describe a system's performance, several metrics need to be reported. Finally, an important point concerns the quality of gold standards."
  ],
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_14",
  "x": "We discussed several issues in the comparison of short answer evaluation systems. To that end, we gave an overview of the existing systems and picked two for a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a ) and the Texas system <cite>(Mohler et al., 2011)</cite> . In comparing the two, it was necessary to turn CoMiC-EN into a scoring system because the Texas corpus as the chosen gold standard contains numeric scores assigned by humans. Taking a step back from the concrete comparison, we gave a more general description of what is necessary to compare short answer evaluation systems. We observed that more datasets need to be publicly available in order for performance comparisons to have meaning, a point also made earlier by Pulman and Sukkarieh (2005) .",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_15",
  "x": "Incidentally, the technique is also used in the earlier CAM system (Bailey and Meurers, 2008) , but called \"Givenness filter\" there, following the long tradition of research on givenness (Schwarzschild, 1999) as a notion of information structure investigated in formal pragmatics. To produce the final system score, the Texas system uses two machine learning techniques based on Support Vector Machines (SVMs), SVMRank and Support Vector Regression (SVR). Both techniques are trained with several combinations of the dependency alignment and BOW features. While with SVR one trains a function to produce a score on the 0-5 scale itself, SVMRank produces a ranking of student answers which does not produce a 0-5 grade. Therefore,<cite> Mohler et al. (2011)</cite> employ isotonic regression to map the ranking to the 0-5 scale.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_16",
  "x": [
   "As mentioned before, CoMiC-EN performs meaning comparison based on a system of categories while the Texas system is a scoring approach, trying to predict a grade. While the former is a classification task, the latter is better characterized as a regression problem because of the desired numerical outcome. Of course, one could simply pretend that individual grades are classes and treat scoring as a classification task. However, a classification approach has no knowledge of numerical relationships, i.e., it does not 'know' that 4 is a higher grade than 3 and a much higher grade than 1 (assuming a 0-5 scale). As a result, if an evaluation metric such as Pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others."
  ],
  "y": "background motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_0",
  "x": "We consider the important task of producing a natural language description of a rich world state represented as an over-determined database of event records. This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_1",
  "x": "This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence.",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_2",
  "x": "We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task<cite> (Angeli et al., 2010)</cite> . We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (Hochreiter and Schmidhuber, 1997) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description. Our model first encodes the full set of over-determined event records using a bidirectional LSTM-RNN.",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_3",
  "x": "Soricut and Marcu (2006) propose a language generation system that uses the WIDL-representation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework<cite> (Angeli et al., 2010)</cite> . Recent work seeks to solve the full selective generation problem through a single framework.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_4",
  "x": "<cite>Angeli et al. (2010)</cite> propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model. Similar to other work, they train their model using external alignments from Liang et al. (2009) . Generation then follows as inference over this model, where they first choose an event record, then the record's fields (i.e., attributes), and finally a set of templates that they then fill in with words for the selected fields. Their ability to model long-range dependencies relies on their choice of features for the log-linear model, while the template-based generation further employs some domain-specific features for fluent output. Konstas and Lapata (2012) propose an alternative method that simultaneously optimizes the content selection and surface realization problems.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_5",
  "x": "Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work<cite> (Angeli et al., 2010)</cite> . We later discuss an alternative k-nearest neighbor-based beam filter (see Sec 6.2). ---------------------------------- **EXPERIMENTAL SETUP** Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability.",
  "y": "similarities"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_6",
  "x": "Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability. Following <cite>Angeli et al. (2010)</cite> , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively. For ROBOCUP, we follow the evaluation methodology of previous work (Chen and Mooney, 2008) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth. Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and \u03b3. We then report the standard average performance (weighted by the number of scenarios) over these four splits.",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_7",
  "x": "**PRIMARY RESULTS (WEATHERGOV)** We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_8",
  "x": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ---------------------------------- **BEAM FILTER WITH K-NEAREST NEIGHBORS**",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_9",
  "x": "Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ---------------------------------- **BEAM FILTER WITH K-NEAREST NEIGHBORS** We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset<cite> (Angeli et al., 2010)</cite> . As an alternative, we consider a beam filter based on a knearest neighborhood.",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_10",
  "x": "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by Bahdanau et al. (2014) . Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner. Encoder Ablation Next, we consider the effectiveness of the encoder. Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN. We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005;<cite> Angeli et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_0",
  "x": "We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015).",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_1",
  "x": "Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_2",
  "x": "RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex-56 ----------------------------------",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_3",
  "x": "RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex-56 ----------------------------------",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_4",
  "x": "We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015) . Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments.",
  "y": "similarities uses"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_5",
  "x": "RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_6",
  "x": "RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_7",
  "x": "RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_8",
  "x": "These models learn a regression function using the features to estimate a numerical target value. We also use them after a dimensionality reduction and mapping step with partial least squares (PLS) (Specia et al., 2009 ). We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are in <cite>(Bi\u00e7ici and Way, 2014b</cite>; Bi\u00e7ici et al., 2014) . We optimize the learning parameters by selecting \u03b5 close to the standard deviation of the noise in the training set (Bi\u00e7ici, 2013) since the optimal value for \u03b5 is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998) .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_9",
  "x": "Domain specific RTM models obtain improved performance in those domains<cite> (Bi\u00e7ici and Way, 2014b)</cite> . STS English test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however for evaluation, STS use a subset of the test set, 375, 750, 375, 750, and 750 instances respectively from the corresponding domains. This may lower the performance of RTMs by causing FDA5 to select more domain specific data and less task specific since RTMs use the test set to select interpretants and build a task specific RTM prediction model. (Bi\u00e7ici and Way, 2014b) are presented in Table 6, where we have used the top results from domain specific RTM models for headlines and images domains in the overall model results. Top 3 individual RTM model performance on the training set with further optimized learning model parameters after the challenge are presented in Table 7 .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_10",
  "x": "---------------------------------- **RTMS ACROSS TASKS AND YEARS** We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8 , we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval-2013 , from SemEval-2014<cite> (Bi\u00e7ici and Way, 2014b)</cite> , and and from quality estimation task (QET) (Bi\u00e7ici and Way, 2014a ) of machine translation (Bojar et al., 2014) . RTMs at SemEval-2013 contain results from STS.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_0",
  "x": "According to (Becker, 1985; Huang, 1985; Gu et al., 1991; Chung, 1993; Kuo, 1995; Fu et al., 1996; Lee et al., 1997; Hsu et al., 1999; Chen et al., 2000; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (Chang et al., 1991; Hsu et al., 1993; Hsu, 1994; Hsu et al., 1999; Kuo, 1995; Lua and Gan, 1992) , arbitrary codes based (Fan et al., 1988) and structure scheme based (Huang, 1985) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (Chung, 1993) , online handwriting and speech recognition (Fu et al., 1996; Chen et al., 2000) . Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_1",
  "x": "Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion. On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) .",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_2",
  "x": "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) . As per (Chung, 1993; Fong and Chung, 1994; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system. Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy. Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (Hsu, 1994; Fu et al., 1996; Hsu et al., 1999; Kuo, 1995; Tsai and Hsu, 2002) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (Lin and Tsai, 1987; Gu et al., 1991; Fu et al., 1996; Ho et al., 1997; Sproat, 1990; Gao et al., 2002; Lee 2003) . From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_3",
  "x": "From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_4",
  "x": "From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_5",
  "x": "Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (Hsu, 1994; Fu et al., 1996; Hsu et al., 1999; Kuo, 1995; Tsai and Hsu, 2002) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (Lin and Tsai, 1987; Gu et al., 1991; Fu et al., 1996; Ho et al., 1997; Sproat, 1990; Gao et al., 2002; Lee 2003) . From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_6",
  "x": "Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows. In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.",
  "y": "motivation"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_7",
  "x": "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows. In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database. Then, we develop a word support model with the WP database to perform STW conversion on identifying words from the Chinese syllables.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_8",
  "x": "The system dictionary provides the knowledge of words and their corresponding pinyin syllable-words. The pinyin syllable-words were translated by phoneme-to-pinyin mappings, such as \"\u3129\u02ca\"-to-\"ju2.\" ---------------------------------- **AUTO-GENERATION OF WP DATABASE** Following<cite> (Tsai, 2005)</cite> , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in<cite> (Tsai, 2005))</cite> Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (Chen et al., 1986; Tsai et al., 2004) with the system dictionary.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_9",
  "x": "**STW EXPERIMENT RESULTS OF THE WSM** The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database. The comparative system is the WP identifier<cite> (Tsai, 2005)</cite> . Table  2 is the experimental results. The WP database and system dictionary of the WP identifier is same with that of the WSM.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_10",
  "x": "**STW EXPERIMENT RESULTS OF CHINESE INPUT SYSTEMS WITH THE WSM** We selected Microsoft Input Method Editor 2003 for Traditional Chinese (MSIME) as our experimental commercial Chinese input system. In addition, following<cite> (Tsai, 2005)</cite> , an optimized bigram model called BiGram was developed. The BiGram STW system is a bigrambased model developing by SRILM (Stolcke, 2002) with Good-Turing back-off smoothing (Manning and Schuetze, 1999) , as well as forward and backward longest syllable-word first strategies (Chen et al., 1986; Tsai et al., 2004) . The system dictionary of the BiGram is same with that of the WP identifier and the WSM.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_11",
  "x": "a STW accuracies and improvements of the words identified by the MSIME (Ms) with the WP identifier b STW accuracies and improvements of the words identified by the MSIME (Ms) with the WSM Table 3a . The results of tonal and toneless STW experiments for the MSIME, the MSIME with the WP identifier and with the WSM. From Table 3a , the tonal and toneless STW improvements of the MSIME by using the WP identifier and the WSM are (18.9%, 10.1%) and (25.6%, 16.6%), respectively. From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively. (Note that, as per<cite> (Tsai, 2005)</cite> , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%).",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_12",
  "x": "(1) The coverage of unknown word problem for tonal and toneless STW conversions is similar. In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) . The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem. This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different.",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_13",
  "x": "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem. This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of<cite> (Tsai, 2005)</cite> . From Table 4 , the major improving targets of tonal STW performance are the HS errors because more than 50% tonal STW errors caused by HS problem.",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_14",
  "x": "In this paper, we present a word support model (WSM) to improve the WP identifier<cite> (Tsai, 2005)</cite> and support the Chinese Language Processing on the STW conversion problem. All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus. We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words. The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing. Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_0",
  "x": "Although biasing is improved by these techniques, they do not address cross-lingual recognition. In [15] , contextual biasing has been used to assist recognition of foreign words. With the phoneme mapping from a foreign language phoneme set to the recognizer's phoneme set, foreign words are modeled as a phoneme-level contextual FST for biasing. It is unclear whether such an approach can be directly applied to E2E models. Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general <cite>[16,</cite> 17] , but shows better recognition of rare words and proper nouns.",
  "y": "background"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_1",
  "x": "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models <cite>[16,</cite> 17] . We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages). In inference, given a list of foreign words, we bias the recognition using an English phoneme-level biasing FST, which is built by first tokenizing the words into foreign phonemes and then mapping them to English phonemes using [15] .",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_2",
  "x": "We use a pronunciation lexicon to obtain phoneme sequences of words. Since phonemes show strength in recognizing rare words<cite> [16]</cite> , we want to present these words as phonemes more often. In a target sentence, we decide to randomly present the i th word as phonemes with a probability , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus. Therefore, the words that appear T times or less will be presented as phonemes with probability p0.",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_3",
  "x": "For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 . Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs. We use context-independent phonemes as in<cite> [16]</cite> . ---------------------------------- **BIASING FST FOR PHONEMES**",
  "y": "similarities uses"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_4",
  "x": "The biasing FST is then used to rescore the phoneme outputs of the wordpiecephoneme model on the fly, using Eq. (1). Figure 1 : Contextual FST for the word \"Cr\u00e9teil\" using a sequence of English phonemes \"k r\\ E t E j\". ---------------------------------- **DECODING GRAPH** To generate words as outputs, we search through a decoding graph similar to<cite> [16]</cite> but accept both phonemes and wordpieces.",
  "y": "differences similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_5",
  "x": "For clarity, we omitted most wordpieces for the state 0. Based on<cite> [16]</cite> , we add two improvements to the decoding strategy. First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input. Second, we merge paths that have the same output symbols. Given the nature of our training and decoding, a given word can be output either directly in wordpieces, or transduced from phonemes to wordpieces.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_6",
  "x": "Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model. We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in<cite> [16]</cite> . Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST. This further reduces the WER by 2%, as shown in the bottom row in Table 1 . This shows that wordpiece and phoneme biasing are complementary to each other.",
  "y": "similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_7",
  "x": "The wordpiecephoneme model performs a little better than the grapheme model, and we attribute that to the higher frequency of wordpieces during training. Compared to the wordpiece model, the wordpiece-phoneme model has a slight degradation (0.1% absolute WER). This is due to the introduction of phonemes in modeling. One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] . However, we note that the regression is significantly smaller than the all-phoneme model in<cite> [16]</cite> .",
  "y": "differences"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_0",
  "x": "---------------------------------- **INTRODUCTION** Semantic parsing is a task of transducing natural language to meaning representations, which in turn can be expressed through many different semantic formalisms including lambda calculus (Zettlemoyer and Collins, 2012) , DCS (Liang et al., 2013) , Discourse Representation Theory (DRT) (Kamp and Reyle, 2013) , AMR (Banarescu et al., 2013) and so on. This availability of annotated data in English has translated into the development of a plethora of models, including encoder-decoders (Dong and Lapata, 2016; Jia and Liang, 2016) as well as tree or graph-structured decoders Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Yin and Neubig, 2017) . Whereas the majority of semantic banks focus on English, recent effort has focussed on *Work done when Jingfeng Yang was an intern and Federico Fancellu a post-doc at the University of Edinburgh building multilingual representations, e.g. PMB (Abzianidze et al., 2017) , MRS (Copestake et al., 1995) and FrameNet (Pad\u00f3 and Lapata, 2005) .",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_1",
  "x": "Figure 1 shows an example DRT for the sentence 'I sat down and opened my laptop' in its canonical 'box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. 'speaker'). DRS can be linked to each other via logic operator (e.g. \u00ac, \u2192, \u22c4) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORA-TION, etc.). To test our approach we leverage the DRT parser of <cite>Liu et al. (2018)</cite> , an encoder-decoder architecture where the meaning representation is reconstructed in three stages, coarse-to-fine, by first building the DRS skeleton (i.e. the 'box' structures) and then fill each DRS with predicates and variables. Whereas the original parser utilizes a sequential Bi-LSTM encoder with monolingual lexical features, we experiment with languageindependent features in the form of cross-lingual word-embeddings, universal PoS tags and universal dependencies.",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_2",
  "x": "In this section, we describe the modifications to the coarse-to-fine encoder-decoder architecture of <cite>Liu et al. (2018)</cite> ; for more detail, we refer the reader to the original paper. ---------------------------------- **ENCODER** BiLSTM. We use <cite>Liu et al. (2018)</cite> 's Bi-LSTM as baseline.",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_3",
  "x": "We use <cite>Liu et al. (2018)</cite> 's Bi-LSTM as baseline. However, whereas the original model represents each token in the input sentence as the concatenation of word (e w i ) and lemma embeddings, we discard the latter and add a POS tag embedding (e p i ) and dependency relation embedding (e d i ) feature. These embeddings are concatenated to represent the input token. The final encoder representation is obtained by concatenating both final forward and backward hidden states. TreeLSTM.",
  "y": "uses"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_4",
  "x": "Computation is shown in Equation (3). ---------------------------------- **DECODER** The decoder of <cite>Liu et al. (2018)</cite> reconstructs the DRS in three steps, by first predicting the overall structure (the 'boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous. During predicate prediction, the decoder uses a copying mechanism to predict those unary predicates that are also lemmas in the input sentence (e.g. 'eat').",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_5",
  "x": "For the those that are not, soft attention is used instead. No modifications were done to the decoder; for more detail, we refer the reader to the original paper. ---------------------------------- **DATA** We use the PMB v. In order to be used as input to the parser, <cite>Liu et al. (2018)</cite> first convert the DRS into treebased representations, which are subsequently linearized into PTB-style bracketed sequences.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_6",
  "x": "Counter looks for the best alignment between the predicted and gold DRS and computes precision, recall and F1. For further details about Counter, the reader is referred to Van Noord et al. (2018) . It is worth reminding that unlike other work on the PMB (e.g. van Noord et al., 2018), <cite>Liu et al. (2018)</cite> does not deal with presupposition. In the PMB, presupposed variables are extracted from a main box and included in a separate one. In our work, we revert this process so to ignore presupposed boxes.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_7",
  "x": "For semantic parsing, encoder-decoder mod- els have achieved great success. Amongst these, tree or graph-structured decoders have recently shown to be state-of-the-art Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Cheng et al., 2017; Yin and Neubig, 2017) . ---------------------------------- **CONCLUSIONS** We go back to the questions in the introduction: Can we train a semantic parser in a language where annotation is available?.",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_0",
  "x": "Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_1",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004 ) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages.",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_2",
  "x": "Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_3",
  "x": "I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German).",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_0",
  "x": "Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern. In contrast, we find that generators trained using our structured adversary produce samples that satisfy rhyming constraints with much higher frequency.",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_1",
  "x": "This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern.",
  "y": "motivation"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_2",
  "x": "Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. They train the classifier along with a language model in a multi-task setup. In contrast, we find that generators trained using our structured adversary produce samples that satisfy rhyming constraints with much higher frequency.",
  "y": "differences"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_3",
  "x": "---------------------------------- **NEURAL GENERATION MODEL** Our generator is a hierarchical neural language model ( Figure 1 ) that first generates a sequence of line-ending words, and thereafter generates the poem's lines conditioned on the ending words. We use recurrent neural networks for ending word generation as well line generation conditioned on ending words. Following prior work<cite> (Lau et al., 2018)</cite>, we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_4",
  "x": "Following prior work<cite> (Lau et al., 2018)</cite>, we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first. Letx represent a sample from the current generator distribution, denoted by p \u03b8 , where \u03b8 represents the generator parameters. We initialize the word embeddings in the generator with pre-trained word embeddings<cite> (Lau et al., 2018)</cite> trained on a separate non-sonnet corpus. ---------------------------------- **STRUCTURED DISCRIMINATOR**",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_5",
  "x": "**EXPERIMENTS AND RESULTS** ---------------------------------- **DATASETS** We work with the Shakespeare SONNET dataset<cite> (Lau et al., 2018</cite> ) and a new LIMERICK corpus. Each sonnet in the Sonnet dataset is made up of 3 quatrains of 4 lines each, and a couplet.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_6",
  "x": "Towards this end, we get 10K samples from each model without any constraints (except avoiding UNK -unknown tokens). Fol-lowing prior work<cite> (Lau et al., 2018)</cite> , words are sampled with a temperature value between 0.6 and 0.8. We use CMU dictionary (Weide, 1998) to look up the phonetic representation of a word, and extract the sequence of phonemes from the last stressed syllable onward. Two words are considered to be rhyming if their extracted sequences match (Parrish, 2015) . We consider a generated quatrain to have an acceptable pattern if the four ending words follow one of the three rhyming patterns: AABB, ABBA, ABAB.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_7",
  "x": "Our model without adversarial learning i.e. RHYME-LM, achieves a test set NLL of 3.97. DEEP-SPEARE reports a test set NLL of 4.38. Note that our language model is hierarchical while DEEP-SPEARE has a linear model. The NLL for RHYME-LM and RHYME-GAN are very similar, though RHYME-GAN gets much better sampling efficiency scores than RHYME-LM. Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_8",
  "x": "Note that our language model is hierarchical while DEEP-SPEARE has a linear model. The NLL for RHYME-LM and RHYME-GAN are very similar, though RHYME-GAN gets much better sampling efficiency scores than RHYME-LM. Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> . The main difference is that we first generate all the line-ending words and then condition on them to generate the remaining words. The change was made to make it more amenable to our proposed discriminator.",
  "y": "extends"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_9",
  "x": "---------------------------------- **HUMAN EVALUATIONS** Following prior work<cite> (Lau et al., 2018)</cite> , we requested human annotators to identify the humanwritten poem when presented with two samples at a time -a quatrain from the Sonnet corpus and a machine-generated quatrain, and report the annotator accuracy on this task. Note that a lower accuracy value is favorable as it signifies higher quality of machine-generated samples. Using 150 valid samples (i.e. samples belonging to one of the allowed rhyming patterns), we observe 56.0% annotator accuracy for RHYME-GAN, and 53.3% for DEEP-SPEARE -indicating that the post-rejection sampling outputs from the two methods are of comparable quality (the difference in annotator accuracy is not statistically significant as per McNemar's test under p < 0.05).",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_10",
  "x": "**RELATED WORK** Early works on poetry generation mostly used rule based methods (Gerv\u00e1s, 2000; Wu et al., 2009; Oliveira, 2017) . More recently, neural models for poetry generation have been proposed (Zhang and Lapata, 2014; Ghazvininejad et al., 2016 Ghazvininejad et al., , 2017 Hopkins and Kiela, 2017;<cite> Lau et al., 2018</cite>; Liu et al., 2019) . Yan et al. (2013) retrieve high ranking sentences for a given user query, and repeatedly swap words to satisfy poetry constraints. Ghazvininejad et al. (2018) worked on poetry translation using an unconstrained machine translation model and separately learned Finite State Automata for enforcing rhythm and rhyme.",
  "y": "background"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_0",
  "x": "Several diagnostic datasets have been proposed with this goal in mind, highlighting various flaws in existing tasks (Johnson et al., 2017; Zhang et al., 2015) . Our paper is a contribution to these efforts, showing that the field may have moved too fast from noun to sentence interpretation, overlooking difficulties in understanding other parts-of-speech. Our paper expands the existing FOIL dataset <cite>(Shekhar et al., 2017)</cite> . FOIL consists of a set of images matched with captions containing one single mistake. The mistakes are always nouns referring to objects not actually present in the image.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_1",
  "x": "**THE FOIL METHODOLOGY** We follow the methodology highlighted in<cite> Shekhar et al. (2017)</cite> , which consists of replacing a single word in a human-generated caption with a 'foil' item, making the caption unsuitable to describe the original image. Given such replacements, the system should be able to perform three tasks: a) a classification task (T1): given an image and a caption, the model has to predict whether the caption is correct or inappropriate for the image (evaluating whether the model has a coarse understanding of the linguistic and visual inputs and their relations); b) a foil word detection task (T2): given an image and a foil caption, detect the foil word in the caption (evaluating whether the model reaches a fine-grained representation of the linguistic input); a foil word correction task (T3): given an image, a foil caption and the foil word, the model has to correct the mistake (verifying whether the model reaches a fine-grained representation of the image). Four models are tested on tasks 1-3: one baseline (a 'blind' model), and three state-of-the-art models from the Visual Question Answering (VQA) and Image Captioning (IC) literature. Blind Model: this model is based on the caption only; in other words, the system does not have access to the visual data.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_2",
  "x": "For T2, we subsequently occlude one word (Goyal et al. (2016) ) at a time and calculate the probability of the new caption to be good vs. foil. The model selects as foil word, the one which has generated the caption with the highest probability. For T3, we regress over all<cite> Shekhar et al. (2017)</cite> . the target words on the position of the foil word and select the one which generates the caption with the highest probability to be \"good\". Due to the generative nature of IC models, adapting IC-Wang for the classification purpose is less straightforward.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_3",
  "x": "---------------------------------- **DATASET CREATION** Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns. The introduced error can also be an adjective (an object's attribute), a verb (an action), a preposition (a relation between objects) or an adverb (a manner of action).",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_4",
  "x": "**DATASET CREATION** Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns. The introduced error can also be an adjective (an object's attribute), a verb (an action), a preposition (a relation between objects) or an adverb (a manner of action). In total, we produce 196,284 datapoints, each corresponding to an <image, original, foil> triple.",
  "y": "extends"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_5",
  "x": "Using these prepositions, we generate target/foil pairs by coupling prepositions which belong to the same class. We obtain a total of 206 pairs (110, 90 and 6 for place, direction and device respectively). Nouns: The target/foil noun pairs are built using words that belong to the same category in MS-COCO (e.g., bird/dog, from the MS-COCO category ANIMAL). In order to obtain a balanced dataset across the various PoS, we only use a subset of the FOIL-COCO dataset of<cite> Shekhar et al. (2017)</cite> . From the FOIL dataset, we retain the 37,536 images for which foil captions could be generated, using the target/foil pairs extracted from the resources mentioned above.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_6",
  "x": "The number of unique pairs per PoS is provided in Table 1 . ---------------------------------- **FOIL CAPTION GENERATION** From the word pair lists above, foil captions are generated from MS-COCO original captions. The foil captions are generated by replacing nouns are directly extracted from the FOIL dataset by<cite> Shekhar et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_7",
  "x": "The comparatively low performance on adverbs may be explained by the fact that all generated target/foil pairs are antonyms which behave very similarly from a distributional point of view (e.g. upwards/downwards, partially/completely, etc). HieCoAtt is the overall best performing model, but we note that it only outperforms the blind model by a few points. These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks. As shown in Table 3 , the blind model's accuracy is still reasonable on T1, but lower than chance for nouns and adverbs.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_8",
  "x": "These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks. As shown in Table 3 , the blind model's accuracy is still reasonable on T1, but lower than chance for nouns and adverbs. In the case of nouns, the visual input helps obtaining a higher accuracy, whereas this is not the case for the other PoS. This could be due to the ability of vision models to 'see' objects but not their properties (adjectives) or relations (verbs, prepositions) . It is a known shortcoming of such systems that they have difficulties in recognising anything that is not straightforwardly defined by a bounding box (Johnson et al. (2017) ).",
  "y": "background"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_0",
  "x": "This tutorial draws connections from theories of deep reinforcement learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) . We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_1",
  "x": "We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016) , semi-supervised text classification (Wu et al., 2018) , coreference (Clark and Manning, 2016; Yin et al., 2018) , knowledge graph reasoning (Xiong et al., 2017 ), text games (Narasimhan et al., 2015; He et al., 2016a) , social media (He et al., 2016b; Zhou and Wang, 2018) , information extraction (Narasimhan et al., 2016; Qin et al., 2018) , language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018) , etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_2",
  "x": "To conclude this part, we discuss interesting applications of DRL in NLP, including information extraction and reasoning. \u2022 Lessons Learned, Future Directions, and Practical Advices for DRL in NLP Third, we switch from the theoretical presentations to an interactive demonstration and discussion session: we aim at providing an interactive session to transfer the theories of DRL into practical insights. More specifically, we will discuss three important issues, including problem formulation/model design, exploration vs. exploitation, and the integration of linguistic structures in DRL. We will show case a recent study<cite> (Wang et al., 2018b</cite> ) that leverages hierarchical deep reinforcement learning for language and vision, and extend the discussion. Practical advice including programming advice will be provided as a part of the demonstration.",
  "y": "uses"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_0",
  "x": "Even inner-domain shifts, such as, e.g., moving from hematology (Ohta et al., 2002) to the genetics of cancer (Kulick et al., 2004) within the field of molecular biology may have drastic consequences in the sense that entirely new meta data sets have to produced by annotation teams. Thus, reducing the human efforts for the creation of adequate training material is a major challenge. Active learning (AL) copes with this problem as it intelligently selects the data to be labeled. It is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process. AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality<cite> (Engelson and Dagan, 1996</cite>; Ngai and Yarowsky, 2000; Hwa, 2001; Tomanek et al., 2007a) .",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_1",
  "x": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement. Although in our experiments we considered the NLP task of named entity recognition (NER) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well. In Tomanek et al. (2007a) we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time.",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_2",
  "x": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement. Although in our experiments we considered the NLP task of named entity recognition (NER) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well. In Tomanek et al. (2007a) we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time.",
  "y": "background similarities"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_3",
  "x": "We employed the committee-based AL approach described in Tomanek et al. (2007a) . The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996) . In each AL iteration, each classifier is trained on a randomly , L being the set of all examples seen so far. Disagreement is measured by vote entropy<cite> (Engelson and Dagan, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_0",
  "x": "This indicates that choosing a strong baseline is crucial for reporting reliable experimental results. ---------------------------------- **INTRODUCTION** In the relatively short time since its introduction, neural machine translation has risen to prominence in both academia and industry. Neural models have consistently shown top performance in shared evaluation tasks (Bojar et al., 2016; Cettolo et al., 2016) and are becoming the technology of choice for commercial MT service providers<cite> (Wu et al., 2016</cite>; Crego et al., 2016) .",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_1",
  "x": "Our starting point for experimentation is a standard baseline neural machine translation system implemented using the Lamtram 1 and DyNet 2 toolkits (Neubig, 2015; Neubig et al., 2017) . This system uses the attentional encoder-decoder architecture described by Bahdanau et al. (2015) , building on work by Sutskever et al. (2014) . The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512. Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> . Maximum training sentence length is set to 100 words.",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_2",
  "x": "**TRANSLATION SYSTEM** Our starting point for experimentation is a standard baseline neural machine translation system implemented using the Lamtram 1 and DyNet 2 toolkits (Neubig, 2015; Neubig et al., 2017) . This system uses the attentional encoder-decoder architecture described by Bahdanau et al. (2015) , building on work by Sutskever et al. (2014) . The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512. Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_3",
  "x": "While effective, this approach can be time consuming and relies on hand-crafted learning schedules that may not generalize to different models and data sets. To eliminate the need for schedules, subsequent NMT work trained models using the Adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training (Zeiler, 2012) . Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b) . More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> .",
  "y": "background motivation differences"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_4",
  "x": "Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b) . More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness.",
  "y": "background motivation"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_5",
  "x": "More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms.",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_6",
  "x": "While Adam's use of momentum can be considered a form of \"self-annealing\", we also evaluate the novel extension of explicitly annealing the maximum step size by applying the same halving and restarting process used for SGD. It is important to note that while restarting SGD has no effect beyond changing the learning rate, restarting Adam causes the optimizer to \"forget\" the per-parameter learning rates and start fresh. For all training, we use a mini-batch size of 512 words. 8 For WMT systems, we evaluate dev 6 Adam is the default optimizer for the Lamtram, Nematus (https://github.com/rsennrich/nematus), and Marian toolkits (https://github.com/amunmt/ marian). 7 Learning rates of 0.5 for SGD and 0.0002 for Adam or very similar are shown to work well in NMT implementations including GNMT<cite> (Wu et al., 2016)</cite> , Nematus, Marian, and OpenNMT (http://opennmt.net).",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_8",
  "x": "We measure the effects of byte pair encoding by training full-word and BPE systems for all data sets as described in \u00a72.1 with the incremental improvement of using Adam with rate annealing ( \u00a73). As<cite> Wu et al. (2016)</cite> show different levels of effectiveness for different sub-word vocabulary sizes, we evaluate running BPE with 16K and 32K merge operations. As shown in Table 2 , sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies. Based on these results, we recommend 32K as a generally effective vocabulary size and 16K as a contrastive condition when building systems on less than 1 million parallel sentences.",
  "y": "similarities uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_9",
  "x": "This trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting (Sennrich et al., 2016a; <cite>Wu et al., 2016)</cite> . Even though the English-French data is still relatively small (220K sentences), BPE leads to a smaller vocabulary of more general translation units, effectively reducing sparsity, while annealing Adam can avoid getting stuck in poor local optima. These techniques already lead to better generalization without the need for dropout. Finally, we can observe a few key properties of data bootstrapping, the best performing technique on fully strengthened systems. Unlike lexicon bias and pre-translation, it modifies only the training data, allowing \"purely neural\" models to be learned from random initialization points.",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_1",
  "x": "In recent years, there has been increasing interest in statistical approaches to semantic parsing. However, most of this research has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text<cite> (Liang et al., 2009)</cite> . Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009 ). However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (Poon and Domingos, 2009 ).",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_3",
  "x": "We study our set-up on the weather forecast data<cite> (Liang et al., 2009)</cite> where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example). The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group. Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 . This compares favorably with 69.1% shown by a semi-supervised learning approach, though, as expected, does not reach the score of the model which, in training, observed semantics states for all the 750 documents (77.7% F 1 ). The rest of the paper is structured as follows.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_4",
  "x": "Section 3 redescribes the semantics-text correspondence model<cite> (Liang et al., 2009)</cite> in the context of our learning scenario. In section 4 we provide an empirical evaluation of the proposed method. We conclude in section 5 with an examination of additional related work. ---------------------------------- **INFERENCE WITH NON-CONTRADICTORY DOCUMENTS**",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_5",
  "x": "The semantics m can be represented either as a logical formula (see, e.g., (Poon and Domingos, 2009 )) or as a set of field values if database records are used as a meaning representation<cite> (Liang et al., 2009</cite> ). The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (Poon and Domingos, 2009) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem. In semantic parsing, we aim to find the most likely underlying semantics and alignment given the text: In the supervised case, where a and m are observable, estimation of the generative model parameters is generally straightforward. However, in a semi-supervised or unsupervised case variational techniques, such as the EM algorithm (Dempster et al., 1977) , are often used to estimate the model.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_6",
  "x": "Though the semantics m k (k / \u2208 n\u222a{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent. It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4). An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages. As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in<cite> (Liang et al., 2009)</cite> . The induced alignments a 1 ,..., a K of semantics m to texts w 1 ,..., w K at the same time induce alignments between the texts.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_7",
  "x": "In this paper, we use semantic structures as a pivot for finding the best alignment in the hope that presence of meaningful text alignments will improve the quality of the resulting semantic structures by enforcing a form of agreement between them. ---------------------------------- **A MODEL OF SEMANTICS** In this section we redescribe the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) with an extension needed to model examples with latent states, and also explain how the inference algorithm defined in section 2 can be applied to this model. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_9",
  "x": "When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step<cite> (Liang et al., 2009)</cite> . However, when the state is latent, dependencies are not local anymore, and approximate inference is required. We use the algorithm described in section 2 (figure 2) to infer the state. In the context of the semantics-text correspondence model, as we discussed above, semantics m defines the subset of admissible world states. In order to use the algorithm, we need to understand how the conditional probabilities of the form P (m |m) are computed, as they play the key role in the inference procedure (see equation (2)).",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_10",
  "x": "Instead of predicting the most probable semanticsm j we search for the most probable pair (\u00e2 j ,m j ), thus assuming that the probability mass is mostly concentrated on a single alignment. The alignment a j is then discarded and not used in any other computations. Though the most likely alignment\u00e2 j for a fixed semantic representationm j can be found efficiently using a Viterbi algorithm, computing the most probable pair (\u00e2 j ,m j ) is still intractable. We use a modification of the beam search algorithm, where we keep a set of candidate meanings (partial semantic representations) and compute an alignment for each of them using a form of the Viterbi algorithm. As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in<cite> (Liang et al., 2009</cite> ): the state s is no longer latent and we can run efficient inference on the E-step.",
  "y": "similarities uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_11",
  "x": "---------------------------------- **EXPERIMENTS** To perform the experiments we used a subset of the weather dataset introduced in<cite> (Liang et al., 2009</cite> ). The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average. We randomly chose 100 texts along with their world states to be used as the labeled data.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_12",
  "x": "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following<cite> Liang et al. (2009)</cite> we evaluate the models on how well they predict these alignments. When estimating the model parameters, we followed the training regime prescribed in<cite> (Liang et al., 2009)</cite> . Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_13",
  "x": "Following<cite> Liang et al. (2009)</cite> we evaluate the models on how well they predict these alignments. When estimating the model parameters, we followed the training regime prescribed in<cite> (Liang et al., 2009)</cite> . Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model. Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM. Instead of prohibiting records from crossing punctuation, as suggested by<cite> Liang et al. (2009)</cite> , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_14",
  "x": "Instead of prohibiting records from crossing punctuation, as suggested by<cite> Liang et al. (2009)</cite> , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records. To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset. step of the EM algorithm, as it significantly reduces the search space. Similarly, though we preserved all records which refer to the first time period, for other time periods we removed all the records which declare that the corresponding event (e.g., rain or snowfall) is not expected to happen. This preprocessing results in the oracle recall of 93%.",
  "y": "differences"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_15",
  "x": "Additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state. 8 To confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover (table 2). Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories. However, correlation between rain and overcast, as also noted in<cite> (Liang et al., 2009)</cite> , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_16",
  "x": "To combat it, we proposed a simple iterative inference algorithm. We showed how it can be instantiated for the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) and evaluated it on a dataset of weather forecasts. Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning. There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations. A promising and challenging possibility is to consider models which induce full semantic representations of meaning.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_0",
  "x": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing -93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%. ---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 .",
  "y": "background"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_1",
  "x": "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_3",
  "x": "**PREVIOUS WORK** We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two<cite> (Vinyals et al., 2015</cite>; Dyer et al., 2016) are parsing models that have the current best results in NN parsing. ---------------------------------- **LSTM-LM**",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_4",
  "x": "1 ---------------------------------- **DATA** We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (McClosky et al., 2006; Huang et al., 2010; <cite>Vinyals et al., 2015)</cite> for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011 ) with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_6",
  "x": "**RESULTS** ---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs<cite> (Vinyals et al., 2015)</cite> and RNNG (Dyer et al., 2016) , both of which are trained on the WSJ only. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_7",
  "x": "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC)<cite> (Vinyals et al., 2015)</cite> ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in Table 3 . LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_8",
  "x": "In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016) . We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of<cite> Vinyals et al. (2015)</cite> and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.",
  "y": "differences"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_0",
  "x": "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015)</cite> , humor recognition was modeled as a binary classification task In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work<cite> (Yang et al., 2015)</cite> , a new corpus was constructed from a Pun of the Day website.",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_1",
  "x": "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015)</cite> , humor recognition was modeled as a binary classification task In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work<cite> (Yang et al., 2015)</cite> , a new corpus was constructed from a Pun of the Day website.",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_2",
  "x": "In Bertero and Fung (2016b), CNN was found to be the best model that uses both acoustic and lexical cues for humor recognition. By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (Lafferty et al., 2001) . From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing.",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_3",
  "x": "Following (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , we selected the same sizes (n = 4726) of humorous and non-humorous sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure 1 , a negative instance (corresponding to 'sent-2') was selected from the nearby sentences ranging from 'sent-7' and 'sent+7'. ---------------------------------- **METHODS**",
  "y": "background uses"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_4",
  "x": "In Bertero and Fung (2016b), CNN was found to be the best model that uses both acoustic and lexical cues for humor recognition. By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (Lafferty et al., 2001) . From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing.",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_5",
  "x": "---------------------------------- **CONVENTIONAL MODEL** Following<cite> Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to do humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300).",
  "y": "uses"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_6",
  "x": "Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in<cite> Yang et al. (2015)</cite> . In our experiment, we firstly divided each corpus into two parts. The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set. parameters used in text classifiers.",
  "y": "similarities"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_7",
  "x": "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting. 7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in<cite> Yang et al. (2015)</cite> . In particular, precision has been greatly increased from 0.762 to 0.864. On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%). The empirical evaluation results suggests that the CNN-based model has an advantage on the humor recognition task.",
  "y": "background similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_0",
  "x": "Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable<cite> (Munteanu and Marcu, 2006)</cite> . Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007) . Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_1",
  "x": "2 Related Work<cite> (Munteanu and Marcu, 2006)</cite> is the first attempt to extract parallel fragments from comparable sentences. They extract sub-sentential parallel fragments by using a Log-Likelihood-Ratio (LLR) lexicon estimated on an external parallel corpus and a smoothing filter. They show the effectiveness of fragment extraction for SMT. This study has the drawback that they do not locate the source Figure 1 : Parallel fragment extraction system. and target fragments simultaneously, which cannot guarantee that the extracted fragments are translations of each other. We solve this problem by using an alignment model to locate the source and target fragments simultaneously.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_2",
  "x": "The first lexicon we use is the IBM Model 1 lexicon, which is obtained by running GIZA++ 3 that implements sequential word-based statistical alignment model of IBM models. The second lexicon we use is the LLR lexicon. <cite>Munteanu and Marcu (2006)</cite> show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction. One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_3",
  "x": "One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon. Vuli\u0107 and Moens (2012) propose an associative approach for lexicon extraction from par-allel corpora that relies on the paradigm of data reduction.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_4",
  "x": "<cite>Munteanu and Marcu (2006)</cite> show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction. One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon.",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_5",
  "x": "They extract translation pairs from many smaller sub-corpora that are randomly sampled from the original corpus, based on some frequency-based criteria of similarity. They show that their method outperforms IBM Model 1 and other associative methods such as LLR in terms of precision and F-measure. We extract SampLEX lexicon from a parallel corpus using the same method as (Vuli\u0107 and Moens, 2012) . Aiming to gain new knowledge that does not exist in the lexicon, we apply a smoothing filter similar to<cite> (Munteanu and Marcu, 2006)</cite> . For each aligned word pair in the fragment candidates, we set scores to the words in two directions according to the extracted lexicon.",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_6",
  "x": "After this process, we get initial scores for the words in the fragment candidates in two directions. We then apply an averaging filter to the initial scores to obtain filtered scores in both directions. The averaging filter sets the score of one word to the average score of several words around it. We think the words with initial positive scores are reliable, because they satisfy two strong constraints, namely alignment by IBM models and existence in the lexicon. Therefore, unlike<cite> (Munteanu and Marcu, 2006)</cite>, we only apply the averaging filter to the words with negative scores.",
  "y": "extends differences"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_7",
  "x": "---------------------------------- **EXPERIMENTS** In our experiments, we compared our proposed fragment extraction method with<cite> (Munteanu and Marcu, 2006)</cite> . We manually evaluated the accuracy of the extracted fragments. Moreover, we used the extracted fragments as additional MT training data, and evaluated the effectiveness of the fragments for MT.",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_8",
  "x": "---------------------------------- **EXTRACTION EXPERIMENTS** We first applied sentence extraction on the quasicomparable corpora using our system, and 30k comparable sentences of chemistry domain were extracted. We then applied fragment extraction on the extracted comparable sentences. We compared our proposed method with<cite> (Munteanu and Marcu, 2006)</cite>.",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_9",
  "x": "\u2022 External: Use a small number of external parallel sentences together with the comparable sentences (In our experiment, we used chemistry domain data of the parallel corpus described in Section 4.1.1, containing 11k sentences). We also compared IBM Model 1, LLR and SampLEX lexicon for filtering. All lexicons were extracted from the parallel corpus. Table 1 shows the results for fragment extraction. We can see that the average size of fragments extracted by<cite> (Munteanu and Marcu, 2006</cite> ) is unusually long, which is also reported in (Quirk et al., 2007) .",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_10",
  "x": "To evaluate accuracy, we randomly selected 100 fragments extracted by the different methods. We manually evaluated the accuracy based on the number of exact match. Note that exact match criteria has a bias against<cite> (Munteanu and Marcu, 2006)</cite> , because their method extacts subsentential fragments which are quite long. We found that only one of the fragments extracted by \"Munteanu+, 2006\" is exact match, while for the remainder only partial matches are contained in long fragments. Our proposed method have a accuracy over 80%, while the remainder are partial matches.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_11",
  "x": "We trained a 5-gram language model on the Japanese side of the parallel corpus using the SRILM toolkit 8 . Table 2 : Results for Chinese-to-Japanese translation experiments (\" \u2020\" and \" \u2021\"denotes the result is better than \"Baseline\" significantly at p < 0.05 and p < 0.01 respectively, \" * \" denotes the result is better than \"+Munteanu+, 2006\" significantly at p < 0.05). Translation results evaluated on BLEU-4, are shown in Table 2 . We can see that appending the extracted comparable sentences have a positive effect on translation quality. Adding the fragments extracted by<cite> (Munteanu and Marcu, 2006)</cite> has a negative impact, compared to appending the sentences.",
  "y": "differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_0",
  "x": "**ABSTRACT** The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Dinners from Hell.\" Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task<cite> (Chambers and Jurafsky, 2008)</cite> . ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_1",
  "x": "**INTRODUCTION** A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977) . Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora<cite> (Chambers and Jurafsky, 2008</cite>; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_2",
  "x": "Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem-plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) ,<cite> Chambers and Jurafsky (2008)</cite> propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_3",
  "x": "It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by<cite> Chambers and Jurafsky (2008)</cite> , a narrative chain is \"a partially ordered set of narrative events that share a common actor,\" where a narrative event is \"a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.\" To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document. For example, the sentence \"John drove to the store where he bought some ice cream.\" would generate two narrative events corresponding to the protagonist John: (DRIVE, nsubj) followed by (BUY, nsubj). Over these extracted chains of narrative events, pointwise mutual information (PMI) is computed between all pairs of events. These PMI scores are then used to predict missing events from such chains, i.e. the narrative cloze evaluation.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_4",
  "x": "To learn the restaurant script from our dataset, we implement the models of<cite> Chambers and Jurafsky (2008)</cite> and Jans et al. (2012) , as well as the unigram baseline of Pichotta and Mooney (2014) . To evaluate our success in learning the restaurant script, we perform a modified version of the narrative cloze task, predicting only verbs that we annotate as \"restaurant script-relevant\" and comparing the performance of each model. Note that these annotations are not used for training. ---------------------------------- **METHODS**",
  "y": "similarities uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_5",
  "x": "Note that these annotations are not used for training. ---------------------------------- **METHODS** This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model<cite> (Chambers and Jurafsky, 2008)</cite> and extending to the modifications of Jans et al. (2012) . As part of this work, we are releasing a program called NaChos, our integrated Python implementation of each of the methods for learning narrative chains described in this section.",
  "y": "extends differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_6",
  "x": "e n , at insertion point k. The original model, proposed by<cite> Chambers and Jurafsky (2008)</cite> , predicts the event that maximizes unordered pmi, where V is the set of all observed events (the vocabulary) and C(e 1 , e 2 ) is symmetric. Two additional models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, pmi(e, e i ) (4) where C(e 1 , e 2 ) is asymmetric, i.e., C(e 1 , e 2 ) counts only cases in which e 1 occurs before e 2 .",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_7",
  "x": "where p(e 2 |e 1 ) = C(e 1 ,e 2 ) C(e 1 , * ) and C(e 1 , e 2 ) is asymmetric. Discounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by<cite> Chambers and Jurafsky (2008)</cite> . For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting.",
  "y": "similarities uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_8",
  "x": "In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank<cite> (Chambers and Jurafsky, 2008)</cite> , mean reciprocal rank, and recall at 50 (Jans et al., 2012) . Baseline The baseline we use for the narrative cloze task is to rank events by frequency. This is the \"unigram model\" employed by Pichotta and Mooney (2014) , a competitive baseline on this task. For each model and scoring metric, we perform a complete grid search over all possible parameter settings to find the best-scoring combination on a cloze tests from a set-aside development set of ten documents.",
  "y": "similarities uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_0",
  "x": "Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices [6] . Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models [8, 9] . To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) [6] and a large conventional model [8] , a two-pass framework has been proposed in<cite> [10]</cite> , which uses a non-streaming LAS decoder to rescore the RNN-T hypotheses. The rescorer attends to audio encoding from the encoder, and computes sequence-level log-likelihoods of first-pass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T [6] and has a similar WER to a large conventional model [8] .",
  "y": "background"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_1",
  "x": "We apply additional encoder (AE) layers and minimum WER (MWER) training [22] to further improve quality. The results show that our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring<cite> [10]</cite> in VS WER, and up to 15% for proper noun recognition. Joint training further improves VS slightly (2%) but significantly for a proper noun test set: 9%. As a result, our best deliberation model achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model [8] (6.3% VS WER). Lastly, we analyze the computational complexity of the deliberation model, and show some decoding examples to understand its strength.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_2",
  "x": "---------------------------------- **DELIBERATION BASED TWO-PASS E2E ASR** ---------------------------------- **MODEL ARCHITECTURE** As shown in Fig. 1 , our deliberation network consists of three major components: A shared encoder, an RNN-T decoder [1] , and a deliberation decoder, similar to<cite> [10,</cite> 16] .",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_3",
  "x": "We keep the audio encoder unidirectional due to latency considerations. Then, two attention layers are followed to attend to acoustic encoding and first-pass hypothesis encoding separately. The two context vectors, c b and ce, are concatenated as inputs to a LAS decoder. There are two major differences between our model and the LAS rescoring<cite> [10]</cite> . First, the deliberation model attends to both e and yr, while<cite> [10]</cite> only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while<cite> [10]</cite> only relies on unidirectional encoding e for decoding.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_4",
  "x": "However, we find training a two-pass model from scratch tends to be unstable in practice<cite> [10]</cite> , and thus use a two-step training process: Train the RNN-T as in [6] , and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in [7, <cite>10]</cite> . ---------------------------------- **MWER LOSS** We apply the MWER loss [22] in training which optimizes the expected word error rate by using n-best hypotheses: where y i d is the ith hypothesis from the deliberation decoder, and W (y i d , y * ) the number of word errors for y i d w.r.t the ground truth target y * .P (y i d |x) is the probability of the ith hypothesis normalized over all other hypotheses to sum to 1.",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_5",
  "x": "The joint training is similar to \"deep finetuning\" in<cite> [10]</cite> but without a pre-trained decoder. ---------------------------------- **DECODING** Our decoding consists of two passes: 1) Decode using the RNN-T model to obtain the first-pass sequence yr, and 2) Attend to both yr and e, and perform the second beam search to generate y d . We are also curious how rescoring performs given bidirectional encoding from yr.",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_6",
  "x": "We are also curious how rescoring performs given bidirectional encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode<cite> [10]</cite> . Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses. We compare rescoring and beam search in Sect. 4.",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_7",
  "x": "In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode<cite> [10]</cite> . Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses. We compare rescoring and beam search in Sect. 4. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_8",
  "x": "To evaluate the performance of proper noun recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets [6] . The SxS set contains utterances where the LAS rescoring model<cite> [10]</cite> performs inferior to a state-of-the-art conventional model [8] , and one reason is due to proper nouns. The voice command test sets include 3 TTS test sets created using parallel-wavenet [25] : Songs, Contacts-TTS, and Apps, where the commands include song, contact, and app names, respectively. The Contacts-Real set contains anonymized and hand-transcribed utterances from Google traffic to communicate with a contact, for example, \"Call Jon Snow\". ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_9",
  "x": "**RESCORING** We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring<cite> [10]</cite> . Table 5 shows that the deliberation rescoring (E8) performs 5% relatively better than LAS rescoring (B3). AE layers are added to both models. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_10",
  "x": "AE layers are added to both models. ---------------------------------- **COMPARISONS** From the above analysis, an MWER trained 8-hypothesis deliberation model with AE layers performs the best, and thus we use that for comparison below. In Table 4 , we compare deliberation models with an RNN-T [6] and LAS rescoring model<cite> [10]</cite> To understand where the improvement comes from, in Fig. 2 we show an example of deliberation attention distribution on the RNN-T hypotheses (x-axis) at every step of the second-pass decoding (yaxis).",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_0",
  "x": "The target to predict in Task 1 is HTER (human-targeted translation edit rate) scores (Snover et al., 2006) and binary classification of word-level translation errors and the target in Task 2 is multi-dimensional quality metrics (MQM) (Lommel, 2015) . Table 1 lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the RTM models (M for million). We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_1",
  "x": "SVR, which obtains accuracy with 5% error compared with estimates obtained with known noise level (Cherkassky and Ma, 2004) and set = \u03c3/2. Martins et al. (2017) used a hybrid stacking model to combine the word-level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_2",
  "x": "We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE). We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) .",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_3",
  "x": "We filter out those results with higher than 1 relative evaluation metric scores. We also use stacking to build higher level models using predictions from base prediction models where they can also use the probability associated with the predictions (Ting and Witten, 1999) . The stacking models use the predictions from predictors as features and build second level predictors. For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_4",
  "x": "Table 1 lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the RTM models (M for million). We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. With the enlarging parallel and monolingual corpora made available by WMT, the capability of the interpretant datasets selected by RTM models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection (Bi\u00e7ici, 2019) .",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_5",
  "x": "Martins et al. (2017) used a hybrid stacking model to combine the word-level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE).",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_6",
  "x": "---------------------------------- **MIXTURE OF EXPERTS MODELS** We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) . We only use the MIX prediction if we obtain better results on the training set.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_7",
  "x": "We filter out those results with higher than 1 relative evaluation metric scores. We also use stacking to build higher level models using predictions from base prediction models where they can also use the probability associated with the predictions (Ting and Witten, 1999) . The stacking models use the predictions from predictors as features and build second level predictors. For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_0",
  "x": "During the selection of linguistic indicators, we have taken into consideration previously studied features of readability (Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) , L2 Swedish curricula (Levy Scherrer and Lindemalm, 2009; Folkuniversitet, 2013) and aspects of Good Dictionary Examples (GDEX) (Hus\u00e1k, 2010; Kilgarriff et al., 2008) , being that we believe they have some properties in common with exercise items. The current version of the machine learning model distinguishes sentences readable by students at an intermediate level of proficiency from sentences of a higher readability level. The approaches have been implemented and integrated into an online Intelligent ComputerAssisted Language Learning (ICALL) platform, L\u00e4rka . Besides a module where users can experiment with the filtering of corpus hits, a module with inflectional and vocabulary exercises (making use of the selected sentences with our method) is also available. An initial evaluation with students, teachers and linguists indicated that more than 70% of the sentences selected were understandable, and about 60% of them would be suitable as exercise items according to the two latter respondent groups.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_1",
  "x": "It measures not only linguistic difficulty, but also cohesion in texts. Research on L1 readability for Swedish, using machine learning, is described in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> . Heimann M\u00fchlenbock (2013) examined readability along five dimensions: surface features, word usage, sentence structure, idea density and human interest. Mean dependency distance, subordinate clauses and modifiers proved good predictors for L1 Swedish. Although a number of readability formulas exist for native language users, these might not be suitable predictors of L2 difficulty being that the acquisition processes of L1 and L2 present a number of differences (Beinborn et al., 2012) .",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_2",
  "x": "These required less sophisticated text processing and had previously been used in several studies with success (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . We computed sentence length as the number of tokens including punctuation, and token length as the number of characters per token. Part of the syntactic features was based on the depth (length) and direction of dependency arcs (features 5-8). Another group of these features relied on the type of dependency relations. In feature 9 (Mod) nominal pre-modifiers (e.g. adjectives) and post-modifiers (e.g. relative clauses, prepositional phrases) were counted, similarly to Heimann M\u00fchlenbock (2013).",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_3",
  "x": "Variation features (ModVar, AdvVar) measured the ratio of a morphosyntactic category to the number of lexical (content) words in the sentence, as in Vajjala and Meurers (2012) . These lexical categories comprised nouns, verbs, adverbs and adjectives. Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively. The lexical-morphological features (features 13-25) constituted the largest group.",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_4",
  "x": "Variation features (ModVar, AdvVar) measured the ratio of a morphosyntactic category to the number of lexical (content) words in the sentence, as in Vajjala and Meurers (2012) . These lexical categories comprised nouns, verbs, adverbs and adjectives. Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively. The lexical-morphological features (features 13-25) constituted the largest group.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_5",
  "x": "The lexical-morphological features (features 13-25) constituted the largest group. Difficulty at the lexical level was determined based on both the TTR feature mentioned above, expressing vocabulary diversity, and on the basis of the rarity of words (features 13-17) according to the Kelly list and the Wikipedia word list. An analogous approach was adopted also by Fran\u00e7ois and Fairon (2012) , Vajjala and Meurers (2012) and Heimann M\u00fchlenbock (2013) with positive results. The LexD feature considers the ratio of lexical words (nouns, verbs, adjectives and adverbs) to the sum of tokens in the sentence (Vajjala and Meurers, 2012) . The NN/VB ratio feature, which has a higher value in written text, can also indicate a more complex sentence (Biber et al., 2004;<cite> Heimann M\u00fchlenbock, 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_6",
  "x": "This feature was computed by counting the number of sense IDs per token according to a lexical-semantic resource for Swedish, SALDO (Borin et al., 2013) , and dividing this value by the number of tokens in the sentence. As pronouns indicate a potentially more difficult text (Graesser et al., 2011), we included PN/NN in our set. Both NomR and PN/NN capture idea density, i.e. how complex the relation between the ideas expressed are<cite> (Heimann M\u00fchlenbock, 2013)</cite>. ---------------------------------- **CLASSIFICATION RESULTS**",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_7",
  "x": "The SVM classified 7 out of 10 sentences accurately. The precision and recall values for the identification of B1 sentences was 73% and 68%. Previous classification results for a similar task obtained an average of 77.25% of precision for the classification of easy-to-read texts within an L1 Swedish text-level readability study<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Another classification at the sentence level, but for Italian and from an L1 perspective achieved an accuracy of 78.2%, thus 7% higher compared to our results (Dell'Orletta et al., 2011) . The 73% precision of our SVM model for classifying B1 sentences was close to the precision of 75.1% obtained for the easy-to-read sentences from Dell'Orletta et al. (2011) .",
  "y": "differences"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_8",
  "x": "terestingly, although semantic features represented the smallest group, they performed 2% better than traditional or syntactic features. The largest group of features including lexical-morphological indicators performed around 10% more accurately than other feature groups. Among the 10 features that influenced most the decisions of our SVM classifier, we can find attributes from different feature groups. The ID of these features together with the SVM weights are reported in Table 5 . An informative traditional measure was sentence length, similarly to the results of previous studies (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) .",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_9",
  "x": "Moreover, in the case of Swedish L1 text readability the noun/pronoun ratio and modifiers proved to be indicative of textlevel difficulty<cite> (Heimann M\u00fchlenbock, 2013</cite> ), but at the sentence level from the L2 perspective only the latter seemed influential in our experiments. The data used for the experiments was labeled for CEFR levels at the text level, not at the sentence level. This introduced some noise in the data and made the classification task somewhat harder. In the future, the availability of data labeled at the sentence level could contribute to more accurate results. Excluding potentially lower level sentences from those appearing in higher level texts based on the distance between feature vectors could also be explored, in a similar fashion to Dell'Orletta et al. (2011).",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_0",
  "x": "More broadly, this model-a smaller neutral model within a larger neutral model-could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants. English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_1",
  "x": "More broadly, this model-a smaller neutral model within a larger neutral model-could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants. English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_2",
  "x": "English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_3",
  "x": "In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_4",
  "x": "In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_5",
  "x": "As N t grows, so does the minimum frequency needed to break into the top 100. As the \"bar\" is raised, words are more likely to 'die' before they ever reach the bar by stochastic walk [43] . As a result, turnover in the Top y can slow down over time and growth of N t . The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> . In the FNM, the expected exponent \u03b2 is 1.0, as the number of different variants (vocabulary) normally scales linearly with \u00b5N t [11] .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_6",
  "x": "In the case of conformity bias-where agents choose high-frequency words with even greater probability than just in proportion to frequency-both the Zipf law and turnover deteriorate under strong conformity in ways that mis-match with the data. What did ultimately work very well was our partial-sampling Neutral model, or PNM (Fig 1b) , which models a growing sample from a fixed-sized FNM. Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent.",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_7",
  "x": "The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon. Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts. \" In our PNM representation, there is no network structure between words at all, such as \"inter-word statistical dependencies\" [44] or grammar as a hierarchical network structure between words [20] .",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_8",
  "x": "Due to the exponentially increasing sample size, the ratio of vocabulary size over corpus size becomes increasingly small, thus the model gives us the sub-linear relationship described by v t = N \u03b2 t , where \u03b2 < 1. On the double-logarithmic plot in Fig 3a, the Heaps law exponent is equivalent to the slope of the data series. The PNM matches the 1-gram data with Heaps exponent (slope) of about 0.5, whereas the FNM, with exponent about 1.0, does not. Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M). We also The PNM yields Heaps law exponent \u03b2 \u2248 0.52 \u00b1 0.006, within the range of English corpora, whereas the FNM yields a mismatch with the data of \u03b2 \u2248 1 \u00b1 0.002 (Fig 3b) .",
  "y": "background similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_9",
  "x": "As in a previous study [1], we removed 1-grams that are common symbols or numbers, and 1-grams containing the same consonant three or more times consecutively. As in our other studies [1, 8, 6 ], we normalized the count of 1-grams using the yearly occurrences of the most common English word, the. Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc). The code used for modeling is available at: https://github.com/dr2g08/Neutral-evolution-and-turnover-over-centuries-of-English-word-popularity. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_10",
  "x": "**INTRODUCTION** English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] .",
  "y": "background differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_11",
  "x": "At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_12",
  "x": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_13",
  "x": "The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_14",
  "x": "If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words. This can be measured in terms of how many words are replaced through time on \"Top y\" ranked lists of different sizes y of most frequently-used words [12, 17, 19, 23] . We can define this turnover z y (t) as the number of new words to have entered the top y most common words in year t, which is equivalent to the the top y in that year. The plotting of turnover z y for different list sizes y can therefore be useful in characterising turnover dynamics [2] .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_15",
  "x": "As a result, turnover in the Top y can slow down over time and growth of N t . The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> . In the FNM, the expected exponent \u03b2 is 1.0, as the number of different variants (vocabulary) normally scales linearly with \u00b5N t [11] . While the FNM has been a powerful null model, in the case of books, we can make a notable improvement to account for the fact that most published material goes unnoticed while a relatively small portion of the corpus is highly visible. To name a few examples across the centuries, literally billions of copies of the Bible and the works of Shakespeare have been read since the seventeenth century, as well as tens or hundreds of millions of copies of works by Voltaire, Swift, Austen, Dickens, Tolkien, Fleming, Rawling and so on.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_16",
  "x": "Due to the exponentially increasing sample size, the ratio of vocabulary size over corpus size becomes increasingly small, thus the model gives us the sub-linear relationship described by v t = N \u03b2 t , where \u03b2 < 1. On the double-logarithmic plot in Fig 3a, the Heaps law exponent is equivalent to the slope of the data series. The PNM matches the 1-gram data with Heaps exponent (slope) of about 0.5, whereas the FNM, with exponent about 1.0, does not. Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M). We also The PNM yields Heaps law exponent \u03b2 \u2248 0.52 \u00b1 0.006, within the range of English corpora, whereas the FNM yields a mismatch with the data of \u03b2 \u2248 1 \u00b1 0.002 (Fig 3b) .",
  "y": "background similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_19",
  "x": "Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent. The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English.",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_20",
  "x": "As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon. Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts. \" In our PNM representation, there is no network structure between words at all, such as \"inter-word statistical dependencies\" [44] or grammar as a hierarchical network structure between words [20] . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_21",
  "x": "**1-GRAM DATA** The 1-gram data are available as csv files directly from Google's Ngrams site [25] . As in a previous study [1] , we removed 1-grams that are common symbols or numbers, and 1-grams containing the same consonant three or more times consecutively. As in our other studies [1, 8, 6] , we normalized the count of 1-grams using the yearly occurrences of the most common English word, the. Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc).",
  "y": "uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_0",
  "x": "The sub-tasks are learned independently, and exact inference is used to find highest-scoring maximum spanning connected acyclic graph that contains all the concepts identified in the first stage. Later work by <cite>Wang et al. (2015b)</cite> adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph. They start from the dependency parse and learn a transition-based parser that converts it into an AMR graph. To learn the parser, <cite>Wang et al. (2015b)</cite> define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to be taken during testing. This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_1",
  "x": "They start from the dependency parse and learn a transition-based parser that converts it into an AMR graph. To learn the parser, <cite>Wang et al. (2015b)</cite> define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to be taken during testing. This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy. In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of <cite>Wang et al. (2015b)</cite> with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy.",
  "y": "differences extends"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_2",
  "x": "In the following subsections we focus on the differences from previous work and in particular that of <cite>Wang et al. (2015b)</cite> who introduced the transitionbased dependency-to-AMR paradigm we follow. We initialise the main algorithm with a stack of the nodes in the dependency tree, root node first. This stack is termed \u03c3. A second stack, \u03b2 is initialised with all children of the top node in \u03c3. The state at any time is described by \u03c3, \u03b2, and the current graph (which starts as the dependency tree).",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_3",
  "x": "All our experiments use node alignments from the system of Pourdamghani et al. (2014) . ---------------------------------- **ACTION SPACE** Flanigan et al. (2014) and <cite>Wang et al. (2015b)</cite> , both use AMR fragments as their smallest unit, which may consist of more than one AMR concept. Instead, we always work with the individual AMR nodes, and rely on Insert actions to learn how to build common fragments, such as country names.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_4",
  "x": "Swap, Reattach and ReplaceHead change this structure, but always retain a tree structure. ReplaceHead covers two distinct actions in <cite>Wang et al. (2015b)</cite> ; ReplaceHead and Merge. Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label. We leave that decision to a later NextEdge action.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_5",
  "x": "Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label. We leave that decision to a later NextEdge action. We permit a Reattach action to use parameter \u03ba equal to any node within six edges from \u03c3 0 , excluding any that would disconnect the graph or creating a cycle. The Insert action inserts a new node as a parent of the current \u03c3 0 .",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_6",
  "x": "From the top the actions are Insert(dateentity); NextNode(WORD); NextEdge(year); second diagram; NextNode(WORD); ReplaceHead to remove \"in\"; third diagram; NextNode(WORD); NextEdge(mod); Reattach to move \"date-entity\"; fourth diagram; NextNode(VERB); ReplaceHead to remove \"by\"; NextEdge(ARG0); NextEdge(time); NextNode(strike-01). <cite>Wang et al. (2015b)</cite> use all AMR concepts and relations that appear in the training set as possible parameters (l c and l r ) if they appear in any sentence containing the same lemma as \u03c3 0 and \u03b2. We reduce this to just concepts that have been aligned to the current lemma. We initially run the expert policy over the training set, and track the AMR concept assigned for each lemma. These provide the possible l c that will be used for NextNode actions.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_7",
  "x": "---------------------------------- **FEATURES** All features used are detailed in Table 2 , largely based on <cite>Wang et al. (2015b)</cite> . All are 0-1 indicator functions. inserted is 1 if the node was inserted by the parser; dl is the dependency label in the original dependency tree; ner the named entity tag; POS the part-of-speech tag; prefix is the string before the hyphen if word is hyphenated; suffix is the string after the hyphen; brown is the 100-class Brown cluster id with cuts at 4, 6, 10 and 20 2 ; deleted is the lemma of any child node previously deleted by the parser; merged is the lemma of any node merged into this node by a ReplaceHead action; distance is the distance between the tokens in the sentence; path concatenates lemmas and dls between the tokens in the dependency tree; POSpath concatenates POS tags between the tokens; NERpath concatenates NER tags between the tokens.",
  "y": "similarities uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_8",
  "x": "All features used are detailed in Table 2 , largely based on <cite>Wang et al. (2015b)</cite> . All are 0-1 indicator functions. inserted is 1 if the node was inserted by the parser; dl is the dependency label in the original dependency tree; ner the named entity tag; POS the part-of-speech tag; prefix is the string before the hyphen if word is hyphenated; suffix is the string after the hyphen; brown is the 100-class Brown cluster id with cuts at 4, 6, 10 and 20 2 ; deleted is the lemma of any child node previously deleted by the parser; merged is the lemma of any node merged into this node by a ReplaceHead action; distance is the distance between the tokens in the sentence; path concatenates lemmas and dls between the tokens in the dependency tree; POSpath concatenates POS tags between the tokens; NERpath concatenates NER tags between the tokens. The key differences to <cite>Wang et al. (2015b)</cite> are the inclusion of the brown, POSpath, NERpath, prefix and suffix feature types. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_0",
  "x": "In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) , or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017) . However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution. Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation with semantic coherence (Semeniuta et al., 2018) .",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_1",
  "x": "RNNs enable generating variable-length sequences, conditioning each token on the tokens generated in previous time steps. We leverage this characteristic in our approximation model ( \u00a74.1). A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation. One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution<cite> (Press et al., 2017)</cite> . This keeps the model differentiable and enables end-to-end training through the discriminator.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_2",
  "x": "As a result, LM metrics cannot be applied to evaluate the generated text. Consequently, other metrics have been proposed: \u2022 N-gram overlap:<cite> Press et al., 2017)</cite> : Inspired by BLEU (Papineni et al., 2002) , this measures whether n-grams generated by the model appear in a held-out corpus. A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\"). To mitigate this, self-BLEU has been proposed (Lu et al., 2018) as an additional metric, where overlap is measured between two independently sampled texts from the model.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_3",
  "x": "The output token (one-hot) is denoted by o t . In RNNbased GANs, the previous output token is used at inference time as the input x t Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) . In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input. Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs. Nevertheless, this allows us to compare the quality of text produced by GANs and LMs on an equal footing.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_0",
  "x": "Currently, state of the art uses lightweight neural networks [1, 2,<cite> 3,</cite> 4] , which can perform inference in real-time even on low-end devices [4, 5] . Despite the popularity of voice-enabled products, web applications have yet to make use of keyword spotting. This is surprising, since modern web applications are supported on billions of devices ranging from desktops to smartphones. Also, an in-browser KWS system would be able to perform the aforementioned simple commands recognition and wakeword detection. Thus, we attempt to close the gap between KWS systems and web applications in both research literature and industrial applications, building and evaluating such an in-browser system.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_1",
  "x": "Unfortunately, the browser is a highly inefficient platform for deploying neural networks, mainly due to poorly optimized matrix multiply routines. Fortunately, in recent years, the art of compressing neural networks has made significant advances in both general [6, 7, 8] and keyword spotting literature [4, 9] . On our task, we demonstrate that network slimming [6] is a simple yet highly effective method to achieve low latency with minimal impact on accuracy. Thus, our main contributions are as follows: first, we develop a novel web application with an in-browser KWS system based on previous state-of-the-art<cite> [3]</cite> models. Second, we provide the first set of comprehensive experimental results for the latency of an in-browser KWS system on a broad range of devices.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_2",
  "x": "Keyword spotting. KWS is the task of detecting a spoken phrase in audio, applicable to simple command recognition <cite>[3,</cite> 10] and wake-word detection [2, 1] . A typical requirement is that such a KWS system must be small-footprint at inference time, since the target platforms are mobile phones, Internet-of-things (IoT) devices, and other portable electronics. To achieve this goal, resource-efficient architectures using convolutional neural networks (CNNs) <cite>[3,</cite> 1] and recurrent neural networks (RNNs) [2] have been proposed, while other works make use of low-bitwidth weights [4, 9] . However, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of JavaScript-based deep learning toolkits, implementing on-device KWS systems in web applications has never been done before.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_3",
  "x": "A typical requirement is that such a KWS system must be small-footprint at inference time, since the target platforms are mobile phones, Internet-of-things (IoT) devices, and other portable electronics. To achieve this goal, resource-efficient architectures using convolutional neural networks (CNNs) <cite>[3,</cite> 1] and recurrent neural networks (RNNs) [2] have been proposed, while other works make use of low-bitwidth weights [4, 9] . However, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of JavaScript-based deep learning toolkits, implementing on-device KWS systems in web applications has never been done before. Compressing neural networks. Sparse matrix storage leads to inefficient computation and storage in general-purpose hardware; thus, inducing structured sparsity in neural networks, e.g., on entire rows and columns, has been the cornerstone of various compression techniques [6, 8] .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_4",
  "x": "**DATA AND IMPLEMENTATION** For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases. To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format.",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_5",
  "x": "**DATA AND IMPLEMENTATION** For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases. To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format.",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_6",
  "x": "As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format. We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively <cite>[3,</cite> 10] . ---------------------------------- **INPUT PREPROCESSING** First, for dataset augmentation, the input is randomly mixed with additive noise from the background noise set [10] -this helps to decrease the generalization error [12] and improve the robustness of the model under noisy conditions.",
  "y": "similarities"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_7",
  "x": "We use the res8 and res8-narrow architectures from Tang and Lin<cite> [3]</cite> as a starting point, which represent prior state of the art in residual CNNs [13] for KWS. In both models, given the input X \u2208 R 101\u00d740 , we first expand the input channel-wise by applying a 2D convolution layer with weights W \u2208 R Cout\u00d71\u00d7(3\u00d73) and padding of one on all sides. This step results in an output ofX \u2208 R Cout\u00d7101\u00d740 , which we then downsample using an average pooling layer with a kernel size of (4, 3). Next, inspired by insights in image classification [13] , the output is passed through a series of three residual blocks comprising convolution and batch normalization [11] layers- Figure 2 illustrates one such block. Finally, we average-pool across the channels and pass the features through a softmax layer across the twelve classes.",
  "y": "similarities uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_1",
  "x": "For example, we don't replace entity names (like genes) with shorter alternatives as is done with the noun phrases in the present version and with the gene names in our earlier version <cite>11</cite> . We also avoided hard-coding the words in the rules created to split sentences with relative clauses. These measures enhance the domain adaptability of the system. Table  2 are preserved from the original tree, {C1\u2026Cq} are removed from the original tree and added as a separate tree, and {D1\u2026Dr} are removed from the original tree. To make sure that the rules are optimized for biomedical sentences, we manually examined each sentence in GENIA biomedical corpus and designed rules that would create the largest possible \"bag of simplified sentences\" based on Halliday's formalism for sentence simplification.",
  "y": "differences"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_2",
  "x": "PIE returns two kinds of results -one with a high precision, which we call tight PIE; and the other with low precision, which we call light PIE. We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_3",
  "x": "For the purpose of evaluating the impact of sentence simplification, we use AIMed 17 corpus (which is extensively used in comparing PPI extraction methods) and PIE 18 (a machine-learning based approach available as a web service that uses the parse tree information from the Collins statistical parser as its key component). PIE returns two kinds of results -one with a high precision, which we call tight PIE; and the other with low precision, which we call light PIE. We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability.",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_0",
  "x": "Recent papers <cite>[3]</cite> , [4] in neural machine translation have proposed the strict use of attention mechanisms in networks such as the Transformer over previous approaches such as recurrent neural networks (RNNs) [5] and convolutional neural networks (CNNs) [6] . In other words, these approaches dispense with recurrences and convolutions entirely. In practice, attention mechanisms have mostly been used with recurrent architectures because removing the recurrent nature of the architecture makes the training more efficient by the removal of necessary sequential steps. This paper contributes by continuing to pursue the removal of sequential operations within encoder-decoder models. These operations are removed through the parallelization of previously stacked encoder layers.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_1",
  "x": "Google's earlier and path-breaking endto-end translation approach [9] uses 16 LSTM layers with attention; once again, ensembling produces the best results. Facebook's end-to-end translation approach [10] depends entirely on CNNs with attention mechanism. Our work reported in this paper is based on another translation work by Google. Google's Vaswani et al. <cite>[3]</cite> proposed the reduction in the sequential steps seen in CNNs and RNNs. The sole use of attention mechanisms and feed-forward networks within the common encoder-decoder sequential model replaces the necessity of deep convolutions for distant dependent relationships, and the memory and computation intensive operations required within recurrent networks.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_2",
  "x": "**III. ARCHITECTURE** The Transformer architectures proposed by Vaswani et al. <cite>[3]</cite> , seen in Figure 1 , inspires this paper's work. We have made modifications to this architecture, to make it more efficient. However, our modifications can be applied to any encoder-decoder based model and is architecture-agnostic. These alterations follow from the following two hypotheses.",
  "y": "similarities"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_3",
  "x": "Each of these layers contains two main sub-layers including multi-head self attention, which feeds a simple feed-forward network, and a final layer of normalization. Around each of the main sub-layers, a skip or residual connection [13] is also used. This same structure is used in the decoder with an attention mask to avoid attending to subsequent positions. The attention mechanism used by Vaswani et al. <cite>[3]</cite> can be thought of as a function that maps a query and set of keyvalue pairs to an output. The query, keys, values and output are all vectors.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_4",
  "x": "**A. PARALLEL ENCODING BRANCHES** A motivation for creating the Transformer model was the sluggish training and generation times of other common sequence-to-sequence models such as RNNs and CNNs <cite>[3]</cite> . This was done by simplifying and limiting sequential operations and computational requirements while also increasing the model's ability to exploit current hardware architecture. This paper proposes that removal of the previously stacked branches of the encoder (there is a stack of N encoder and other blocks on the left side of Figure 1 ), parallelizing these separate encoder 'trees', and incorporating their learned results for the decoder, will further eliminate sequential steps and accelerate learning within current sequence-to-sequence models. The architectures discussed are modeled in Figure 2 .",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_5",
  "x": "All proposed architectures including the base Transformer model <cite>[3]</cite> are trained over the International Workshop on Spoken Language Translation (IWSLT) 2016 corpus and tested similarly over the IWSLT 2014 test corpus [15] . The training corpus includes over 200,000 parallel sentence pairs, and 4 million tokens for each language. The testing set contains 1,250 sentences, and 20-30 thousand tokens for French and German. This paper also performed experiments over the larger WMT data set including 4.5 and 36 million training sentence pairs for the EN-DE and EN-FR tasks respectively. The testing set for these experiments was the standard Newstest 2014 test set including around 3000 sentence pairs for each language task.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_6",
  "x": "---------------------------------- **B. MACHINE TRANSLATION** On the much larger WMT English-German test set, all our models achieve better results then Vaswani et al. <cite>[3]</cite> . Our model with five parallel encoding branches has a BLEU score of 62.69 compared to 60.95 and 61.00 for the two Transformers shown in Table III . Our approach also takes considerably less time than the large Transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller Transformer model reported by Vaswani et al. <cite>[3]</cite> .",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_7",
  "x": "---------------------------------- **B. MACHINE TRANSLATION** On the much larger WMT English-German test set, all our models achieve better results then Vaswani et al. <cite>[3]</cite> . Our model with five parallel encoding branches has a BLEU score of 62.69 compared to 60.95 and 61.00 for the two Transformers shown in Table III . Our approach also takes considerably less time than the large Transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller Transformer model reported by Vaswani et al. <cite>[3]</cite> .",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_0",
  "x": "gaining momentum. The PeerRead dataset<cite> (Kang et al., 2018)</cite> is an excellent resource towards research and study on this very impactful and crucial problem. With our ongoing effort towards the development of an Artificial Intelligence (AI)-assisted peer review system, we are intrigued with: What if there is an additional AI reviewer which predicts decisions by learning the high-level interplay between the review texts and the papers? How would the sentiment embedded within the review texts empower such decision-making? Although editors/program chairs usually go by the majority of the reviewer recommendations, they still need to go through all the review texts corresponding to all the submissions. A good use case of this research would be: slot-filling the missing reviewer, providing an additional perspective to the editor in cases of contrasting/borderline reviews.",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_1",
  "x": "Recently we (Ghosal et al., 2018b,a) investigated the impact of various features in the editorial pre-screening process. Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts. We carry our current investigations on a portion of the recently released PeerRead dataset<cite> (Kang et al., 2018)</cite> . Study towards automated support for peer review was otherwise not possible due to the lack of rejected paper instances and corresponding reviews. Our approach achieves significant performance improvement over the two tasks defined in<cite> Kang et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_2",
  "x": "Our approach achieves significant performance improvement over the two tasks defined in<cite> Kang et al. (2018)</cite> . We attribute this to the use of deep neural networks and augmentation of review sentiment information in our architecture. ---------------------------------- **DATA DESCRIPTION AND ANALYSIS** The PeerRead dataset consists of papers, a set of associated peer reviews, and corresponding accept/reject decisions with aspect specific scores of papers collected from several top-tier Artificial Intelligence (AI), Natural Language Processing (NLP) and Machine Learning (ML) conferences.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_3",
  "x": "**DATA DESCRIPTION AND ANALYSIS** The PeerRead dataset consists of papers, a set of associated peer reviews, and corresponding accept/reject decisions with aspect specific scores of papers collected from several top-tier Artificial Intelligence (AI), Natural Language Processing (NLP) and Machine Learning (ML) conferences. Table 1 shows the data we consider in our experiments. We could not consider NIPS and arXiv portions of PeerRead due to the lack of aspect scores and reviews, respectively. For more details on the dataset creation and the task, we request the readers to refer to<cite> Kang et al. (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_4",
  "x": "For more details on the dataset creation and the task, we request the readers to refer to<cite> Kang et al. (2018)</cite> . We further use the submissions of ICLR 2018, corresponding reviews and aspect scores to boost our training set for the decision prediction task. One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in<cite> Kang et al. (2018)</cite> . However, from the heatmap in Figure 1 we can see that the reviewer's sentiments (compound/positive) embedded within the review texts have visible correlations with the aspects like Recommendation, Appropriateness and Overall Decision. This also seconds our recent finding that determining the scope or appropriateness of an article to a venue is the first essential step in peer review (Ghosal et al., 2018a) .",
  "y": "motivation"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_5",
  "x": "The reason we extract features from both is to simulate the editorial workflow, wherein ideally, the editor/chair would look at both into the paper and the corresponding reviews to arrive at a judgement. ---------------------------------- **MULTI-LAYER PERCEPTRON** We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error. CNN variant as in<cite> (Kang et al., 2018</cite> ) is used as the comparing system.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_6",
  "x": "We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error. CNN variant as in<cite> (Kang et al., 2018</cite> ) is used as the comparing system. representation as where \u03b8 predict represents the parameters of the MLP Predict. We also extract features from the review sentiment representation x rs via another MLP (MLP Senti).",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_7",
  "x": "**EXPERIMENTAL SETUP** As we mention earlier, we undertake two tasks: Task 1: Predicting the overall recommendation score (Regression) and Task 2: Predicting the Accept/Reject Decision (Classification). To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_8",
  "x": "To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set. For Task 1, n 1 is 666 and n 2 is 98 while for Task 2, n 1 is 1494 and n 2 is 525. We employ a grid search for hyperparameter optimization.",
  "y": "differences extends"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_9",
  "x": "We employ a grid search for hyperparameter optimization. For Task 1, F is 256, l is 5. ReLU is the non-linear function g(), learning rate is 0.007. We train the model with SGD optimizer, set momentum as 0.9<cite> (Kang et al., 2018</cite> ) is feature-based and considers only paper, and not the reviews. and batch size as 32. We keep dropout at 0.5.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_10",
  "x": "Table 2 and Table 3 show our results for both the tasks. We propose a simple but effective architecture in this work since our primary intent is to establish that a sentiment-aware deep architecture would better suit these two problems. For Task 1, we can see that our review sentiment augmented approach outperforms the baselines and the comparing systems by a wide margin (\u223c 29% reduction in error) on the ICLR 2017 dataset. With only using review+sentiment information, we are still able to outperform<cite> Kang et al. (2018)</cite> by a margin of 11% in terms of RMSE. A further relative error reduction of 19% with the addition of paper features strongly suggests that only review is not sufficient for the final recommendation.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_11",
  "x": "For Task 2, we observe that the handcrafted feature-based system by<cite> Kang et al. (2018)</cite> performs inferior compared to the baselines. This is because the features were very naive and did not 4 https://github.com/aritzzz/DeepSentiPeer address the complexity involved in such a task. We perform better with a relative improvement of 28% in terms of accuracy, and also our system is end-toend trained. Presumably, to some extent, our deep neural network learned to distinguish between the probable accept versus probable reject by extracting useful information from the paper and review data. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_12",
  "x": "Here we observe a relative error reduction of 4.8% and 14.5% over the comparing system for ACL 2017 and CoNLL 2016, respectively (Table 2 ). For the decision prediction task, the comparing system performs even worse, and we outperform them by a considerable margin of 28% (ACL 2017) and 26% (CoNLL 2017), respectively ( Table 3 ). The reason is that the work reported in<cite> Kang et al. (2018)</cite> relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture. However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in<cite> Kang et al. (2018)</cite> for ACL 2017. This again seconds that inclusion of paper is vital in recommendation decisions.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_13",
  "x": "However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in<cite> Kang et al. (2018)</cite> for ACL 2017. This again seconds that inclusion of paper is vital in recommendation decisions. Only paper is enough for a human reviewer, but with the current state of AI, an AI reviewer would need the supervision of her human counterparts to arrive at a recommendation. So our system is suited to cases where the editor needs an additional judgment regarding a submission (such as dealing with missing/non-responding reviewers, an added layer of confidence with an AI which is aware of the past acceptances/rejections of a specific venue). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_0",
  "x": "First, they can model the argument of an implicit discourse relation as dense vectors and suffer less from the data sparsity problem that is typical of the traditional feature engineering paradigm. Second, they should be easily extended to other languages as they do not require human-annotated lexicons. However, despite the many nice properties of neural network models, it is not clear how well they will fare with a small dataset, typicalley found in discourse annotation projects. Moreover, it is not straightforward to construct a single vector that properly represents the \"semantics\" of the ar-guments. As a result, neural network models that use dense vectors have been shown to have inferior performance against traditional systems that use manually crafted features, unless the dense vectors are combined with the hand-crafted surface features <cite>(Ji and Eisenstein, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_1",
  "x": "---------------------------------- **RELATED WORK** The prevailing approach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009) , reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes. Along the same vein, Brown cluster assignments have also been used as a general purpose lexicon that requires no human manual annotation (Rutherford and Xue, 2014) . However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_2",
  "x": "For a fair comparison with the sequential model, we apply the same formulation of LSTM on the binarized constituent parse tree. The hidden state vector now corresponds to a constituent in the tree. These hidden state vectors are then used in the same fashion as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015) . This model is similar to the recursive neural networks proposed by<cite> Ji and Eisenstein (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_4",
  "x": "The annotation is done as another layer on the Penn Treebank on Wall Street Journal sections. Each relation consists of two spans of text that are minimally required to infer the relation, and the sense is organized hierarchically. The classification problem can be formulated in various ways based on the hierarchy. Previous work in this task has been done over three schemes of evaluation: top-level 4-way classification (Pitler et al., 2009 ), second-level 11-way classification (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> , and modified second-level classification introduced in the CoNLL 2015 Shared Task . We focus on the second-level 11-way classification because the labels are fine-grained enough to be useful for downstream tasks and also because the strongest neural network systems are tuned to this formulation.",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_5",
  "x": "---------------------------------- **EXPERIMENT ON THE SECOND-LEVEL SENSE IN THE PDTB** We want to test the effectiveness of the interargument interaction and the three models described above on the fine-grained discourse relations in English. The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> . Preprocessing All tokenization is taken from the gold standard tokenization in the PTB (Marcus et al., 1993) .",
  "y": "similarities uses"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_6",
  "x": "**EXPERIMENT ON THE SECOND-LEVEL SENSE IN THE PDTB** We want to test the effectiveness of the interargument interaction and the three models described above on the fine-grained discourse relations in English. The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> . Preprocessing All tokenization is taken from the gold standard tokenization in the PTB (Marcus et al., 1993) . We use the Berkeley parser to parse all of the data (Petrov et al., 2006 too little data, 50-dimensional WSJ-trained word vectors have previously been shown to be the most effective in this task <cite>(Ji and Eisenstein, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_7",
  "x": "**RESULTS AND DISCUSSION** The feedforward model performs best overall among all of the neural architectures we explore (Table 2) . It outperforms the recursive neural network with bilinear output layer introduced by<cite> Ji and Eisenstein (2015)</cite> (p < 0.05; bootstrap test) and performs comparably with the surface feature baseline (Lin et al., 2009) , which uses various lexical and syntactic features and extensive feature selection. Tree LSTM achieves inferior accuracy than our best feedforward model. The Figure 3: Inter-argument interaction can be modeled effectively with hidden layers.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_9",
  "x": "Although this first comes as a surprise to us, the results are consistent with recent works that use sequential LSTM to encode syntactic information. For example, Vinyals et al. (2015) use sequential LSTM to encode the features for syntactic parse output. Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015) . Furthermore, the benefits of tree LSTM are not readily apparent for a model that discards the syntactic categories in the intermediate nodes and makes no distinction between heads and their dependents, which are at the core of syntactic representations. Another point of contrast between our work and<cite> Ji and Eisenstein's (2015)</cite> is the modeling choice for inter-argument interaction.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_10",
  "x": "Secondly, using linear interaction allows us to use high dimensional word vectors, which we found to be another important component for the performance. The recursive model by<cite> Ji and Eisenstein (2015)</cite> is limited to 50 units due to the bilinear layer. Our choice of linear interargument interaction and high-dimensional word vectors turns out to be crucial to building a competitive neural network model for classifying implicit discourse relations. 6 Extending the results across label sets and languages Do our feedforward models perform well without surface features across different label sets and languages as well? We want to extend our results to another label set and language by evaluating our models on non-explicit discourse relation data used in English and Chinese CoNLL 2016 Shared Task.",
  "y": "differences"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_0",
  "x": "The research community in artificial intelligence (AI) has witnessed a series of dramatic advances in the AI tasks concerning language and vision in recent years, thanks to the successful applications of deep learning techniques, particularly convolutional neural networks (CNN) and recurrent neural networks (RNN). AI has moved on from naming the entities in the image (Mei et al. 2008; Wang et al. 2009) , to describing the image with a natural sentence (Vinyals et al. 2015; Xu et al. 2015; Karpathy and Li 2015) and then to answering specific questions about the image with the advent of visual question answering (VQA) task <cite>(Antol et al. 2015)</cite> . However, current VQA task is focused on generating a short answer, mostly single words, which does not fully take advantage of the wide range of expressibility inherent in human natural language. Just as we moved from merely naming entities in the image to description of the images with natural sentence, it naturally follows that VQA will also move towards full-sentence answers. One way to tackle this issue would be to apply appropriate linguistic rules after a single-word answer is generated.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_1",
  "x": "**RELATED WORK** A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_2",
  "x": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category.",
  "y": "background motivation"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_3",
  "x": "However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category. (Saito et al. 2016) proposed DualNet, in which both addition and multiplication of the input features are performed, in order to fully take advantage of the discriminative features in the data.",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_4",
  "x": "Collecting full-sentence annotations from crowd-sourcing tools can be highly costly. We circumvent this financial cost by converting the answers in the original VQA dataset <cite>(Antol et al. 2015)</cite> to full-sentence answers by applying a number of linguistic rules using natural language processing techniques. Furthermore, we also provide an augmented version of dataset by converting the human-written captions provided in the MS COCO (Lin et al. 2014) . We generated questions with a set of rules, for which the caption itself becomes the answer. Both versions of FSVQA dataset along with the features used in our experiment, as will be described in the Experiment Section, are publicly available for download.",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_5",
  "x": "Batch size was 500 and training was performed for 300 epochs. We trained only with the answers that appear twice or more in train split, as using all unique answers in the dataset fails to run, with required memory far beyond the capacity of most of the contemporary GPUs, NVIDIA Tesla 40m in our case. 20,130 answers appear more than once in regular version, covering 95,340 questions from 62,292 images, and 23,400 answers appear more than once in the augmented version, which cover 105,563 questions from 64,060 images. This is only about 25% and 15.5% of the entire train split in respective version, which again shows a striking contrast with the original VQA dataset, in which only 1,000 answers covered up to 86.5% of the dataset. Following <cite>(Antol et al. 2015)</cite> , we examined the effect of Conversely, we also examined an approach where only image features are concerned.",
  "y": "uses"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_6",
  "x": "It is notable that using image features alone performs very poorly, whereas using question features alone results in performances comparable to using both features. One plausible explanation is that, since using image features alone always generates the same answer for the same image regardless of the question, it can only get 1 out of k questions correctly at best, where k is the number of questions per image. On the contrary, using question features alone essentially reduces the problem to a semantic Q&A task, which can be handled one at a time. This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues.",
  "y": "similarities"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_7",
  "x": "It is notable that using image features alone performs very poorly, whereas using question features alone results in performances comparable to using both features. One plausible explanation is that, since using image features alone always generates the same answer for the same image regardless of the question, it can only get 1 out of k questions correctly at best, where k is the number of questions per image. On the contrary, using question features alone essentially reduces the problem to a semantic Q&A task, which can be handled one at a time. This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues.",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_0",
  "x": "Modern Machine Translation (MT) systems perform consistently well on clean, in-domain text. However human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation. In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset<cite> (Michel and Neubig, 2018)</cite> to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_1",
  "x": "Machine Translation (MT) systems have been shown to exhibit severely degraded performance when presented with translation of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017) . This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance<cite> (Michel and Neubig, 2018)</cite> . Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has further demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data through two primary research questions: * These authors contributed equally 1.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_2",
  "x": "We approach the problem of adapting to noisy data through two primary research questions: * These authors contributed equally 1. Can we artificially synthesize the types of noise common to social media text in otherwise clean data? 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013;<cite> Michel and Neubig, 2018)</cite> as naturally occurring in internet and social media based text. We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <cite>(Michel and Neubig, 2018</cite> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_3",
  "x": "We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <cite>(Michel and Neubig, 2018</cite> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. The primary contributions of this work are our Synthetic Noise Induction model which specifically introduces types of noise unique to social media text and the introduction of back translation (Sennrich et al., 2015a) as a means of emulating target noise. Szegedy et al. (2013) demonstrate the fragility of neural networks to noisy input. This fragility has been shown to extend to MT systems (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018) where both artificial and natural noise are shown to negatively affect performance. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_4",
  "x": "Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT<cite> (Michel and Neubig, 2018)</cite> . Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to handle noise from this perspective. Notable approaches include training on varying amounts of data from the target domain (Li et al., 2010; Axelrod et al., 2011) , Luong and Manning (2015) suggest the use of fine-tuning on varying amounts of target domain data, and Barone et al. (2017) note a logarithmic relationship between the amount of data used in fine-tuning and the relative success of MT models. Other approaches to domain adaptation include weighting of domains in the system objective function (Wang et al., 2017) and specifically curated datasets for adaptation (Blodgett et al., 2017) . Kobus et al. (2016) introduce a method of domain tagging to assist neural models in differentiating domains.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_5",
  "x": "Our baseline MT model architecture consists of a bidirectional Long Short-Term Memory (LSTM) network encoder-decoder model with two layers. The hidden and embedding sizes are set to 256 and 512, respectively. We also employ weighttying (Press and Wolf, 2016) between the embedding layer and projection layer of the decoder. For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<cite> Michel and Neubig (2018)</cite> , which allows us to provide comparative results across a variety of settings. Other model parameters reflect the implementation outlined in<cite> Michel and Neubig (2018)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_6",
  "x": "Other model parameters reflect the implementation outlined in<cite> Michel and Neubig (2018)</cite> . In all experimental settings we employ Byte-Pair Encoding (BPE) (Sennrich et al., 2015b) using Google's SentencePiece 2 . ---------------------------------- **EXPERIMENTAL APPROACHES** We propose two primary approaches to increasing the resilience of our baseline model to the MTNT data, outlined as follows:",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_7",
  "x": "**SYNTHETIC NOISE INDUCTION (SNI)** For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in<cite> Michel and Neubig (2018)</cite> . For every token we choose to introduce the different types of noise with some probability on both French and English sides in 100k sentences of EP. Specifically, we fix the probabilities of error types as follows: spelling (0.04), profanity (0.007), grammar (0.015) and emoticons (0.002). To simulate spelling error, we randomly add or drop a character in a given word.",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_0",
  "x": "Qualia Structures have been originally introduced by <cite>(Pustejovsky, 1991)</cite> and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996) , co-composition and coercion <cite>(Pustejovsky, 1991)</cite> as well as for bridging reference resolution (Bos et al., 1995) . Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993) . One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004) .",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_1",
  "x": "**QUALIA STRUCTURES** According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (Kronlid, 2003) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework <cite>(Pustejovsky, 1991)</cite> reused Aristotle's basic factors for the description of the meaning of lexical elements. In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in <cite>(Pustejovsky, 1991)</cite> however seem to have a more restricted interpretation.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_2",
  "x": "Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in <cite>(Pustejovsky, 1991)</cite> however seem to have a more restricted interpretation. In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence. The Formal role normally consists in typing information about the object, i.e. its hypernym or superconcept. Finally, the Telic role describes the purpose or function of an object either by a verb or nominal phrase.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_3",
  "x": "In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy. In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon <cite>(Pustejovsky, 1991)</cite> . ---------------------------------- **THE FORMAL ROLE** To derive qualia elements for the Formal role, we first download for each of the clues in Table 1 the first 10 abstracts matching the clue and then process them offline matching the patterns defined over part-of-speech-tags 5 thus yielding up to 10 different qualia element candidates per clue.",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_4",
  "x": "Busa, 1996) or <cite>(Pustejovsky, 1991)</cite> , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining. We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6. The evaluation of our approach consists on the one hand of a discussion of the weighted qualia structures, in particular comparing them to the ideal structures form the literature. On the other hand, we also asked a student at our institute to assign credits to each of the qualia elements from 0 (incorrect) to 3 (totally correct) whereby 1 credit meaning 'not totally wrong' and 2 meaning 'still acceptable'. ----------------------------------",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_5",
  "x": "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase.",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_6",
  "x": "**QUALITATIVE EVALUATION & DISCUSSION** In this section we provide a more subjective evaluation of the automatically learned qualia structures by comparing them to ideal qualia structures discussed in the literature wherever possible. In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_7",
  "x": "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of <cite>(Pustejovsky, 1991)</cite> as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought.",
  "y": "motivation"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_8",
  "x": "The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ). Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in <cite>(Pustejovsky, 1991)</cite> , while thing at the 3rd position is certainly too general. Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results. Very interesting are the results concoction and libation for the Formal role of beer, which unfortunately were rated low by our evaluator (compare Figure 3) . Overall, the discussion has shown that the results produced by our method are reasonable when compared to the qualia structures from the literature.",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_0",
  "x": "---------------------------------- **INTRODUCTION** Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs<cite> (Zoph et al., 2016</cite>; Koehn and Knowles, 2017) . A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009) .",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_1",
  "x": "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English. In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of <cite>Zoph et al. (2016)</cite> does not always work, but it is still possible to use the parent model to considerably improve the child model. The basic idea is to exploit the relationship between the parent and child language lexicons. Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents.",
  "y": "motivation uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_2",
  "x": "However, it still lags behind other statistical methods on very lowresource language pairs<cite> (Zoph et al., 2016</cite>; Koehn and Knowles, 2017) . A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009) . However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, <cite>Zoph et al. (2016)</cite> train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.",
  "y": "background"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_3",
  "x": "For the purposes of this paper, the most important detail of the model is that (as in many other models) the word types of both the source and target languages are mapped to vector representations called word embeddings, which are learned automatically with the rest of the model. ---------------------------------- **LANGUAGE TRANSFER** We follow the transfer learning approach proposed by <cite>Zoph et al. (2016)</cite> . In their work, a parent model is first trained on a high-resource language pair.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_4",
  "x": "---------------------------------- **METHOD** The basic idea of our method is to extend the transfer method of <cite>Zoph et al. (2016)</cite> to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding. In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages. Thus, we need to process the data to make these two assumptions hold as much as possible.",
  "y": "uses extends"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_5",
  "x": "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of <cite>Zoph et al. (2016)</cite> ( \u00a72.2): one where the target word embeddings are fine-tuned, and one where they are frozen. ---------------------------------- **RESULTS AND ANALYSIS** Our results are shown in Table 3 . In this lowresource setting, we find that transferring wordbased models does not consistently help.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_6",
  "x": "We surmise that the improvement is primarily due to the vocabulary overlap created by BPE (see Table 4 ). ---------------------------------- **CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_7",
  "x": "We surmise that the improvement is primarily due to the vocabulary overlap created by BPE (see Table 4 ). ---------------------------------- **CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
  "y": "extends"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_0",
  "x": "Their output exhibits a disproportionate replication of common n-grams and full captions seen in the training set [9, 11,<cite> 26]</cite> . Contributing to this problem is a combination of biased datasets and insufficient quality metrics. While the main benchmarking dataset for Image Captioning, MS COCO, makes available over 120k images with 5 human-annotated captions each [6] , the selection process for the images suggests a lack of diversity in both content and composition [11, 20] . Furthermore, the standard benchmarking metrics, based on ngram level overlap between generated captions and ground-truth captions, reward models with a bias towards common n-grams. This leads to the (indirect and unwanted) consequence of incentivizing models that output generic captions that are likely to fit a range of similar images, despite missing the goal of describing the relevant aspects specific to each image.",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_1",
  "x": "**MEASURING CAPTION QUALITY** The subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of Image Captioning models [5,<cite> 26]</cite> . Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> . Moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with CIDEr and METEOR having the highest correlations among them [5] .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_2",
  "x": "The subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of Image Captioning models [5,<cite> 26]</cite> . Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> . Moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with CIDEr and METEOR having the highest correlations among them [5] . With the recognition of these limitations, there has been a growing interest in developing metrics that measure other desirable qualities in captions.",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_3",
  "x": "This, again, encourages the ''safe'' generic captions that we want to move away from. ---------------------------------- **DIVERSITY METRICS** In an effort to measure the amount of generic captions produced by various Image Captioning models, [11] explores the concept of caption diversity. More recently, this concept has been employed as the focus for training and evaluation<cite> [26,</cite> 29] , and it has been proposed that improving caption diversity leads to more human-like captions <cite>[26]</cite> .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_4",
  "x": "More recently, this concept has been employed as the focus for training and evaluation<cite> [26,</cite> 29] , and it has been proposed that improving caption diversity leads to more human-like captions <cite>[26]</cite> . This research direction is still new and lacks clear benchmarks and standardized metrics. We propose the following set of metrics to evaluate the diversity of a model: \u2500 novelty -percentage of generated captions where exact duplicates are not found in the training set [11,<cite> 26,</cite> 29 ] \u2500 diversity -percentage of distinct captions (where duplicates count as a single distinct caption) out of the total number of generated captions [11] \u2500 vocabulary size -number of unique words used in generated captions <cite>[26]</cite> ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_5",
  "x": "In [22] , the training is cooperative rather than competitive; both systems adjust to the other to provide the best joint results. We take a slightly different approach from both the joint training in [22] and recent applications of GAN training in Image Captioning [9,<cite> 26]</cite> . Instead of allowing both systems to learn from each other, we freeze the NLU side and allow only the NLG to learn from the NLU; the NLU model is pre-trained on ground-truth captions, without any input from the NLG. Consequently, we avoid one of the problems observed in [22] where both systems adapt to each other and develop their own protocol of communication which gradually degrades the resemblance to human language. We also avoid the instability in training and difficulty in loss monitoring commonly seen in GANs.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_7",
  "x": "The non-contrastive models are based on single runs. As can be seen in Table 1 , our models demonstrate increased diversity and novelty, outperforming previously reported results. The vocabulary size also increases but is lower than in <cite>[26]</cite> . When it comes to the specificity metrics, our contrastive models have the advantage over our non-contrastive ones. They all improve the overall mean rank, but the latter do not show the increase in smaller k recalls that the contrastive models do.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_8",
  "x": "For completeness, we include the best models from [29] in Table 2 ; however, they only report diversity results on multiple (up to 10) candidates per image (where duplicates of a novel caption are counted as multiple novel captions), so they are not directly comparable to the single-best-caption models. Note that [12, 29] use different data splits, while our models and <cite>[26]</cite> use the Karpathy 5k splits [17] . Table 3 . Standard text metric results for single-best-caption models. All metrics are n-gram based except for SPICE which is based on scene graphs automatically inferred from the captions.",
  "y": "similarities uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_10",
  "x": "baseline: a man riding a snowboard down a snow covered slope DP: a snowboarder doing a trick in the air Cos: a snowboarder doing a trick in the air CDP: a snowboarder is jumping in the air on a snowboard CCos: a snowboarder is jumping in the air on a snowboard HUMAN CAPTIONS a picture of a man in the air on a snowboard a man doing tricks on a snowboard a man riding a snowboard through the air on a ski slope a snowboarder flies into the air under a chair lift a snowboarder does a trick while jumping through the air GENERATED CAPTIONS baseline: a man walking on the beach with a surfboard DP: a person walking on the beach with a surfboard Cos: a person walking on a beach with a surfboard CDP: a person walking on a beach with a dog CCos: a man walking on the beach with a dog HUMAN CAPTIONS a person walking their dog on the beach a man on a beach holding something while walking along it a single person walking the beach with a dog a person walking their dog on the beach a person walking their dog along the shoreline GENERATED CAPTIONS baseline: a man is doing a trick on a skateboard DP: a man doing a trick on a skateboard Cos: a man doing a trick on a skateboard CDP: a man is doing a trick on a wooden structure CCos: a man is doing a trick on a wooden structure HUMAN CAPTIONS a man on a skateboard performing a trick a man flying through the air on top of a skateboard a person on a skateboard in the air at a skate park a male skateboarder skateboards on a wall in an enclosed area a male on a skateboard performing a trick on a halfpipe Another example of GAN training is <cite>[26]</cite> where the Discriminator classifies whether a multi-sample set of captions are human-written or generated. In contrast, our evaluator only requires a single caption and uses a much simpler loss function. Furthermore, we let the NLU remain frozen during training, making the training stable and producing more informative learning curves. A similar approach can be found in [10] where Contrastive Learning is used in a GAN-like setting.",
  "y": "differences"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_0",
  "x": "To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a vector space using word embeddings [MSC + 13] . This idea is motivated by the recent success of word embeddings-based methods to learn syntactic and semantic structures automatically when provided with large datasets.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_1",
  "x": "To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a vector space using word embeddings [MSC + 13] . This idea is motivated by the recent success of word embeddings-based methods to learn syntactic and semantic structures automatically when provided with large datasets.",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_2",
  "x": "In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising F 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_3",
  "x": "In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising F 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings.",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_4",
  "x": "Recent improvements to Skip-gram model make it better able to handle less frequent words, especially when negative sampling is used [MSC + 13]. ---------------------------------- **FEATURES CONSIDERED** Gang member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets/profile text may carry acronyms which can only be deciphered by others involved in gang culture <cite>[BWDS16]</cite> . These gang-related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_5",
  "x": "Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible. Instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to YouTube videos reflecting their music preferences and affinity. In this section, we briefly discuss the feature types and their broad differences in gang and non-gang member profiles. An in-depth explanation of these feature selection can be found in <cite>[BWDS16]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_6",
  "x": "In this section, we briefly discuss the feature types and their broad differences in gang and non-gang member profiles. An in-depth explanation of these feature selection can be found in <cite>[BWDS16]</cite> . ---------------------------------- **TWEET TEXT:** In our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on Twitter <cite>[BWDS16]</cite> .",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_7",
  "x": "Further, we found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre <cite>[BWDS16]</cite> . Moreover, we found that eight YouTube links are shared on average by a gang member. The top 5 terms used in YouTube videos shared by gang members were shit, like, nigga, fuck, and lil while like, love, peopl, song, and get were the top 5 terms in nongang members' video data. Figure 1 gives an overview of the steps to learn word embeddings and to integrate them into a classifier. We first convert any non-textual features such as emoji and profile images into textual features.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_8",
  "x": "Then we discuss the 10-fold cross validation experiments and the evaluation matrices used. Finally we present the results of the experiments. ---------------------------------- **EVALUATION SETUP** We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work <cite>[BWDS16]</cite> .",
  "y": "uses"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_9",
  "x": "The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in <cite>[BWDS16]</cite> . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every YouTube video shared by them. Table 1 provides statistics about the number of words found in each type of feature in the dataset.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_10",
  "x": "To build the classifiers we used three different learning algorithms, namely Logistic Regression (LR), Random Forest (RF), and Support Vector Machines 4 https://moz.com/followerwonk/bio (SVM) fiers. An open source tool of Python, Gensim [\u0158S10] was used to generate the word embeddings. We compare our results with the two best performing systems reported in <cite>[BWDS16]</cite> which are the two state-of-theart models for identifying gang members in Twitter. Both baseline models are built from a random forest classifier trained over term frequencies for unigrams in tweet text, emoji, profile data, YouTube video data and image tags.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_0",
  "x": "These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks.",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_1",
  "x": "Separately, Gu et al. (2018) and Lee et al. (2018) developed non-autoregressive approaches to machine translation where the entire output can be generated in parallel in constant time. These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_2",
  "x": "These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks.",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_3",
  "x": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation.",
  "y": "background similarities"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_4",
  "x": "In the classical setting, output sequences are produced by repeatedly appending tokens to the rightmost end of the hypothesis until an endof-sequence token is generated. Though highperforming across a wide range of application areas, this approach lacks the flexibility to accommodate other types of inference such as parallel generation, constrained decoding, infilling, etc. Moreover, it also leaves open the possibility that a non-left-to-right factorization of the joint distribution over output sequences could outperform the usual monotonic ordering. To address these concerns, several recent approaches have been proposed for insertion-based sequence modeling, in which sequences are con-structed by repeatedly inserting tokens at arbitrary locations in the output rather than only at the right-most position. We use one such insertion-based model, the Insertion Transformer <cite>(Stern et al., 2019)</cite> , for our empirical study.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_5",
  "x": "Second, the Insertion Transformer pads the lengthn decoder input on both ends so that n + 2 output vectors are produced. It then concatenates adjacent pairs of output vectors to obtain n + 1 slot representations, which in turn inform the conditional distributions over tokens within each slot, p(c | l). Lastly, it performs an additional attention step over the slot representations to obtain a location distribution p(l), which is multiplied with the conditional content distributions to obtain the full joint distribution: p(c, l) = p(c | l)p(l). A schematic of the architecture is given in Figure 1 for reference. We note that<cite> Stern et al. (2019)</cite> also experimented with a number of other architectural variants, but we use the baseline version of the model described above in our experiments for simplicity.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_6",
  "x": "Here, A * is the set of all valid actions. The temperature \u03c4 \u2208 (0, \u221e) controls the sharpness of the distribution, where \u03c4 \u2192 0 results in a one-hot distribution with all mass on the best-scoring action under the order function O(a), and \u03c4 \u2192 \u221e results in a uniform distribution over all valid actions. Intermediate values of \u03c4 result in distributions which are biased towards better-scoring actions but allow for other valid actions to be taken some of the time. Having defined the target distribution, we take the slot loss L for insertions within a particular slot to be the KL-divergence between the oracle distribution q oracle and the model distribution p. Substituting L in for the slot loss within the training framework of<cite> Stern et al. (2019)</cite> then gives the full sequence generation loss, which we can use to train an Insertion Transformer under any oracle policy rather than just the specific one they propose. We describe a wide variety of generation orders which can be characterized by different order functions O(a) in the subsections that follow.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_7",
  "x": "Conversely, the \"hard\" variant uses O(a) = \u2212 log p(a) which encourages the model to place probability mass on what it thinks are the hardest valid actions. This is akin to a negative feedback system whose stationary condition is the uniform distribution. ---------------------------------- **ROLL-IN POLICY** We follow<cite> Stern et al. (2019)</cite> and use a uniform roll-in policy when sampling partial outputs at training time in which we first select a subset size uniformly at random, then select a random subset of the output of that size.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_8",
  "x": "Parallel decode (common-first): out using sacreBLEU 2 (Post, 2018) . In both cases, we train all models for 1M steps using sequencelevel knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016 ) from a base Transformer (Vaswani et al., 2017) . We perform a sweep over temperatures \u03c4 \u2208 {0.5, 1, 2} and EOS penalties \u2208 {0, 0.5, 1, 1.5, . . . , 8} <cite>(Stern et al., 2019)</cite> ---------------------------------- **ABILITY TO LEARN DIFFERENT ORDERS**",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_9",
  "x": "**TEST RESULTS** Next, we measure the quality of our models by evaluating their performance on their respective test sets. The BLEU scores are reported in Table 3 . The uniform loss proposed by<cite> Stern et al. (2019)</cite> serves as a strong baseline for both language pairs, coming within 0.6 points of the original Transformer for En-De at 26.72 BLEU, and attaining a respectable score of 33.1 BLEU on En-Zh. We note that there is a slightly larger gap between the normal Transformer and the Insertion Transformer for the latter of 2.7 points, which we hypothesize is a result of the larger discrepancy between word orders in the two languages combined with the more difficult nature of the Insertion Transformer training objective.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_10",
  "x": "On the other hand, we note that while the soft left-to-right and right-to-left losses perform substantially better than the hard loss employed in the original work by<cite> Stern et al. (2019)</cite> , performance does suffer when using parallel decoding for those models, which is generally untrue of the other orderings. We believe this is due in part to exposure bias issues arising from the monotonic ordering as compared with the uniform roll-in policy that are not shared by the other losses. ---------------------------------- **PERFORMANCE VS. SENTENCE LENGTH** For additional analysis, we consider how well our models perform relative to one another conditional on the length of the source sentence.",
  "y": "differences"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_0",
  "x": "In this case topic modelling can be applied to transcribed text to extract the key issues and emerging topics of discussion. In this study we propose a method for simulating various types of transcription errors. We then test the robustness of a popular topic modelling algorithm, Latent Dirichlet Allocation (LDA) using a topic stability measure introduced by <cite>Greene et al. (2014)</cite> over a variety of corpora. ---------------------------------- **TOPIC MODELLING AND METRICS**",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_1",
  "x": "---------------------------------- **TOPIC MODELLING AND METRICS** Blei et al. (Blei et al., 2003) introduced Latent Dirichlet Allocation (LDA) as a generative probabilistic model for text corpora. LDA regulates the probabilistic distributions between document, topic and word and it is an unsupervised learning model. For the evaluation of topic models, we follow the approach by <cite>Greene et al. (2014)</cite> for measuring topic model agreement .",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_2",
  "x": "(Greene et al., 2014) ---------------------------------- **DATASETS** In this paper, we explore two datasets bbc and wikilow<cite> (Greene et al., 2014)</cite> with different document size and corpus size. The bbc corpus includes general BBC news articles.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_3",
  "x": "With high level systematic errors, topic models with 3 times the correct number of topics are most robust. In some corpora, redundant number of topics helps the LDA model through severe systematic errors (Figure 1(b) ). This complements previous work by <cite>Greene et al. (2014)</cite> who investigated how topic stability is influenced by number of topics over noisefree corpora. This suggests that transcribers should perhaps consider omitting words when the uncertainty is high. The topic model is less influenced with a random missing term than an erroneous replacement.",
  "y": "similarities"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_0",
  "x": "Linguistic adaptation is twofold. In one part, collective voice unifies opinions and decisions in a complex process, ideas are biased, and consequently people start acting similarly, talking similarly, and so writing similarly. Twitter conversations<cite> (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013 ) and popular memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> prove this similarity in social media. In the other part, when people have a well-defined goal at the end, they tend to reshape their arguments. In the presence of distinct winning and losing sides and social hierarchy, people at lower status show both cooperation through that at the higher status and competition among each other.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_1",
  "x": "We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge, e.g., commonsense, and so richness of opinions kept in each individual is relatively unimportant. We can further argue that commonsense drives an adaptation in extracting knowledge. To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> . In this paper, our main concerns are firstly to construct discussion groups including agents having different social powers and serving opposite aims. Secondly, we investigate how we can track the progress of opinions together with their influences on decisions in oral conversations.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_2",
  "x": "Complex knowledge extraction process in micro state suddenly becomes less valuable and group decision gains (Conover et al. 2011) . We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge, e.g., commonsense, and so richness of opinions kept in each individual is relatively unimportant. We can further argue that commonsense drives an adaptation in extracting knowledge. To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> . In this paper, our main concerns are firstly to construct discussion groups including agents having different social powers and serving opposite aims.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_3",
  "x": "To analyze adaptation induced by both cooperation and competition, we consider court conversations: they are held in clearly stated winner and loser groups with distinct hierarchy in decisionmaking due to the presence of Justices and lawyers. To this end, we evaluate the open access data of the United States Supreme Court (Hawes, Lin, and Resnik 2009; Hawes 2009; <cite>Danescu-Niculescu-Mizil et al. 2012</cite> ), prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and finally provide a comparison between the groups and the discovered linguistic relations. The rest of the paper is organized as follows: the first section presents the dataset we consider and designed conversation groups out of the data; the second section describes our algorithm in detail; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic relations; finally, we provide experimental results and conclude the paper. ---------------------------------- **SUPREME COURT DATA**",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_4",
  "x": "**SUPREME COURT DATA** We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices. Both the original data and the most updated version used here are publicly available<cite> (Danescu-NiculescuMizil et al. 2012)</cite> . The data gathers oral speeches before the Supreme Court and hosts 50,389 conversational exchanges among Justices and lawyers. Distinct hierarchy between Justices (high power) and lawyers (low power) impose lawyers to tune their arguments under the perspective and understandings of Justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_5",
  "x": "The data gathers oral speeches before the Supreme Court and hosts 50,389 conversational exchanges among Justices and lawyers. Distinct hierarchy between Justices (high power) and lawyers (low power) impose lawyers to tune their arguments under the perspective and understandings of Justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns. Tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech. Moreover, lawyers show even more cooperation to unfavorable Justices than favorable ones.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_6",
  "x": "We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_7",
  "x": "1-6 summarize all potential roles present in the data. In 1-4, who supported by the Justice is given in the middle. Furthermore, the last indicates the side of lawyer the Justice speaks to. Referring exchange theory (Willer 1999; Thye, Willer, and Markovsky 2006) and the measured coordination<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , one can order the relative power of each Justice and lawyer pair where J and l represent Justices and lawyers, respectively (note that for comparing individually following the social exchange theory, P (J) > P (l) for both supportive and unsupported Justices).",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_8",
  "x": "Referring exchange theory (Willer 1999; Thye, Willer, and Markovsky 2006) and the measured coordination<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , one can order the relative power of each Justice and lawyer pair where J and l represent Justices and lawyers, respectively (note that for comparing individually following the social exchange theory, P (J) > P (l) for both supportive and unsupported Justices). The subscript u indicates that Justice doesn't support the side of lawyer and the supportive version is described by s. For instance, in Table 1 , in the communications of 1 and 5; 4 and 6, Justices show supports and play as J s , whereas that of 3 and 5; 2 and 6, lawyers are unsupported by J u . The scenarios and pairs guide to construct groups with different cooperation level induced by P as illustrated in Table 2 . We further add another dimension in the relative power: Winners and Losers, haven't been investigated in the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_9",
  "x": "Unlike the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , entirely tracking back and forth utterances and proving the adaptation, e.g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations already preserve the adaptation and any other complex collective linguistic process induced by both cooperation and competition in different power groups. We expect that the variation in M I(R, \u03ba) of gathered utterances of each relative power group, independent of the utterance order, suggests which relations can distinguish the difference in the groups and the magnitude of M I(R, \u03ba) of that difference highlights which relative power groups drastically influence the applied language. We will analyze M I(R, \u03ba) following this discussed understanding in coming Section. ---------------------------------- **RESULTS**",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_0",
  "x": "The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner. The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible. The proposed system outperforms the baseline keyword spotting model in [<cite>1</cite>] due to increased optimizability. Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_1",
  "x": "Recently introduced end-to-end trainable DNN approaches [<cite>1,</cite> 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, <cite>1</cite>] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_2",
  "x": "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] . Recently introduced end-to-end trainable DNN approaches [<cite>1,</cite> 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, <cite>1</cite>] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_3",
  "x": "In [<cite>1</cite>] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR. Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty. In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy. Prior work of CTC-training [15] or sequence-to-sequence training [16] dont require on frame level alignment information. However those approaches need to train full-sized encoders, which require fully transcribed speech data.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_4",
  "x": "---------------------------------- **SMOOTHED MAX POOLING LOSS FOR TRAINING ENCODER/DECODER KEYWORD SPOTTING MODEL** The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. In [<cite>1</cite>] , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
  "y": "similarities differences uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_5",
  "x": "We conclude with discussions in Section 5. ---------------------------------- **SMOOTHED MAX POOLING LOSS FOR TRAINING ENCODER/DECODER KEYWORD SPOTTING MODEL** The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. In [<cite>1</cite>] , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_6",
  "x": "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the <cite>baseline</cite>. ---------------------------------- **BASELINE END-TO-END KEYWORD SPOTTING MODEL** Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_7",
  "x": "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the <cite>baseline</cite>. ---------------------------------- **BASELINE END-TO-END KEYWORD SPOTTING MODEL** Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_8",
  "x": "Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_9",
  "x": "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_10",
  "x": "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t. In [<cite>1</cite>] , target label sequence consists of intervals of repeated labels which we call runs.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_11",
  "x": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [<cite>1</cite>] . Both the <cite>baseline</cite> and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_12",
  "x": "**EXPERIMENTAL SETUP** We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [<cite>1</cite>] . Both the <cite>baseline</cite> and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_13",
  "x": "Details of the setup are discussed below. ---------------------------------- **FRONT-END** We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details.",
  "y": "similarities uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_15",
  "x": "The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details. ---------------------------------- **MODEL SETUP** We selected E2E 318K architecture in [<cite>1</cite>] as the baseline and use the same structure for testing all other models. As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
  "y": "similarities uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_17",
  "x": "We call the <cite>baseline model</cite> as Baseline CE CE where encoder and decoder submodels are trained with CE loss. We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss. We also performed ablation study by testing other models that use different losses. Table 1 summarizes all the tested models. Model Max1-Max3 uses SMP (smoothed max pooling) loss for the decoder, but uses different losses for the encoder.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_18",
  "x": "CTC loss doesnt need alignments, but it learns peaky activations whose peak values are not highly stable. Max2 NA SMP has no encoder loss (i.e. \u03b1 = 0), s.t. the entire network is trained by decoder loss only. Max3 CE SMP used <cite>baseline</cite> CE loss for encoder. Model Max4-Max7 are tested to measure the importance of the smoothing operation.",
  "y": "uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_19",
  "x": "**DATASET** The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google. Data augmentation similar to [<cite>1</cite>] has been used for better robustness. Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition. Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_21",
  "x": "Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set). Fig.3 shows the ROC curves of various models (<cite>baseline</cite>, Max1-Max4) across different conditions. Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_22",
  "x": "Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite> Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_23",
  "x": "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ---------------------------------- **CONCLUSION** We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_24",
  "x": "Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ---------------------------------- **CONCLUSION** We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_25",
  "x": "---------------------------------- **CONCLUSION** We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the <cite>baseline</cite>.",
  "y": "differences"
 },
 {
  "id": "e75e14ff2812f34ff456eb472a36d2_0",
  "x": "The presentation of this tutorial is arranged into five parts. First of all, we share the current status of researches on natural language understanding, statistical modeling and deep neural network and explain the key issues in deep Bayesian learning for discrete-valued observation data and latent semantics. A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking. Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; Gehring et al., 2017) , CNN (<cite>Kalchbrenner et al., 2014</cite>; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_0",
  "x": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_1",
  "x": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_2",
  "x": "Distributed representations of multimodal embeddings (Feng & Lapata, 2010) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_3",
  "x": "In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and<cite> Lazaridou et al. (2015)</cite> . Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & Robertson (2000) by incorpo-rating the word-level visual modality directly into the sentence context. This model represents an advancement of the existing literature surrounding multimodal skip-gram, as well as multimodal distributional semantic models in general, by greatly simplifying the method of situating the words in the visual context and reducing the number of hyperparameters to tune by directly incorporating multimodal words into the existing objective function and hiearchical softmax formulations of the skip-gram models. Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013) .",
  "y": "extends"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_4",
  "x": "**RELATED WORK** In the last few years, there has been a wealth of literature on multimodal representational models. As explained in<cite> Lazaridou et al. (2015)</cite> , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics. Bruni et al. (2014) utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_5",
  "x": "In Kiela & Bottou (2014) , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014) , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by<cite> Lazaridou et al. (2015)</cite> takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_6",
  "x": "For our text corpus, keeping with the existing literature, we use a preprocessed dump of Wikipedia 3 containing approximately 800M tokens. For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding Wordnet hierarchy (Miller, 1995) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus. This yields approximately 5,100 \"visual\" words. To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by<cite> Lazaridou et al. (2015)</cite> . In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image.",
  "y": "similarities uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_7",
  "x": "Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by<cite> Lazaridou et al. (2015)</cite> and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments. In all cases, results are reported on the full set of word similarity pairs. under a max-margin framework.; the former constrains the dimensionality of the visual features to be the same as the word embeddings, while the latter learns an explicit mapping between the textual and visual spaces. We also include baseline results for pure-text skip-gram embeddings (SKIP-GRAM)).",
  "y": "uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_8",
  "x": "---------------------------------- **NEURAL WEIGHT INITIALIZATION** On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved. In the cases of capturing general relatedness and pure visual similarity, the multimodal model of<cite> Lazaridou et al. (2015)</cite> performs better. However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & Lapata, 2014) and a point below the non-mapping MMSKIP-GRAM-A).",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_0",
  "x": "For example, semantics of sentences in human languages is believed to be carried by not merely a linear concatenation of words; instead, sentences have parse structures (Manning & Sch\u00fctze, 1999) . Image understanding, as another example, benefits from recursive modeling over structures, which yielded the state-of-the-art performance on tasks like scene segmentation (Socher et al., 2011) . In this paper, we extend LSTM to tree structures, in which we learn memory cells that can reflect the history memories of multiple child cells and hence multiple descendant cells. We call the model S-LSTM. Compared with previous recursive neural networks (<cite>Socher et al., 2013</cite>; 2012) , S-LSTM has the potentials of avoiding gradient vanishing and hence may model long-distance interaction over trees.",
  "y": "differences motivation"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_1",
  "x": "This is a desirable characteristic as many of such structures are deep. S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network togetherStanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) to determine the sentiment for different granularities of phrases in a tree. <cite>The dataset</cite> has favorable properties: in addition to being a benchmark for much previous work, <cite>it</cite> provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM. We experimentally show that S-LSTM outperforms a stateof-the-art recursive model by simply replacing the original tensor-enhanced composition with the S-LSTM memory block we propose here. We showed that utilizing the given structures is helpful in achieving a better performance than that without considering the structures.",
  "y": "motivation"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_2",
  "x": "In recent years, recursive neural networks (RvNN) have been introduced and demonstrated to achieve state-of-the-art performances on different problems such as semantic analysis in natural language processing and image segmentation (<cite>Socher et al., 2013</cite>; 2011) . <cite>These networks</cite> are defined over recursive tree structures-a tree node is a vector computed from its children. In a recursive fashion, the information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner through the tree. Derivatives of errors are computed with backpropagation over structures (Goller & Kchler, 1996) . In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004) , amongst others.",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_3",
  "x": "In (Irsoy & Cardie, 2014) , a deep recursive neural network is proposed . Nevertheless, over the often deep structures, the networks are potentially subject to the vanishing gradient problem, resulting in difficulties in leveraging long-distance dependencies in the structures. In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_4",
  "x": "We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results. Recurrent neural networks and LSTM Unlike a feedforward network, a recurrent neural network (RNN) shares their hidden states across time. The sequential history is summarized in a hidden vector. RNN also suffers from the decaying of gradient, or less frequently, blowing-up of gradient problem.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_5",
  "x": "Figure 1 is composed of a S-LSTM memory block. We present a specific wiring of such a block in Figure 2 . Each memory block contains one input gate and one output gate. The number of forget gates depends on the structure, i.e., the number of children of a node. In this paper, we assume there are two children at each nodes, same as in (<cite>Socher et al., 2013</cite>) and therefore we use <cite>their data</cite> in our experiments.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_6",
  "x": "where \u03c3 is the element-wise logistic function used to confine the gating signals to be in the range of [0, 1]; f L and f R are the left and right forget gate, respectively; b is bias and W is network weight matrices; the sign \u2297 is a Hadamard product, i.e., element-wise product. The subscripts of the weight matrices indicate what they are used for. For example, W ho is a matrix mapping a hidden vector to an output gate. Backpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; <cite>Socher et al., 2013</cite>) . The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_7",
  "x": "For example, W ho is a matrix mapping a hidden vector to an output gate. Backpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; <cite>Socher et al., 2013</cite>) . The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2 . We will discuss the specific objective function later in experiments.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_8",
  "x": "Objective over trees The objective function defined over structures can be complicated, which could consider the output structures depending on the properties of problem. Following (<cite>Socher et al., 2013</cite>) , the overall objective function we used to learn S-LSTM in this paper is simply minimizing the overall cross-entropy errors and a sum of that at all nodes. ---------------------------------- **EXPERIMENT SET-UP** As discussed earlier, recursion is a basic process inherent to many problems.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_9",
  "x": "As discussed earlier, recursion is a basic process inherent to many problems. In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages. We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (<cite>Socher et al., 2013</cite>) . In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_10",
  "x": "More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_11",
  "x": "In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_12",
  "x": "In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM. ---------------------------------- **DATA SET** The Stanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005) . The sentences were parsed with the Stanford parser (Klein & Manning, 2003) .",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_13",
  "x": "Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_14",
  "x": "Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_15",
  "x": "---------------------------------- **TRAINING DETAILS** As mentioned before, we follow (<cite>Socher et al., 2013</cite>) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings. For all phrases, the error is calculated as a regularized sum: where y seni \u2208 R c\u00d71 is predicted distribution and t i \u2208 R c\u00d71 the target distribution.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_16",
  "x": "c is the number of classes or categories, and j \u2208 c denotes the j-th element of the multinomial target distribution; i iterates over nodes, \u03b8 are model parameters, and \u03bb is a regularization parameter. We tuned our model against the development data set as split in (<cite>Socher et al., 2013</cite>) . ---------------------------------- **RESULTS** To understand the modeling advantages of S-LSTM over the structures, we conducted four sets of experiments.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_17",
  "x": "In the default setting, we conducted experiments as in (<cite>Socher et al., 2013</cite>) . Table 1 shows the accuracies of different models on the test set of the <cite>Stanford Sentiment Tree Bank</cite>. We present the results on 5-category sentiment prediction at both the sentence level (i.e., the ROOTS column in the table) and for all phrases including roots (the PHRASES column) 3 . In Table 1 , NB and SVM are naive Bayes and support vector machine classifiers, respectively; <cite>RvNN</cite> corresponds to RNN in (<cite>Socher et al., 2013</cite>) . As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_20",
  "x": "Table 1 shows the accuracies of different models on the test set of the <cite>Stanford Sentiment Tree Bank</cite>. We present the results on 5-category sentiment prediction at both the sentence level (i.e., the ROOTS column in the table) and for all phrases including roots (the PHRASES column) 3 . In Table 1 , NB and SVM are naive Bayes and support vector machine classifiers, respectively; <cite>RvNN</cite> corresponds to RNN in (<cite>Socher et al., 2013</cite>) . As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks. RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_21",
  "x": "As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks. RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions. Table 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (<cite>Socher et al., 2013</cite>) . The S-LSTM results reported here were obtained by setting the size of the hidden units to be 100, batch size to be 10, and learning rate to be 0.1. In our experiments, we only tuned these hyper-parameters, and we feel that more finer tuning, such as discriminating the classification weights between the leaves (word embedding) and other nodes, using different numbers of hidden units for the memory blocks (e.g., for the hidden layers of words), or different initializations of word embedding, may further improve the performances reported here.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_23",
  "x": "Also, annotating all phrases is expensive. However, these should not be regarded as comments on the value of the <cite>Sentiment Tree Bank</cite>. Detailed annotations in the <cite>tree bank</cite> enable much interesting work to be possible, e.g., the study of the effect of negation in changing sentiment (Zhu et al., 2014) . The second setting, corresponding to model (3) and (4) in Table 2 , is only slightly different, in which we keep annotation for the tree leafs as well, to simulate that a sentiment lexicon is available and it covers all leafs (words) (LEAF LBLS along the side of the model names stands for leaf labels), and so there is no out-of-vocabulary concern. Using real sentiment lexicons is expected to have a performance between the two settings here.",
  "y": "future_work"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_24",
  "x": "First, a special case for the S-LSTM model is considered, in which no sentential structures are given. Instead, words are read from left to right and combined in that order. We call it left recursive S-LSTM, or S- LSTM-LR in short. Similarly, we also experimented with a right recursive S-LSTM, S-LSTM-RR, in which words are read from right to left instead. Since for these models, phrase-level training signals are not available-the nodes here do not correspond to that in the <cite>original Standford Sentiment Tree Bank</cite>, but the roots and leafs annotations are still the same, so we run two versions of our experiments: one uses only training signals from roots and the other includes also leaf annotations.",
  "y": "similarities differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_0",
  "x": "<cite>Krishna et al. (2016)</cite> is currently the state of the art in Sanskrit word segmentation. The system treats the problem as an iterative query expansion problem. Using a shallow parser for Sanskrit (Goyal et al., 2012) , an input sentence is first converted to a graph of possible candidates and desirable nodes are iteratively selected using Path Constrained Random Walks (Lao and Cohen, 2010) . To further catalyse the research in word segmentation for Sanskrit, Krishna et al. (2017) has released a dataset for the word segmentation task. The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_1",
  "x": "To further catalyse the research in word segmentation for Sanskrit, Krishna et al. (2017) has released a dataset for the word segmentation task. The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser. The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word. The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation (Krishna et al., 2017) .So far, no system successfully predicts the morphological information of the words in addition to the final word form. Though <cite>Krishna et al. (2016)</cite> has designed their system with this requirement in mind and outlined the possible extension of <cite>their</cite> system for the purpose, <cite>the system</cite> currently only predicts the final word-form.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_2",
  "x": "This should not be confused with sequence length. Here, we mean the 'word' as per the original vocabulary and is common for all the competing systems. For all the strings with up to 10 words, our system 'attnSegSeq2Seq' consistently outperforms all the systems in terms of both precision and recall. The current state of the art performs slightly better than our system, for sentences with more than 10 words. It needs to be noted that the average length of a string in the Digital Corpus of Sanskrit is 6.7 (<cite>Krishna et al., 2016</cite>) .",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_3",
  "x": "The word-level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi. String matching approaches often result in low precision results. Though this is not semantically meaningful it is lexically valid. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_4",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_5",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_6",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_7",
  "x": "Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_8",
  "x": "The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. Krishna et al. (2017) states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_9",
  "x": "The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself. So we achieve scalability at the cost of missing out on providing valuable linguistic information. Models that use linguistic resources are at an advantage here. Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now.",
  "y": "background motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_10",
  "x": "Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. But, we leave this work for future.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_11",
  "x": "So we achieve scalability at the cost of missing out on providing valuable linguistic information. Models that use linguistic resources are at an advantage here. Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. In our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_12",
  "x": "But, we leave this work for future. ---------------------------------- **CONCLUSION** In this work we presented a model for word segmentation in Sanskrit using a purely engineering based appraoch. Our model with attention outperforms the current state of the art (<cite>Krishna et al., 2016</cite>) .",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_13",
  "x": "Our model with attention outperforms the current state of the art (<cite>Krishna et al., 2016</cite>) . Since, we tackle the problem with a non-linguistic approach, we hope to extend the work to other Indic languages as well where sandhi is prevalent such as Hindi, Marathi, Malayalam, Telugu etc. Since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017) , where different novel approaches in using attention are experimented with. Our experiments in line with the measures reported in <cite>Krishna et al. (2016)</cite> show that our system performs robustly across strings of varying word size. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_0",
  "x": "Unfortunately, sizes of treebanks are generally small and insufficient, which results in a common problem of data sparseness. Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_1",
  "x": "**** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_0",
  "x": "For instance, some GEC systems use large parallel corpora and synthetic data (Ge et al., 2018; Xie et al., 2018) . We introduce an unsupervised method based on MT for GEC that does not use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018) ,<cite> Artetxe et al. (2018b)</cite> , and Lample et al. (2018) . These methods are based on phrase-based statistical machine translation (SMT) and phrase table refinements. Forward refinement used by Marie and Fujita (2018) simply augments a learner corpus with automatic corrections.",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_1",
  "x": "We verified our GEC system through experiments for a low resource track of the shared task at Building Educational Applications 2019 (BEA2019). The experimental results show that our system achieved an F 0.5 score of 28.31 points. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from<cite> Artetxe et al. (2018b)</cite> . First, the cross-lingual phrase embeddings are acquired.",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_2",
  "x": "The source-to-target lexical translation model lex(f |e) considers the word with the highest translation probability in a target phrase for each word in a source phrase. The score of the lexical translation model was calculated based on the product of respective phrase translation probabilities. \u03f5 is a constant term for the case where no alignments are found. As in<cite> Artetxe et al. (2018b)</cite> , the term was set to 0.001. The backward lexical translation probability lex(e|f ) is calculated in a similar manner.",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_3",
  "x": "Therefore, we update the phrase table using an SMT system. The SMT system trained on synthetic data eliminates the noisy phrase pairs using 2 As in<cite> Artetxe et al. (2018b)</cite> , \u03c4 is estimated by maximizing the phrase translation probability between an embedding and the nearest embedding on the opposite side. language models trained on the target-side corpus. This process corresponds to lines 6-10 in Algorithm 1. The phrase table is refined with forward refinement (Marie and Fujita, 2018) .",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_4",
  "x": "For forward refinement, target synthetic data were generated from the source monolingual data using the source-to-target phrase s\u2192t was then created with this target synthetic corpus. This operation was executed N times. Construction of a comparable corpus This unsupervised method is based on the assumption that the source and target corpora are comparable. In fact, Lample et al. (2018) ,<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as the source-side data.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_5",
  "x": "These data are tokenized by spaCy v1.9.0 6 and the en_core_web_sm-1.2.0 model. We used moses truecaser for the training data; this truecaser model is learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016) learned from processed English News Crawl; the number of operations is 50K. The implementation proposed by<cite> Artetxe et al. (2018b)</cite> 7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013) 8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features.",
  "y": "extends"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_6",
  "x": "Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT <cite>(Artetxe et al., 2018b</cite> Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data. Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 . this study, we apply the USMT method of<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_7",
  "x": "Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT <cite>(Artetxe et al., 2018b</cite> Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data. Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 . this study, we apply the USMT method of<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data.",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_0",
  "x": "Even inner-domain shifts, such as, e.g., moving from hematology (Ohta et al., 2002) to the genetics of cancer (Kulick et al., 2004) within the field of molecular biology may have drastic consequences in the sense that entirely new meta data sets have to produced by annotation teams. Thus, reducing the human efforts for the creation of adequate training material is a major challenge. Active learning (AL) copes with this problem as it intelligently selects the data to be labeled. It is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process. AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Hwa, 2001; <cite>Tomanek et al., 2007a)</cite> .",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_1",
  "x": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy (Engelson and Dagan, 1996) as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement. Although in our experiments we considered the NLP task of named entity recognition (NER) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well. In <cite>Tomanek et al. (2007a)</cite> we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time.",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_2",
  "x": "For our experiments on approximating the learning curves for AL-based selection, we chose named entity recognition (NER) as the annotation task in focus. We employed the committee-based AL approach described in <cite>Tomanek et al. (2007a)</cite> . The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996) . In each AL iteration, each classifier is trained on a randomly , L being the set of all examples seen so far.",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_3",
  "x": "In our NER scenario, complete sentences are selected by AL. While we made use of ME classifiers during the selection, we employed a NE tagger based on Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) during evaluation time to determine the learning curves. We have already shown that in this scenario, ME classifiers perform equally well for AL-driven selection as CRFs when using the same features. This effect is truly beneficial, especially for real-world annotation projects, due to much lower training times and, by this, shorter annotator idle times <cite>(Tomanek et al., 2007a)</cite> . For the AL simulation, we employed two simulation corpora: The CONLL corpus, based on the English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) , which consists of newspaper articles annotated with respect to person, location, and organisation entities.",
  "y": "background"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_0",
  "x": "Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model <cite>(Freitag and Khadivi, 2007)</cite> . ---------------------------------- **SEQUENCE ALIGNMENT MODEL** Let e = e 1 e 2 . . . e n and f = f 1 f 2 . . .",
  "y": "background differences"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_1",
  "x": "For fixed sequences e and f the function s(e, f ) can be efficiently computed using a dynamic programming algorithm, Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences . In this paper, we employ the same features as those used by<cite> Freitag and Khadivi (2007)</cite> . All local feature functions \u03c6(a k , e, i, f , j) are conjunctions of the alignment operation a k and forward or backward-looking character m-grams in sequences e and f at positions i and j respectively.",
  "y": "background uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_2",
  "x": "Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences . In this paper, we employ the same features as those used by<cite> Freitag and Khadivi (2007)</cite> . All local feature functions \u03c6(a k , e, i, f , j) are conjunctions of the alignment operation a k and forward or backward-looking character m-grams in sequences e and f at positions i and j respectively. For the source sequence f both forward and backwardlooking m-gram features are included.",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_3",
  "x": "We apply our model to the real-world ArabicEnglish name transliteration task on a data set of 10,084 Arabic names from the LDC. The data set consists of Arabic names in an ASCII-based alphabet and its English rendering. Table 1 shows a few examples of Arabic-English pairs in our data set. We use the same training/development/testing (8084/1000/1000) set as the one used in a previous benchmark study <cite>(Freitag and Khadivi, 2007)</cite> . The development and testing data were obtained by randomly removing entries from the training data.",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_4",
  "x": "Unlike previ-ous generative transliteration models, no additional language model feature is used. We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features <cite>(Freitag and Khadivi, 2007)</cite> . The SMT system directly models the posterior probability P r(e|f ) using a log-linear combination of several sub-models: a characterbased phrase translation model, a character-based lexicon model, a character penalty and a phrase penalty. In the PTEM model, the update rule only considers the best target sequence and modifies the parameters w \u03c4 +1 = w \u03c4 + \u03a6(a, e, f ) \u2212 \u03a6(a \u2032 , e \u2032 , f ) if the score s(e \u2032 , f ) \u2265 s(e, f ). Table 2 shows the 1-best and 5-best accuracy of each model trained on the combined train+dev data set.",
  "y": "uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_0",
  "x": "In this paper, we assume that the languages are sufficiently closely related to allow some translation pairs to be identified on the basis of orthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005) , a gold seed lexicon <cite>(Mikolov et al., 2013b)</cite> , or document-aligned comparable corpora (Vuli\u0107 and Moens, 2015) . Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014) , temporal and web-based features (Irvine and Callison-Burch, 2013) , or BabelNet (Wang and Sitbon, 2014) . Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping procedure.",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_1",
  "x": "Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014) , temporal and web-based features (Irvine and Callison-Burch, 2013) , or BabelNet (Wang and Sitbon, 2014) . Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping procedure. The seed set is constructed by identifying words with similar spelling (cognates). We filter out non-translation pairs that look similar but differ in meaning (false friends) by imposing a relative-frequency constraint. We then use this noisy seed lexicon to train context vectors via neural network <cite>(Mikolov et al., 2013b)</cite> , inducing a cross-lingual transformation that approximates semantic similarity.",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_2",
  "x": "Although the initial accuracy of the transformation is low, it is sufficient to identify a certain number of correct translation pairs. Adding the high-confidence pairs to the seed lexicon allows us to refine the cross-lingual transformation matrix. We proceed to iteratively expand our lexicon by alternating the two steps of translation pair identification, and transformation induction. We conduct a series of experiments on English, French, and Spanish. The results demonstrate a substantial error reduction with respect to a word-vector-based method of<cite> Mikolov et al. (2013b)</cite> , when using the same word vectors on six source-target pairs.",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_3",
  "x": "The pseudo-code of the algorithm is shown in Figure 1. ---------------------------------- **LEXICON EXPANSION** Since our task is to find translations for each of a given set of source-language words, which we refer to as the source vocabulary, we must expand the seed lexicon to cover all such words. We adapt the approach of<cite> Mikolov et al. (2013b)</cite> for learning a linear transformation between the source and target vector spaces to enable it to function given only a small, noisy seed.",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_4",
  "x": "We adapt the approach of<cite> Mikolov et al. (2013b)</cite> for learning a linear transformation between the source and target vector spaces to enable it to function given only a small, noisy seed. We use WORD2VEC (Mikolov et al., 2013a) to map words in our source and target corpora to ndimensional vectors. The mapping is derived in a strictly monolingual context of both the source and target languages. While<cite> Mikolov et al. (2013b)</cite> derive the translation matrix using five thousand translation pairs obtained via Google Translate, for c iterations do",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_5",
  "x": "We use the cosine similarity sim(T \u00b7 u, v) to calculate the confidence score for the corresponding candidate translation pair (s, t). An important innovation of our algorithm is considering not only the fitness of t as a translation of s, but also of s as a translation of t. Distinct translation matrices are derived in both directions: source-to-target (T) and target-to-source (T ). We define the score of a pair (s, t) corresponding to the pair of vectors (u, v) as the average of the two cosine similarity values: Unlike<cite> Mikolov et al. (2013b)</cite> , our algorithm iteratively expands the lexicon, which gradually increases the accuracy of the translation matrices. The initial translation matrices, derived from a small, noisy seed, are sufficient to identify a small number of correct translation pairs, which are added to the lexicon.",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_6",
  "x": "We refer to this approach as bootstrapping, and continue the process for a set number of iterations, which is tuned on development data. The output of our algorithm is the set of translation pairs produced in the final iteration, with each source vocabulary word paired (not necessarily injectively) with one target vocabulary word. ---------------------------------- **EXPERIMENTS** In this section we compare our method to two prior methods, our reimplementation of the supervised word-vector-based method of<cite> Mikolov et al. (2013b)</cite> (using the same vectors as our method), and the reported results of an EM-based method of Haghighi et al. (2008) .",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_7",
  "x": "**EVALUATION** We evaluate the induced lexicon after 40 iterations of bidirectional bootstrapping by comparing it to the lexicon after the first iteration in a single direction, which is equivalent to the method of<cite> Mikolov et al. (2013b)</cite> . Following Haghighi et al. (2008) , we also report the accuracy of an ED-ITDIST baseline method, which matches words in the source and target vocabularies. We use an implementation of the Hungarian algorithm 1 (Kuhn, 1955) to solve the minimum bipartite matching problem, where the edge cost for any source-target pair is the normalized edit distance between the two words. The results in Table 2 show that the method of<cite> Mikolov et al. (2013b)</cite> (MIK13-Auto) , represented by the first translation matrix derived on our automatically extracted the seed lexicon, performs well below the edit distance baseline.",
  "y": "similarities"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_8",
  "x": "Following Haghighi et al. (2008) , we also report the accuracy of an ED-ITDIST baseline method, which matches words in the source and target vocabularies. We use an implementation of the Hungarian algorithm 1 (Kuhn, 1955) to solve the minimum bipartite matching problem, where the edge cost for any source-target pair is the normalized edit distance between the two words. The results in Table 2 show that the method of<cite> Mikolov et al. (2013b)</cite> (MIK13-Auto) , represented by the first translation matrix derived on our automatically extracted the seed lexicon, performs well below the edit distance baseline. By contrast, our bootstrapping approach (BootstrapAuto) achieves an average accuracy of 85% on the six datasets. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_0",
  "x": "Using the sense-tagged corpus of 192,800 word occurrences reported in<cite> (Ng and Lee, 1996)</cite> , I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier. Finally, I suggest that intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for word sense disambiguation research. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_1",
  "x": "In this view, a large sense-tagged corpus is critical as well as necessary to achieve broad coverage, high accuracy WSD. The rest of this paper is organized as follows. In Section 2, I briefly discuss the utility of WSD in practical NLP tasks like information retrieval and machine translation. I also address some objections to WSD research. In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses<cite> (Ng and Lee, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_2",
  "x": "This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al., 1994) . When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ).",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_3",
  "x": "This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al., 1994) . When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ).",
  "y": "motivation"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_4",
  "x": "This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> . We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (hot I will only focus on common noun in this paper and ignore proper noun. 2 mographic) sense distinction is needed, say for some NLP applications. Indeed, the WORDNET software has an option for grouping noun senses into a smaller number of sense classes.",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_5",
  "x": "However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ). This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> . We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (hot I will only focus on common noun in this paper and ignore proper noun.",
  "y": "extends"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_6",
  "x": "A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996) , were tested on a small number of words like \"line\" and \"interest\". Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994) , contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994) , there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a running text in SEMCOR. To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET<cite> (Ng and Lee, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_7",
  "x": "These 192,800 word occurrences consist of only 121 nouns and 70 verbs which are the most frequently occurring and most ambiguous words of English. 2 To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in<cite> (Ng and Lee, 1996)</cite> ). For each word, 10 random trials were conducted and the accuracy figures were averaged over the I0 trials. In each trial, I00 examples were randomly selected to form the test set, while the remaining examples (randomly shuffled) were used for training.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_8",
  "x": "The performance figures of LEXAS in Table 1 are higher than those reported in<cite> (Ng and Lee, 1996)</cite> . The classification accuracy of the nearest neighbor algorithm used by LEXAS (Cost and Salzberg, 1993) is quite sensitive to the number of nearest neighbors used to select the best matching example. By using 10-fold cross validation (Kohavi and John, 1995) to automatically pick the best number of nearest neighbors to use, the performance of LSXAS has improved. ---------------------------------- **WORD SENSE DISAMBIGUATION IN THE**",
  "y": "differences"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_0",
  "x": "Abstract. Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13,<cite> 26]</cite> . Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting. In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise. We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_1",
  "x": "The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23,<cite> 26]</cite> has started exploring the effectiveness of these modalities for automatically assessing ---------------------------------- **ARXIV:1807.07351V1 [CS.CY] 19 JUL 2018** the mental health of a subject, reporting very high accuracy. What is typically done in these studies is to use features based on the subjects' smart phone logs and social media, to predict some self-reported mental health index (e.g., \"wellbeing\", \"depression\" and others), which is provided either on a Likert scale or on the basis of a psychological questionnaire (e.g., PHQ-8 [12] , PANAS [29] , WEMWBS [25] and others). Most of these studies are longitudinal, where data about individuals is collected over a period of time and predictions of mental health are made over a sliding time window.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_2",
  "x": "Again however, the limited number of instances for every user make such models unable to generalize well. In order to overcome these issues, previous work [2, 5, 9, 10, 22,<cite> 26]</cite> has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED). While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual. In this paper we demonstrate the challenges that current state-of-the-art models face, when tested in a real-world setting. We work on two longitudinal datasets with four mental health targets, using different features derived from a wide range of heterogeneous sources.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_3",
  "x": "This poses serious questions about the contribution of the features derived from social media, smartphones and sensors for the task of automatically assessing well-being on a longitudinal basis. Our goal is to flesh out, study and discuss such limitations through extensive experimentation across multiple settings, and to propose a pragmatic evaluation and model-building framework for future research in this domain. ---------------------------------- **RELATED WORK** Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13,<cite> 26]</cite> .",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_4",
  "x": "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22,<cite> 26]</cite> , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score. LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] . From the works listed in Table 2 , only Suhara et al. [23] achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means. ---------------------------------- **PROBLEM STATEMENT**",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_5",
  "x": "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10,<cite> 26]</cite> . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis. Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level. In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. <cite>[26]</cite> and Jaques et al. [9] .",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_6",
  "x": "Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level. In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. <cite>[26]</cite> and Jaques et al. [9] . ---------------------------------- **P1: TRAINING ON PAST VALUES OF THE TARGET (LOIOCV, LOUOCV)**",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_7",
  "x": "Additionally, the target values of these two instances will also be highly correlated due to the moving average filter, making the task artificially easy for large T HIST and not applicable in a real-world setting. While we focus on the approach in [3] , a similar approach with respect to feature extraction was also followed in LiKamWa et al. [13] and Bogomolov et al. [2] , extracting features from the past 2 and 2 to 5 days, respectively. ---------------------------------- **P3: PREDICTING USERS (LOUOCV)** Tsakalidis et al. <cite>[26]</cite> monitored the behaviour of 19 individuals over four months.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_8",
  "x": "Four types of features were extracted from survey and smart devices carried by subjects. Self-reported scores on a daily basis served as the ground truth. The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue. Since different users exhibit different mood scores on average <cite>[26]</cite> , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one. A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_10",
  "x": "We employed the dataset obtained by Tsakalidis et al. <cite>[26]</cite> , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects. From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects. For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period. ---------------------------------- **DATASET 2:**",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_11",
  "x": "**P3: PREDICTING USERS:** We followed the evaluation settings of two past works (see section 3.3), with the only difference being the use of 5-fold CV instead of a train/dev/test split that was used in [9] . The features of every instance are extracted from the past day before the completion of a mood form. In Experiment 1 we follow the setup in <cite>[26]</cite> : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE. We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step.",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_13",
  "x": "Table 6 . P2: Performance (sensitivity/specificity) of the SVM classifier trained over 14 days of smartphone/social media features (FEAT) compared against 3 na\u00efve baselines. ---------------------------------- **P3: PREDICTING USERS** Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> .",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_14",
  "x": "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> . In the MIXED cases, the pattern is consistent with <cite>[26]</cite> , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores. This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity. In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.",
  "y": "similarities"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_15",
  "x": "In MIXED, by identifying who the user is, we have a rough estimate of her mood score, which is by itself a good predictor, if it is compared with the average predictor across all mood scores of all users. Thus, the effect of the features in this setting cannot be assessed with certainty. Table 7 . P3: Results following the evaluation setup in <cite>[26]</cite> (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation. Table 8 displays our results based on Jaques et al. [9] (see section 3.3).",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_0",
  "x": "****BLINDFOLD BASELINES FOR EMBODIED QA**** **ABSTRACT** We explore blindfold (question-only) baselines for <cite>Embodied Question Answering</cite>. The <cite>EmbodiedQA</cite> task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering. Consequently, a blindfold baseline which ignores the environment and visual information is a degenerate solution, yet we show through our experiments on the EQAv1 dataset that a simple question-only baseline achieves state-of-the-art results on the EmbodiedQA task in all cases except when the agent is spawned extremely close to the object.",
  "y": "background motivation"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_1",
  "x": "Recent breakthroughs in static, unimodal tasks such as image classification [16] and language processing [18] has prompted research towards multimodal tasks [1, 8] and virtual environments [4, 15, 25] . This is substantiated by embodiment theories in cognitive science that have argued for agent learning to be interactive and multimodal, mimicking key aspects of human learning [9, 17] . To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being <cite>Embodied Question Answering</cite> (<cite>EmbodiedQA</cite>) <cite>[5]</cite> . The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision. Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?').",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_2",
  "x": "The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision. Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_3",
  "x": "Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_4",
  "x": "The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods.",
  "y": "motivation"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_5",
  "x": "Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods. To our surprise, blindfold baselines achieve state-of-the-art performance on the <cite>EmbodiedQA</cite> task, except in the case when the agent is spawned extremely close to the object. Even in the latter case, blindfold baselines perform surprisingly close to existing state-of-the-art methods.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_6",
  "x": "We hope comparison with our baseline results can more effectively demonstrate how well a method is able to leverage embodiment in the environment. Upon further error analysis of our models and qualitative inspection of the dataset, we find that there exist biases in the EQAv1 dataset that allow blindfold models to perform so well. We acknowledge the active effort of Das et al. <cite>[5]</cite> in removing some biases via entropy-pruning but note that further efforts might be necessary to fully correct these biases. ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_8",
  "x": "We acknowledge the active effort of Das et al. <cite>[5]</cite> in removing some biases via entropy-pruning but note that further efforts might be necessary to fully correct these biases. ---------------------------------- **RELATED WORK** <cite>EmbodiedQA</cite> Methods: Das et al. <cite>[5]</cite> introduced the <cite>PACMAN-RL+Q</cite> model which is bootstrapped with expert shortest-path demonstrations and later fine-tuned with REINFORCE [24] . This model consists of a hierarchical navigation module: a planner and a controller, and a question answering module that acts when the navigation module has given up control.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_9",
  "x": "To get rid of peaky answers, an entropy pruning method was applied by <cite>[5]</cite> where questions with normalized entropy below 0.5 were excluded. However this still leaves an uneven answer distribution that can be exploited. We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs.",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_10",
  "x": "We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs. Results Detailed results are reported in Table 4 . Following Das et al. [6] , we report the agent's top-1 accuracy on the test set when spawned 10, 20 and 50 steps away from the goal, denoted as T 10 , T 20 and T 50 respectively. Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 . We observe that the BoW model outperforms all existing methods except NMC(BC+A3C) in the case where agent is spawned very close to the target.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_11",
  "x": "Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 . We observe that the BoW model outperforms all existing methods except NMC(BC+A3C) in the case where agent is spawned very close to the target. The Nearest Neighbour method also does pretty well, and only falls behind to <cite>PACMAN</cite> (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case. The difference in performance b/w the Nearest Neighbour method and BoW is primarily due to the fact that the BoW method leverages validation metrics more effectively, uses distributed word representations and differs in optimization. We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes.",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_12",
  "x": "We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes. For completeness, we also include a question only baseline derived directly from the <cite>EmbodiedQA</cite> codebase, which uses only the Question LSTM in the <cite>PACMAN</cite> model, termed as <cite>PACMAN</cite> Q-only (LSTM). Note that we only compare the top-1 accuracy of different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]).",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_13",
  "x": "As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]). Our results on the Nearest Neighbour baseline confirm this source of bias and explain largely the text model performance. Viewing these results holistically, we conclude that current methods for the <cite>EmbodiedQA</cite> task are not effective at using context from the environment, and in fact this negatively hampers them. This shows that there is room for building new models that leverage the context and embodiment in the environment. Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_14",
  "x": "As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]). Our results on the Nearest Neighbour baseline confirm this source of bias and explain largely the text model performance. Viewing these results holistically, we conclude that current methods for the <cite>EmbodiedQA</cite> task are not effective at using context from the environment, and in fact this negatively hampers them. This shows that there is room for building new models that leverage the context and embodiment in the environment. Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case.",
  "y": "uses motivation"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_16",
  "x": "We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in <cite>PACMAN</cite> reduces performance to below the text baselines. For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant. As noted in <cite>[5]</cite> , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ---------------------------------- **T 10**",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_17",
  "x": "As noted in <cite>[5]</cite> , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ---------------------------------- **T 10** T 20 T 50 T any Navigation + VQA <cite>PACMAN</cite> (BC) <cite>[5]</cite> 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from [6] for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment. Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object.",
  "y": "uses differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_18",
  "x": "Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in <cite>[5]</cite> Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both. The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_19",
  "x": "The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ---------------------------------- **CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_20",
  "x": "The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ---------------------------------- **CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "future_work"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_0",
  "x": "In order to obtain better embeddings for the questions and answers, we build a convolutional neural network (CNN) structure on top of biLSTM. Secondly, in order to better distinguish candidate answers according to the question, we introduce a simple but efficient attention model to this framework for the answer embedding generation according to the question context. We report experimental results for two answer selection datasets: (1) InsuranceQA<cite> (Feng et al., 2015)</cite> 1 , a recently released large-scale non-factoid QA dataset from the insurance domain. The rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the details of the proposed models; Experimental settings and results of InsuranceQA and TREC-QA datasets are discussed in section 4 and 5 respectively; Finally, we draw conclusions in section 6. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_1",
  "x": "While these methods show effectiveness, they might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity by introducing linguistic tools, such as parse trees and dependency trees. There were prior methods using deep learning technologies for the answer selection task. The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics<cite> (Feng et al., 2015</cite>; Yu et al., 2014; dos Santos et al., 2015) . Secondly, a joint feature vector is constructed based on both the question and the answer, and then the task can be converted into a classification or learning-to-rank problem (Wang & Nyberg, 2015) . Finally, recently proposed models for textual generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals & Le, 2015) .",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_2",
  "x": "There are two major differences between our approaches and the work in<cite> (Feng et al., 2015)</cite> : (1) The architectures developed in<cite> (Feng et al., 2015)</cite> are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2)<cite> Feng et al. (2015)</cite> tackle the question and answer independently, while the proposed structures develop an efficient attentive models to generate answer embeddings according to the question. ---------------------------------- **APPROACH**",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_3",
  "x": "Following the same ranking loss in<cite> (Feng et al., 2015</cite>; Weston et al., 2014; Hu et al., 2014) , we define the training objective as a hinge loss. where a + is a ground truth answer, a \u2212 is an incorrect answer randomly chosen from the entire answer space, and M is constant margin. We treat any question with more than one ground truth as multiple training examples, each for one ground truth. There are three simple ways to generate representations for questions and answers based on the word-level biLSTM outputs: (1) Average pooling; (2) max pooling; (3) the concatenation of the last vectors on both directions. The three strategies are compared with the experimental performance in Section 5.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_4",
  "x": "Dropout operation is performed on the QA representations before cosine similarity matching. Finally, from preliminary experiments, we observe that the architectures, in which both question and answer sides share the same network parameters, is significantly better than the one that the question and answer sides own their own parameters separately, and converges much faster. As discussed in<cite> (Feng et al., 2015)</cite> , this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs. While for the network with separate question and answer parameters, there is no such constraint and the model has doublesized parameters, making it difficult to learn for the optimizer. In the previous subsection, we generate the question and answer representations only by simple operations, such as max or mean pooling.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_5",
  "x": "In this subsection, we resort to a CNN structure built on the outputs of biLSTM, in order to give a more composite representation of questions and answers. The structure of CNN in this work is similar to the one in<cite> (Feng et al., 2015)</cite> , as shown in Figure 2 . Unlike the traditional forward neural network, where each output is interactive with each input, the convolutional structure only imposes local interactions between the inputs within a filter size m. In this work, for every window with the size of m in biLSTM output vectors, ie. , where t is a certain time step, the convolutional filter F = [F(0) \u00b7 \u00b7 \u00b7 F(m \u2212 1)] will generate one value as follows.",
  "y": "similarities"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_6",
  "x": "However, QA-LSTM/CNN with attention can outperform the baselines on both datasets. ---------------------------------- **INSURANCEQA EXPERIMENTS** Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by<cite> Feng et al. (2015)</cite> . The InsuranceQA dataset provides a training set, a validation set, and two test sets.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_7",
  "x": "The InsuranceQA dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between two tests' questions. One can see the details of InsuranceQA data in<cite> (Feng et al., 2015)</cite> . We list the numbers of questions and answers of the dataset in Table 1 . A question may correspond to multiple answers.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_8",
  "x": "Architecture-II in<cite> (Feng et al., 2015)</cite> : Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question. No attention model is used in this baseline. ---------------------------------- **ARCHITECTURE-II WITH GEOMETRICMEAN OF EUCLIDEAN AND SIGMOID DOT PRODUCT (GESD):** GESD is used to measure the distance between the question and answers.",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_9",
  "x": "This is the model which achieved the best performance in<cite> (Feng et al., 2015)</cite> . ---------------------------------- **RESULTS AND DISCUSSIONS** In this section, detailed analysis on experimental results are given. or attention model. They vary on how to utilize the biLSTM output vectors to form sentential embeddings for questions and answers in shown in section 3.1.",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_10",
  "x": "From Row (D) to (F), CNN layers are built on the top of the biLSTM with different filter numbers. We set the filter width m = 2, and we did not see better performance if we increase m to 3 or 4. Row (F) with 4000 filters gets the best validation accuracy, obtained a comparable performance with the best baseline (Row (D) in Table 2 ). Row F shared a highly analogous CNN structure with Architecture II in<cite> (Feng et al., 2015)</cite> , except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input. Row (G) and (H) corresponds to QA-LSTM with the attention model.",
  "y": "differences similarities"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_11",
  "x": "In comparison to Model (C), Model (G) shows over 2% improvement on both validation and Test2 sets. With respect to the model with mean pooling layers (B), the improvement from attention is more remarkable. Model (H) is over 8% higher on all datasets compared to (B), and gets improvements from the best baseline by 3%, 2.8% and 1.2% on the validation, Test1 and Test2 sets, respectively. Compared to Architecture II in<cite> (Feng et al., 2015)</cite> , which involved a large number of CNN filters, (H) model also has fewer parameters. Row (I) corresponds to section 3.4, where CNN and attention mechanism are combined.",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_12",
  "x": "In Table 4 , we list the performance of some prior work on this dataset, which can be referred to (Wang & Nyberg, 2015) . We implemented the Architecture II in<cite> (Feng et al., 2015)</cite> from scratch. Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ---------------------------------- **SETUP**",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_13",
  "x": "Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ---------------------------------- **SETUP** We keep the configurations same as those in InsuranceQA in section 4.1, except the following differences: First, we set the minibatch size as 10; Second, we set the maximum length of questions and answers as 40 instead of 200. Third, following (Wang & Nyberg, 2015) , We use 300-dimensional vectors that were trained and provided by word2vec 3 .",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_14",
  "x": "Moreover, although TREC-QA dataset provided negative answer candidates for each training question, we randomly select the negative answers from all the candidate answers in the training set. Table 5 shows the performance of the proposed models. Compared to Model (A), which is with average pooling on top of biLSTM but without attention, Model (B) with attention improves MAP by 0.7% and MRR by approximately 2%. The combination of CNN with QA-LSTM (Model-C) gives greater improvement on both MAP and MRR from Model (A). Model (D), which combines the ideas of Model (B) and (C), achieves the performance, competitive to the best baselines on MAP, and 2\u223c4% improvement on MRR compared to (Wang & Nyberg, 2015) and<cite> (Feng et al., 2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_0",
  "x": "As an input, this function operates on various statistics relating to different text features. In this paper, we train a readability classification model using a corpus compiled from textbooks and features inherited from our previous works Islam et al. (2012; and features from<cite> Sinha et al. (2012)</cite> . Later we use the model to classify Bangla news articles for children from different well-known news sources from Bangladesh and West Bengal. The paper is organized as follows: Section 2 discusses related work. Section 3 describes cognitive model of children in terms of readability followed by an introduction of the training corpus and news articles in Section 4.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_1",
  "x": "We have inherited features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> , these features achieve reasonable classification accuracy. Children's reading skills is influenced by their cognitive ability. The following section describes children's cognitive model and text readability. ---------------------------------- **TEXT READABILITY AND CHILDREN**",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_2",
  "x": "**READABILITY MODELS FOR BANGLA** Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas. A user study was performed to evaluate their performance. We also inherited two of their best performing models: In their models, they use structural parameters such as average WL, number of jukta-akshars (JUK) or consonant-conjuncts, number of polysyllabic words (PSW).",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_3",
  "x": "**READABILITY MODELS FOR BANGLA** Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas. A user study was performed to evaluate their performance. We also inherited two of their best performing models: In their models, they use structural parameters such as average WL, number of jukta-akshars (JUK) or consonant-conjuncts, number of polysyllabic words (PSW).",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_4",
  "x": "Table 3 : Performance of Bangla readability models proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> . In this paper, we use 20 features to generate feature vectors for the classifier. The following section describes our experiments and results on training corpus and news articles. ---------------------------------- **EXPERIMENTS AND RESULTS**",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_5",
  "x": "**EXPERIMENTS AND RESULTS** In order to find the best performing training model, we use 20 features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> . Note that hundred data sets were randomly generated where 80% of the corpus was used for training and remaining 20% for evaluation. The weighted average of Accuracy and F-score is computed by considering results of all data sets. We use the SMO (Platt, 1998; Keerthi et al., 2001) classifier model implemented in WEKA (Hall et al., 2009) together with the Pearson VII function-based universal kernel PUK (\u00dcst\u00fcn et al., 2006) .",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_6",
  "x": "We use the SMO (Platt, 1998; Keerthi et al., 2001) classifier model implemented in WEKA (Hall et al., 2009) together with the Pearson VII function-based universal kernel PUK (\u00dcst\u00fcn et al., 2006) . ---------------------------------- **TRAINING MODEL** The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014;<cite> Sinha et al., 2012)</cite> . That is why, we did not explore any of the traditional formulas.",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_7",
  "x": "**TRAINING MODEL** The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014;<cite> Sinha et al., 2012)</cite> . That is why, we did not explore any of the traditional formulas. At first we build a classifier using two readability models from <cite>Sinha et al(2012)</cite> . The output of these models are used as input for the readability classifier.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_8",
  "x": "However, informationtransmission based features (i.e., SL and WL prob. and SL and DW prob.) are the best performing features. Therefore, a text with higher average SL become more difficult when it contains more difficult words or more longer words. The classification F-Score rises to 87.87 when we combine features from Islam et al. (2014) and<cite> Sinha et al. (Sinha et al., 2012)</cite> . ---------------------------------- **NEWS ARTICLES CLASSIFICATION**",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_9",
  "x": "Although linguistically motivated features could capture linguistic properties of news articles. Lexical features and features related to information density also have good predictive power to identify text difficulties. The classification results show that candidate articles are appropriate for children. This study also validate that features in our previous study Islam et al. (2014) and features proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> are useful for Bangla text readability analysis. There are many languages in the world which lack a readability measurement tool.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_0",
  "x": "**ABSTRACT** This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015) [3,<cite> 4]</cite> , on the eight datasets with relatively large training data that were used for testing the very deep characterlevel CNN in Conneau et al. (2016) [1]. Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in [1] though the results should be interpreted with some consideration due to the unique pre-processing of [1]. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_1",
  "x": "Text categorization is the task of labeling documents, which has many important applications such as sentiment analysis and topic categorization. Recently, several variations of convolutional neural networks (CNNs) [7] have been shown to achieve high accuracy on text categorization (see e.g., [3, <cite>4,</cite> 9, 1] and references therein) in comparison with a number of methods including linear methods, which had long been the state of the art. Long-Short Term Memory networks (LSTMs) [2] have also been shown to perform well on this task, rivaling or sometimes exceeding CNNs [5, 8] . However, CNNs are particularly attractive since, due to their simplicity and parallel processing-friendly nature, training and testing of CNNs can be made much faster than LSTM to achieve similar accuracy [5] , and therefore CNNs have a potential to scale better to large training data. Here we focus on two CNN studies that report high performances on categorizing long documents (as opposed to categorizing individual sentences):",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_2",
  "x": "However, CNNs are particularly attractive since, due to their simplicity and parallel processing-friendly nature, training and testing of CNNs can be made much faster than LSTM to achieve similar accuracy [5] , and therefore CNNs have a potential to scale better to large training data. Here we focus on two CNN studies that report high performances on categorizing long documents (as opposed to categorizing individual sentences): \u2022 Our earlier work (2015) [3,<cite> 4]</cite> : shallow word-level CNNs (taking sequences of words as input), which we abbreviate as word-CNN. \u2022 Conneau et al. (2016) [1]: very deep character-level CNNs (taking sequences of characters as input), which we abbreviate as char-CNN. Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison.",
  "y": "background motivation"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_3",
  "x": "Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] .",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_4",
  "x": "While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results. Although it may be natural to assume that the error rates reported in [1] well represent the best performance that the deep char-CNNs can achieve, we note that in [1] , documents were clipped and padded so that they all became 1014 characters long, and we do not know how this pre-processing affected their model accuracy.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_5",
  "x": "In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> .",
  "y": "motivation"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_6",
  "x": "In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_7",
  "x": "To experiment with word-CNN, we handle variable-sized documents as variable-sized as we see no merit in making them fixed-sized, though we reduce the size of vocabulary to reduce storage requirements. Considering that, we emphasize that this work is not intended to be a rigorous comparison of word-CNNs and char-CNNs; instead, it should be regarded as a report on the shallow word-CNN performance on the eight datasets used in [1] , referring to the results in [1] as the state-of-the-art performances. ---------------------------------- **PRELIMINARY** We start with briefly reviewing the very deep word-CNN of [1] and the shallow word-CNN of [3,<cite> 4]</cite> .",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_8",
  "x": "---------------------------------- **SHALLOW WORD-LEVEL CNNS AS IN [3,<cite> 4]</cite>** Two types of word-CNN were proposed in [3,<cite> 4]</cite> , which are illustrated in Figure 1 . One is a straightforward application of CNN to text (the base model), and the other involves training of tv-embedding ('tv' stands for two views) to produce additional input to the base model. The models with tv-embedding produce higher accuracy provided that sufficiently large amounts of unlabeled data for tv-embedding learning are available.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_9",
  "x": "explicit word embedding layer before a convolution layer as in, e.g., [6] , which makes x the concatenation of word vectors. See also the supplementary material of<cite> [4]</cite> for the representation power analysis. As illustrated in Figure 1 (a), f (x) is applied to the text regions at every location of a document (ovals in the figure), and pooling aggregates the resulting region vectors into a document vector, which is used as features by a linear classifier. In our experiments with word-CNN without tv-embedding reported below, the one-hot representation used for x was fixed to the concatenation of one-hot vectors with a vocabulary of the 30K most frequent words, and the dimensionality of region embedding (i.e., the number of feature maps) was fixed to 500. That is, our one-hot vectors were 30K-dimensional while any out-of-vocabulary word was converted to a zero vector, and the region embedding f (x) produced 500-dimensional vectors for each region.",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_10",
  "x": "This model can be easily extended to use multiple tv-embeddings, each of which, for example, uses a distinct vector representation of region, and so the region embedding function in the final model (hollow ovals in Figure 1 (b) ) can be written as: is the output of the tv-embedding indexed by i applied to the corresponding text region. In<cite> [4]</cite> , tv-embedding training was done using unlabeled data as an additional resource; therefore, the proposed models were semi-supervised models. In the experiments reported below, due to the lack of standard unlabeled data for the tested datasets, we trained tv-embeddings on the labeled training data ignoring the labels; thus, the resulting models are supervised ones. We trained four tv-embeddings with four distinct one-hot representations of text regions (i.e., input to orange ovals in Figure 1 (b) ): bow representation with region size 5 or 9, and bag-of-{1,2,3}-gram representation with region size 5 or 9.",
  "y": "differences"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_11",
  "x": "**EXPERIMENTAL DETAILS OF WORD-LEVEL CNNS** On all datasets, we held out 10K data points from the training set for use as validation data. Models were trained using the training set minus validation data, and model selection (or hyper parameter tuning) was done based on the performance on the validation data. Tv-embedding training was done as in<cite> [4]</cite> ; weighted square loss was minimized without regularization while the target regions (adjacent regions) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved. Tv-embeddings were fixed (i.e., no weight updating) during the final training with labeled data.",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_12",
  "x": "On each dataset, the best results are shown in bold and the second best results are shown in the italic font. On all datasets, the shallow word-CNN with tv-embeddings performs the best. The second best performer is the shallow word-CNN without tv-embedding on all but Ama.f (Amazon full). Whereas the deep char-CNN underperforms traditional linear models when training data is relatively small, the shallow word-CNNs with and without tv-embedding clearly outperform them on all the datasets. We observe that, as in our previous work<cite> [4]</cite> , additional input produced by tv-embeddings led to substantial improvements.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_13",
  "x": "Reducing the number of tv-embeddings from four to two also reduces the number of parameters with a small degradation of accuracy ('w/ 2 tv (100-dim)' in Table 2 ). ---------------------------------- **SUMMARY OF THE RESULTS** \u2022 The shallow word-CNNs as in [3,<cite> 4]</cite> generally achieved better error rates than those of the very deep char-CNNs reported in [1] . \u2022 The shallow word-CNN computes much faster than the very deep char-CNN.",
  "y": "similarities"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_0",
  "x": "However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art.",
  "y": "background"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_1",
  "x": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_0",
  "x": "**INTRODUCTION** Referring expression generation is classically considered to be the problem of producing a single noun phrase that uniquely identifies a referent (Krahmer and van Deemter, 2012) . This approach is well suited for non-interactive, static contexts, but recently, there has been increased interest in generation for situated dialog<cite> (Stoia, 2007</cite>; . Most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee. Thus the result is often not a single noun phrase but a sequence of installments (Clark and Wilkes-Gibbs, 1986) , consisting of multiple utterances which may be interleaved with feedback from the addressee.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_1",
  "x": "While computational models of this behavior are still scarce, some first steps have been taken. <cite>Stoia (2007)</cite> studies instruction giving in a virtual environment and finds that references to target objects are often not made when they first become visible. Instead interaction partners are navigated to a spot from where an easier description is possible. develop a planning-based approach of this behavior. But once their system decides to generate a referring expression, it is delivered in one unit. Thompson (2009) , on the other hand, proposes a game-theoretic model to predict how noun phrases are split up into installments. While Thompson did not specify how the necessary parameters to calculate the utility of an utterance are derived from the context and did not implement the model, it provides a good theoretical basis for an implementation.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_2",
  "x": "In (3) and (4) a first reference utterance is followed by a separate object manipulation utterance. While in (3) the first reference uniquely identifies the target, in (4) the first utterance simply directs the player's attention to a group of buttons. The second utterance then picks out the target. <cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_3",
  "x": "The second utterance then picks out the target. <cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data. For instance in (5), the IF is asked to turn to directly face the group of buttons containing the target. (5) also shows how IGs monitor their partners' actions and respond to them.",
  "y": "similarities"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_4",
  "x": "This poses interesting new questions for NLG systems, which we have illustrated by discussing the patterns of utterance sequences that IGs and IFs use in our corpus to agree on the objects that need to be manipulated. In line with results from psycholinguistics, we found that the information necessary to establish a reference is often expressed in multiple installments and that IGs carefully monitor how their partners react to their instructions and quickly respond by giving feedback, repeating information or elaborating on previous utterance when necessary. The NLG system thus needs to be able to decide when a complete identifying description can be given in one utterance and when a description in installments is more effective. <cite>Stoia (2007)</cite> as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression. They do not consider cases in which another type of utterance, for instance, one that refers to a group of objects or gives an initial ambiguous description, is used to draw the attention of the IF to a particular area and they do not generate referring expressions in installments.",
  "y": "differences"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_5",
  "x": "It then has to decide how to formulate this feedback. The addressee, e.g., needs to be able to distinguish elaborations from corrections. If the feedback was inserted in the middle of a sentence, if finally has to decide whether this sentence should be completed and how the remainder may have to be adapted. Once we have finished the corpus collection, we plan to use it to study and address the questions discussed above. We are planning on building on the work by <cite>Stoia (2007)</cite> on using machine learning techniques to develop a model that takes into account various contextual factors and on the work by Thompson (2009) on generating references in installments.",
  "y": "future_work"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_0",
  "x": "With the fast advances of deep learning technologies, converting the well matured multi-module speech recognition processes [1] into a single speech-to-text model [2] becomes highly attractive. Such end-to-end speech recognition approaches are primarily based on two distinct models: connectionist temporal classification (CTC) [3, 4, 5] and sequenceto-sequence (Seq2seq) [6, 7, 8] models. By introducing an additional blank symbol and a specially defined loss function aggregating many allowed paths within a graph, CTC model can be optimized to generate the correct character sequences from the speech signals regardless of the blank symbols interspersed among. The seq2seq models, on the other hand, simply maximized the likelihood of observing the decoded sequence given the ground truth at every time step. With many recent results [9, 10, 11, 12, <cite>13]</cite> approaching the stateof-the-art, end-to-end deep learning has definitely been a very important direction for speech recognition.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_1",
  "x": "The two steps here are conducted iteratively: (a) a Criticizing Language Model (CLM) is trained to evaluate the quality score given a text sequence, and (b) and ASR model is trained to minimize the sequence loss calculated with ground truth while maximizing the scores given by CLM. 16, 17] have been developed to address such problem by involving unpaired text data (which are relatively easy to obtain) in the training progress. One approach is to utilize unpaired text data to produce a separately trained language model (LM) to rescore the output of the end-to-end approach [18,<cite> 13,</cite> 19, 20] , but at the price of extra computation during testing. Also, in this way the unpaired text data and paired audio-text data were used separately, and the machines could not learn from them jointly. Another approach is to back-translate (synthesize) speech signals or encoder state sequences [17, 21, 22] from the unpaired text data, so they can be jointly learned in training.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_2",
  "x": "Also, CNN based network is relatively more computationally efficient, which is important in adversarial training. But other network architectures such as RNN-LM<cite> [13]</cite> can also be used here. Loss Function. A major problem here is that soft distribution vectors produced by the ASR model is very different from one-hot vectors for real text data, making the task of CLM trivial, and the ASR model almost always fail to compete against it. Thanks to Wasserstein GAN (WGAN) [24] which addressed the above problem to some good extent.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_3",
  "x": "Network Architecture. Any network architecture for end-toend speech recognition can be used here, while Fig. 3 gives the one used in this work, following the previous work<cite> [13]</cite> of integrating attentioned Seq2seq with CTC. The model takes a sequence of speech features O = o 1 , o 2 , ..., o N with length N as the input. O is encoded into sequence of hidden state H = h 1 , h 2 , ..., h T by the encoder (consists of a VGG extractor performing input downsampling followed by several BLSTM layers) with T being the output sequence length. The decoder is a single layer LSTM maintaining its own hidden state q. For each time index t, location-aware attention mechanism [7] Attention is used to integrate H with the previous decoder state q t\u22121 to generate the context vector c t .",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_4",
  "x": "The ASR model outputs two character sequences,\u1ef9 =\u1ef9 1 ,\u1ef9 2 , ...,\u1ef9 T andy =y 1 ,y 2 , ...,y T , respectively supervised by Seq2seq loss and CTC loss. CLM only takes\u1ef9 as input. During testing,\u1ef9 andy are integrated into a single output sequence just as done in the previous work<cite> [13]</cite> . Seq2seq Loss. The Seq2seq ASR model is to estimate the posterior probability,",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_5",
  "x": "We took the text of other 360 hours of clean speech and 500 hours of noisy speech and utilized them as the unpaired data (text-only). The clean development set and clean test set were used for evaluation. We used the end-to-end speech processing toolkit ESPnet [28] for data preprocessing and customized it for our adversarial training processes. We followed the previous work<cite> [13,</cite> 21] to use 80-dimensional log Mel-filter bank and 3-dimensional pitch features as the acoustic features. Text data are represented by sequences of 5000 subword units one-hot vectors to avoid OOV.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_6",
  "x": "For the CLM model, the dimension of the output of all layers were set to 128 except the last. The first convolution had a window size of 2 and stride of 1, and the second had window size 3 and stride 1. Batch normalization is applied between layers. For the ASR model, the encoder included a 6layer VGG extractor with downsampling used in the previous work<cite> [13]</cite> and a 5-layer BLSTM with 320 units per direction. 300-dimensional location-aware attention [7] was used in the attention layer.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_7",
  "x": "Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM<cite> [13]</cite> , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] . ---------------------------------- **EXPERIMENTAL RESULTS** In the experiments, the ASR model was trained on the 100 hours speech data but combined with different amount of unpaired text utilized in different ways. The results are listed in Table 1 , where \"Baseline\" refers to the plain end-toend speech recognition framework as described in Sec. 2.3, \"+LM\" refers to the shallow fusion decoding with a separately trained RNN language model (RNN-LM)<cite> [13,</cite> 20] and \"+AT\" refers to the adversarial training proposed here.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_8",
  "x": "Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM<cite> [13]</cite> , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] . ---------------------------------- **EXPERIMENTAL RESULTS** In the experiments, the ASR model was trained on the 100 hours speech data but combined with different amount of unpaired text utilized in different ways. The results are listed in Table 1 , where \"Baseline\" refers to the plain end-toend speech recognition framework as described in Sec. 2.3, \"+LM\" refers to the shallow fusion decoding with a separately trained RNN language model (RNN-LM)<cite> [13,</cite> 20] and \"+AT\" refers to the adversarial training proposed here.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_0",
  "x": "These data, by their nature (e.g., raw data or long time-series data), are not easily usable by humans; outlining their crucial need to be synthesized. Recently, numerous works have focused on leveraging structured data in various applications, such as question answering [24, 34] or table retrieval [7, 32] . One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity. This field is referred to as \"data-to-text\" [8] and has its place in several application domains (such as journalism [22] or medical diagnosis [25] ) or wide-audience applications (such as financial [26] and weather reports [30] , or sport broadcasting [4, <cite>39]</cite> ). As an example, Figure 1 shows a data-structure containing statistics on NBA basketball games, paired with its corresponding journalistic description.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_1",
  "x": "Recent datato-text models [18, 28, 29, <cite>39]</cite> mostly rely on an encoder-decoder architecture [2] in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder. Then, a decoder generates words conditioned on this representation. With the introduction of the attention mechanism [19] on one hand, which computes a context focused on important elements from the input at each decoding step and, on the other hand, the copy mechanism Fig. 1 : Example of structured data from the RotoWire dataset. Rows are entities (either a team or a player) and each cell a record, its key being the column label and its value the cell content. Factual mentions from the table are boldfaced in the description.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_2",
  "x": "This way of encoding unordered sequences (i.e. collections of entities) implicitly assumes an arbitrary order within the collection which, as demonstrated by Vinyals et al. [37] , significantly impacts the learning performance. To address these shortcomings, we propose a new structured-data encoder assuming that structures should be hierarchically captured. Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in [28, <cite>39]</cite> . Our contribution is threefold: -We model the general structure of the data using a two-level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities; -We introduce the Transformer encoder [36] in data-to-text models to ensure robust encoding of each element/entities in comparison to all others, no matter their initial positioning; -We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder.",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_3",
  "x": "We report experiments on the RotoWire benchmark<cite> [39]</cite> which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. In the following, we first present a state-of-the art of data-to-text literature (Section 2), and then describe our proposed hierarchical data encoder (Section 3). The evaluation protocol is presented in Section 4, followed by the results (Section 5).",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_4",
  "x": "Modern approaches to the wide range of tasks based on structured-data (e.g. table retrieval [7, 41] , table classification [9] , question answering [12] ) now propose to leverage progress in deep learning to represent these data into a semantic vector space (also called embedding space). In parallel, an emerging task, called \"data-to-text\", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work [1, 15, <cite>39]</cite> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements. To improve these models, a number of work [16, 28, 40] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_5",
  "x": "In parallel, an emerging task, called \"data-to-text\", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work [1, 15, <cite>39]</cite> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements. To improve these models, a number of work [16, 28, 40] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. For example, Puduppully et al. [28] propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_6",
  "x": "These approaches are however designed for single-entity data structures and do not account for delimitation between entities. Our contribution differs from previous work in several aspects. First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence [18, 28, <cite>39]</cite> , we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process. Second, unlike all works in the domain, we exploit the Transformer architecture [36] and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering. Finally, in contrast to [5, 29] that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities, we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention.",
  "y": "differences"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_7",
  "x": "For each data-structure s in D, the objective function aims to generate a description\u0177 as close as possible to the ground truth y. This objective function optimizes the following log-likelihood over the whole dataset D: where \u03b8 stands for the model parameters and P (\u0177 = y | s; \u03b8) the probability of the model to generate the adequate description y for table s. During inference, we generate the sequence\u0177 * with the maximum a posteriori probability conditioned on table s. Using the chain rule, we get: This equation is intractable in practice, we approximate a solution using beam search, as in [18, 17, 28, 29, <cite>39]</cite> . Our model follows the encoder-decoder architecture [2] . Because our contribution focuses on the encoding process, we chose the decoding module used in [28, <cite>39]</cite> : a two-layers LSTM network with a copy mechanism.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_8",
  "x": "where \u03b8 stands for the model parameters and P (\u0177 = y | s; \u03b8) the probability of the model to generate the adequate description y for table s. During inference, we generate the sequence\u0177 * with the maximum a posteriori probability conditioned on table s. Using the chain rule, we get: This equation is intractable in practice, we approximate a solution using beam search, as in [18, 17, 28, 29, <cite>39]</cite> . Our model follows the encoder-decoder architecture [2] . Because our contribution focuses on the encoding process, we chose the decoding module used in [28, <cite>39]</cite> : a two-layers LSTM network with a copy mechanism. In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_9",
  "x": "**HIERARCHICAL ENCODING MODEL** As outlined in Section 2, most previous work [16, 28, 29,<cite> 39,</cite> 40 ] make use of flat encoders that do not exploit the data structure. To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in [18, 28, <cite>39]</cite> .",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_10",
  "x": "To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in [18, 28, <cite>39]</cite> . We present in what follows the record embedding layer and introduce our two hierarchical modules. Record Embedding Layer.",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_11",
  "x": "As in previous work [18, 28, <cite>39]</cite> , each record embedding r i,j is computed by a linear projection on the concatenation [k i,j ; v i,j ] followed by a non linearity: where W r \u2208 R 2d\u00d7d and b r \u2208 R d are learnt parameters. The low-level encoder aims at encoding a collection of records belonging to the same entity while the high-level encoder encodes the whole set of entities. Both the low-level and high-level encoders consider their input elements as unordered. We use the Transformer architecture from [36] .",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_12",
  "x": "**THE ROTOWIRE DATASET** To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the Ro-toWire dataset<cite> [39]</cite> . It includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of Figure 1 . The descriptions are professionally written and average 337 words with a vocabulary size of 11.3K. There are 39 different record keys, and the average number of records (resp. entities) in a single data-structure is 628 (resp. 28).",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_13",
  "x": "The second category designed by<cite> [39]</cite> is more qualitative. BLEU Score. The BLEU score [23] is commonly used as an evaluation metric in text generation tasks. It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams (n \u2208 1, 2, 3, 4) between the generated candidate and the ground truth. We use the implementation code released by [27] .",
  "y": "background uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_14",
  "x": "To do so, we follow the protocol presented in<cite> [39]</cite> . First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE tool will extract the pair (Isaiah Thomas, 23, PTS). Second, we compute three metrics on the extracted information:",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_15",
  "x": "\u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism. \u2022 Li [16] is a standard encoder-decoder with a delayed copy mechanism: text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network. \u2022 Puduppully-plan [28] acts in two steps: a first standard encoder-decoder generates a plan, i.e. a list of salient records from the table; a second standard encoder-decoder generates text from this plan. \u2022 Puduppully-updt [29] . It consists in a standard encoder-decoder, with an added module aimed at updating record representations during the generation process.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_16",
  "x": "---------------------------------- **BASELINES** We compare our hierarchical model against three systems. For each of them, we report the results of the best performing models presented in each paper. \u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism.",
  "y": "background uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_17",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** The decoder is the one used in [28, 29, <cite>39]</cite> with the same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset<cite> (39)</cite> , their embedding size is fixed to 20.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_18",
  "x": "For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset<cite> (39)</cite> , their embedding size is fixed to 20. The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300. We use dropout at rate 0.5. The models are trained with a batch size of 64.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_19",
  "x": "Our results on the RotoWire testset are summarized in Table 1 . For each proposed variant of our architecture, we report the mean score over ten runs, as well as the standard deviation in subscript. Results are compared to baselines [28, 29, <cite>39]</cite> and variants of our models. We also report the result of the oracle (metrics on the gold descriptions). Please note that gold descriptions trivially obtain 100% on all metrics expect RG, as they are all based on comparison with themselves.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_20",
  "x": "We show in Figure  4 a text generated by our best model, which can be directly compared to the gold description in Figure 1 . Generation is fluent and contains domain-specific expressions. As reflected in Table 1 , the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work [16, 28, 29, <cite>39]</cite> , generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. \"[...] he's now averaging 22 points [...].\"). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_0",
  "x": "---------------------------------- **INTRODUCTION** Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> . Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008) . We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_1",
  "x": "---------------------------------- **INTRODUCTION** Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> . Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008) . We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.",
  "y": "background uses"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_2",
  "x": "2 Following<cite> Pad\u00f3 and Lapata (2007)</cite> , we only consider co-occurrences where two target words are connected by certain dependency paths, namely: the top 30 most frequent preposition-mediated noun-to-noun paths (soldier+with+gun), the top 50 transitive-verbmediated noun-to-noun paths (soldier+use+gun), the top 30 direct or preposition-mediated verbnoun paths (kill+obj+victim, kill+in+school), and the modifying and predicative adjective-to-noun paths. Pairs (w 1 , w 2 ) that account for 0.01% or less of the marginal frequency of w 1 were trimmed. The resulting tuple list, with raw counts converted to mutual information scores, contains about 25 million tuples. To test how well graph-based and alternative methods \"scale down\" to smaller corpora, we sampled random subsets of tuples corresponding to 0.1%, 1%, 10%, and 100% of the full list. To put things into perspective, the full list was extracted from a corpus of about 2 billion words; so, the 10% list is on the order of magnitude of the BNC, and the 0.1% list is on the order of magnitude of the Brown corpus.",
  "y": "uses"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_3",
  "x": "The difference between SVD and pure-vector models is negligible and they both obtain the best performance in terms of both cluster entropy (not shown in the table) and purity. Both models' performances are comparable with the previously reported studies, and above that of random walks. Semantic priming: The next dataset comes from Hodgson (1991) and it is of interest since it requires capturing different forms of semantic relatedness between prime-target pairs: synonyms (synonym), coordinates (coord), antonyms (antonym), free association pairs (conass), superand subordinate pairs (supersub) and phrasal associates (phrasacc). Following previous simulations of this data-set<cite> (Pad\u00f3 and Lapata, 2007)</cite> , we measure the similarity of each related target-prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target-prime pair. We report results in terms of differences between unrelated and related pairs, normalized to t-scores, marking significance according to twotailed paired t-tests for the relevant degrees of freedom.",
  "y": "uses"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_0",
  "x": "Language models can be optimized to recognize syntax and semantics with great accuracy [1] . However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input <cite>[2]</cite> . While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling [5] have shown improvements, it does not solve the underlying problem. In creative text generation, the objective is not strongly bound to the ground truth-instead the objective is to generate diverse, unique or original samples. We attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens.",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_1",
  "x": "Our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine-tuning objectiveless decoding tasks like that of creative text. We show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics. Previous work on handling the shortcomings of MLE include length-normalizing sentence probability [6] , future cost estimation [7] , diversity-boosting objective function [8,<cite> 2]</cite> or penalizing repeating tokens [9] . When it comes to poetry generation using generative text models, Zhang and Lapata [10] , Yi et al. [11] and Wang et al. [12] use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding.",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_2",
  "x": "However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN [13] uses a Reinforcement Learning signal from the discriminator, FMD-GAN [14] uses an optimal transport mechanism as an objective function. GumbelGAN [15] uses Gumbel-Softmax distribution that replaces the non-differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients. Li et al. <cite>[2]</cite> use a discriminator for a diversity promoting objective. Yu et al. [16] use SeqGAN to generate poetry and comment on the performance of SeqGAN over MLE in human evaluations, encouraging our study of GANs for creative text generation.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_0",
  "x": "Topic models have proven to be an elegant way to build exploratory interfaces (i.e. topic browsers) for visualizing document collections by presenting to the users lists of topics [6, 15, 14] where they select documents of a particular topic of interest. A topic is traditionally represented by a list of t terms with the highest probability. In recent works, short phrases [11, 4] , images<cite> [3]</cite> or summaries [19] have been used as alternatives. Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] .",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_1",
  "x": "Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] . The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic. In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_2",
  "x": "The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic. In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes. The edges are weighted with a similarity score between the images that connect. Then, an image is selected by re-ranking the candidates using PageRank.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_3",
  "x": "That is the output of the publicly available 16-layer VGG-net [1<cite>3]</cite> trained over the ImageNet dataset [9] . VGG-net provides a 1000 dimensional vector which is the soft-max classification output of ImageNet classes. The input to the network is the concatenation of topic, caption and visual vectors. i.e., This results in a 1600-dimensional input vector.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_4",
  "x": "We evaluate our model on the publicly available data set provided by<cite> [3]</cite> . It consists of 300 topics generated using Wikipedia articles and news articles taken from the New York Times. Each topic is represented by ten terms with the highest probability. They are also associated with 20 candidate image labels and their human ratings between 0 (lowest) and 3 (highest) denoting the appropriateness of these images for the topic. That results into a total of 6K images and their associated textual metadata which are considered as captions.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_5",
  "x": "The 20 candidate image labels per topic are collected by<cite> [3]</cite> using an information retrieval engine (Google). Hence most of them are expected to be relevant to the topic. This jeopardizes the training of our supervised model due to the lack of sufficient negative examples. To address this issue we generate extra negative examples. For each topic we sample another 20 images from random topics in the training set and assign them a relevance score of 0.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_6",
  "x": "This jeopardizes the training of our supervised model due to the lack of sufficient negative examples. To address this issue we generate extra negative examples. For each topic we sample another 20 images from random topics in the training set and assign them a relevance score of 0. These extra images are added into the training data. Our evaluation follows prior work [11, <cite>3]</cite> using two metrics.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_7",
  "x": "**RESULTS AND DISCUSSION** We compare our approach to the state-of-the-art method that uses Personalized PageRank<cite> [3]</cite> to re-rank image candidates (Local PPR) and an adapted version that computes the PageRank scores of all the available images in the test set (Global PPR). We also test other baselines methods: (1) a relevant approach originally proposed for image annotation that learns a joint model of text and image features (WSABIE) [20] , (2) linear regression and SVM models that use the concatenation of the topic, the caption and the image vectors as input, LR (Topic+Caption+VGG) and SVM (Topic+Caption+VGG) respectively. Finally, we test two versions of our own DNN using only either the caption (DNN (Topic+Caption)) or the visual information of the image (DNN (Topic+VGG)). Table 1 shows the Top-1 average and nDCG scores obtained.",
  "y": "uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_1",
  "x": "tackle the KBQA problem <cite>(Liang et al., 2016</cite>; Hao et al., 2017; Lukovnikov et al., 2017; Sorokin & Gurevych, 2017) . We study the application of the Neural Machine Translation paradigm for question parsing using a sequence-to-sequence model within an architecture dubbed Neural SPARQL Machine, previously introduced in Soru et al. (2017) . Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. Although query induction can save a considerable amount of supervision effort <cite>(Liang et al., 2016</cite>; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused).",
  "y": "similarities uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_2",
  "x": "Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. Although query induction can save a considerable amount of supervision effort <cite>(Liang et al., 2016</cite>; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused). On the contrary, our proposed solution relies on manual annotation and a weakly-supervised expansion of question-query templates. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_3",
  "x": "External pre-trained word embeddings help deal with vocabulary mismatch. Knowledge graph jointly embedded with SPARQL operators (Wang et al., 2014) can be utilized in the target space. A curriculum learning (Bengio et al., 2009 ) paradigm can learn graph pattern and SPARQL operator composition, in a similar fashion of<cite> Liang et al. (2016)</cite> . We argue that the coverage of language utterances can be expanded using techniques such as Question (Abujabal et al., 2017; Elsahar et al., 2018; Abujabal et al., 2018) and Query Generation (Zafar et al., 2018) as well as Universal Sentence Encoders (Cer et al., 2018 ). Another problem is the disambiguation between entities having the same surface forms.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_0",
  "x": "Most of the presented works study the interrelationship between words in a text snip- pet (Hill et al., 2016; Kiros et al., 2015; <cite>Le and Mikolov, 2014)</cite> in an unsupervised fashion. Other methods build a task specific representation (Kim, 2014; Collobert et al., 2011) . In this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document. We call our representation DoCoV descriptor. Our descriptor obtains a fixed-length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_1",
  "x": "Our descriptor obtains a fixed-length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements. This makes our work distinguished from to the work of<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet. ---------------------------------- **TOY EXAMPLE** We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector<cite> (Le and Mikolov, 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_2",
  "x": "This makes our work distinguished from to the work of<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet. ---------------------------------- **TOY EXAMPLE** We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector<cite> (Le and Mikolov, 2014)</cite> . First, we used Gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus.",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_3",
  "x": "On the bottom right we show the corresponding covariance matrices as points in a new space after vectorization step. ---------------------------------- **MOTIVATION AND CONTRIBUTIONS** Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , FastSent (Hill et al., 2016) use a shared space between the words and paragraphs.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_4",
  "x": "Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , FastSent (Hill et al., 2016) use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words. Figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> are positioned in the space as in figure 1 . (2) The covariance matrix represents the second order summary statistic of multivariate data.",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_5",
  "x": "(3) The use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision (Tuzel et al., 2006; Hussein et al., 2013; Sharaf et al., 2015) and brain signal analysis (Barachant et al., 2013) . With this global success of this representation, we believe this method can be useful for text-related tasks. (4) The computation of the covariance descriptor is known to be fast and highly parallelizable. Moreover, there is no inference steps involved while computing the covariance matrix given its observations. This is an advantage compared to existing methods for generating paragraph vectors, such as<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_6",
  "x": "Among the approaches of finding word embedding are (Pennington et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013b) . These alternatives share the same objective of finding a fixed-length vectorized representation for words to capture the semantic and syntactic regularities between words. These efforts paved the way for many researchers to judge document similarity based on word embedding. Some efforts aimed at finding a global representation of a text snippet using a paragraph-level representation such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> . Recently other neural-based sentence and paragraph level representations appeared to provide a fixed length representation like Skip-Thought Vectors (Kiros et al., 2015) and FastSent (Hill et al., 2016) .",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_7",
  "x": "---------------------------------- **DATASETS AND BASELINES** We contrast our results against the methods reported in (Hill et al., 2016) . The competing methods are the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , skip-thought vectors (Kiros et al., 2015) , Fastsent (Hill et al., 2016) , Sequential (Denoising) Autoencoders (SDAE) (Hill et al., 2016) . The Mean vector baseline is also implemented.",
  "y": "similarities uses"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_8",
  "x": "We show the correlation values between the similarities computed via DoCoV and the human judgements. We contrast the performance of other representations in table 2. We observe that DoCoV representation outperforms other representations in this task. Other models such as skipthought vectors (Kiros et al., 2015) and SDAE (Hill et al., 2016) requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors<cite> (Le and Mikolov, 2014)</cite> and Fastsent vectors (Hill et al., 2016) , they require a gradient descent inference step to compute the paragraph/sentence vectors.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_9",
  "x": "This means each feature is capturing different discriminating information. This justifies the choice of concatenating DoCoV with other features. We further observe that DoCoV is consistently better than the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , Fastsent and SDAE (Hill et al., 2016) . The overall accuracy of DoCoV is highlighted and it outperforms other methods on the text classification benchmark. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_0",
  "x": "We investigate whether such models learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results. ---------------------------------- **INTRODUCTION** Recent work has shown that state-of-the-art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_1",
  "x": "**BACKGROUND** The recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various NLP tasks, such as language modeling or next word prediction (Tsvetkov et al., 2016; \u00d6stling and Tiedemann, 2017; Malaviya et al., 2017; Tiedemann, 2018) , translation (Dong et al., 2015; Zoph et al., 2016; Firat et al., 2016; <cite>Johnson et al., 2017)</cite> , morphological reinflection (Kann et al., 2017) and more (Bjerva, 2017) . A practical benefit of training models multilingually is to transfer knowledge from high-resource languages to lowresource ones and improve task performance in the latter. Here we aim at understanding how linguistic knowledge is transferred among languages, specifically at the syntactic level, which to our knowledge has not been studied so far. Assessing the syntactic abilities of monolingual neural LMs trained without explicit supervision has been the focus of several recent studies: Linzen et al. (2016) analyzed the performance of LSTM LMs at an English subject-verb agreement task, while Gulordava et al. (2018) extended the analysis to various long-range agreement patterns in different languages.",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_2",
  "x": "For instance, through a crosslingual syntactic priming experiment, Hartsuiker et al. (2004) showed that bilinguals recently exposed to a given syntactic construction (passive voice) in their L1 tend to reuse the same construction in their L2. While the neural networks in this study are not designed to be plausible models of the human mind learning and processing multiple languages, we believe there is interesting potential at the intersection of these research fields. ---------------------------------- **EXPERIMENT** We consider the scenario where L1 is overresourced compared to L2 and train our bilingual models by joint training on a mixed L1/L2 corpus so that supervision is provided simultaneously in the two languages (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "f5bf9a833c3d46b00d70498e4f1c1b_1",
  "x": "Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter<cite> [5]</cite> . ---------------------------------- **METHOD** Our method predicts the language for a tweet t by combining scores from a content model and a graph model that takes social context into account, as per Equation 1: Where \u03b8content are the content model parameters, \u03b8 social the social model parameters.",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_0",
  "x": "****FRUSTRATINGLY EASY SEMI-SUPERVISED DOMAIN ADAPTATION**** **ABSTRACT** In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) <cite>(Daum\u00e9 III, 2007)</cite> . Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target. This semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner.",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_1",
  "x": "Experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method. ---------------------------------- **INTRODUCTION** A domain adaptation approach for sequential labeling tasks in NLP was proposed in <cite>(Daum\u00e9 III, 2007)</cite> . The proposed approach, termed EASYADAPT (EA), augments the source domain feature space using features from labeled data in target domain.",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_2",
  "x": "We have a set of source labeled examples L s (\u223c D s (x, y)) and a set of target labeled examples We also have target unlabeled data denoted by U t (\u223c D t (x)), where |U t | = u t . Our goal is to learn a hypothesis h : X \u2192 Y having low expected error with respect to the target domain. In this paper, we consider linear hypotheses only. However, the proposed techniques extend to non-linear hypotheses, as mentioned in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_3",
  "x": "Similarly, the corresponding expected errors are denoted by \u01eb s (h, f s ) and \u01eb t (h, f t ). Shorthand notions of\u01eb s ,\u01eb t , \u01eb s and \u01eb t have also been used. ---------------------------------- **EASYADAPT (EA)** In this section, we give a brief overview of EASYADAPT proposed in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_4",
  "x": "One of the most appealing properties of EASYADAPT is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space. Almost any standard supervised learning approach for linear classifiers (for e.g., SVMs, perceptrons) can be used to learn a linear hypothesish \u2208 R 3d in the augmented space. As mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended <cite>(Daum\u00e9 III, 2007)</cite> to non-linear hypotheses. Let us denot\u0207 h = h c , h s , h t , where each of h c , h s , h t is of dimension d and represent the common, sourcespecific and target-specific components ofh, respectively. During prediction on target data, the incoming target feature x is transformed to obtain \u03a6 t (x) andh is applied on this transformed feature.",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_5",
  "x": "During prediction on target data, the incoming target feature x is transformed to obtain \u03a6 t (x) andh is applied on this transformed feature. This is equivalent to applying (h c + h t ) on x. A good intuitive insight into why this simple algorithm works so well in practice and outperforms most state-of-the-art algorithms is given in <cite>(Daum\u00e9 III, 2007)</cite> . Briefly, it can be thought to be simultaneously training two hypotheses: w s = (h c + h s ) for source domain and w t = (h c + g t ) for target domain. The commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t , respectively.",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_6",
  "x": "This technique can be easily extended to a multi-domain scenario by making more copies of the original feature space ((K + 1) copies in case of K domains). A kernelized version of the algorithm has also been presented in <cite>(Daum\u00e9 III, 2007)</cite> . ---------------------------------- **USING UNLABELED DATA** As discussed in the previous section, the EASYADAPT algorithm is attractive because it performs very well empirically and can be used in conjunction with any underlying supervised clas-sifier.",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_7",
  "x": "---------------------------------- **SETUP** We follow the same experimental setup used in <cite>(Daum\u00e9 III, 2007)</cite> and perform two sequence labelling tasks (a) named-entity-recognition (NER), and (b) part-of-speech-tagging (POS )on the following datasets: PubMed-POS: Introduced by (Blitzer et al., 2006) , this dataset consists of two domains. task is to perform part-of-speech tagging on unlabeled PubMed abstracts with a classifier trained on labeled WSJ and PubMed data.",
  "y": "similarities uses"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_8",
  "x": "The labeled and unlabeled case start together but with increase in number of samples their gap increases with the unlabeled case resulting in much lower error as compared to the labeled case. Similar trends were observed in other data sets as can be seen in Figure 3(b) . We also note that EA performs poorly for some cases, as was shown <cite>(Daum\u00e9 III, 2007)</cite> earlier. ---------------------------------- **SUMMARY**",
  "y": "similarities"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_9",
  "x": "**FUTURE WORK** In both EA and EA++, we use features from source and target space to construct an augmented feature space. In other words, we are sharing features across source and target labeled data. We term such algorithms as Feature Sharing Algorithms. Feature sharing algorithms are effective for domain adaptation because they are simple, easy to implement as a preprocessing step and outperform many existing state-of-the-art techniques (shown previously for domain adaptation <cite>(Daum\u00e9 III, 2007)</cite> ).",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_0",
  "x": "Subsequent work has proposed alternative optimization objectives to learn better mappings. <cite>Xing et al. (2015)</cite> incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection. Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space. Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language. Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b) , both through the neighbor retrieval method and the training itself .",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_1",
  "x": "In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; <cite>Xing et al., 2015)</cite> . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_2",
  "x": "This last optimization objective coincides with <cite>Xing et al. (2015)</cite> , but <cite>their work</cite> was motivated by an hypothetical inconsistency in Mikolov et al. (2013b) , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by <cite>Xing et al. (2015)</cite> is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to <cite>Xing et al. (2015)</cite> , <cite>who introduce</cite> orthogonality only to ensure that unit length is preserved after mapping. ---------------------------------- **MEAN CENTERING FOR MAXIMUM COVARIANCE**",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_3",
  "x": "In order to speed up the experiments, we follow the authors and perform an approximate evaluation by reducing the vocabulary size according to a frequency threshold of 30,000 (Mikolov et al., 2013a) . Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves. With these settings, we obtain a coverage of 64.98%. We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 . The code for Mikolov et al. (2013b) and <cite>Xing et al. (2015)</cite> is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of <cite>Xing et al. (2015)</cite> (postprocessing instead of constrained training).",
  "y": "uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_4",
  "x": "As discussed before, (Mikolov et al., 2013b) and <cite>(Xing et al., 2015)</cite> were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively. ---------------------------------- **COMPARISON TO OTHER WORK** As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_5",
  "x": "As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings. Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by <cite>Xing et al. (2015)</cite> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality. In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_6",
  "x": "As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings. Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by <cite>Xing et al. (2015)</cite> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality. In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task.",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_7",
  "x": "---------------------------------- **CONCLUSIONS** This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of <cite>Xing et al. (2015)</cite> and Faruqui and Dyer (2014) . It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.",
  "y": "extends differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_0",
  "x": "****MAPPING DISTRIBUTIONAL TO MODEL-THEORETIC SEMANTIC SPACES: A BASELINE**** **ABSTRACT** Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. <cite>(Herbelot and Vecchi, 2015)</cite> explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_1",
  "x": "Word embeddings are one of the main components in many state-of-the-art systems for natural language processing (NLP), such as language modeling (Mikolov et al., 2010 ), text classification (Socher et al., 2013; Kim, 2014; Blunsom et al., 2014; , question answering (Weston et al., 2015; Wang and Nyberg, 2015) , machine translation (Bahdanau et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014) , as well as named entity recognition (Collobert et al., 2011; Lample et al., 2016; Labeau et al., 2015) . Word embeddings can be pre-trained using large unlabeled datasets typically based on token cooccurrences (Mikolov et al., 2013; Collobert et al., 2011; Pennington et al., 2014) . They can also be jointly learned with the task. Understanding what information word embeddings contain is subsequently of high interest. <cite>(Herbelot and Vecchi, 2015)</cite> investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper.",
  "y": "background uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_2",
  "x": "**TASK** In this section, we summarize the task presented in <cite>(Herbelot and Vecchi, 2015)</cite> . The following is an example of a concept along with some of its features, as formatted in one of the two datasets used to evaluate the model:   yam  a vegetable  all  all  all  yam  eaten by cooking  all  most most  yam grows in the ground  all  all  all  yam  is edible  all  most  all  yam  is orange  some most most  yam  like a potato  all  all  all The concept yam has six features (a vegetable, eaten by cooking, grows in the ground, is edible, is orange, and like a potato). Each feature in this dataset is annotated by three different humans. The annotation is a quantifier that reflects how frequently the concept has a feature.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_3",
  "x": "---------------------------------- **MODEL** In the previous section, we have seen how to convert a concept into a model-theoretic vector based on human annotations. The goal of <cite>(Herbelot and Vecchi, 2015)</cite> is to analyze whether there exists a transformation from the word embedding of a concept to its model-theoretic vector, the gold standard being the human annotations. The word embeddings are taken from the word embeddings pre-trained with word2vec GoogleNews-vectors-negative300 1 (300 dimensions), which were trained on part of the Google News dataset, consisting of approximately 100 billion words.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_4",
  "x": "The word embeddings are taken from the word embeddings pre-trained with word2vec GoogleNews-vectors-negative300 1 (300 dimensions), which were trained on part of the Google News dataset, consisting of approximately 100 billion words. The transformation used in <cite>(Herbelot and Vecchi, 2015)</cite> is based on Partial Least Squares Regression (PLSR). The PLSR is fitted on the training set: the inputs are the word embeddings for each concept, and the outputs are the model-theoretic vectors for each concept. To assess the quality of the predictions, the Spearman rank-order correlation coefficient is computed between the predictions and the gold modeltheoretic vectors, ignoring all features for which a concept has not been annotated. The idea is that some of the features might be present but not given as options during annotation.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_5",
  "x": "The method should therefore not be penalized for not suggesting them. Figure 1 illustrates the model. <cite>(Herbelot and Vecchi, 2015)</cite> 's system. The word embedding of a concept is transformed to a modeltheoric vector via a PLSR. The quality of the predicted model-theoric vector is assessed with the Spearman rank-order correlation coefficient between the predictions and the gold model-theoretic vectors.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_6",
  "x": "---------------------------------- **EXPERIMENTS** We compare <cite>(Herbelot and Vecchi, 2015)</cite> 's model (<cite>PLSR + word2vec</cite>) against three baselines: random vectors, mode, and nearest neighbor. \u2022 Mode: A predictor that outputs, for each feature, the most common feature value (i.e., the mode) in the training set. For example, if a feature is annotated as all for most concepts, then the predictor will always output all for this feature.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_7",
  "x": "This is a simple nearest neighbor predictor. \u2022 Random vectors: <cite>(Herbelot and Vecchi, 2015)</cite> used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1). We also apply retrofitting (Faruqui et al., 2014) on the word embeddings in order to leverage relational information from semantic lexicons by encouraging linked words to have similar vector representations. Using (Faruqui et al., 2014 )'s retrofitting tool 2 , we retrofit the word embeddings (GoogleNews-vectorsnegative300) on each of the 4 datasets present in the retrofitting tool (framenet, ppdb-xl, wordnetsynonyms+, and wordnet-synonyms. <cite>(Herbelot and Vecchi, 2015)</cite> ) in the last row.",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_8",
  "x": "This is a simple nearest neighbor predictor. \u2022 Random vectors: <cite>(Herbelot and Vecchi, 2015)</cite> used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1). We also apply retrofitting (Faruqui et al., 2014) on the word embeddings in order to leverage relational information from semantic lexicons by encouraging linked words to have similar vector representations. Using (Faruqui et al., 2014 )'s retrofitting tool 2 , we retrofit the word embeddings (GoogleNews-vectorsnegative300) on each of the 4 datasets present in the retrofitting tool (framenet, ppdb-xl, wordnetsynonyms+, and wordnet-synonyms. <cite>(Herbelot and Vecchi, 2015)</cite> ) in the last row.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_9",
  "x": "**AD** 6 Results and discussion Table 1 presents the results, using the Spearman correlation as the performance metric. The experiment was coded in Python using scikit-learn (Pedregosa et al., 2011) and the source as well as the complete result log and the two datasets are available online 3 . We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_10",
  "x": "QMR has 3 potentially different annotations for each concept-feature pair, while AD has 3 only one annotation for each concept-feature pair: as a result, mode and true-mode have similar results for AD, but potentially different results for QMR. For each run, a train/test split was randomly chosen (60 training samples for AD, 400 for QMR, in order to have the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> 's Table 2 ). ---------------------------------- **AD** 6 Results and discussion Table 1 presents the results, using the Spearman correlation as the performance metric.",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_11",
  "x": "6 Results and discussion Table 1 presents the results, using the Spearman correlation as the performance metric. The experiment was coded in Python using scikit-learn (Pedregosa et al., 2011) and the source as well as the complete result log and the two datasets are available online 3 . We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> . For the AD dataset, our worst run achieved 0.435, and our best run achieved 0.713, which emphasizes the lack of robustness of the results with respect to the train/test split.",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_12",
  "x": "QMR dataset (min: 0.244; max: 0.407), which is expected since QMR is significantly larger than AD. Furthermore, the mode baseline yields results that are good on the AD dataset (0.554, vs. 0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our PLSR + word2vec implementation), and significantly better than all other models on the QMR dataset (0.522, vs. 0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> , i.e. +51% improvement). To get an intuition of why the mode baseline works well, Figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the AD dataset. A similar trend can be found in the QMR dataset. In the AD dataset, there are 54 features, each of them being annotated for all 73 concepts.",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_13",
  "x": "This is expected as applying retrofitting to word embeddings leverages relational information from semantic lexicons by encouraging linked words to have similar vector representations. ---------------------------------- **CONCLUSION** In this paper we have presented several baselines for mapping distributional to model-theoretic semantic spaces. The mode baseline significantly outperforms <cite>(Herbelot and Vecchi, 2015)</cite> 's model on the QMR dataset, and yields competitive results on the AD dataset.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_0",
  "x": "Previous work has mostly treated AA as a supervised text classification or regression task. A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006) , often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003) , and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones <cite>(Yannakoudakis et al., 2011)</cite> . As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976) . Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues.",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_2",
  "x": "Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . ---------------------------------- **EXPERIMENTAL DESIGN & BACKGROUND** We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features. Specifically, we describe a number of different experiments improving on the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> ; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) (Joachims, 2002) are used to explicitly model the grade relationships between scripts.",
  "y": "extends"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_4",
  "x": "For example, an examinee might learn by rote a set of well-formed sentences and reproduce these in an exam in the knowledge that an AA system is not checking for prompt relevance or coherence 1 . ---------------------------------- **DATASET & EXPERIMENTAL SETUP** We use the First Certificate in English (FCE) ESOL examination scripts 2 (upper-intermediate level assessment) described in detail in<cite> Yannakoudakis et al. (2011)</cite> , extracted from the Cambridge Learner Corpus 3 (CLC). The dataset consists of 1,238 texts between 200 and 400 words produced by 1,238 distinct learners in response to two different prompts.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_5",
  "x": "For all experiments, we use a series of 5-fold cross-validation runs on 1,141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models. Moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report the best published results. Validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the AA system. Again, we treat AA as a rank preference learning problem and use SVMs, utilizing the SVM light package (Joachims, 2002) , to facilitate comparison with<cite> Yannakoudakis et al. (2011)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_7",
  "x": "We focus on the development and evaluation of (automated) methods for assessing coherence in learner texts under the framework of AA. Most of the methods we investigate require syntactic analysis. As in<cite> Yannakoudakis et al. (2011)</cite> , we analyze all texts using the RASP toolkit (Briscoe et al., 2006) 4 . ---------------------------------- **'SUPERFICIAL' PROXIES**",
  "y": "similarities uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_8",
  "x": "The AA system described in<cite> Yannakoudakis et al. (2011)</cite> exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades. In coherent texts, constituent clauses and sentences are related and depend on each other for their interpretation. Anaphors such as pronouns link the current sentence to those where the entities were previously mentioned. Pronouns can be directly related to (lack of) coherence and make intuitive sense as cohesive devices. We compute the number of pronouns in a text and use it as a shallow feature for capturing coherence.",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_9",
  "x": "---------------------------------- **SEMANTIC SIMILARITY** We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in<cite> Yannakoudakis et al. (2011)</cite> , none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_10",
  "x": "We examine the predictive power of each of the coherence models/features described in Section 4 by measuring the effect on performance when combined with an AA system that achieves state-of-theart results on the FCE dataset, but does not use discourse coherence features. In particular, we use the system described in<cite> Yannakoudakis et al. (2011)</cite> as our baseline AA system. Discourse coherence is a strong indicator of thorough knowledge of a second language and thus we expect coherence features to further improve performance of AA systems. We evaluate the grade predictions of our models against the gold standard grades in the dataset using Pearson's product-moment correlation coeffi-16 http://goo.gl/yQ0Q0 cient (r) and Spearman's rank correlation coefficient (\u03c1) as is standard in AA research (Briscoe et al., 2010) . Table 1 gives results obtained by augmenting the baseline model with each of the coherence features described above.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_11",
  "x": "Moreover, combining all feature classes together in row 17 does not yield higher results than those obtained with ISA, while \u03c1 is no better than the baseline. In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report results of the final best system. Validating the model on a different exam year also shows us the extent to which it generalizes between years. Table 2 published results on the 2001 texts, getting closer to the upper-bound. The upper-bound on this dataset 20 is 0.796 and 0.792 r and \u03c1 respectively, calculated by taking the average correlation between the FCE grades and the ones provided by 4 senior ESOL examiners 21 .",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_12",
  "x": "The upper-bound on this dataset 20 is 0.796 and 0.792 r and \u03c1 respectively, calculated by taking the average correlation between the FCE grades and the ones provided by 4 senior ESOL examiners 21 . Table 3 also presents the average correlation between our extended AA system's predicted grades and the 4 examiners' grades, in addition to the original FCE grades from the dataset. Again, our extended model improves over the baseline. Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . The previous AA system is unable to downgrade appropriately 'outlier' scripts containing individually high-scoring sentences with poor overall coherence, created by randomly ordering a set of highly-marked texts.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_14",
  "x": "We presented the first systematic analysis of a wide variety of models for assessing discourse coherence on learner data, and evaluated their individual performance as well as their combinations for the AA grading task. We adapted the LOWBOW model for assessing sequential content in texts, and showed evidence supporting our hypothesis that local histograms are useful. We also successfully adapted ISA, an efficient and incremental variant distributional semantic model, to this task. ISA, LOWBOW, the POS IBM model and word length are the best individual features for assessing coherence. A significant improvement over the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> and the best published result on the FCE dataset was obtained by augmenting the system with an ISA-based local coherence feature.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_15",
  "x": "Our contribution is threefold: 1) we present the first systematic analysis of several methods for assessing discourse coherence in the framework of AA of learner free-text responses, 2) we identify new discourse features that serve as proxies for the level of (in)coherence in texts and outperform previously developed techniques, and 3) we improve the best results reported by<cite> Yannakoudakis et al. (2011)</cite> on the publically available 'English as a Second or Other Language' (ESOL) corpus of learner texts (to date, this is the only public-domain corpus that contains grades). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . ---------------------------------- **EXPERIMENTAL DESIGN & BACKGROUND** We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features.",
  "y": "uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_0",
  "x": "**INTRODUCTION** Unsupervised machine translation has become an emerging research interest in recent years (Artetxe et al., 2017; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019;<cite> Lample and Conneau, 2019)</cite> . The common framework of unsupervised machine translation builds two initial translation models at first (i.e., source to target and target to source), and then does iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with the two models using pseudo data generated by each other. The initialization process is crucial to the final translation * Contribution during internship at MSRA. performance as pointed in , Artetxe et al. (2018b) and Ren et al. (2019) .",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_1",
  "x": "The initialization process is crucial to the final translation * Contribution during internship at MSRA. performance as pointed in , Artetxe et al. (2018b) and Ren et al. (2019) . Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models <cite>(Lample and Conneau, 2019)</cite> . However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly.",
  "y": "background motivation"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_2",
  "x": "---------------------------------- **XLM** Based on BERT,<cite> Lample and Conneau (2019)</cite> propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification , machine translation, and unsupervised cross-lingual word embedding. The basic points of XLM are mainly two folds. The first one is to use a shared vocabulary of BPE (Sennrich et al., 2016b ) to provide potential crosslingual information between two languages just as mentioned in , in an inexplicit way though.",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_3",
  "x": "p(y j |x i ) is the model prediction in Softmax outputs. For each source ngram, all of the retrieved k translation candidates are used to calculate the cross entropy loss, which are weighted with their translation probabilities in the n-gram table. Given a language pair X \u2212 Y , we process both languages with the same shared BPE vocabulary using their monolingual sentences together during pre-training. Following Devlin et al. (2018) ;<cite> Lample and Conneau (2019)</cite> , in our CMLM objective, we randomly sample 15% of the BPE ngrams from the text streams, and replace them by [MASK] tokens 70% of the time. During pretraining, in each iteration, a batch is composed of sentences sampled from the same language, and we alternate between MLM and CMLM objectives.",
  "y": "extends differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_4",
  "x": "It is also interesting to explore the usage of pre-trained decoders in the translation model. It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders <cite>(Lample and Conneau, 2019)</cite> , one reason for which could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted substantially in the following tuning process for MT task. In our experiments, we explore some other usage of pre-trained decoders, i.e., we use the pretrained decoder as the feature extractor and feed the outputs into a new decoder consisting of several Transformer layers with the attention to the encoder. We find this method improves the performance of some language translation directions.",
  "y": "differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_5",
  "x": "The NewsCrawl data are used in both pretraining and the following unsupervised NMT iteration process. Our CMLM is optimized based on the pre-trained models released by<cite> Lample and Conneau (2019)</cite> 1 , which are trained with Wikipedia dumps. For fair comparison, we use newstest 2014 as the test set for en-fr, and newstest 2016 for en-de and en-ro. 1 https://github.com/facebookresearch/XLM We use Moses scripts for tokenization, and use fastBPE 2 to split words into subword units with their released BPE codes 1 . The number of shared BPE codes for each language pair is 60,000.",
  "y": "similarities uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_6",
  "x": "The improvement made by<cite> Lample and Conneau (2019)</cite> compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT. However, the crosslingual information learned with this method during pre-training is mostly from the shared subword space, which is inexplicit and not strong enough. Our proposed method can give the model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with<cite> Lample and Conneau (2019)</cite> (with the significance level of p<0.01). ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_7",
  "x": "Notice that by doing another iteration (\"Iter 2\") with updated ngram tables as described in Section 3.4, we further improve the performance a bit for most translation directions with the improvements of en2fr and ro2en bigger than 0.5 BLEU point, which confirms the potential that fine-tuned machine translation models contain more beneficial cross-lingual information than the initial n-gram translation tables, which can be used to enhance the pre-trained model iteratively. The improvement made by<cite> Lample and Conneau (2019)</cite> compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT. However, the crosslingual information learned with this method during pre-training is mostly from the shared subword space, which is inexplicit and not strong enough. Our proposed method can give the model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with<cite> Lample and Conneau (2019)</cite> (with the significance level of p<0.01).",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_8",
  "x": "Given two parallel sentences in English and French respectively, we feed each sentence into the pre-trained cross-lingual encoder and get its respective outputs. Then, we calculate the similarities between the outputs of the two sentences and choose target words with max similarity scores as the alignments of corre- sponding source words. We compare the context-unaware method (i.e., directly calculating the similarity scores between unsupervised cross-lingual embeddings (Artetxe et al., 2018a ) of source and target words), XLM <cite>(Lample and Conneau, 2019)</cite> and our proposed CMLM pre-training method in the Table 3 . In this experiment, we leave out all the OOV words and those torn apart by the BPE operations. Table 3 : Results of word alignment tasks using different cross-lingual word embeddings.",
  "y": "similarities"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_10",
  "x": "Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervised machine translation, some following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) . They all build unsupervised PBSMT systems, and all of their models are initialized with language models and phrase tables inferred from cross-lingual word or n-gram embeddings and then use the initial PBSMT models to do iterative back-translation. also build a hybrid system by combining the best pseudo data that SMT models generate into the training of the NMT model while Ren et al. (2019) alternately train SMT and NMT models with the framework of posterior regularization. More recently,<cite> Lample and Conneau (2019)</cite> reach new state-of-the-art performance on unsupervised en-fr and en-de translation tasks.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_0",
  "x": "They observed that English readability formulas did not work well on Bengali texts <cite>[11]</cite> , [21] . This observation is not surprising, because Bengali is very different than English. Bengali is a highly inflected language, follows subject-object-verb ordering in sentences, and has a rich morphology. Further, Bengali shows word compounding and diglossia, i.e. formal and informal language variants (sadhu bhasha and cholit bhasha). All these factors complicate readability scoring in Bengali.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_1",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] . They found that the two-parameter model was a better predictor of readability than the one-parameter model. Note, however, that Das and Roychoudhury's corpus was small (only seven documents), thereby calling into question the validity of their results. Sinha et al. alleviated these problems by considering six parameters instead of just two [21] .",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_2",
  "x": "Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . They designed a Bengali readability classifier on lexical and information-theoretic features, resulting in an F-score 50% higher than that from traditional scoring approaches. While all the above studies are very important and insightful, none of them explicitly performed an inter-rater agreement study. For reasons mentioned in Section 1, an inter-rater agreement study is very important when we talk about readability assessment. Further, none of these studies made available their readability-annotated gold standard datasets, thereby stymieing further research.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_3",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . While all the above studies are very important and insightful, none of them explicitly performed an inter-rater agreement study. Further, none of these studies made available their readability-annotated gold standard datasets, thereby stymieing further research. We attempt to bridge these gaps in our work.",
  "y": "motivation"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_4",
  "x": "We plan to release this dataset for future research. We are working on readability modeling in Bengali, and this dataset will be very helpful. An important limitation of our study is the small corpus size. We only have 30 annotated passages at our disposal, whereas Islam et al. <cite>[11]</cite> had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours. Note also that our dataset is larger than both Sinha et al.'s 16document dataset [21] , and Das and Roychoudhury's seven document dataset [6] .",
  "y": "differences"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_0",
  "x": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \"insults\", whereas Nobata et al. (2016) classifies similar remarks as \"hate speech\" or \"derogatory language\". Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while <cite>Davidson et al. (2017)</cite> distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and <cite>Davidson et al. (2017)</cite> . To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_1",
  "x": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \"insults\", whereas Nobata et al. (2016) classifies similar remarks as \"hate speech\" or \"derogatory language\". Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while <cite>Davidson et al. (2017)</cite> distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and <cite>Davidson et al. (2017)</cite> . To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_2",
  "x": "Much of the work on abusive language subtasks can be synthesized in a two-fold typology that conExplicit Implicit Directed \"Go kill yourself\", \"You're a sad little f*ck\" (Van Hee et al., 2015a) , \"@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga\"<cite> (Davidson et al., 2017)</cite> , \"Youre one of the ugliest b*tches Ive ever fucking seen\" (Kontostathis et al., 2013) . \"Hey Brendan, you look gorgeous today. What beauty salon did you visit?\" (Dinakar et al., 2012), \"(((@User) )) and what is your job? Writing cuck articles and slurping Google balls? #Dumbgoogles\" (Hine et al., 2017) , \"you're intelligence is so breathtaking!!!!!!\" (Dinakar et al., 2011) Generalized \"I am surprised they reported on this crap who cares about another dead n*gger?\", \"300 missiles are cool! Love to see um launched into Tel Aviv! Kill all the g*ys there!\" (Nobata et al., 2016) , \"So an 11 year old n*gger girl killed herself over my tweets?\u02c6\u02c6thats another n*gger off the streets!!\" (Kwok and Wang, 2013) . \"Totally fed up with the way this country has turned into a haven for terrorists. Send them all back home.\" (Burnap and Williams, 2015) , \"most of them come north and are good at just mowing lawns\" (Dinakar et al., 2011 ), \"Gas the skypes\" (Magu et al., 2017) siders whether (i) the abuse is directed at a specific target, and (ii) the degree to which it is explicit. Starting with the targets, abuse can either be directed towards a specific individual or entity, or it can be used towards a generalized Other, for example people with a certain ethnicity or sexual orientation.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_3",
  "x": "To better illustrate this, the first row of Table 1 shows examples from the literature of directed abuse, where someone is either mentioned by name, tagged by a username, or referenced by a pronoun. 2 Cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. The second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016;<cite> Davidson et al., 2017)</cite> , although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate. The other dimension is the extent to which abusive language is explicit or implicit.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_4",
  "x": "Explicit abusive lan-guage is that which is unambiguous in its potential to be abusive, for example language that contains racial or homophobic slurs. Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012;<cite> Davidson et al., 2017)</cite> , with abusive terms being used in a colloquial manner or by people who are victims of abuse. Implicit abusive language is that which does not immediately imply or denote abuse. Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014) . Social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed \"microaggressions\" (Sue et al., 2007) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_5",
  "x": "Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions<cite> (Davidson et al., 2017)</cite> , others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016) . In such cases, expert annotators with domain specific knowledge are preferred as they tend to produce more accurate classifications (Waseem, 2016a) . Ultimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators. <cite>Davidson et al. (2017)</cite> , for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism. As such, it is important that researchers consider the social biases that may lead people to disregard certain types of abuse.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_6",
  "x": "Ultimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators. <cite>Davidson et al. (2017)</cite> , for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism. As such, it is important that researchers consider the social biases that may lead people to disregard certain types of abuse. The type of abuse that researchers are seeking to identify should guide the annotation strategy. Where subtasks occupy multiple cells in our typology, annotators should be allowed to make nuanced distinctions that differentiate between different types of abuse.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_7",
  "x": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015;<cite> Davidson et al., 2017)</cite> . Typed dependencies offer a more sophisticated way to capture the relationship between terms (Burnap and Williams, 2015) . Overall, there are many tools that researchers can use to model the relationship between abusive language and targets, although many of these require high-quality annotations to use as training data. Generalized abuse. Generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities (Silva et al., 2016) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_8",
  "x": "Further research is necessary to determine if there are underlying syntactic structures associated with generalized abusive language. Explicit abuse Explicit abuse, whether directed or generalized, is often indicated by specific keywords. Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016) , although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013;<cite> Davidson et al., 2017)</cite> . Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015) . Implicit abuse.",
  "y": "future_work"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_0",
  "x": "Prior work on conversation thread disentanglement is often based on pairwise message compar- ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006) , while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros\u00e9, 2010; Elsner and Charniak, 2008 , 2011 Mayfield et al., 2012) . Recent work by <cite>(Jiang et al., 2018</cite>; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps). However, linguistic theories (Biber and Conrad, 2019) differentiate the following three concepts: register, genre and style, to describe the text varieties. Register refers to the linguistic features such as the choice of words in content.",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_1",
  "x": "The contribution of this work is two-fold: 1) We propose context-aware deep learning models for thread detection and it advances the state-ofthe-art; 2) Based on the dataset in<cite> (Jiang et al., 2018)</cite> , we develop and release a more realistic multi-party multi-thread conversation dataset for future research. ---------------------------------- **METHODOLOGY** We model thread-detection as a topic detection and tracking task by deciding whether an incoming message starts a new thread or belongs to an existing thread (Allan, 2002) . The goal is to assign each message m i in the sequence, M = {m i } N i=1 , a thread label t i , such that the complete thread label sequence T = {t i } N i=1 contains multiple threads (T 1 , T 2 , \u00b7 \u00b7 \u00b7), where each thread T l contains all messages with the same label.",
  "y": "uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_2",
  "x": "**CONTEXT-AWARE THREAD DETECTION** We first adopt the Universal Sentence Encoder 2 with deep averaging network (USE) (Cer et al., 2018) to get a static feature representation for each message in the form of sentence embeddings. We encode each message m j as enc(m j ), by concatenating the USE output with two 20dimensional embeddings: (1) User-identity difference between m j and m i . (2) Time difference by mapping the time difference between m j and m i into 11 ranges (from 1 minutes to 72 hours, details in Appendix A). These two features are also used in<cite> (Jiang et al., 2018)</cite> , and another baseline model GTM uses only these features (Elsner and Charniak, 2008) .",
  "y": "similarities"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_3",
  "x": "Intuitively, if they are close, both FLOW and MATCH will be considered equally for prediction. Otherwise, the model dynamically computes the weights of MATCH and FLOW. Training Procedure: Following<cite> (Jiang et al., 2018)</cite> , apart from a new thread, we consider the candidate threads (Active Threads) in Eq. 1 only from those appearing in one hour time-frame before m i . During training, we treat the messages of a channel as a single sequence, and optimize Eq. 1 with training examples, containing m i and its active threads. Though messages are sorted by time, the training examples are shuffled during training.",
  "y": "extends differences"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_4",
  "x": "We strictly follow<cite> (Jiang et al., 2018)</cite> to construct our data. Comments under a post can be treated as messages in a single conversational thread, and we merge all comments in a ---------------------------------- **GADGETS** Iphones Politics NMI ARI F1 NMI ARI F1 NMI ARI F1 Table 2: CATD models are compared with baselines wrt.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_5",
  "x": "We take three sub-reddits to build three datasets, Gadgets, IPhones and Politics. 3 The data statistics and examples are shown in Appendix B. Reddit Dataset Improvement: We use the same pre-processing method in<cite> (Jiang et al., 2018)</cite> : we discard the messages which have less than 10 words or more than 100 words. Conversations less than 10 messages are also discarded. We guarantee that no more than 10 conversations happen at the same time.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_6",
  "x": "Baseline: (1) CISIR-SHCNN<cite> (Jiang et al., 2018)</cite> : A recently proposed model based on CNN and ranking message pairs. (2) CISIR-USE: We replace CNN encoder in CISIR with a USE to test the effect of different sentence encoders. (3) GTM (Elsner and Charniak, 2008) : A graph-theoretical model with chat and content specific features. ---------------------------------- **MODEL VARIATIONS**",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_7",
  "x": "833 .431 .433 Evaluation Metrics: Normalized mutual information (NMI), Adjusted rand index (ARI) and F1 score, following<cite> (Jiang et al., 2018)</cite> . F1 is computed based on all message pairs in a test set. Also, following their work, we assume the candidate threads of each message for our models and baselines are obtained from the ones which have messages in the previous hour. For examples, the CISIR-SHCNN models will take pairs only within the one-hour frame. Main Results: Table 2 compares the CATD models and baselines on NMI, ARI and F1.",
  "y": "similarities uses"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_0",
  "x": "The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009 ). Current approaches for learning such patterns include bootstrapping techniques<cite> (Huang and Riloff, 2012a</cite>; Yangarber et al., 2000) , weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006) , fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009 ) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_1",
  "x": "All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIER light<cite> (Huang and Riloff, 2012a</cite> ) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_2",
  "x": "**APPROACH** In this work, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007;<cite> Huang and Riloff, 2012a</cite>; Huang and Riloff, 2012b) , the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations.",
  "y": "differences"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_3",
  "x": "We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arson. These are represented by templates containing various slots for each piece of information that should be extracted from the document (perpetrators, human targets, physical targets, etc). Following previous works (Huang and Riloff, 2011;<cite> Huang and Riloff, 2012a)</cite> , we only consider the \"String Slots\" in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type).",
  "y": "similarities uses"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_4",
  "x": "We used the extra-trees ensemble classifier implemented in (Pedregosa et al., 2011) , with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of features to consider when looking for the best split is \u221a number f eatures. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&W50, HLBL-50) 3 and finally to another State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33 word representation construction on the domainspecific data (W2V-50) 4 . Figure 1: F1-score results for event role labeling on MUC-4 data, for different size of training data, of \"String Slots\" on the TST3+TST4 with different parameters, compared to the learning curve of TIER<cite> (Huang and Riloff, 2012a)</cite> . The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon.",
  "y": "differences"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_0",
  "x": "Purandare and Litman analyzed humorous spoken conversations from a classic comedy television show. They used standard supervised learning classifiers to identify humorous speech (Purandare and Litman, 2006 ). Taylor and Marlack focused on a specific type of humor, wordplays. Their algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes (Taylor and Mazlack, 2004) . Later,<cite> Yang et al. (2015)</cite> formulated a classifier to distinguish between humorous and non-humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives: incongruity, ambiguity, interpersonal effect and phonetic style.",
  "y": "background"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_2",
  "x": "(b) the datasets in most studies are English corpus. (c) the evaluations are isolated from other research. In our work, we build the humor recognizer by using CNNs with extensive filter size and number, and the result shows higher accuracy from previous CNNs models. We conducted experiments on two different dataset, which were used in the previous studies. One is Pun of the Day <cite>(Yang et al., 2015)</cite> , and the other is 16000 One-Liners (Mihalcea and Strapparava, 2005) .",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_3",
  "x": "To fairly evaluate the performance on humor recognition, we need the dataset to consist of both humorous (positive) and non-humorous (negative) samples. The datasets we use to construct humor recognition experiments includes four parts: Pun of the Day <cite>(Yang et al., 2015)</cite> , 16000 OneLiners (Mihalcea and Strapparava, 2005) , Short Jokes dataset and PTT jokes. The four datasets have different joke types, sentence lengths, data sizes and languages that allow us to conduct more comprehensive and comparative experiments. We would like to thank Yang and Mihalcea for their kindly provision of two former datasets. And we depict how we collect the latter two datasets in the following subsections. Table 1 shows the statistics of four datasets.",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_4",
  "x": "We set the baseline on the previous works of<cite> Yang et al. (2015)</cite> by Random Forest with Word2Vec + Human Centric Feature (Word2Vec + HCF) and Chen and Lee (2017) by Convolutional Neural Networks. We choose a dropout rate at 0.5 and test our model's performance with two factors F and HN. F means the increase of filter size and number as we mentioned in section 4. Otherwise, the window sizes would be (5, 6, 7) and filter number is 100 that is the same with Chen and Lee (2017)'s. HN indicates that we use the highway layers to train deep networks and we set the HN layers = 3 because it has better stability and accuracy in training step. We could observe that when we use both F and 115 Table 3 presents the result of Short Jokes and PTT Jokes datasets.",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_0",
  "x": "Paragraph vectors (Le and Mikolov, 2014 ) are a recent method for embedding pieces of natural language text as fixed-length, real-valued vectors. Extending the word2vec framework <cite>(Mikolov et al., 2013b)</cite> , paragraph vectors are typically presented as neural language models, and compute a single vector representation for each paragraph. Unlike word embeddings, paragraph vectors are not shared across the entire corpus, but are instead local to each paragraph. When interpreted as a latent variable, we expect them to have higher uncertainty when the paragraphs are short. Recently, Barkan (2017) proposed a probabilistic view of word2vec that has motivated research on combining word2vec with other priors (Bamler and Mandt, 2017) .",
  "y": "background motivation"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_1",
  "x": "**RELATED WORK** Paragraph embeddings are built on top of word embeddings, a set of dimensionality reduction tools that map words from a large vocabulary to a dense vector representation. Most word embedding methods learn a point estimate for each embedding vector (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Goldberg and Levy, 2014; Pennington et al., 2014) . Barkan (2017) pointed out that the skip-gram model with negative sampling, also known as word2vec <cite>(Mikolov et al., 2013b)</cite> , admits a Bayesian interpretation. The Bayesian skip-gram model allows uncertainty to be taken into account in a principled way, and lays the basis for our proposed Bayesian paragraph vector model.",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_2",
  "x": "The Bayesian skip-gram model (Barkan, 2017 ) is a probabilistic interpretation of word2vec <cite>(Mikolov et al., 2013b)</cite> . The left part of Figure 1 shows the generative process. For each word i in the vocabulary, the model draws a latent word embedding vector U i \u2208 R E and a latent context embedding vector V i \u2208 R E from a Gaussian prior N (0, \u03bb 2 I). Here, E is the embedding dimension and \u03bb is a hyperparameter. The model then constructs N pairs labeled pairs of words following a twostep process.",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_3",
  "x": "is the sigmoid function. The pairs with label z ij = 1 form the so-called positive examples, and are assumed to correspond to occurrences of the word i in the context of word j somewhere in the corpus. The so-called negative examples with label z ij = 0 do not correspond to any observation in the corpus. When training the model, we resort to the heuristics proposed in <cite>(Mikolov et al., 2013b)</cite> to create artificial evidence for the negative examples (see Section 3.2 below). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_4",
  "x": "Here, w n,t is the word class of the t th token, t runs over all tokens in document n, \u03b4 runs from \u2212c to c where c is a small context window size, and we exclude \u03b4 = 0. Negative examples are not observed in the corpus. Following<cite> Mikolov et al. (2013b)</cite> , we construct artificial evidence X \u2212 n for negative pairs by sampling from the noise distribution , where f is the empirical unigram frequency across the training corpus. The log-likelihood of the entire data is thus",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_5",
  "x": [
   "Word2vec (Mikolov et al., 2013b ) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection."
  ],
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_0",
  "x": "To this end, various sense-specific word embeddings have been proposed to account for the contextual subtlety of language (Reisinger and Mooney, 2010b,a;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015; Tian et al., 2014; Li and Jurafsky, 2015; Arora et al., 2016) . A majority of these methods propose to learn multiple vectors for each word via clustering. (Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings. Side information such as topical understanding (Liu et al., 2015b,a) or paralleled foreign language data (Guo et al., 2014; \u0160uster et al., 2016; Shyam et al., 2017) have also been exploited for clustering different meanings of multi-sense words. Another trend is to forgo word embeddings in favor of sentence or paragraph embeddings for specific tasks (? Kiros et al., 2015; Le and Mikolov, 2014) .",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_1",
  "x": "A majority of these methods propose to learn multiple vectors for each word via clustering. (Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings. Side information such as topical understanding (Liu et al., 2015b,a) or paralleled foreign language data (Guo et al., 2014; \u0160uster et al., 2016; Shyam et al., 2017) have also been exploited for clustering different meanings of multi-sense words. Another trend is to forgo word embeddings in favor of sentence or paragraph embeddings for specific tasks (? Kiros et al., 2015; Le and Mikolov, 2014) . While being more flexible and adaptive to context, all these approaches require sophisticated neural network structures and are problem specific, taking away the advantage offered by the unsupervised embedding approaches of single-sense embeddings.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_2",
  "x": "We evaluate our approach on various tasks that require contextual understanding of words, combining existing and new test datasets and evaluation metrics: word-sense induction ( (Koeling et al., 2005; Bartunov et al., 2015) ), contextual word similarity<cite> ((Huang et al., 2012</cite> ) and a new test set), and relevance detection ( (Arora et al., 2016) and a new test set). To the best of our knowledge, no prior literature has provided a comprehensive evaluation of all these multisense-specific tasks. Our simple, intuitive model retains almost all advantages offered by more complicated multisense embedding models, and often surpasses the performance of nonlinear \"deep\" models. Our code and data are at https://github.com/ dingwc/multisense/ To summarize, the contributions of our paper are as follows: 1. We propose an extremely simple model for learning polysemous word representations 2.",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_3",
  "x": "This idea of local similarity has been widely used to obtain context sense representation<cite> Huang et al., 2012</cite>; Le and Mikolov, 2014; Neelakantan et al., 2015) . It was also used to decompose unisense vector into sense specific vectors (Arora et al., 2016) . In this paper, we exploit this intuition and model the contextual embedding of a word as a linear combination of its contexts. Specifically, we consider a corpus drawn from a vocabulary V = (word 1 , . . . , word V ). We define the normalized cooccurence matrix as the V \u00d7 V (sparse 1 ) symmetric matrix W where",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_5",
  "x": "Recall that \u03b1 = 0 is the dot product and \u03b1 = 1 is cosine similarity. To show the effect of \u03b1 when only relevant (w, S) pairs are used, Figure 3 .2 plots the SCWS score (see Section 4; also<cite> (Huang et al., 2012)</cite> ) for varying \u03b1, for the top-performing embedding from each method. Cos-distance works best for all embeddings. However, for embeddings from<cite> (Huang et al., 2012)</cite> and (Neelakantan et al., 2015) , which all have norm \u2248 1, the choice of \u03b1 makes little difference. On the other hand, using the embeddings from and our method, which both have highly varying norms, the choice of \u03b1 greatly affects performance.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_7",
  "x": "In each case, we use their pre-trained model and choose the embedding of a target word that is closest to the context representation (as they suggest). Since the code in<cite> (Huang et al., 2012)</cite> allows choosing various distance functions, we pick all and report the best scores. For (Neelakantan et al., 2015; we use the cosine distance as recommended. Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet.",
  "y": "motivation"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_8",
  "x": "For (Neelakantan et al., 2015; we use the cosine distance as recommended. Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet. We now describe each task in detail. Word-Context Relevance (WCR) This task is proposed in (Arora et al., 2016) and aims to detect when word-context pairs are relevant.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_9",
  "x": "We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet. We now describe each task in detail. Word-Context Relevance (WCR) This task is proposed in (Arora et al., 2016) and aims to detect when word-context pairs are relevant. In<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015; , the relevance metric can be seen as the distance (cosine or Euclidean) between the query word and the context cluster center. In our method, we rely on the filtering effect of W ij values to diminish the norm of words in irrelevant contexts; thus we propose the 2 -norms of the contextual embedding as the metric of relevance, where the target word is excluded from the context if present.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_12",
  "x": "set to only include words present in vocabularies available to all embeddngs. Following<cite> (Huang et al., 2012)</cite> , we sort all the n = 2003 test pairs based on predicted similarity score and compare such ranking against the ground-truth ranking indicated by the average human evaluation score. The distance between two rank-lists is measured using the Spearman correlation score. Example of SCWS test for admission and confession ... the reason the Buddha himself gave was that the admission of women would weaken the Sangha and shorten its lifetime ... ... They included a confession said to have been inadvertently included on a computer disk that was given to the press... avg. human-given sim. score: 2.3 Table 8 : An example of a pair of word-contexts for a single SCWS task.",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_13",
  "x": "Example of SCWS test for admission and confession ... the reason the Buddha himself gave was that the admission of women would weaken the Sangha and shorten its lifetime ... ... They included a confession said to have been inadvertently included on a computer disk that was given to the press... avg. human-given sim. score: 2.3 Table 8 : An example of a pair of word-contexts for a single SCWS task. We note that in<cite> (Huang et al., 2012</cite> ) the similarity between two word-context pairs is the measured using avgSimC, a weighted average of cosine similarities between all possible representation vectors of w 1 and w 2 . This metric, however, can not be applied to our approach since we have an infinite number of possible contextual representation for each word. Therefore, we use the cosine similarity without averaging, which is reasonable for all the embedding approaches.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_15",
  "x": "For each word we create 80 \u2212 20% train-test splits, train a K-NN multiclass classifier with Euclidean distance between contextual word embeddings, and report the mean classification accuracy (Acc) averaged across all words. Discussion We have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from existing test sources and formed ourselves through WordNet. Overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a WordNet based model. One thing to note is that the SCWS Spearman scores of the<cite> (Huang et al., 2012)</cite> listed here are much smaller than that first reported. This is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words.",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_0",
  "x": "It is generally believed that natural language understanding systems would benefit from incorporating common-sense knowledge about prototypical sequences of events and their participants. Early work focused on structured representations of this knowledge (called scripts (Schank & Abelson, 1977) ) and manual construction of script knowledge bases. However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs<cite> (Regneri et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_1",
  "x": "Early work focused on structured representations of this knowledge (called scripts (Schank & Abelson, 1977) ) and manual construction of script knowledge bases. However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs<cite> (Regneri et al., 2010)</cite> . These graphs are scenario-specific, nodes in them correspond to events (and associated with sets of potential event mentions) and arcs encode the temporal precedence relation.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_2",
  "x": "In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the expected ordering of events. Both the parameters of the compositional process for computing the event representation and the ranking component of the model are estimated from data. In order to get an intuition why the embedding approach may be attractive, consider a situation where a prototypical ordering of events the bus disembarked passengers and the bus drove away needs to be predicted. An approach based on frequency of predicate pairs (Chambers & Jurafsky, 2008) , is unlikely to make a right prediction as driving usually precedes disembarking. Similarly, an approach which treats the whole predicate-argument structure as an atomic unit<cite> (Regneri et al., 2010)</cite> will probably fail as well, as such a sparse model is unlikely to be effectively learnable even from large amounts of data.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_3",
  "x": "In previous work on learning inference rules (Berant et al., 2011) , it has been shown that enforcing transitivity constraints on the inference rules results in significantly improved performance. The same is true for the event order- ing task, as scripts have largely linear structure, and observing that a \u227a b and b \u227a c is likely to imply a \u227a c. Interestingly, in our approach we implicitly learn the model which satisfies transitivity constraints, without the need for any explicit global optimization on a graph. The approach is evaluated on crowdsourced dataset of <cite>Regneri et al. (2010)</cite> and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%). ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_4",
  "x": "The learning procedure is sketched in Algorithm 1. Additionally, we use a Gaussian prior on weights, regularizing both the embedding parameters and the vector w. We initialize word representations using the SENNA embeddings (Collobert et al., 2011) . ---------------------------------- **EXPERIMENTS** We evaluate our approach on crowdsourced data collected for script induction by <cite>Regneri et al. (2010)</cite> , though, in principle, the method is applicable in arguably more general setting of Chambers & Jurafsky (2008) .",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_5",
  "x": "**EXPERIMENTS** We evaluate our approach on crowdsourced data collected for script induction by <cite>Regneri et al. (2010)</cite> , though, in principle, the method is applicable in arguably more general setting of Chambers & Jurafsky (2008) . ---------------------------------- **DATA AND TASK** <cite>Regneri et al. (2010)</cite> collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions provided in the OMICS corpus (Gupta & Kochenderfer, 2004) .",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_6",
  "x": "{go to coffee maker} \u2192 {fill water in coffee maker} \u2192 {place the filter in holder} \u2192 {place coffee in filter} \u2192 {place holder in coffee maker} \u2192 {turn on coffee maker} Though individual ESDs may seem simple, the learning task is challenging because of the limited amount of training data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be mentioned in a ESD), different granularity of events and variability in the ordering (e.g., coffee may be put in a filter before placing it in a coffee maker). Unlike our work, <cite>Regneri et al. (2010)</cite> relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not<cite> (Regneri et al., 2010</cite> ).",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_7",
  "x": "As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not<cite> (Regneri et al., 2010</cite> ). ---------------------------------- **3** The model was estimated as explained in Section 2.2 with the order of events in ESDs treated as gold standard.",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_0",
  "x": "<cite>Zhou et al. (2017)</cite> utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated. In the work of a multi-perspective context matching algorithm is employed. Harrison and Walker (2018) use a set of rich linguistic features along with a NQG model. (Song et al., 2018) used the matching algorithm proposed by to compute the similarity between the target answer and the passage for collecting relevant contextual information under the different perspective, so that contextual information can be better considered by the encoder. More recently, Kim et al. (2018) has claimed to improve the performance of QG model by replacing the target answer in the original passage with a special tokens.",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_2",
  "x": "The word vectors, the embedded world knowledge feature vectors and the answer position indicator embedding vectors are concatenated and passed as input to the Bi-LSTM encoder. ---------------------------------- **ENTITY LINKING** In previous works<cite> (Zhou et al., 2017</cite>; Harrison and Walker, 2018) , named entity type features have been used. These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.",
  "y": "motivation"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_3",
  "x": "SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles. We used the same split as<cite> (Zhou et al., 2017)</cite> . MS MARCO datasets contains 1 million queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents. We picked a subset of MS MARCO data where answers (<= 10 words) are sub-spans within the passages (<= 600 words), and use dev set as test set (7, 849), and split train set with ratio 90%-10% into train (1, 36, 337) and dev (15, 148) sets.",
  "y": "similarities uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_0",
  "x": "The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases. This makes standard similarity calculations as proposed in (Lin, 1998;<cite> Curran, 2002</cite>; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasible. These approaches first calculate an information measure between each word and the according context and then calculate the similarity between all words, based on the information measure for all shared contexts. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_1",
  "x": "These pairs are subsequently used for the computation of the similarities between terms, leading to a second-order relation. The representation can be formalized by the pair <x,y> where x is the term and y represents the context feature. The position of x in y is denoted by the hole symbol '@'. As an example the dependency relation (nsub;gave 2 ;I 1 ) could be transferred to <gave 2 ,(nsub;@;I 1 )> and <I 1 ,(nsub;gave 2 ;@)>. This representation scheme is more generic then the schemes introduced in (Lin, 1998;<cite> Curran, 2002)</cite> , as it allows to characterise pairs by several holes, which could be used to learn analogies, cf. (Turney and Littman, 2005) .",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_2",
  "x": "Features of low saliency generally should not contribute much to the similarity of terms and also could lead to spurious similarity scores. Afterwards, all terms are aggregated by their features, which allows us to compute similarity scores between all terms that share at least one such feature. Whereas the method introduced by (Pantel and Lin, 2002) is very similar to the one proposed in this paper (the similarity between terms is calculated solely by the number of features two terms share), they use PMI to rank features and do not use pruning to scale to large corpora, as they use a rather small corpus. Additionally, they do not evaluate the effect of such pruning. In contrast to the best measures proposed by Lin (1998;<cite> Curran (2002</cite>; Pantel et al. (2009; Goyal et al. (2010) we do not calculate any information measure using frequencies of features and terms (we use significance ranking instead), as shown in Table 1 .",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_4",
  "x": "We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget's 1911 thesaurus, Moby Thesaurus, Merriam Webster's Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure<cite> (Curran, 2002)</cite> to evaluate the DTs. Furthermore, we introduce a WordNet-based method. To calculate the similarity between two terms, we use the WordNet::Similarity path (Pedersen et al., 2004) measure. While its absolute scores are hard to interpret due to inhomogenity in the granularity of WordNet, they are well-suited for relative comparison. The score between two terms is inversely proportional to the shortest path between all the synsets of both terms.",
  "y": "uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_5",
  "x": "Apart from the PMI measure, Curran's measure leads to the weakest results. We could not confirm that his measure outperforms Lin's measure as stated in<cite> (Curran, 2002)</cite> 1 . An explanation for this results might be the use of a different parser, very few test words and also a different gold standard thesaurus in his evaluation. Comparing our method using LMI to Lin's method, we achieve lower scores with our method using small corpora, but surpass Lin's measure from 10 million sentences onwards. Next, we show the results of the WordNet evaluation measure in Figure 2 .",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_6",
  "x": "We can see that our method using PMI does not decline for larger corpora, as the limit on first-order features is not reached and frequent features are still being used. Comparing our LMI DT is en par with Lin's measure for 10 million sentences, and makes better use of large data when using the complete dataset. Again, the inverse ranking and the WordNet Path measure are highly correlated. 2 Building a gold standard thesaurus following<cite> Curran (2002)</cite> needs access to all the used thesauri. Whereas for some, programming interfaces exist, often with limited access and licence restrictions, others have to be extracted manually.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_0",
  "x": "In this paper we study the feasibility of the application of different features for representing safely information about clues related to fake reviews. We focus our study in a variant of the stylistic feature character n-grams named character n-grams in tokens. We also study an emotion-based feature and a linguistic processes feature based on LIWC variables. We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We show an experimental study evaluating the single features and combining them with the intention to obtain better features.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_1",
  "x": "**EXPERIMENTAL STUDY** In order to evaluate our proposal, we have performed some experimental study on the first publicly available opinion spam dataset gathered and presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We first describe the corpus and then we show the different experiments made. Finally we compare our results with those published previously. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_2",
  "x": "---------------------------------- **OPINION SPAM CORPUS** The Opinion Spam corpus presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013 ) is composed of 1600 positive and negative opinions for hotels with the corresponding gold-standard. From the 800 positive reviews (<cite>Ott et al., 2011</cite>) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area. All reviews were written in English, have at least 150 characters and correspond to users who had posted opinions previously on TripAdvisor (non first-time authors).",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_3",
  "x": "From the 800 positive reviews (<cite>Ott et al., 2011</cite>) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area. All reviews were written in English, have at least 150 characters and correspond to users who had posted opinions previously on TripAdvisor (non first-time authors). The 400 deceptive opinions correspond to the same 20 hotels and were gathered using Amazon Mechanical Turk crowdsourcing service. From the 800 negative reviews (Ott et al., 2013) , the 400 truthful where mined from TripAdvisor, Expedia, Hotels.com, Orbitz, Priceline and Yelp. The reviews are 1 or 2-star category and are about the same 20 hotels in Chicago.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_4",
  "x": "In (<cite>Ott et al., 2011</cite> ) <cite>the authors</cite> used the 80 dimensions of LIWC2007, unigrams and bigrams as set of features with a SVM classifier. In (Feng and Hirst, 2013) , profile alignment compatibility features combined with unigrams, bigrams and syntactic production rules were proposed for representing the opinion spam corpus. Then, a multivariate performance measures version of SVM classifier (named SVM perf ) was trained. In (Hern\u00e1ndez Fusilier et al., 2015b ) the authors studied two different representations: character n-grams and word n-grams. In particular, the best results were obtained with a Na\u00efve Bayes classifier using character 4 and 5 grams as features.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_5",
  "x": "In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone. From the Table 4 we can observe that the combination of 13 independent variables seems to have the lowest prediction accuracy (accuracy = 70.50%). About the last result, the authors in (Banerjee and Chua, 2014) concluded that only articles and pronouns (over the 13 variables) could significantly distinguish true from false reviews. The accuracy of the semi-supervised model is slightly lower (86.69%) than that of our approach (89%), although good enough.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_6",
  "x": "As we stated before, two kinds of comparisons are shown: an indirect (we could not obtain the complete set of results reported by the authors) and a direct (the authors kindly made available the results and a statistical comparison can be performed). In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone. From the Table 4 we can observe that the combination of 13 independent variables seems to have the lowest prediction accuracy (accuracy = 70.50%). About the last result, the authors in (Banerjee and Chua, 2014) concluded that only articles and pronouns (over the 13 variables) could significantly distinguish true from false reviews.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_7",
  "x": "About the last result, the authors in (Banerjee and Chua, 2014) concluded that only articles and pronouns (over the 13 variables) could significantly distinguish true from false reviews. The accuracy of the semi-supervised model is slightly lower (86.69%) than that of our approach (89%), although good enough. The authors concluded that the good performance of the semi-supervised model is due the topic information captured by the model combined with the examples and their similarity (Ren et al., 2014) . Then, they could obtain an accurate SVM classifier. Regarding the experiments with the 5 fold cross-validation, we obtained similar results to those of (<cite>Ott et al., 2011</cite>) and slightly lower than the ones of (Feng and Hirst, 2013) .",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_8",
  "x": "**MODEL ACCURACY** 10 fold cross-validation (Banerjee and Chua, 2014) 70.50% (Ren et al., 2014) 86.69% Our approach 89% 5 fold cross-validation (<cite>Ott et al., 2011</cite>) 89.8% (Feng and Hirst, 2013) 91.3% Our approach 89.8% Table 4 : Indirect comparison of the performance. Deceptive opinions detection for positive reviews of Opinion Spam corpus (800 opinions). In Table 5 we can observe the direct comparison of the performance for the positive and negative polarities reviews of the Opinion Spam corpus considering the proposal of (Hern\u00e1ndez Fusilier et al., 2015b) .",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_9",
  "x": "For the experimental study we have used the positive and negative polarities reviews corresponding to the corpora proposed by (<cite>Ott et al., 2011</cite>; Ott et al., 2013) with 800 reviews each one (400 true and 400 false opinions). We have used both corpora in a separate way but we have performed experiments joining both polarities reviews in a combined corpus of 1600 reviews. From the results obtained with the different features we have concluded that character 4-grams in tokens with LIWC variables performs the best using a SVM classifier. We made also a comparison with the approach of (Hern\u00e1ndez Fusilier et al., 2015b) and the results were similar (no statistically significant difference was found), but our low dimensionality representation makes our approach more efficient. For future work we plans to investigate another emotion/sentiment features in order to study the contributions in tasks of deception detection of opinion spam.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_0",
  "x": "Then, we explore a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. Cumulatively up to 20% error reduction is achieved relative to the standard <cite>Boulis and Ostendorf (2005)</cite> algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic corpus exceeds 95%. ---------------------------------- **INTRODUCTION** Speaker attributes such as gender, age, dialect, native language and educational level may be (a) stated overtly in metadata, (b) derivable indirectly from metadata such as a speaker's phone number or userid, or (c) derivable from acoustic properties of the speaker, including pitch and f0 contours (Bocklet et al., 2008) .",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_1",
  "x": "While most of the prior work in sociolinguistics has been approached from a non-computational perspective, Koppel et al. (2002) employed the use of a linear model for gender classification with manually assigned weights for a set of linguistically interesting words as features, focusing on a small development corpus. Another computational study for gender classification using approximately 30 weblog entries was done by Herring and Paolillo (2006) , making use of a logistic regression model to study the effect of different features. While small-scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of <cite>Boulis and Ostendorf (2005)</cite> and show how gender and other attributes can be accurately predicted based on the following original contributions: 1. Modeling Partner Effect: A speaker may adapt his or her conversation style depending on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_2",
  "x": "2. Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of <cite>Boulis and Ostendorf (2005)</cite> . 3. Application to Arabic Language: We also report results for Arabic language and show that the ngram model gives reasonably high accuracy for Arabic as well. Furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in English. 4. Application to Email Genre: We show how the models explored in this paper extend to email genre, showing the wide applicability of general text-based features.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_3",
  "x": "5. Application to new attributes: We show how the lexical model of <cite>Boulis and Ostendorf (2005)</cite> can be extended to Age and Native vs. Non-native prediction, with further improvements gained from our partner-sensitive models and novel sociolinguistic features. ---------------------------------- **RELATED WORK** Much attention has been devoted in the sociolinguistics literature to detection of age, gender, social class, religion, education, etc.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_4",
  "x": "There has also been some work in developing computational models based on linguistically interesting clues suggested by the sociolinguistic literature for detecting gender on formal written texts (Singh, 2001; Koppel et al., 2002; Herring and Paolillo, 2006) but it has been primarily focused on using a small number of manually selected features, and on a small number of formal written texts. Another relevant line of work has been on the blog domain, using a bag of words feature set to discriminate age and gender (Schler et al., 2006; Burger and Henderson, 2006; Nowson and Oberlander, 2006) . Conversational speech presents a challenging domain due to the interaction of genders, recognition errors and sudden topic shifts. While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work.",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_5",
  "x": "<cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper. Finally, we also explore and evaluate original model performance on additional latent speaker attributes including age and native vs. non-native English speaking status. ---------------------------------- **CORPUS DETAILS**",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_6",
  "x": "While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work. <cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper.",
  "y": "motivation"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_7",
  "x": "Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc. of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation).",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_8",
  "x": "In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc. of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_9",
  "x": "The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication. This resulted in a training set of 9000 speakers with 17587 conversation sides and a test set of 1000 speakers with 2008 conversation sides. The Switchboard corpus was much smaller and consisted of 543 speakers, with 443 speakers used for training and 100 speakers used for testing, resulting in a total of 4062 conversation sides for training and 808 conversation sides for testing. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_11",
  "x": "**MODELING GENDER VIA NGRAM FEATURES (<cite>BOULIS AND OSTENDORF, 2005</cite>)** As our reference algorithm, we used the current state-of-the-art system developed by <cite>Boulis and Ostendorf (2005)</cite> using unigram and bigram features in a SVM framework. We reimplemented <cite>this</cite> model as our reference for gender classification, further details of which are given below: ---------------------------------- **TRAINING VECTORS**",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_12",
  "x": "However, stopwords were retained in the feature set as various sociolinguistic studies have shown that use of some of the stopwords, for instance, pronouns and determiners, are correlated with age and gender. Also, only the ngrams with frequency greater than 5 were retained in the feature set following <cite>Boulis and Ostendorf (2005)</cite> . This resulted in a total of 227,450 features for the Fisher corpus and 57,914 features for the Switchboard corpus. ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_13",
  "x": "After extracting the ngrams, a SVM model was trained via the SVM light toolkit (Joachims, 1999) using the linear kernel with the default toolkit settings. Table 1 shows the most discriminative ngrams for gender based on the weights assigned by the linear SVM model. It is interesting that some of the gender-correlated words proposed by sociolinguistics are also found by this empirical approach, including the frequent use of \"oh\" by females and also obvious indicators of gender such as \"my wife\" or \"my husband\", etc. Also, named entity \"Mike\" shows up as a discriminative unigram, this maybe due to the self-introduction at the beginning of the conversations and \"Mike\" being a common male name. For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 .",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_14",
  "x": "It is interesting that some of the gender-correlated words proposed by sociolinguistics are also found by this empirical approach, including the frequent use of \"oh\" by females and also obvious indicators of gender such as \"my wife\" or \"my husband\", etc. Also, named entity \"Mike\" shows up as a discriminative unigram, this maybe due to the self-introduction at the beginning of the conversations and \"Mike\" being a common male name. For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 . Furthermore, the ngram-based approach scales well with varying the amount of conversation utilized in training the model as shown in Figure 1 . The \"<cite>Boulis and Ostendorf</cite>, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 .",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_15",
  "x": "---------------------------------- **INCORPORATING SOCIOLINGUISTIC FEATURES** The sociolinguistic literature has shown gender differences for speakers due to features such as speaking rate, pronoun usage and filler word usage. While ngram features are able to reasonably predict speaker gender due to their high detail and coverage and the overall importance of lexical choice in gender differences while speaking, the sociolinguistics literature suggests that other nonlexical features can further help improve performance, and more importantly, advance our understanding of gender differences in discourse. Thus, on top of the standard <cite>Boulis and Ostendorf (2005)</cite> model, we also investigated the following features motivated by the sociolinguistic literature on gender differences in discourse (Macaulay, 2005) :",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_16",
  "x": "Each row indicates an additive effect in the feature ablation, showing the result of adding the current sociolinguistic feature with the set of features mentioned in the rows above. Table 4 combines the results of the experiments reported in the previous sections, assessed on both the Fisher and Switchboard corpora for gender classification. The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is <cite>Boulis and Ostendorf (2005)</cite> , and all cited relative error reductions are based on this established standard, as implemented in this paper.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_17",
  "x": "As noted before, the standard reference algorithm is <cite>Boulis and Ostendorf (2005)</cite> , and all cited relative error reductions are based on this established standard, as implemented in this paper. Also, as a second reference, performance is also cited for the popular \"Gender Genie\", an online gender-detector 7 , based on the manually weighted word-level sociolinguistic features discussed in Argamon et al. (2003) . The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the <cite>Boulis and Ostendorf (2005)</cite> with the work reported by <cite>Boulis and Ostendorf (2005)</cite> ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker. Table 5 shows the result of each model on a per-speaker basis using a majority vote of the predictions made on the individual conversations of the respective speaker. The consensus model when applied to Switchboard corpus show larger gains as it has 9.38 conversations per speaker on average as compared to 1.95 conversations per speaker on average in Fisher.",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_21",
  "x": "We can see that the ngrambased approach for gender also gives reasonable performance on other speaker attributes, and more importantly, both the partner-model and sociolinguistic features help in reducing the error rate on age and native language substantially, indicating their usefulness not just on gender but also on other diverse latent attributes. Table 8 shows the most discriminative ngrams for binary classification of age, it is interesting to see the use of \"well\" right on top of the list for older speakers, also found in the sociolinguistic studies for age (Macaulay, 2005) . We also see that older speakers talk about their children (\"my daughter\") and younger speakers talk about their parents (\"my mom\"), the use of words such as \"wow\", \"kinda\" and \"cool\" is also common in younger speakers. To give maximal consistency/benefit to the <cite>Boulis and Ostendorf (2005)</cite> n-gram-based model, we did not filter the self-reporting n-grams such as \"im forty\" and \"im thirty\", putting our sociolinguisticliterature-based and discourse-style-based features at a relative disadvantage. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_22",
  "x": [
   "This paper has presented and evaluated several original techniques for the latent classification of speaker gender, age and native language in diverse genres and languages. A novel partner-sensitve model shows performance gains from the joint modeling of speaker attributes along with partner speaker attributes, given the differences in lexical usage and discourse style such as observed between same-gender and mixed-gender conversations. The robustness of the partner-model is substantially supported based on the consistent performance gains achieved in diverse languages and attributes. This paper has also explored a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. In addition to these novel models, the paper also shows how these models and the previous work extend to new languages and genres."
  ],
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_1",
  "x": "Furthermore, the problem of modeling semantic plausibility has itself been used as a testbed for exploring various knowledge representations. In this work, we focus specifically on modeling physical plausibility as presented by <cite>Wang et al. (2018)</cite> . This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible (Table 1) . We show that in the original supervised setting a distributional model, namely a novel application of BERT (Devlin et al., 2019) , significantly outperforms the best existing method which has access to manually labeled physical features<cite> (Wang et al., 2018)</cite> . Still, the generalization ability of supervised models is limited by the coverage of the training set.",
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_2",
  "x": "We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on <cite>Wang et al. (2018)</cite> 's physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010) , and find that Wikipedia triples result in better performance. arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017) .",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_3",
  "x": "We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010) , and find that Wikipedia triples result in better performance. arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017) . More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019) . Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003) , or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019) .",
  "y": "background uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_4",
  "x": "---------------------------------- **SELECTIONAL PREFERENCE** Closely related to semantic plausibility is selectional preference (Resnik, 1996) which concerns the semantic preference of a predicate for its arguments. Here, preference refers to the typicality of arguments: while it is plausible that a gorilla rides a camel, it is not preferred. Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility<cite> (Wang et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_5",
  "x": "---------------------------------- **TASK** Following existing work, we focus on the task of single-event, physical plausibility. This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible. We use <cite>Wang et al. (2018)</cite> 's physical plausibility dataset for evaluation.",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_6",
  "x": "We follow the same evaluation procedure as previous work and perform cross validation on the 3,062 labeled triples<cite> (Wang et al., 2018)</cite> . ---------------------------------- **LEARNING FROM TEXT** We also present the problem of learning to model physical plausibility directly from text. In this new setting, a model is trained on events extracted from a large corpus and evaluated on a physical plausibility task.",
  "y": "similarities uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_7",
  "x": "For NELL, we filter out triples with nonalphabetic characters or less than 5 occurrences, resulting in a total 2.5 million unique triples with a cumulative 112 million occurrences. For evaluation, we split <cite>Wang et al. (2018)</cite>'s 3,062 triples into equal sized validation and test sets. Each set thus consists of 1,531 triples. ---------------------------------- **METHODS**",
  "y": "extends"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_8",
  "x": "**NN** As a baseline, we consider the performance of a neural method for selectional preference (Van de Cruys, 2014). This method is a two-layer artificial neural network (NN) over static embeddings. Supervised. We reproduce the results of <cite>Wang et al. (2018)</cite> using GloVe embeddings and the same hyperparameter settings.",
  "y": "similarities"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_9",
  "x": "**RESULTS** ---------------------------------- **SUPERVISED** For the supervised setting, we follow the same evaluation procedure as <cite>Wang et al. (2018)</cite> : we perform 10-fold cross validation on the dataset of 3,062 s-v-o triples, and report the mean accuracy of running this procedure 20 times all with the same model initialization (Table 3) . BERT outperforms existing methods by a large margin, including those with access to manually labeled physical features.",
  "y": "similarities uses"
 }
]