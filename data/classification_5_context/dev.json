[
 {
  "id": "02521fd9721c264ee05315dec9b31d_0",
  "x": "In low-resource conditions using only 10 hours of labeled data, we achieve Word Error Rates (WER) of 10.2% and 23.5% on the standard test \"clean\" and \"other\" benchmarks of the Librispeech dataset, which is almost on bar with previously published work that uses 10 times more labeled data. Moreover, compared to previous work that uses two models in tandem <cite>(Baevski et al., 2019b)</cite> , by using one model for both BERT pre-trainining and fine-tuning, our model provides an average relative WER reduction of 9%. 1 ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_1",
  "x": "The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> . Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss.",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_2",
  "x": "Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\". To understand the nature of their learned representations, we train models using the continuous and the discrete BERT approaches on spectral features, e.g. Mel-frequency cepstral coefficients (MFCC), as well as on pre-trained Wav2vec features . These comparisons provide insights on how complementary the acoustically motivated contrastive loss function is to the other masked language model one. The unsupervised and semi-supervised ASR approaches is in need for test suites like the unified downstream tasks available for language representation models (Devlin et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_3",
  "x": "**VQ-WAV2VEC** vq-wav2vec <cite>(Baevski et al., 2019b)</cite> learns vector quantized (VQ) representations of audio data using a future time-step prediction task. Similar to wav2vec, there is a convolutional encoder and decoder networks f : X \u2192 Z and g :\u1e90 \u2192 C for feature extraction and aggregation. However, in between them there is a quantization module q : Z \u2192\u1e90 to build discrete representations which are input to the aggregator. First, 30ms segments of raw speech are mapped to a dense feature representation z at a stride of 10ms using the encoder f .",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_4",
  "x": "Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes. To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step.",
  "y": "extends"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_5",
  "x": "---------------------------------- **DISCRETE BERT** Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_6",
  "x": "To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ). ---------------------------------- **CONTINUOUS BERT**",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_7",
  "x": "**MODELS** ---------------------------------- **QUANTIZED INPUTS TRAINING** We first train the vq-wav2vec quantization model following the gumbel-softmax recipe described in <cite>(Baevski et al., 2019b)</cite> . After training this model For quantizing MFCC and log-mel filterbanks we first compute dense features using the scripts from the Kaldi (Povey) toolkit.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_8",
  "x": "We train on 128 GPUs with a batch size of 3072 tokens per GPU giving a total batch size of 393k tokens (Ott et al., 2018) . Each token represents 10ms of audio data. To mask the input sequence, we follow <cite>(Baevski et al., 2019b)</cite> and randomly sample p = 0.05 of all tokens to be a starting index, without replacement, and mask M = 10 consecutive tokens from every sampled index; spans may overlap. ---------------------------------- **CONTINUOUS INPUTS TRAINING**",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_9",
  "x": "During training we discard longer examples and during evaluation we discard randomly chosen tokens from each example until they are at most 2048 tokens long. We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in<cite> Baevski et al. (2019b)</cite> . The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_10",
  "x": "The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning. Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features. The discrete BERT training augments the wav2vec features with a higher level of representation that captures the sequential structure of the full utterance through the masked language modeling loss.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_11",
  "x": "---------------------------------- **DISCUSSION AND RELATED WORK** The the success of BERT (Devlin et al., 2018) and Word2Vec (Mikolov et al., 2013) for NLP tasks motivated more research on self-supervised approaches for acoustic word embedding and unsupervised acoustic feature representation (Bengio and Heigold; Levin et al.; Chung et al., b; He et al.; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> , either by predicting masked discrete or continuous input, or by contrastive prediction of neighboring or similarly sounding segments using distant supervision or proximity in the audio signal as an indication of similarity. In (Kamper et al.) a dynamic time warping alignment is used to discover similar segment pairs. Our work is inspired by the research efforts in reducing the dependence on labeled data for building ASR systems through unsupervised unit discovery and acoustic representation leaning (Park and Glass, 2008; Glass; et.",
  "y": "background"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_0",
  "x": "Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] . Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention Networks [12] and Pythia v1.0 [13] . Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model <cite>[14]</cite> . Finally, we discuss the observations and future directions.",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_1",
  "x": "[20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential Modules [30] , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA Challenge 2018 1 . The architecture is similar to Teney et al. <cite>[14]</cite> with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models. Differential Networks [20] : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN [22] . The differential modules [30] are used to refine the features in both text and images.",
  "y": "similarities"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_2",
  "x": "We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model <cite>[14]</cite> . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets.",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_3",
  "x": "In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance. ---------------------------------- **V. CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_0",
  "x": "However, relations between arguments are often a lot more diverse than simple contrasts that can be captured through antonyms, and may rely on world knowledge (Kishimoto et al., 2018) . It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_1",
  "x": "It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . These models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013) , GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1).",
  "y": "motivation"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_2",
  "x": "We used the Penn Discourse Tree Bank (Prasad et al., 2008) , the largest available manually annotated discourse corpus. It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in<cite> Bai and Zhao (2018)</cite> , we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009) , which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015) , which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 0-22, as promoted in . We also follow the standard in the literature to formulate the task as an 11-way classification task.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_3",
  "x": "Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful.",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_4",
  "x": "In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_5",
  "x": "We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ---------------------------------- **METHOD**",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_6",
  "x": "The biomedical domain is very different from the WSJ or the data on which the BERT model was trained. The BioDRB contains a lot of professional words / phrases that are extremely hard to model. In order to test the ability of the BERT model on cross-domain data, we performed finetuning on PDTB while testing on BioDRB. We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_8",
  "x": "From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90",
  "y": "differences"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_0",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools.",
  "y": "background"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_1",
  "x": "Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "motivation future_work"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_2",
  "x": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "future_work motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_3",
  "x": "Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_4",
  "x": "However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods. More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_0",
  "x": "Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative constituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a ; * The research reported in this paper was partially supported by the Research Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model 1 based on these previous works.",
  "y": "background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_1",
  "x": "Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. ---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner.",
  "y": "background uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_2",
  "x": "Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. ---------------------------------- **DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_3",
  "x": "The factorization of the parsing model allows us to develop an exact decoding algorithm for it. This algorithm is more complex than the approximate decoding algorithm of<cite> Huang (2008)</cite> .",
  "y": "differences"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_5",
  "x": "In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002) . A factor \u03bb is introduced to balance the two models. It is tuned on a development set using the gold sec- (2003) 90.70 Carreras et al. (2008) 91.1 Re-scoring Collins (2000) 89.70 Charniak and Johnson (2005) 91.02 The parser of Charniak and Johnson 91.40 43.54<cite> Huang (2008)</cite> 91.69 43.5 Combination Fossum and Knight (2009) 92. 4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (Kiefer, 1953) . The parameters \u03b8 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and<cite> Huang (2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_0",
  "x": "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_1",
  "x": "In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed. Other possible variations are the removal of stop words and the lemmatization [21] . In a similar way, the number of characters and variations related to lemmatization and stop words removal could also be considered at sentence level.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_2",
  "x": "Basically, this law states that the bigger the whole, the smaller its parts and vice-versa. However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in<cite> [28]</cite> asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words. In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic. ---------------------------------- **SIMILARITIES BETWEEN DISTRIBUTIONS**",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_3",
  "x": "It is expected that h * = 0.5. Using the The Brothers Karamazov as example again, Fig. 4a and 4b illustrate, respectively, F (m) and the differences between the Hurst exponents for all six series. We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * \u223c 0.5, implying in long-range correlations. All the series from the other books reflects this behavior (h \u223c 0.75). This result is consistent with the multifractal analysis performed in<cite> [28]</cite> .",
  "y": "similarities"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_0",
  "x": "This principle has already been used to explain the origins of other linguistic laws: Zipf's law of abbreviation, namely, the frequency of more frequent words to be shorter [3,<cite> 4]</cite> , and Menzerath's law, the tendency of a larger linguistic construct to be made of smaller components [5] . Our argument combines two constraints for compression: (1) non-singular coding, i.e. any two different words should not be represented by the same string of letters or phonemes, and (2) unique decipherability, i.e. given a continuous sequence of letters or phonemes, there should be only one way of segmenting it into words [6] . The former is needed to reduce the cost of retrieving the original meaning. The latter is required to reduce the cost of determining word boundaries. Thus both constraints on compression and compression itself, are realistic cognitive pressures that are vital to fight against the now-or-never bottleneck of linguistic processing [7] . Suppose that words are coded using an alphabet of N letters (or phonemes) and that p i and l i are the probability and the length of the i-th most probable word.",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_2",
  "x": "Mandelbrot's derivation assumes that typing at random determines the probability of a word, which has two key implications. First, a relationship between the length of a word and its probability<cite> [4]</cite> l = a log p + b, where a and b are constants (a < 0) defined on the parameters of the model as and Eq. 7 can be interpreted, approximately, as a linear generalization of the relationship between l and p of optimal uniquely decipherable codes in Eq.",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_3",
  "x": "Second, random typing is based exclusively on random choices but its parameters cannot be set at random: indeed, a precise tuning of the parameters is needed to mimic Zipf's law with \u03b1 = 1 [8] . In contrast, our argument only requires N to be large enough. Third, its assumptions are far reaching: compression allows one to shed light on the origins of three linguistic laws at the same time: Zipf's law for word frequencies, Zipf's law of abbreviation and Menzerath's law with the unifying principle of compression [3, <cite>4,</cite> 5] . There are many ways of explaining the origins of power-law-like distributions such as Zipf's law for word frequencies [12] but compression appears to be as the only one that can lead to a compact theory of statistical laws of language. Although uniquely decipherable codes are a subset of non-singular codes, it is tempting to think that both optimal non-singular coding and optimal uniquely decipherable coding cannot be satisfied to a large extend simultaneously.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_0",
  "x": "The traditional clustering measure of F-Score (Zhao et al., 2005 ) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_1",
  "x": "A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_2",
  "x": "The outcome of this process might be more helpful for systems that produce a large number of clusters. In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies. Subsequently, we present the use of V-measure <cite>(Rosenberg and Hirschberg, 2007)</cite> as an evaluation measure that can overcome the current limitations of F-Score. Finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of WSD performance. The proposed evaluation setting will be applied in the SemEval-2010 WSI task.",
  "y": "extends differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_3",
  "x": "Finally, the F-Score of the entire clustering solution is defined as the weighted average of the F-Scores of each GS sense (Formula 1), where q is the number of GS senses and N is the total number of target word ings 1 gs 2 gs 3 cl 1 500 100 100 cl 2 100 500 100 cl 3 100 100 500 stances. If the clustering is identical to the original classes in the datasets, F-Score will be equal to one. In the example of Table 1 , F-Score is equal to 0.714. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Homogeneity refers to the degree that each cluster consists of data points, which primarily belong to a single GS class.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_4",
  "x": "As it can be observed, a system with low entropy (high purity) does not necessarily achieve high F-Score. This is due to the fact that entropy and purity only measure the homogeneity of a clustering solution. For that reason, the 1c1inst baseline achieves a perfect entropy and purity, although its clustering solution is far from ideal. On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution. However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_5",
  "x": "This is due to the fact that entropy and purity only measure the homogeneity of a clustering solution. For that reason, the 1c1inst baseline achieves a perfect entropy and purity, although its clustering solution is far from ideal. On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution. However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_6",
  "x": "V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Recall that homogeneity refers to the degree that each cluster consists of data points which primarily belong to a single GS class. V-measure assesses homogeneity by examining the conditional entropy of the class distribution given the proposed clustering, i.e. H(GS|C). H(GS|C) quantifies the remaining entropy (uncertainty) of the class distribution given that the proposed clustering is known. As a result, when H(GS|C) is 0, we have the perfectly homogeneous solution, since each cluster contains only those data points that are members of a single class.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_7",
  "x": "**FORMULAS 2 AND 3 DEFINE H(GS) AND H(GS|C).** When there is only a single class (H(GS) = 0), any clustering would produce a perfectly homogeneous solution. In the worst case, the class distribution within each cluster is equal to the overall class distribution (H(GS|C) = H(GS)), i.e. clustering provides no new information. Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1\u2212 H(GS|C) H(GS) in any other case <cite>(Rosenberg and Hirschberg, 2007)</cite> . Symmetrically to homogeneity, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster.",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_8",
  "x": "---------------------------------- **H(C|GS) H(C)** in any other case. In the worst case, completeness will be equal to 0, particularly when H(C|GS) is maximal and equal to H(C). This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_9",
  "x": "However, its completeness is not 0, as one might intuitively expect. This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . This worst solution is not equivalent to the 1c1inst, hence completeness of 1c1inst is greater than 0. Additionally, completeness of this baseline benefits from the fact that around 18% of GS senses have only one instance in the test set. Note however, that on average this baseline achieves a lower completeness than most of the systems.",
  "y": "similarities"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_0",
  "x": "The usefulness of affordances in cognitive robotics is in the fact that they capture essential properties of environment objects in terms of the actions that a robot is able to perform with them [7, 8] . Some authors have suggested an alternative computational model called Object-Action Complexes (OACs) [9] , which links low-level sensorimotor knowledge with high-level symbolic reasoning hierarchically in autonomous robots. In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_1",
  "x": "In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings. That framework assumes that the robot action is known a priori during the training phase (e. g., the information \"grasping\" during a grasping experiment is given), and the resulting model can be used at testing to make inferences about the environment, including estimating the most likely action, based on evidence from other pieces of information. Several neuroscience and psychology studies build upon the theory of mirror neurons which we brought up in the Introduction. These studies indicate that perceptual input can be linked with the human action system for predicting future outcomes of actions, i. e., the effect of actions, particularly when the person possesses concrete personal experience of the actions being observed in others [16, 17] .",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_2",
  "x": "One difference between this line of research and ours is that we use real, noisy data acquired from robots and sensors to test our models, rather than virtual simulations. ---------------------------------- **PROPOSED APPROACH** In this paper, we combine (1) the robot affordance model of <cite>[10]</cite> , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements. We consider three manipulative gestures corresponding to physical actions performed by agent(s) onto objects on a table (see Fig. 1 ): grasp, tap, and touch.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_3",
  "x": "Our main contribution is that of extending <cite>[10]</cite> by relaxing the assumption that the action is known during the learning phase. This assumption is acceptable when the robot learns through self-exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another (human) agent. We estimate the action performed by a human user during a human-robot collaborative task, by employing statistical inference methods and Hidden Markov Models (HMMs). This provides two advantages. First, we can infer the executed action during training.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_4",
  "x": "First, we can infer the executed action during training. Secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances. ---------------------------------- **BAYESIAN NETWORK FOR AFFORDANCE-WORDS MODELING** Following the method adopted in <cite>[10]</cite> , we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it.",
  "y": "similarities uses"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_5",
  "x": "The symbolic variables and their discrete values are listed in Table 1 . In addition to the symbolic variables, the model also includes word variables, describing Figure 3 : Structure of the HMMs used for human gesture recognition, adapted from [4] . In this work, we consider three independent, multiple-state HMMs, each of them trained to recognize one of the considered manipulation gestures. the probability of each word co-occurring in the verbal description associated to a robot experiment in the environment. This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> .",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_6",
  "x": "**COMBINING THE BN WITH GESTURE HMMS** In this study we wish to generalize the model of <cite>[10]</cite> by observing external (human) agents, as shown in Fig. 1 . For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs. This corresponds to the Gesture HMMs block in Fig. 2 . The Affordance-Words Bayesian Network (BN) model and the Gestures HMMs may be combined in different ways [19] :",
  "y": "extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_7",
  "x": "1. the Gesture HMMs may provide a hard decision on the action performed by the human (i. e., considering only the top result) to the BN, 2. the Gesture HMMs may provide a posterior distribution (i. e., soft decision) to the BN, 3. if the task is to infer the action, the posterior from the Gesture HMMs and the one from the BN may be combined as follows, assuming that they provide independent information: In the experimental section, we will show that what the robot has learned subjectively or alone (by self-exploration, knowing the action identity as a prior <cite>[10]</cite> ), can subsequently be used when observing a new agent (human), provided that the actions can be estimated with Gesture HMMs as in [4] . ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_0",
  "x": "---------------------------------- **INTRODUCTION** Apart from its application to machine translation, the encoder-decoder or sequence-to-sequence (seq2seq) paradigm has been successfully applied to monolingual text-to-text tasks including simplification <cite>(Nisioi et al., 2017)</cite> , paraphrasing (Mallinson et al., 2017) , style transfer (Jhamtani et al., 2017) , sarcasm interpretation (Peled and Reichart, 2017) , automated lyric annotation (Sterckx et al., 2017) and dialogue systems (Serban et al., 2016) . A sequence of input tokens is encoded to a series of hidden states using an encoder network and decoded to a target domain by a decoder network. During decoding, an attention mechanism is used to indicate which are the relevant input tokens at each step.",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_1",
  "x": "We train on the Wikilarge collection used by Zhu (2010) . Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) . These are complex sentences paired with simplifications provided by Amazon Mechanical Turk workers and provide a more reliable evaluation of the task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_2",
  "x": "We extend the OpenNMT (Klein et al., 2017) framework with functions for attention generation and release our code as a submodule. We use a similar archi- Table 2 : Quantitative evaluation of existing baselines from previous work and seq2seq with prior attention from the CVAE when choosing an optimal z sample for BLEU scores. tecture as Zhu et al. (2010) and<cite> Nisioi et al. (2017)</cite> : 2 layers of stacked unidirectional LSTMs with bi-linear global attention as proposed by Luong et al. (2015) , with hidden states of 512 dimensions. The vocabulary is reduced to the 50,000 most frequent tokens and embedded in a shared 500-dimensional space. We train using SGD with batches of 64 samples for 13 epochs after which the autoencoder is trained by translating sequences from training data.",
  "y": "uses"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_3",
  "x": "Inclusion of an attention mechanism was instrumental to match existing baselines. Our standard seq2seq model with attention, without prior attention, obtains a score of 89.92 BLEU points, which is close to scores obtained by similar models used in existing 1 Fleish-Kincaid Grade Level index. work on neural text simplification (Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . In Table 2 , we compare our seq2seq model with attention and without prior attention. A value for BLEU of 90.14 is found for z =[\u22122,0] which was tuned on a development set.",
  "y": "similarities"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_4",
  "x": "For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments. For increasing values of z 1 , attention becomes more vertical and focused on single encoder states. This type of attention gives more control to the language model, as exemplified by output samples shown in Table 1 . Output from this region is far longer and less related to the source sentence.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_0",
  "x": "****IMPROVING CONTEXT MODELLING IN MULTIMODAL DIALOGUE GENERATION**** **ABSTRACT** In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_1",
  "x": "So far, most conversational agents are uni-modal -ranging from opendomain conversation (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to task oriented dialogue systems Lemon, 2010, 2011; Young et al., 2013; Singh et al., 2000; Wen et al., 2016) . While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_2",
  "x": "This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_3",
  "x": "This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_4",
  "x": "While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) .",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_5",
  "x": "Finally, output is generated by passing h dec n,m through an affine transformation followed by a softmax activation. The model is trained using cross entropy on next-word prediction. During generation, the decoder conditions on the previous output token. <cite>Saha et al. (2017)</cite> propose a similar baseline model for the <cite>MMD</cite> dataset, extending HREDs to include the visual modality. However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance.",
  "y": "similarities"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_6",
  "x": "In the following, we empirically show that our extension leads to better results in terms of text-based similarity measures, as well as quality of generated dialogues. Example contexts for a given system utterance; note the difference in our approach from <cite>Saha et al. (2017)</cite> when extracting the training data from the original chat logs. For simplicity, in this illustration we consider a context size of 2 previous utterances. '|' differentiates turns for a given context. We concatenate the representation vector of all images in one turn of a dialogue to form the image context.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_7",
  "x": "Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_8",
  "x": "Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ). ----------------------------------",
  "y": "differences motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_9",
  "x": "4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 . 5 Using attention clearly helps: even T-HRED-attn outperforms M-HRED (without attention) for the same context size. We also tested whether multimodal input has an impact on the generated outputs.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_10",
  "x": "We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points. 6 This can be primarily attributed to the way we created the input for our model from raw chat logs, as well as incorporating more information during decoding via attention. Figure 4 provides example output utterances using M-HRED-attn with a context size of 5.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_11",
  "x": "**CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) .",
  "y": "motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_12",
  "x": "**CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_13",
  "x": "In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) . Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_14",
  "x": "**ANALYSIS AND RESULTS** We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts. 4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder).",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_15",
  "x": "---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_16",
  "x": "The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ).",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_0",
  "x": "Stance detection (also called stance identi cation or stance classication) is one of the considerably recent research topics in natural language processing (NLP). It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} <cite>[12]</cite> . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_1",
  "x": "Nevertheless, in sentiment analysis, the sentiment of the author of a piece of text usually as Positive, Negative, and Neutral is explored while in stance detection, the stance of the author of the text for a particular target (an entity, event, etc.) either explicitly or implicitly referred to in the text is considered. Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems <cite>[12]</cite> . Previous work on stance detection include [16] where a stance classi er based on sentiment and arguing features is proposed in addition to an arguing lexicon automatically compiled. e ultimate approach performs be er than distribution-based and unigram-based baseline systems [16] . In [17] , the authors show that the use of dialogue structure improves stance detection in on-line debates.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_2",
  "x": "Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed. SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_3",
  "x": "SCIFNET creates networks of people who are stance targets, automatically from the related document collections [3] using stance expansion and re nement techniques to arrive at stance-coherent networks. A tweet data set annotated with stance information regarding six prede ned targets is proposed in [11] where this data set is annotated through crowdsourcing. e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_4",
  "x": "e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available. e domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_5",
  "x": "**STANCE DETECTION EXPERIMENTS USING SVM CLASSIFIERS** It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as <cite>[12]</cite> . In order to be used as a baseline and reference system for further studies on stance detection in Turkish tweets, we have trained two SVM classi ers (one for each target) using unigrams as features. Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets. e stopword list used is the list presented in [8] which, in turn, is the slightly extended version of the stopword list provided in [2] .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_6",
  "x": "e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> .",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_7",
  "x": "Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_8",
  "x": "e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_9",
  "x": "On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in [15] ) have been reported to achieve be er F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results.",
  "y": "uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_10",
  "x": "erefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_11",
  "x": "When employing such a procedure, other stance classes like Neither can be considered as well. e procedure will improve the quality the data set as well as the quality of prospective systems to be trained and tested on it. \u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set.",
  "y": "future_work uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_12",
  "x": "When employing such a procedure, other stance classes like Neither can be considered as well. e procedure will improve the quality the data set as well as the quality of prospective systems to be trained and tested on it. \u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set.",
  "y": "future_work"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_0",
  "x": "This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3] . We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite>), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] .",
  "y": "uses"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_1",
  "x": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite> ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_0",
  "x": "Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data. However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling <cite>(Bowman et al., 2016)</cite> . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_1",
  "x": "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_2",
  "x": "However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_3",
  "x": "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss <cite>(Bowman et al., 2016</cite>; , or resort to designing more sophisticated model structures (Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) .",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_4",
  "x": "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 .",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_5",
  "x": "The the second term is the KL divergence of the approximate posterior from prior, i.e., a regularisation pushing the learned posterior to be as close to the prior as possible. ---------------------------------- **VARIATIONAL AUTOENDODER WITH HOLISTIC REGULARISATION** In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_6",
  "x": "Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q \u03c6t and P (z t )) will contribute to the overall KL loss of the ELBO. By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form KL(Q \u03c6t (z t |x) P (z t )). ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works <cite>(Bowman et al., 2016</cite>; . The weight between these two terms of our model is simply 1 : 1.",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_7",
  "x": "**EXPERIMENTAL SETUP** ---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews.",
  "y": "background uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_8",
  "x": "**IMPLEMENTATION DETAILS** For the PTB dataset, we used the train-test split following <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018) . For the E2E dataset, we used the train-test split from the original dataset (Novikova et al., 2017) and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings (Mikolov et al., 2013) . We set the dimension of the hidden layers of both encoder and decoder to 256.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_9",
  "x": "KL annealing is used to tackled the latent variable collapse issue <cite>(Bowman et al., 2016)</cite> ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (Yang et al., 2017) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance. Table 2 shows the language modelling results of our approach and the baselines.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_10",
  "x": "These observations suggest that our approach can learn a better generative model for data. Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 . These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing <cite>(Bowman et al., 2016)</cite> , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_0",
  "x": "Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks. While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in <cite>(Conneau et al., 2017)</cite> . We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models. We show that by adding such constraints, superior sentence embeddings can be achieved. We compare our method with the original implementation and show improvements in several tasks.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_1",
  "x": "FastSent (Hill et al., 2016) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (Klein et al., 2015) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_2",
  "x": "While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work. AE Reg and LM Reg refers to the Auto-Encoder and Language-Model regularization terms described in 2.1 and Combined refers to optimizing with both terms. Bi-AE Reg and Bi-LM Reg refers to the bi-directional Auto-Encoder and bi-directional Language-Model regularization terms described in 2.2.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_3",
  "x": "We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ----------------------------------",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_4",
  "x": "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_5",
  "x": "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> . ---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_6",
  "x": "We denote { \u2212 \u2192 h t } and { \u2190 \u2212 h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling). The original model of <cite>(Conneau et al., 2017)</cite> was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 \u2212s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_7",
  "x": "We call the second regularization term in (3) a bi-directional auto-encoder regularization and in (4) a bi-directional language model regularization term. Again, \u03bb 1 and \u03bb 2 are hyper-parameters controlling the amount of regularization and were set to 0.5 in our experiments. ---------------------------------- **EXPERIMENTS** Following <cite>(Conneau et al., 2017)</cite> we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -Pang and Lee (2005) , SST -Socher et al. (2013) ), question-type (TREC -Li and Roth (2002) ), product reviews (CR - Hu and Liu (2004) ), subjectivity/objectivity (SUBJ - Pang and Lee (2005) ) and opinion polarity (MPQA -Wiebe et al. (2005) ).",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_10",
  "x": "Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations <cite>(Conneau et al., 2017)</cite> . However, every supervised learning tasks is prone to overfit. In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks. We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models. We note that the added regularization terms do come at the price of increasing the model size by ld parameters (where d and l are the dimensions of the word embedding and the LSTM hidden state, respectively) due to the added linear transformation (see 2.1).",
  "y": "motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_0",
  "x": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; .",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_1",
  "x": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; .",
  "y": "background motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_2",
  "x": "We present a detailed theoretical comparison of the efficiency of these three types of methods in \u00a73.3. ---------------------------------- **PRIOR APPROACHES** The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) . Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path \u03c0.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_3",
  "x": "The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) . Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path \u03c0. The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> . For this model, the embedding of a length-n path \u03a6 \u03c0 \u2208 R d\u00d7d is defined as the matrix product of the sequence of relation matrices for the relations in \u03c0.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_4",
  "x": "This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_5",
  "x": "---------------------------------- **PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_6",
  "x": "The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb .",
  "y": "differences uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_7",
  "x": "Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).**",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_8",
  "x": "We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T . Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).** Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients.",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_9",
  "x": "After this computation, the time to compute the scores of training positive and negative triples is O d2(\u03b7 + 1)E kb L . The time to increment gradients using each triple considered in training is O dEL 2 . The evaluation time memory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using precomputation. Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_10",
  "x": "Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively. Table 1 : KB completion results on NCI-PID test: comparison of our compositional learning approach (ALL-PATHS+NODES) with baseline systems. d is the embedding dimension; sampled paths occurring less than c times were pruned in PRUNED-PATHS.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_11",
  "x": "Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in<cite> (Guu et al., 2015)</cite> versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models? (iii) Does modeling intermediate path nodes improve the accuracy of KB completion? Datasets We used two datasets for evaluation: NCI-PID and WordNet. For the first set of experiments, we used the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) as our knowledge base, which was created by editors from the Nature Publishing Groups, in collaboration with the National Cancer Institute. It contains a collection of highquality gene regulatory networks (also referred to as pathways).",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_12",
  "x": "We identified genes belonging to the same family via the common letter prefix in their names, which adds 1936 triples to training. As a second dataset, we used a WordNet KB with the same train, dev, and test splits as<cite> Guu et al. (2015)</cite> . There are 38,696 entities and 11 types of knowledge base relations. The KB includes 112,581 triples for training, 2,606 triples for validation, and 10,544 triples for testing. WordNet does not contain textual relations and is used for a more direct comparison with recent works.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_13",
  "x": "The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_14",
  "x": "6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES additionally models nodes in the paths. PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_15",
  "x": "We also included the BILINEAR-DIAG baseline. Implementation Details We used batch training with RProp (Riedmiller and Braun, 1993) . The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results.",
  "y": "extends"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_16",
  "x": "However, when nodes are modeled, the compositional learning approach gains in accuracy as well, especially when text is jointly embedded. ---------------------------------- **NCI-PID RESULTS** Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation.",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_17",
  "x": "The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR-DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. 11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_18",
  "x": "(Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_19",
  "x": "Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS. (Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_20",
  "x": "(Guu et al., 2015) and our implementation. The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_21",
  "x": "The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5. As can be seen, lower count cutoff performed better for paths up to length 3, but we could not run the method with path lengths up to 5 and count cutoff of 1, due to excessive memory requirements (more than 248GB). When using count cutoff of 10, paths up to length 5 performed worse than paths up to length 3. This performance degradation could be avoided with 12 We ran the trained model distributed by<cite> Guu et al. (2015)</cite> and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5. Due to the discrepancy, we report the original results from the authors' paper which lack MAP values instead.",
  "y": "uses"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_0",
  "x": "Most approaches utilize complex features to re-estimate the tree structures of a given sentence [1, 2, 3] . Unfortunately, sizes of treebanks are generally small and insufficient, which results in a common problem of data sparseness. Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8,</cite> 9 ].",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_1",
  "x": "**** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8</cite>, 9] . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_2",
  "x": "The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [<cite>8</cite>] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_0",
  "x": "Machine translation systems are generally trained on clean data, without spelling errors. Yet machine translation may be used in settings in which robustness to such errors is critical: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018) , and interactive settings in which users must enter text on a mobile device. Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) .",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_1",
  "x": "Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques. Drawing inspiration from dropout (Srivastava et al., 2014) and noise-based regularization methods, we explore the space of random noising methods at training time, and evaluate performance on both clean text and text corrupted by natural noise based on real spelling mistakes on Wikipedia (Max and Wisniewski, 2010 ).",
  "y": "background motivation"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_2",
  "x": "<cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length. In Section 3.2, we show that they are indeed the primary contributors in improving our model's robustness to natural noise.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_3",
  "x": "1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length. In Section 3.2, we show that they are indeed the primary contributors in improving our model's robustness to natural noise. During training, we used a balanced diet of all four noise types by sampling the noise, for each to-ken, from a multinomial distribution of 60% clean (no noise) and 10% probability for each type of noise.",
  "y": "background motivation"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_4",
  "x": "Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise. Moreover, the negative effects of training on synthetic noise seem to be limited to both negative and positive fluctuations that are smaller than 1 BLEU point. ----------------------------------",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_5",
  "x": "We used beam search for generating the translations (5 beams), and computed BLEU scores to measure performance on the test 3 https://github.com/pytorch/fairseq set. Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise. Moreover, the negative effects of training on synthetic noise seem to be limited to both negative and positive fluctuations that are smaller than 1 BLEU point.",
  "y": "differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_6",
  "x": "Table 3 shows the model's performance on the German-to-English dataset when training with various mixtures of noise. We find that deletion is by far the most effective synthetic noise in preparing our model for natural orthographical errors, followed by insertion. The French and Czech datasets exhibit the same trend. We conjecture that the importance of deletion and insertion is that they distort the typical distances between characters, requiring the CNN character encoder to become more invariant to unexpected character movements. The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while <cite>previous work</cite> that trained only on substitutions and swaps was not able to do so <cite>(Belinkov and Bisk, 2018)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_7",
  "x": "Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_8",
  "x": "Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder.",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_9",
  "x": "In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_10",
  "x": "In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise. An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",
  "y": "background motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_0",
  "x": "However, evaluating measures of phrase-level similarity directly against human judgments of similarity ignores the problem that it is not always possible to determine meaning in a compositional manner. If we compose the meaning representations for red and herring, we might expect to get a very different representation from the one which could be directly inferred from corpus observations of the phrase red herring. Thus any judgements of the similarity of two composed phrases may be confounded by the degree to which those phrases are compositional. In this paper, we use a compound noun compositionality dataset (<cite>Reddy et al., 2011</cite>) to investigate the extent to which the underlying definition of context has an effect on a model's ability to support composition. We compare the Anchored Packed Tree (APT) model (Weir et al., 2016) , where composition is an integral part of the distributional model, with the commonly employed approach of applying na\u00efve compositional operations to state-of-the-art distributional representations.",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_1",
  "x": "Compositionality detection (<cite>Reddy et al., 2011</cite>) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. <cite>Reddy et al. (2011)</cite> introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase spelling bee is deemed to have high literalness in its use of the first constituent, low literalness in its use of the second constituent and a medium level of literalness with respect to the whole phrase. Assuming the distributional hypothesis (Harris, 1954) , the observed co-occurrences of compositional target phrases are highly likely to have occurred with one or both of the constituents independently.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_2",
  "x": "On the other hand, the observed cooccurrences of non-compositional target phrases are much less likely to have occurred with either of the constituents independently. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100).",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_3",
  "x": "A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) .",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_4",
  "x": "Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 . For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_5",
  "x": "For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus.",
  "y": "motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_6",
  "x": "We consider both the use of probabilities 2 and positive pointwise mutual information (PPMI) 1 Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the <cite>Reddy et al. (2011)</cite> evaluation task when trained on the BNC. However, these results were still significantly lower than those reported by <cite>Reddy et al. (2011)</cite> using the larger ukWaC corpus. 2 referred to as normalised counts by Weir et al. (2016) values.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_7",
  "x": "Levy et al. (2015) showed that the use of context distribution smoothing (\u03b1 = 0.75) in the PMI calculation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment with \u03b1 = 0.75 and \u03b1 = 1. 3 Having constructed elementary APTs, the APT composition process involves aligning and composing these elementary APTs. We investigate using INT , which takes the minimum of each of the constituent's feature values and UNI , which performs pointwise addition. Following <cite>Reddy et al. (2011)</cite> , when using the UNI operation, we experiment with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A 2 is the APT associated with the head of the phrase and A \u03b4 1 is the properly aligned APT associated with the modifier where \u03b4 is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as:",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_8",
  "x": "Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation. Table 3 : Average \u03c1 using APT representations. APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space.",
  "y": "differences"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_9",
  "x": "Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation. Table 3 : Average \u03c1 using APT representations. APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space.",
  "y": "differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_0",
  "x": "Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like Fast-Text (Bojanowski et al., 2017) , and, more recently, pretrained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT<cite> (Devlin et al., 2019)</cite> . In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_1",
  "x": "Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance<cite> (Devlin et al., 2019)</cite> . The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_2",
  "x": "Thus, for some common Basque words such as etxerantz (to the house) or medikuarenera (to the doctor), the subword tokenization generated by the monolingual BERT we trained will substantially differ from the output produced by the multilingual BERT: mBERT: Et #xer #ant #z ours: Etxera #ntz mBERT: Medi #kua #rene #ra ours: Mediku #aren #era More specifically, mBERT's subwords tend to be shorter and less interpretable, while our subwords are closer to linguistically interpretable strings, like mediku (doctor) aren ('s) and era (to the). Furthermore, most of the time the released models have been thoroughly tested only in English. Alternatively, multilingual versions have been tested in transfer learning scenarios for other languages, where they have not been compared to monolingual versions<cite> (Devlin et al., 2019)</cite> . The goal of this paper is to compare publicly available models for Basque with analogous models which have been trained with a larger, better quality corpus.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_3",
  "x": "More recently,<cite> Devlin et al. (2019)</cite> introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019) . When pre-training mBERT the corpora sizes in different languages are very diverse, with English corpora being order of magnitudes larger than that of the minority languages. The authors alleviate this issue by oversampling examples of lower resource languages.",
  "y": "motivation"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_4",
  "x": "Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik et al., 2018) , outperforming other well-known approaches such as BERT and ELMO <cite>(Devlin et al., 2019</cite>; Peters et al., 2018) . In any case, Flair is of interest to us because they distribute their own Basque pre-trained embedding models obtained from a corpus of 36M tokens (combining OPUS and Wikipedia). Flair-BMC models: We train our own Flair embeddings using the BMC corpus with the following parameters: Hidden size 2048, sequence length of 250, and a mini-batch size of 100. The rest of the parameters are left in their default setting. Training was done for 5 epochs over the full training corpus.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_5",
  "x": "For text classification tasks, the computed Flair embeddings are fed into a BILSTM 6 to produce a document level embedding which is then used in a linear layer to make the class prediction. Although for best results they recommend to stack their own Flair embeddings with additional static embeddings such as FastText, in this paper our objective is to compare the official pretrained Flair embeddings for Basque with our own Flair-BMC embeddings. ---------------------------------- **BERT LANGUAGE MODELS** We have trained a BERT<cite> (Devlin et al., 2019)</cite> model for Basque Language using the BMC corpus motivated by the rather low representation this language has in the original multilingual BERT model.",
  "y": "uses"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_6",
  "x": "We set the coverage percentage to 99.95. Model Architecture In the same way as the original BERT architecture proposed by<cite> Devlin et al. (2019)</cite> our model is composed by stacked layers of Transformer encoders (Vaswani et al., 2017) . Our approach follows the BERT BASE configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters. Pre-training objective Following BERT original implementation, we train our model on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) tasks. Even if the necessity of the NSP task has been questioned by some recent works (Yang et al., 2019; Liu et al., 2019; Lample and Conneau, 2019) we have decided to keep it as in the original paper to allow for head-to-head comparison.",
  "y": "extends differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_7",
  "x": "An upgraded version of BERT LARGE 7 has proven that WWM has substantial benefits in comparison with previous masking that was done after the sub-word tokenization. Pre-training procedure Similar to<cite> (Devlin et al., 2019)</cite> we use Adam with learning rate of 1e \u2212 4, \u03b2 1 = 0.9, \u03b2 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 0000 steps, and linear decay of the learning rate. The dropout probability is fixed to 0.1 on all the layers. As the attentions are quadratic to the sequence length, making longer sequences much more expensive, we pre-train the model with sequence length of 128 for 90% of the steps and sequence length of 512 for 10% of the steps. In total we train for 1, 000, 000 steps and a batch size of 256.",
  "y": "similarities"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_8",
  "x": "Flair is tuned on the development data using the test only for the final evaluation. We do not use the development set for training. For comparison between BERT models we fine-tune on the training data provided for each of the four tasks with both the official multilingual BERT<cite> (Devlin et al., 2019)</cite> model and with our BERTeus model (trained as described in Section 3.3.). Every reported result for every system is the average of five randomly initialized runs. The POS and NER experiments using mBERT and BERTeus are performed using the transformers library (Wolf et al., 2019) where it is recommended to remove the seed for random initialization.",
  "y": "similarities uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_0",
  "x": "Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems.",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_1",
  "x": "Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_2",
  "x": "Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_3",
  "x": "This led to convolution-only CTC models for long-range temporal dependencies [9] [10] [11] . However, these models have to be very deep (e.g., 17-19 convolutional layers on LibriSpeech [23] ) to cover the same context (Table 1) . While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_4",
  "x": "Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3.",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_5",
  "x": "Maximum path length Table 1 : Operation complexity of each layer type, based on [22] . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, <cite>[27]</cite> argue their LAS self-attention heads are differentiated phoneme detectors.",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_6",
  "x": "This is especially prohibitive for self-attention in terms of memory: recall that an attention matrix of dimension \u2208 R T \u00d7T is created, giving the T 2 factor in Table 1 . A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one <cite>[27]</cite> . Note that CTC will still require U \u2264 T /k, however.",
  "y": "uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_7",
  "x": "Self-attention is inherently content-based [22] , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define for position t. We consider three approaches: content-only [21] , which forgoes position encodings; additive [19] , which takes demb = dh and adds the encoding to the embedding; and concatenative, where one takes demb = 40 and concatenates it to the embedding. The latter was found necessary for self-attentional LAS <cite>[27]</cite> , as additive encodings did not give convergence. However, the monotonicity of CTC is a further positional inductive bias, which may enable the success of content-only and additive encodings.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_8",
  "x": "We also evaluate design choices in Table 4 . Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_9",
  "x": "We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers.",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_10",
  "x": "Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head). Character labels gave forward-and backward-attending heads (incidentally, averaging these would retrieve the bimodal distribution in [26] ) at all layers. This suggests a gradual expansion of context over depth, as is often engineered in convolutional CTC.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_11",
  "x": "We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances.",
  "y": "differences similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_0",
  "x": "Automatic essay scoring (AES) is the task of assigning grades to essays written in an educational setting, using a computer-based system with natural language processing capabilities. The aim of designing such systems is to reduce the involvement of human graders as far as possible. AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; <cite>Dong and Zhang, 2016</cite>; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task <cite>(Dong and Zhang, 2016)</cite> . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_1",
  "x": "The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . ---------------------------------- **METHOD** String kernels. Kernel functions (Shawe-Taylor and Cristianini, 2004) capture the intuitive notion of similarity between objects in a specific domain.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_2",
  "x": "Data set. To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle. The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_3",
  "x": "The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_4",
  "x": "We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . For the in-domain experiments, we use 5-fold cross-validation. The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_5",
  "x": "We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_6",
  "x": "For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0.",
  "y": "uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_7",
  "x": "Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_8",
  "x": "We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) . For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_9",
  "x": "For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) . For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases. We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_10",
  "x": "In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_11",
  "x": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results. Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts.",
  "y": "similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_12",
  "x": "Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts. The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_13",
  "x": "We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_14",
  "x": "We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_15",
  "x": "We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_16",
  "x": "---------------------------------- **CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_0",
  "x": "The task of referring expression generation (REG) has been studied since the 1970s [40, 22, 30, 7] , with most work focused on studying particular aspects of the problem in some relatively constrained datasets. Recent approaches have pushed this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> .",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_1",
  "x": "The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image. Similar to these recent methods, we also take a deep learning approach to referring expression generation and comprehension.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_2",
  "x": "Three recent approaches for referring expression generation <cite>[26]</cite> and comprehension [14, 33] also take a deep learning approach. However, we add visual object comparisons and tie together language generation for multiple objects. Referring expression generation has been studied for many years [40, 22, 30] in linguistics and natural language processing. These works were limited by data collection and insufficient computer vision algorithms. Together Amazon Mechanical Turk and CNNs have somewhat mitigated these limitations, allowing us to revisit these ideas on large-scale datasets.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_3",
  "x": "Recently, a large-scale referring expression dataset was collected by Kazemzadeh et al [19] featuring natural objects in the real world. Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19,<cite> 26]</cite> . The availability of large-scale referring expression datasets allows us to train deep learning models. Additionally, our analysis of these datasets motivates our incorporation of visual comparisons between same-type objects, and the need to tie together choices for referring expression generation between objects. ----------------------------------",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_4",
  "x": "**MODELS** We implement several model variations for referring expression generation and comprehension. The first set of models are recent state of the art deep learning approaches from Mao et al <cite>[26]</cite> . We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2).",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_5",
  "x": "**BASELINES** For comparison, we implement both the baseline and strong model of Mao et al <cite>[26]</cite> . Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation. In particular, object and context are modeled as features from a CNN trained to recognize 1,000 object categories [36] from ImageNet [34] . Specifically, the visual representation is composed of:",
  "y": "motivation uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_6",
  "x": "In Mao et al's baseline <cite>[26]</cite> , the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features. In addition, they also propose a stronger model that uses maximum mutual information (MMI) training to consider whether a listener would interpret a referring expression unambiguously. They impose this by penalizing the model if a generated referring expression could also be generated by some other object within the image. We implement both their original model and MMI model in our experiments. We subsequently refer to these two models as Baseline and MMI, respectively.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_7",
  "x": "W m and b m project the concatenation of the five types of features to be the final representation. ---------------------------------- **JOINT LANGUAGE GENERATION** For the referring expression generation task, rather than generating sentences for each object in an image separately [15] <cite>[26]</cite>, we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image. This makes sense intuitively -when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_8",
  "x": "We make use of 3 referring expression datasets in our work, all collected on top of the Microsoft COCO image collection [24] . One dataset, RefCOCOg <cite>[26]</cite> is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19] . In the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments . ---------------------------------- **DATASETS & ANALYSIS**",
  "y": "background uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_9",
  "x": "We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach <cite>[26]</cite> . The second type is people-vs-objects splits. One thing we observe from analyzing the datasets is that about half of the referred objects are people. Therefore, we create a split for RefCOCO and RefCOCO+ datasets that evaluates images containing multiple people (testA) vs images containing multiple instances of all other objects (testB). In this split all objects from an image will appear either in the training or testing sets, but not both.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_10",
  "x": "**EXPERIMENTS** We first perform some experiments to analyze the use of context in referring expressions (Sec 5.1). Given these findings, we then perform experiments evaluating the usefulness of our proposed visual and language innovations on the comprehension (Sec 5.2) and generation tasks (Sec 5.3). In experiments for the referring expression comprehension task, we use the same evaluation as Mao et al <cite>[26]</cite> , namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box. If the IOU is larger than 0.5 we count it as a true positive.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_12",
  "x": "**ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_13",
  "x": "**ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_14",
  "x": "For RefCOCOg, we evaluate on the per-object split as previous work <cite>[26]</cite> . Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO. Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_15",
  "x": "We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits. Similar improvements are also observed on top of the MMI model. In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_16",
  "x": "In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by <cite>[26]</cite> , which were trained uisng Multibox [4] . Results on shown in the bottom half of Table 2 . Although all comprehension accuracies drop due to imperfect detections, the improvements of our models over Baseline and MMI are still observed.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_0",
  "x": "Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works<cite> (Rubin et al., 2016</cite>; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try to model the interaction of sentences within the document. We observed a pattern in the way sentences cluster in different kind of news articles. Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_1",
  "x": "We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method. ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_2",
  "x": "<cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_3",
  "x": "Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> . McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_4",
  "x": "In this work, we show that our proposed model generalizes to articles from unseen publication sources. <cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model.",
  "y": "differences"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_5",
  "x": "We use SLN: Satirical and Legitimate News Database<cite> (Rubin et al., 2016)</cite> , RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines, \u2022 CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (Kim, 2014) with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_6",
  "x": "We conduct experiments across various settings and datasets. We report macro-averaged scores in Table 3 : 4-way classification results for different models. We only report F1-score following the SoTA paper. similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper<cite> (Rubin et al., 2016)</cite> reports a 10fold cross validation number on SLN.",
  "y": "differences"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_0",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_1",
  "x": "Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_2",
  "x": "The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics. Our concern for units of measurement is not a simple matter of precision but one of great theoretical importance: if the length of a dependency is measured in units of word length (e.g., syllables or phonemes) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. Therefore, pressure to reduce dependency lengths implies pressure for compression [25, 26] , linking a principle of word order with a principle that operates (nonexclusively) on individual words. An understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case.",
  "y": "background uses"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_3",
  "x": "Therefore we agree with [1] on the convenience of the term distance. A less flashy contribution of [6] has been promoting the need of controlling for sentence length (as a predictor of dependency length in their mixed-effects regression model) in research on dependency length minimization, an important methodological issue [15] that was addressed early [2] but neglected in subsequent research (e.g., [16, 17, 18] ). Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_4",
  "x": "A less flashy contribution of [6] has been promoting the need of controlling for sentence length (as a predictor of dependency length in their mixed-effects regression model) in research on dependency length minimization, an important methodological issue [15] that was addressed early [2] but neglected in subsequent research (e.g., [16, 17, 18] ). Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_5",
  "x": "Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics. Our concern for units of measurement is not a simple matter of precision but one of great theoretical importance: if the length of a dependency is measured in units of word length (e.g., syllables or phonemes) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words.",
  "y": "background uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_0",
  "x": "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010</cite>; Post and Gildea, 2009) . DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: \u2022 A natural recurring structure here would be the structure \"[ N P the [ N N president]]\", yet it occurs not at all in the data.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_1",
  "x": "---------------------------------- **PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_2",
  "x": "**PROBABILISTIC MODEL** In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees. 3",
  "y": "extends"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_3",
  "x": "**INFERENCE** Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) .",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_4",
  "x": "It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002;<cite> Cohn and Blunsom, 2010)</cite> . Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. ---------------------------------- **EVALUATION RESULTS** We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23.",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_5",
  "x": "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ). Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees. As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 depends mostly on TSG rules.",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_6",
  "x": "We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ). Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees. As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 depends mostly on TSG rules. If we look at the most likely derivations for the test data, TIG 0 assigns 663 insertions (351 left insertions) in the parsing of entire Section 23, meanwhile TIG assigns 3924 (2100 left insertions).",
  "y": "similarities uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_0",
  "x": "**INTRODUCTION** Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. The first is the advent of deep learning techniques (Goodfellow et al., 2016) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries<cite> (Hill et al., 2015</cite>; Hermann et al., 2015) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_1",
  "x": "In a pragmatic approach, recent work<cite> (Hill et al., 2015)</cite> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text (Hermann et al., 2015) . In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_2",
  "x": "We present a novel iterative, alternating attention mechanism that, unlike existing models<cite> (Hill et al., 2015</cite>; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) . It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks. ---------------------------------- **TASK DESCRIPTION**",
  "y": "differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_4",
  "x": "**TASK DESCRIPTION** One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_5",
  "x": "The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token. The dataset is divided into four subsets depending on the type of the word replaced. The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by<cite> (Hill et al., 2015)</cite> . The CNN 2 corpus was generated from news articles available through the CNN website.",
  "y": "motivation"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_6",
  "x": "where q i, t are the query attention weights and A q \u2208 R 2h\u00d7s , where s is the dimensionality of the inference GRU state, and a q \u2208 R 2h . The attention we use here is similar to the formulation used in<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) , but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in (Luong et al., 2015) . Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t\u22121 .",
  "y": "differences similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_8",
  "x": "Table 2 reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from<cite> (Hill et al., 2015)</cite> and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016) . ---------------------------------- **CBT** Main result Our model (line 7) sets a new stateof-the-art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader (line 5).",
  "y": "uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_10",
  "x": "These include, but are not limited to, handwriting recognition (Graves, 2013) , digit classification (Mnih et al., 2014) , machine translation (Bahdanau et al., 2015) , question answering (Sukhbaatar et al., 2015; Hermann et al., 2015) and caption generation (Xu et al., 2015) . In general, attention models keep a memory of states that can be accessed at will by learned attention policies. In our case, the memory is represented by the set of document and query contextual encodings. Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016;<cite> Hill et al., 2015)</cite> , which were also applied to question answering. The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by (Kadlec et al., 2016) , which in turn was based on the earlier Pointer Networks of (Vinyals et al., 2015) .",
  "y": "similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_11",
  "x": "The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models. Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015;<cite> Hill et al., 2015)</cite> . In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately. Moreover, we substitute the simple linear update with a GRU network.",
  "y": "similarities"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_0",
  "x": "Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) .",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_1",
  "x": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) . Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice.",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_2",
  "x": "The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the experiments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007) . We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, \u03c4 B , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1 , and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling<cite> (Koehn, 2004)</cite> , which is widely used in MT evaluation.",
  "y": "motivation differences"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_3",
  "x": "A bootstrap pseudo-sample consists of the translations by the two systems (X b , Y b ) of a bootstrapped test set<cite> (Koehn, 2004)</cite> , constructed by sampling with replacement from the original test set translations. The bootstrap distribution S boot of the test statistic is estimated by calculating the value of the pseudo-statistic The null hypothesis distribution S H 0 can be estimated from S boot by applying the shift method (Noreen, 1989 ), which assumes that S H 0 has the same shape but a different mean than S boot . Thus, S boot is transformed into S H 0 by subtracting the mean bootstrap statistic from every value in S boot . Once this shift-to-zero has taken place, the null hypothesis is rejected if the probability of observing a more extreme value than the actual statistic is lower than a predetermined p-value \u03b1, which is typically set to 0.05. In other words, the score difference is significant at level 1 \u2212 \u03b1.",
  "y": "background uses"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_0",
  "x": "**QVEC AND QVEC-CCA** We introduce QVEC-CCA-an intrinsic evaluation measure of the quality of word embeddings. Our method is a modification of QVEC-an evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource <cite>(Tsvetkov et al., 2015)</cite> . We review QVEC, and then describe QVEC-CCA. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_1",
  "x": "Resources that capture more coarse-grained, general properties can be used instead, for example, WordNet for semantic evaluation (Fellbaum, 1998) , or Penn Treebank (Marcus et al., 1993, PTB) for syntactic evaluation. Since these properties are not an exact match to the task, the intrinsic evaluation tests for a necessary (but possibly not sufficient) set of generalizations. ---------------------------------- **SEMANTIC VECTORS.** To evaluate the semantic content of word vectors,<cite> Tsvetkov et al. (2015)</cite> exploit supersense annotations in a WordNetannotated corpus-SemCor (Miller et al., 1993 Table 2 : Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB.",
  "y": "background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_2",
  "x": "The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. We extend the setup of<cite> Tsvetkov et al. (2015)</cite> with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016) , and the second is dependency parsing (Parse), using the stack-LSTM model of . ---------------------------------- **RESULTS.**",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_0",
  "x": "using solely public social media information. The advantage of such an effort is that the resulting tool provides non-intrusive and cost-effective means to detect and warn at-risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_1",
  "x": "The advantage of such an effort is that the resulting tool provides non-intrusive and cost-effective means to detect and warn at-risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_2",
  "x": "However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals. The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_3",
  "x": "Golder and Macy (2011) and Dodds et al. (2011) are interested in the temporal changes of mood on social media. Mysl\u00edn et al. (2013) focus on understanding the perception of emerging tobacco products by analyzing tweets. Social media, especially Twitter, has been recently utilized as a popular source of data for public health monitoring, such as tracking diseases (Ginsberg et al., 2009; YomTov et al., 2014; Nascimento et al., 2014; Greene et al., 2011; Chew and Eysenbach, 2010) , mining drug-related adverse events (Bian et al., 2012) , predicting postpartum psychological changes in new mothers (De Choudhury et al., 2013) , and detecting life satisfaction (Schwartz et al., 2013) and obesity (Chunara et al., 2013; Cohen-Cole and Fletcher, 2008; Fernandez-Luque et al., 2011) . We focus our attention on the language of food on social media to identify overweight communities and individuals. In the last couple of years, several variants of this problem have been considered<cite> (Fried et al., 2014</cite>; Abbar et al., 2015; Culotta, 2014; Ardehaly and Culotta, 2015) .",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_4",
  "x": "Generally, they use state-level populations, e.g., one of their classification tasks is to label whether a state is more overweight than the national median. Overweight rate is the percentage of adults whose Body Mass Index (BMI) is larger than a normal range defined by NIH. The classification task is to label whether a state is more overweight than the national median. Individuals' tweets are localized at state level as a single instance to train several classifier models, and the performance of models is evaluated using leave-one-out cross-validation. Importantly,<cite> Fried et al. (2014)</cite> train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health.",
  "y": "motivation"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_5",
  "x": "Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010) . These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_6",
  "x": "**AN INTERPRETABLE MODEL FOR COMMUNITY CLASSIFICATION** Our main data-collection idea is to use a playful 20-questions-like survey, automatically generated from a community-based model, which can be widely deployed to acquire training data on individuals. Our approach is summarized in Figure 1 . The first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions. To this end, we started with the same settings as<cite> Fried et al. (2014)</cite> : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_7",
  "x": "Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state. For each state, we extracted features based on unigram (i.e., single) words and hashtags from all the above tweets localized to the corresponding state. To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates<cite> (Fried et al., 2014</cite> Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data. motivation for this decision was interpretability: as shown below, decision trees can be easily converted into a series of if . . . then . . . else . . . statements, which form the building blocks of the quiz.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_8",
  "x": "Majority baseline 50.89 SVM<cite> (Fried et al., 2014)</cite> 80.39 RF (food + hashtags) 82.35 Discretized RF (food + hashtags) 78.43 Table 2 : Random forest (RF) classifier performance on state-level data relative to majority baseline and<cite> Fried et al. (2014)</cite> 's best classifier. We include two versions of our classifier: the first keeps numeric features (e.g., word counts) as is, whereas the second discretizes numeric features to three bins. of How often do you mention \"fruit\" in your tweets? Table 1 shows the questions and corresponding answers we used for the three left-most decision nodes in Figure 2 . Conversion to natural language questions was as consistent as possible.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_9",
  "x": "This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%. As discussed earlier, this discretization step was necessary to create intelligible Likert-scaled questions (Likert, 1932) .",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_10",
  "x": "This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%. As discussed earlier, this discretization step was necessary to create intelligible Likert-scaled questions (Likert, 1932) .",
  "y": "differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_0",
  "x": "However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> . OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual).",
  "y": "similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_1",
  "x": "OLID is an annotated dataset with a three-level annotation model. We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_2",
  "x": "We show that breaking down offensive content into sub-categories by taking the type and target of offenses into account results in a flexible annotation model that can relate to the phenomena captured by previously annotated datasets such as the one by . Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> . Section 5 presents an analysis of the results of the shared task, and, finally, Section 6 concludes this paper presenting avenues for future work.",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_3",
  "x": "Offensive language: The GermEval 2 (Wiegand et al., 2018) shared task focused on Offensive language identification in German tweets. A dataset of over 8,500 annotated tweets was provided for a course-grained binary classification task in which systems were trained to discriminate between offensive and non-offensive tweets and a second task where the organizers broke down the offensive class into three classes: profanity, insult, and abuse. Toxic comments: The Toxic Comment Classification Challenge was an open competition at Kaggle which provided participants with comments from Wikipedia labeled in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this. Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks.",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_4",
  "x": "**TASK DESCRIPTION AND EVALUATION** The training and testing material used for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, built for this task. OLID was annotated using a hierarchical three-level annotation model introduced in <cite>Zampieri et al. (2019)</cite> . We use the annotation of each of the three layers in OLID to each sub-task in OffensEval as follows: ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_5",
  "x": "In this Section we summarize OLID, the dataset used for this task. A detailed description of the data collection process and annotation is presented in <cite>Zampieri et al. (2019)</cite> . OLID is a large collection of English tweets annotated using a hierarchical three-layer annotation model. It contains 14,100 annotated tweets divided in a training partition containing 13,240 tweets and a test partition containing 860 tweets. Additionally, a small trial set containing 320 tweets was made available before the start of the competition.",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_6",
  "x": "**CONCLUSION** In this paper, we presented the results of SemEval-2016 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval). In OffensEval we used OLID<cite> (Zampieri et al., 2019)</cite> , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers. message (sub-task B); and 3) what is the target of the offensive (sub-task C). OLID is publicly available to the research community.",
  "y": "similarities uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_0",
  "x": "In phrase-based statistical MT (PSMT), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_1",
  "x": "In phrase-based statistical MT (PSMT), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis.",
  "y": "background motivation"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_2",
  "x": "We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2). We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_3",
  "x": "In order to use features within the system's log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT ( \u00a73). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation ( \u00a73.2). We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_4",
  "x": "---------------------------------- **REORDERING-AS-PREPROCESSING** The reordering-as-preprocessing approach addresses the PSMT reordering difficulty by removing word order differences prior to translation. This is done with a preprocessing step where the input sentence is parsed and a reordered alternative created on the basis of the resulting parse tree. Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> .",
  "y": "extends"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_5",
  "x": "This is done with a preprocessing step where the input sentence is parsed and a reordered alternative created on the basis of the resulting parse tree. Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation. The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_6",
  "x": "Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_7",
  "x": "The authors note this finding but do not analyse it further. ---------------------------------- **FEATURES FOR IMPROVED TRANSLATION** Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_8",
  "x": "Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering. Our work has some similarities to that of Zwarts and Dras (2008) but uses the log-linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation. This idea of using linguistic features to improve statistical MT has appeared in a number of recent papers.",
  "y": "background differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_9",
  "x": "\u00a73.1 introduces the lattice input format, by which we provide the system with two variants of the input sentence: the original and the reordered alternative produced by the preprocessing step. \u00a73.2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives. Our system is built upon the PSMT system Moses . For reordering, we use the Berkeley parser (Petrov et al., 2006 ) and the rules given by <cite>Collins et al. (2005)</cite> , but any reordering preprocessing step could equally be used. Further details of our systems are given in \u00a74.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_10",
  "x": "5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1). The reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above. We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_11",
  "x": "4 We run all of our experiments using the Moses Experiment Management System; configuration files and scripts to reproduce our experiments are available online. 5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1). The reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_12",
  "x": "We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths. confidence features (+FEATURES). We do not explore different subsets of the features here. For evaluation we use the standard BLEU metric (Papineni et al., 2002) , which measures n-gram overlap between the candidate translation and the given reference translation.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_13",
  "x": "The final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus. Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_14",
  "x": "The final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus. Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_15",
  "x": "However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle.",
  "y": "background uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_16",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_17",
  "x": "In the case where these reordered alternatives are all possible combinations of parts of one reordering process, our system approaches the work described in \u00a72.4, and in fact those systems will probably be more suitable as the preprocessing takes over the role of the PSMT distortion model. Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements. The next major phase of this work is to extend and explore the feature space.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_18",
  "x": "For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements. The next major phase of this work is to extend and explore the feature space. This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system. ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_19",
  "x": "We find that providing the system with this choice results in improved translation performance, achieving a BLEU score of 21.39, 0.62 higher than the baseline. We then augment the translation model of our system with a number of features to express our confidence in the reordering. While these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically-informed reordering. While our reordering step is a reimplementation of the <cite>Collins et al. (2005)</cite> system, contrary to their findings we do not see an improvement using the reordering step alone. This provides evidence against the idea that reordering improves translation performance absolutely.",
  "y": "differences uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_0",
  "x": "****DECIPHERMENT OF SUBSTITUTION CIPHERS WITH NEURAL LANGUAGE MODELS**** **ABSTRACT** Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by<cite> Nuhn et al. (2013)</cite> .",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_1",
  "x": "On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. ---------------------------------- **INTRODUCTION** Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011;<cite> Nuhn et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_2",
  "x": "However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time. ---------------------------------- **DECIPHERMENT MODEL** We use the notation from<cite> Nuhn et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_3",
  "x": "\u03c6 is called an extension of \u03c6, if f is fixed in \u03c6 such that \u03b4(\u03c6 (f ), \u03c6(f )) yields true \u2200f \u2208 V f which are already fixed in \u03c6 where \u03b4 is Kronecker delta. Decipherment is then the task of finding the \u03c6 for which the probability of the deciphered text is maximized. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm<cite> (Nuhn et al., 2013)</cite> which incrementally finds the most likely substitutions using the language model scores as the ranking. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_4",
  "x": "P (S) = P (w 1 , w 2 , w 3 , ..., w N ) ---------------------------------- **BEAM SEARCH** Algorithm 1 is the beam search algorithm<cite> (Nuhn et al., 2013</cite> (Nuhn et al., , 2014 Hs. ADD((\u2205,0)) 5:",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_5",
  "x": "States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given subsequence. In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer 2 . ---------------------------------- **1:1 SUBSTITUTION CIPHERS** In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) ,<cite> Nuhn et al. (2013)</cite> and Hauer et al. (2014) .",
  "y": "similarities uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_6",
  "x": "**I H A V E D E P O S I T E D I N T H E C O U N T Y O F B E D F O R D A B O U T F O U R M I L E S F R O M B U F O R D S I N A N E X C A V A T I O N O R V A U L T S I X F E E T B E L O W T H E S U R F A C E O F T H E G R O U N D T H E F O L L O W I N G A R T I C L E S B E L O N G I N G J O I N T L Y T O T H E P A** ---------------------------------- **AN EASY CIPHER: ZODIAC-408** Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms. Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm<cite> (Nuhn et al., 2013)</cite> with beam size of 10M with a 6-gram LM which gives an SER of 2%.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_7",
  "x": "Nuhn et al. (2014) present various improvements to the beam search algorithm in<cite> Nuhn et al. (2013)</cite> including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model. They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search. Their approach is the best for short ciphers. Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_8",
  "x": "---------------------------------- **CONCLUSION** This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from<cite> Nuhn et al. (2013</cite>; and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
  "y": "extends differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_0",
  "x": "We provide several popular schedulers, e.g., the inverse square-root scheduler from <cite>Vaswani et al. (2017)</cite> and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) . Reproducibility and forward compatibility. FAIRSEQ includes features designed to improve reproducibility and forward compatibility. For example, checkpoints contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed. FAIRSEQ also provides forward compatibility, i.e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading.",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_1",
  "x": "Inference. FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017;<cite> Vaswani et al., 2017</cite>; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used. This can speed up a na\u00efve implementation without caching by up to an order of magnitude, since only new states are computed for each token. For some models, this requires a component-specific caching implementation, e.g., multi-head attention in the Transformer architecture. During inference we build batches with a variable number of examples up to a user-specified number of tokens, similar to training.",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_2",
  "x": "FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; Edunov et al., 2018b,a; Chen et al., 2018; Song et al., 2018; Wu et al., 2019) , language modeling , abstractive document summarization (Fan et al., 2018a; Narayan et al., 2018) , story generation (Fan et al., 2018b , error correction (Chollampatt and Ng, 2018) , multilingual sentence embeddings (Artetxe and Schwenk, 2018) , and dialogue (Miller et al., 2017; Dinan et al., 2019) . ---------------------------------- **MACHINE TRANSLATION** We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_3",
  "x": "**MACHINE TRANSLATION** We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ).",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_4",
  "x": "For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_5",
  "x": "The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate .",
  "y": "differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_6",
  "x": "FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ---------------------------------- **LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_7",
  "x": "FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and ---------------------------------- **ABSTRACTIVE DOCUMENT SUMMARIZATION**",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_0",
  "x": "Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_1",
  "x": "Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about language and morphology can be transferred. ---------------------------------- **INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings.",
  "y": "background motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_2",
  "x": "Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure.",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_3",
  "x": "Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_4",
  "x": "However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning -they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure. Several possibilities exist: (i) learning of target tag specific word transformations from the high-resource language (trans); (ii) training of the character language model of the decoder (LM); (iii) learning a bias to copy a large part of the input (copy), since members of the same paradigm mostly share the same stem; (iv) a general regularization effect obtained by multitask training (reg).",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_5",
  "x": "Even between two morphologically rich languages transfer is difficult if they are unrelated, since inflections often mark dissimilar subcategories and word forms do not share similarities. However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other. They make this possible by training an encoder-decoder model and appending a special tag (i.e., embedding) for each language to the input of the system, similar to (Johnson et al., 2016) . It is currently unclear, though, what the nature of this transfer is, motivating our work which explores this in more detail. Model description.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_6",
  "x": "They make this possible by training an encoder-decoder model and appending a special tag (i.e., embedding) for each language to the input of the system, similar to (Johnson et al., 2016) . It is currently unclear, though, what the nature of this transfer is, motivating our work which explores this in more detail. Model description. The model<cite> Kann et al. (2017)</cite> use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) . It is trained on maximizing the following log-likelihood:",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_7",
  "x": "In order to answer the questions raised in the introduction, we conduct the following experiments. ---------------------------------- **DATA** We use the Romance and Arabic language data from<cite> Kann et al. (2017)</cite> . In particular, each training file contains 12, 000 high-resource examples mixed with 50 or 200 fixed Spanish instances.",
  "y": "uses"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_8",
  "x": "We can explain this with the model learning to copy -it has no intrinsic way of knowing which input character equals which output character in the vocabulary unless it has seen it at least once. However, for 200 Spanish examples, we can expect all characters to appear in the Spanish training data, such that the character language model and tag-output correspondence get more important. This explains the unexpected result that l-emb performs best for Arabic (200) and Portuguese (200): both source languages potentially confuse the language model; in Portuguese we contribute this to a big overlap of lemmata in the two languages with Portuguese often inflecting in a different way<cite> (Kann et al., 2017)</cite> . Further, the differences in performance between original and t-emb show that the model indeed learns information from the tags, supposedly which output sequence is more likely to appear with which tag. The l-emb-sep and t-emb-sep results show that a separation symbol clearly improves the model's performance.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_9",
  "x": "There exists much work on multi-task learning with encoderdecoder RNNs for machine translation (Johnson et al., 2016; Dong et al., 2015; Firat et al., 2016; Ha et al., 2016) . Alonso and Plank (2016) explored multi-task learning empirically, analyzing when it improves performance. Here, we focus on how transfer via multi-task learning works. Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> , in order to encourage the development of systems for the task.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_0",
  "x": "In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_1",
  "x": "In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed. The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_2",
  "x": "The model consists of both an extractor and abstractor, where the extractor picks out salient sentences first from a source article, and then the abstractor rewrites and compresses the extracted sentences into a complete summary. It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences. To resolve the problem, we present a novel neural extractor exploiting the pre-trained LMs (BERT in this work) which are expected to perform better according to the recent studies (Liu, arXiv:1909 .08752v3 [cs.CL] 26 Sep 2019 2019 Zhang et al., 2019c) .",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_3",
  "x": "**BACKGROUND** ---------------------------------- **SENTENCE REWRITING** In this paper, we focus on single-document multisentence summarization and propose a neural abstractive model based on the Sentence Rewriting framework<cite> (Chen and Bansal, 2018</cite>; Xu and Dur-rett, 2019) which consists of two parts: a neural network for the extractor and another network for the abstractor. The extractor network is designed to extract salient sentences from a source article.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_4",
  "x": "We use LSTM Pointer Network (Vinyals et al., 2015) as the decoder to select the extracted sentences based on the above sentence representations. The decoder extracts sentences recurrently, producing a distribution over all of the remaining sentence representations excluding those already selected. Since we use the sequential model which selects one sentence at a time step, our decoder can consider the previously selected sentences. This property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already. As the decoder structure is almost the same with the previous work, we convey the equations of<cite> Chen and Bansal (2018)</cite> to avoid confusion, with minor modifications to agree with our notations.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_5",
  "x": "Our abstractor is practically identical to the one proposed in<cite> Chen and Bansal (2018)</cite> . ---------------------------------- **TRAINING** In our model, an extractor selects a series of sentences, and then an abstractor paraphrases them. As they work in different ways, we need different training strategies suitable for each of them.",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_6",
  "x": "As usual in RL for sequence prediction, we pre-train submodules and apply RL to fine-tune the extractor. ---------------------------------- **TRAINING SUBMODULES** Extractor Pre-training Starting from a poor random policy makes it difficult to train the extractor agent to converge towards the optimal policy. Thus, we pre-train the network using cross entropy (CE) loss like previous work (Bahdanau et al., 2017;<cite> Chen and Bansal, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_7",
  "x": "This encourages the model to extract additional sentences only when they are expected to increase the final return. Following<cite> Chen and Bansal (2018)</cite> , we use the Advantage Actor Critic (Mnih et al., 2016) method to train. We add a critic network to estimate a value function V t (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ), which then is used to compute advantage of each action (we will omit the current state (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ) to simplify): where Q t (s i ) is the expected future reward for selecting s i at the current step t. We maximize this advantage with the policy gradient with the where \u03b8 \u03c0 is the trainable parameters of the actor network (original extractor).",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_8",
  "x": "---------------------------------- **EVALUATION** We evaluate the performance of our method using different variants of ROUGE metric computed with respect to the gold summaries. On the CNN/Daily Mail and DUC-2002 dataset, we use standard ROUGE-1, ROUGE-2, and ROUGE- L (Lin, 2004) on full length F 1 with stemming as previous work did (Nallapati et al., 2017; See et al., 2017;<cite> Chen and Bansal, 2018)</cite> . On NYT50 dataset, following Durrett et al. (2016) and Paulus et al. (2018) , we used the limited length ROUGE recall metric, truncating the generated summary to the length of the ground truth summary.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_9",
  "x": "Extractive Summarization As See et al. (2017) showed, the first 3 sentences (lead-3) in an article form a strong summarization baseline in CNN/Daily Mail dataset. Therefore, the very first objective of extractive models is to outperform the simple method which always returns 3 or 4 sentences at the top. However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks. Our extractor network with BERT (BERT-ext), as a single model, outperforms those models with large margins.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_10",
  "x": "This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_11",
  "x": "This shows the effectiveness of our learning method. Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_12",
  "x": "In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_13",
  "x": "With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> . In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_14",
  "x": "Sentence-matching finds sentences with the highest ROUGE-L score for each sentence in the gold summary. This search method matches with the best reward from<cite> Chen and Bansal (2018)</cite> . Greedy Search is the same method explained for extractor pre-training in section 4.1. ---------------------------------- **COMBINATION SEARCH SELECTS A SET OF SENTENCES**",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_15",
  "x": "---------------------------------- **COMBINATION SEARCH SELECTS A SET OF SENTENCES** ---------------------------------- **MODELS** Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_16",
  "x": "This result supports our training strategy; pretraining with Greedy Search and final optimization with the combinatorial return. Additionally, we experiment to verify the contribution of our training method. We train the same model with different training signals; Sentencelevel reward from<cite> Chen and Bansal (2018)</cite> and combinatorial reward from ours. The results are shown in Table 4 . Both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence-level reward.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_17",
  "x": "Relevance is based on the summary containing important, salient information from the input article, being correct by avoiding contradictory/unrelated information, and avoiding repeated/redundant information. Readability is based on the summarys fluency, grammaticality, and coherence. To evaluate both these criteria, we design a Amazon Mechanical Turk experiment based on ranking method, inspired by Kiritchenko and Mohammad (2017) . We randomly select 20 samples from the CNN/Daily Mail test set and ask the human testers (3 for each sample) to rank summaries (for relevance and readability) produced by 3 different models: our final model, that of<cite> Chen and Bansal (2018)</cite> and that of Liu (2019) . 2, 1 and 0 points were given according to the ranking.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_18",
  "x": "See et al. (2017) introduced Pointer Generator network that implicitly combines the abstraction with the extraction, using copy mechanism (Gu et al., 2016; Zeng et al., 2016) . More recently, there have been several studies that have attempted to improve the performance of the abstractive summarization by explicitly combining them with extractive models. Some notable examples include the use of inconsistency loss (Hsu et al., 2018) , key phrase extraction (Li et al., 2018; Gehrmann et al., 2018) , and sentence extraction with rewriting<cite> (Chen and Bansal, 2018)</cite> . Our model improves Sentence Rewriting with BERT as an extractor and summary-level rewards to optimize the extractor. Reinforcement learning has been shown to be effective to directly optimize a non-differentiable objective in language generation including text summarization (Ranzato et al., 2016; Bahdanau et al., 2017; Paulus et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018) .",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_0",
  "x": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_1",
  "x": "Statistical approaches to semantic parsing have recently received considerable attention. However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007) . The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_2",
  "x": "From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010) . In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S\u00e9aghdha, 2010) , and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets.",
  "y": "differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_3",
  "x": "In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S\u00e9aghdha, 2010) , and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_4",
  "x": "In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_5",
  "x": "Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1 , and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In both cases, we follow <cite>(Poon and Domingos, 2009</cite> ) in using the corpus of biomedical abstracts.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_6",
  "x": "**SEMANTIC PARSING** In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and assumptions relevant to our model. Unlike <cite>(Poon and Domingos, 2009</cite> ), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002) . The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in<cite> (Poon and Domingos, 2009)</cite> .",
  "y": "background differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_7",
  "x": "Second, we assume that the semantic arguments are local in the dependency tree; that is, one lexical item can be a semantic argument of another one only if they are connected by an arc in the dependency tree. This is a slight simplification of the semantic role labeling problem but one often made. Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does not explicitly assume contiguity of lexical items and does not make this directionality assumption but their clustering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in <cite>(Poon and Domingos, 2009</cite> ), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_8",
  "x": "This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009) ), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_9",
  "x": "The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive. The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sentences, they would require marginalization over semantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible.",
  "y": "background motivation"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_10",
  "x": "In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm. The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to fragments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more natural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models. Our approach makes one step in this direction by proposing a non-parametric version of such generative model.",
  "y": "background motivation differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_11",
  "x": "**EMPIRICAL EVALUATION** We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003) , a dataset of 1999 biomedical abstracts, and a set of questions produced by<cite> (Poon and Domingos, 2009)</cite> . A example question is shown in Figure 3 . All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_12",
  "x": "Argument types of the induced classes also show a tendency to correspond to semantic roles. For example, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from. The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, . . . ) . We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in <cite>(Poon and Domingos, 2009</cite> ). The first set of baselines looks for answers by attempting to match a verb and its argument in the question with the input text.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_13",
  "x": "There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; ), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model <cite>(Poon and Domingos, 2009</cite> ), another unsupervised method has been proposed in (Goldwasser et al., 2011) . In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006) , however, they do not attempt to discover frames and deal only with isolated predicates.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_0",
  "x": "The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (Mikolov et al., 2013) . The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words Hahn, 2016a, 2017) . Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990) ) are not affected by this problem. Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) .",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_1",
  "x": "There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_2",
  "x": "The use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., WORDSMITH 3 or the UCS TOOLKIT, 4 as well as interactive websites for exploring precompiled corpora, e.g., the \"advanced\" interface for Google Books (Davies, 2014) or DIACOLLO (Jurish, 2015) . Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_3",
  "x": "The resulting models have a size of 32 GB and are available for download on JESEME's Help page. 9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and \u03c7 2 . These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space. Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus. In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and <cite>Hamilton et al. (2016</cite> words above the minimum frequency threshold used during PPMI and \u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices.",
  "y": "similarities uses"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_4",
  "x": "We follow Kim et al. (2014) in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> . We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA. be potentially misleading by implying a constant meaning of those words used as the background (which are actually positioned by their meaning at a single point in time). Typical Context offers two graphs, one for \u03c7 2 and one for PPMI, arranged in tabs. Values in typical context graphs are normalized to make them comparable across different metrics.",
  "y": "differences"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_6",
  "x": "We presented JESEME, the Jena Semantic Explorer, an interactive website and REST API for exploring changes in lexical semantics over long periods of time. In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (Levy et al., 2015;<cite> Hamilton et al., 2016</cite>; Hahn, 2016a, 2017) and provides access to five popular corpora for the English and German language. JESEME is also the first tool of its kind and under continuous development. Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and provide optional stemming routines. Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
  "y": "future_work"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_0",
  "x": "Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware. For the quantization approach, It is important to fine-tune models with quantized weights to reduce the performance loss with quantized networks. Here we refer quantization with fine-turning as quantization training.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_1",
  "x": "However, to ensure high performance, those models are of large scale computation and memory intense, which makes it a challenge to deploy for real industrial applications with limited computation resources and memory. Our paper is focused on increasing the computation efficiency for AED models while maintaining their accuracies, so that AED deployment for resource-constraint industrial applications is feasible. Compression of neural networks has been explored in broad context. We focus on two widely used and effective methods for deep models: low-rank matrix factorization and and quantization. Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> .",
  "y": "background uses"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_2",
  "x": "Model f is trained using cross-entropy loss: L = \u2212 (I,y)\u2208DL C c=1 {w c y c log f c (I) + (1 \u2212 y c ) log(1 \u2212 f c (I))}, where w c is the penalty of positive mis-classification of class c. Specifically we focus on the RNN-based model in this paper. Compared to CNNs, it is more memory efficient and easier to deploy on devices with constrained resources. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32).",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_3",
  "x": "We start by formulating the multi-class audio event detection problem. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> .",
  "y": "background uses"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_4",
  "x": "The evaluation is based on detection error tradeoff (DET) curve (false negative rate vs. false positive rate). We compute area under curve (AUC) and equal error rate (EER) as the two quantitative measures. As the distribution of weight matrices' eigenvalues can be different across different LSTM layers, we follow the practice of <cite>[11]</cite> to set the same threshold \u03c4 across layers as the fraction of retained singular values, defined as \u03c4 = Table 1 summarizes the results of low-rank matrix factorization compared to our baseline 3-layer LSTM. There is no accuracy degradation when \u03c4 is reduced to 0.6, which we hypothesize to be related to the regularizing effects. Table 2 summarizes the results with quantization compared to our baseline.",
  "y": "uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_0",
  "x": "**INTRODUCTION** Lipreading is the process of understanding speech by using solely visual features, i.e. images of the lips of a speaker. In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] .",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_1",
  "x": "In communication between humans, lipreading has a twofold relevance [1] : First, visual cues play a role in spoken conversation [2] ; second, hearing-impaired persons may use lipreading as a means to follow verbal speech. With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_2",
  "x": "With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_3",
  "x": "Since then, several end-to-end trainable systems were presented<cite> [7,</cite> 8, 9] . The current state-of-the-art accuracy on the GRID corpus is 3.3% error [9] using a very large set of additional training data; so their result is not directly comparable to ours. In domain adaptation, it is assumed that a learning task exhibits a domain shift between the training (or source) and test (or target) data. This can be mitigated in several ways [35] ; we apply domain-adversarial training [12] , where an intermediate layer in a multi-layer network is driven to learn a representation of the input data which is optimized to be domain-agnostic, i.e. to make it difficult to detect whether an input sample is from the source or the target domain. A great advantage of this approach is the end-to-end trainability of the entire system.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_4",
  "x": "We follow the data preprocessing protocol from <cite>[7]</cite> . We use the GRID corpus [13] , which consists of video and audio recordings of 34 speakers (which we name s1 to s34) saying 1000 sentences each. All sentences have a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \"Place red at J 2, please\", where the number of alternative words is given in parentheses. There are 51 distinct words; alternatives are randomly distributed so that context cannot be used for classification. Each sentence has a length of 3 seconds at 25 frames per second, so the total data per speaker is 3000 seconds (50 minutes).",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_5",
  "x": "We converted the \"normal\" quality videos (360 \u00d7 288 pixels) to greyscale and extracted 40\u00d740 pixel windows containing the mouth area, as described in <cite>[7]</cite> . The frames were contrastnormalized and z-normalized over the training set, independently for each speaker. Unreadable videos were discarded. All experiments have one dedicated target speaker on which this experiment is evaluated, and one, four, or eight source speakers on which supervised training is performed. Speakers are chosen consecutively, for example, the experiments on four training speakers on the development data are (s1 . . .",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_6",
  "x": "**METHODS AND SYSTEM SETUP** The system is based on the lipreading setup from <cite>[7]</cite> , reimplemented in Tensorflow [36] . Raw 40 \u00d7 40 lip images are used as input data, without any further preprocessing except normalization. We stack several fully connected feedforward layers, optionally followed by Dropout [37] , and one LSTM recurrent layer to form a network which is capable of recognizing sequential video data. The final layer is a softmax with 51 word targets.",
  "y": "extends differences"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_7",
  "x": "Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> . ---------------------------------- **EXPERIMENTS AND RESULTS**",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_8",
  "x": "**EXPERIMENTS AND RESULTS** ---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> . We run the lipreader as a single-speaker system with different topologies, optionally using Dropout (always with 50% dropout ratio) to avoid overfitting the training set.",
  "y": "similarities uses"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_0",
  "x": "So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts would obviously increase both considerably. This is especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%. This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions. 1 <cite>(Tetreault and Chodorow, 2008b)</cite> challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_1",
  "x": "In fact, the usability of AMT for text annotation has been demostrated in those studies by showing that non-experts' annotation converges to the gold standard developed by expert annotators. However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_2",
  "x": "<cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. In their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained 46 raters? 2. If so, how many raters does it take to match trained raters? In the experiment, a Turker was presented with a sentence from Microsoft's Encarta encyclopedia, with one preposition in that sentence replaced with a blank. There were 194 HITs (sentences) in all, and we requested 10 Turker judgments per HIT.",
  "y": "background extends"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_3",
  "x": "While the previous results look quite encouraging, the task they are based on, preposition selection in well-formed text, is quite different from, and less challenging than, the task that a system must perform in detecting errors in learner writing. To examine the reliability of Turker preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were in-structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment. The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, <cite>(Tetreault and Chodorow, 2008b)</cite> found kappa between two raters averaged 0.630.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_0",
  "x": "Readability assessment refers to the task of (automatically) linking a text to the appropriate target audience based on its complexity. A diverse spectrum of potential application domains has been identified for this task in the literature, ranging from the design and evaluation of education materials, to information retrieval, and text simplification. Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010) . For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_1",
  "x": "This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets. To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by <cite>Hancke et al. (2012)</cite> , but double in size. 1 The longstanding success of these outlets with their target audiences provides some external validity to the nature of the implicit linguistic adaptation of the language used. As showed for German secondary school textbooks, this is not necessarily the case across all linguistic dimensions and adjustments may even be limited to only the surface level of text, sentence, and word length. We conducted a series of analyses on these two data sets to accomplish the following objectives:",
  "y": "extends"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_2",
  "x": "They show, that the typical aggregation of word frequencies across documents are less informative than richer representations including frequency standard deviations. In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> , but also Russian (Reynolds, 2016) or French (Fran\u00e7ois and Fairon, 2012) . The readability checker DeLite of Vor der Br\u00fcck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German. The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_3",
  "x": "The readability checker DeLite of Vor der Br\u00fcck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German. The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings. However, due to the specific nature of the data, the robustness of the approach across genres is unclear. Municipal administration language is so particular that results are unlikely to generalize to educational or literary materials, which are more attractive in first and second language acquisition contexts. Later work by <cite>Hancke et al. (2012)</cite> also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_4",
  "x": "The GEO/GEOlino data set consists of online articles from one of the leading German monthly educational magazines, GEO, and the counterpart for children, GEOlino. 3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_5",
  "x": "3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus. We removed all material flagged as non-article contents by GEO as well as all articles that contained less than 15 words. We further cleaned our data from crawling artifacts and performed near-duplicate detection with the Simhash algorithm.",
  "y": "uses"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_6",
  "x": "For model performance evaluation, we report classification accuracy and the classification confusion matrices, and random baselines as reference point. Table 6 shows the accuracy of our SMO models on both data sets and compares them with a random baseline. Both models clearly outperform the baseline of 50.0%. On GEO/GEOlino S , the performance is comparable to the performance observed by <cite>Hancke et al. (2012)</cite> on the original GEO/GEOlino data. 11 As Table 7a shows, erroneous classifications are roughly balanced across both classes, showing that the model does not prefer one class over the other.",
  "y": "similarities"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_7",
  "x": "We compared this with a newly compiled GEO/GEOlino corpus consisting of online articles of two magazines for adults and children by the same publisher discussing the similar topics. Based on these two data sets, we presented within-corpus (10-fold CV) and cross-corpus experiments and built binary classification models of German educational media text readability that perform with very high accuracy across both data sets. The model is based on a broad range of features that are highly informative for both data sets. This model is a valuable contribution since i) it is based on a considerably broader data basis than previous approaches to German readability, and ii) it successfully generalizes across the data sets, illustrating surprising robustness across rather different text types. The approach presented thus extends the state-of-the-art in <cite>Hancke et al. (2012)</cite> in terms of the breadth of features integrated and the accuracy and generalizability of the model -and provides two new data sources for this line of research.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_0",
  "x": "We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle).",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_1",
  "x": "To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports. Third, even if the bug reports are parallel, we find many cases of context-peculiar paraphrases, i.e., a pair of phrases that have the same meaning in a very narrow context.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_2",
  "x": "Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_3",
  "x": "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_4",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases \"a VGA monitor\" and \"a monitor\" are represented as \"DT 1 JJ NN 2 \" \u2194 \"DT 1 NN 2 \", where the subscripts denote common words.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_5",
  "x": "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>. The following provides a summary of <cite>their technique</cite>. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_6",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. <cite>The authors</cite> first used identical words and phrases as seeds to find and score contextual patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_7",
  "x": "Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_8",
  "x": "Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2. Parallel bug reports are usually noisy. At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_9",
  "x": "At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom). For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_10",
  "x": "This is defined by the following formula: where n is the number of parallel bug reports with the two phrases occurring in parallel, and s i is the score for the i'th occurrence. s i is computed as follows: 1. We compute the set of patterns with affixed pattern scores based on (<cite>Barzilay and McKeown, 2001</cite> ). 2. For the i'th parallel occurrence of the pair of phrases we want to score, we try to find a pattern that matches the occurrence and assign the pattern score to the pair of phrases as s i .",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_11",
  "x": "All possible pairings of chunks are then formed. This set of chunk pairs are later fed to the method in (<cite>Barzilay and McKeown, 2001</cite> ) to produce a set of patterns with affixed scores. With this we compute our global-context based scores. The cooccurrence based scores are computed following the approach described above. Two thresholds are used and candidate paraphrases whose scores are below the respective thresholds are removed.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_12",
  "x": "We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_13",
  "x": "We get in total 53,363 duplicate bug report pairs. As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_14",
  "x": "As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_15",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_16",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_17",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases.",
  "y": "extends motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_18",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_19",
  "x": "In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns. Using these patterns we compute the global context-based scores S g . We also compute the co-occurrence scores S c . We rank and extract top-k paraphrases based on these scores. We consider 4 different methods: We can use either S g or S c to rank the discovered paraphrases.",
  "y": "uses extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_20",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. From the figure we can see that our holistic approach using global-context score to rank and co-occurrence score to filter (i.e., Rk-S g +Ft-S c ) has higher precision than the <cite>baseline approach</cite> (i.e., <cite>BL</cite>) in all ks.",
  "y": "differences"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_21",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. Interestingly, the graph shows that using only one of the scores alone (i.e., Rk-S g and Rk-S c ) does not result in a significantly higher precision than the <cite>baseline approach</cite>.",
  "y": "differences similarities"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_0",
  "x": "This supervision takes form of sentence-aligned parallel data [5] , pre-built word translation pairs [11,<cite> 19]</cite> or document-aligned comparable data [21] . 1 Recently, methods for inducing shared cross-lingual embedding spaces without the need for any bilingual signal (not even word translation pairs) have been proposed [1, 3] . These methods exploit inherent structural similarities of induced monolingual embedding spaces to learn vector space transformations that align the source language space to the target language space, with strong results observed for bilingual lexicon extraction. In this work, we show that these unsupervised cross-lingual word embeddings offer strong support to the construction of fully unsupervised adhoc CLIR models. We propose two different CLIR models: 1) termby-term translation through the shared cross-lingual space, and 2) query and document representations as IDF-weighted sums of constituent word vectors.",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_1",
  "x": "We then explain in detail the query and document representations as well as the ranking functions of our CLIR models. ---------------------------------- **CROSS-LINGUAL WORD VECTOR SPACES** For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs <cite>[19]</cite> ; and 3) no bilingual data at all [3] . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_2",
  "x": "This class of models [1, 11,<cite> 19]</cite> focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces. Let { S w i } V S i =1 , S w i \u2208 R ds be the monolingual word embedding space of the source language L S with V S vectors, and { T w i } V T i =1 , T w i \u2208 R dt the monolingual space for the target language L T containing V T vectors; ds and dt are the respective space dimensionalities. The models learn a parametrized mapping function f ( |\u03b8) that projects the source language vectors into the target space: . The projection parameters \u03b8 are learned using the training set of K word translation pairs: , typically via second-order stochastic optimisation techniques.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_3",
  "x": "According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).**",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_4",
  "x": "We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_5",
  "x": "2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ---------------------------------- **CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x).",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_6",
  "x": "In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network. In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. <cite>[19]</cite> . ---------------------------------- **UNSUPERVISED CLIR MODELS**",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_7",
  "x": "For the CL-CD embeddings, the BWESG model trains on full documentaligned Wikipedias 8 using SGNS with suggested parameters from prior work [22] : 15 negative samples, global decreasing learning rate is .025, subsampling rate is 1e \u2212 4, window size is 16. The CL-WT embeddings of Smith et al. <cite>[19]</cite> use 10K translation pairs obtained from Google Translate to learn the linear mapping functions. The CL-UNSUP training setup closely follows the default setup of Conneau et al. [3] : we refer the reader to the original 4 Note that with both variants of BWE-Agg, we effectively ignore both query and document terms that are not represented in the cross-lingual embedding space. 5 If the representation of a query term t q i is not present in the cross-lingual embedding space, we retain the query term t q i itself. We have also attempted eliminating out-ofvocabulary query terms, but the former consistently leads to better performance.",
  "y": "differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_8",
  "x": "The reported performance on bilingual lexicon extraction (BLE) using cross-lingual embedding spaces is also lower for EN-NL compared to EN-IT (see, e.g., <cite>[19]</cite> ). We observe the same pattern (4-5% lower BLE performance for EN-NL than for EN-IT) with the CL-UNSUP embedding spaces. The weighted variant of BWE-Agg (BWE-Agg-IDF) outperforms the simpler non-weighted summation model (BWE-Agg-Add) across the board. These results suggest that the common IR assumption about document-specific terms being more important than the terms occurring collection-wide is also valid for constructing dense document representations by summing word embeddings. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_0",
  "x": "These approaches can be usually categorized into: a) sentence encoding models, and b) other neural network models. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.1% (Kim et al., 2018) and 86.7% (Devlin et al., 2018) respectively. However, a big question w.r.t to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint. Work in progress. is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise.",
  "y": "motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_1",
  "x": "In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> .",
  "y": "differences extends similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_2",
  "x": "In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ---------------------------------- **RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_3",
  "x": "All of the models perform well on the SNLI dataset, reaching near stateof-the-art accuracy in the sentence encoding and the other category respectively. KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by <cite>Glockner et al. (2018)</cite> . For BiLSTM-max we used the Adam optimizer (Kingma and Ba, 2014) and a learning rate of 5e-4. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve. We used a batch size of 64.",
  "y": "background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_4",
  "x": "In contrast to the findings of <cite>Glockner et al. (2018)</cite> , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained language model did not improve the results significantly. ---------------------------------- **CONCLUSION** In this paper we have shown that neural network models for NLI fail to generalize across different NLI benchmarks.",
  "y": "differences"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_5",
  "x": "For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset. b) NLI is to some extent also genre/context dependent.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_6",
  "x": "We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset.",
  "y": "future_work"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_0",
  "x": "Bracketing evaluation may count a single error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSE-VAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003) . At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_1",
  "x": "At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_2",
  "x": "In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus. <cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_3",
  "x": "Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_4",
  "x": "<cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_5",
  "x": "2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora.",
  "y": "motivation"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_6",
  "x": "This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora.",
  "y": "extends"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_7",
  "x": "3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ---------------------------------- **THE CORPORA USED** As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T\u00fcBa-D/Z, which differ significantly in the syntactic representations used -thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_8",
  "x": "The goal of the following experiments is a comparison of parsing performance across different types of evaluation metrics for parsers trained on Negra (Ver. 2) and T\u00fcBa-D/Z (Ver. 2). ---------------------------------- **DATA PREPARATION** Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_9",
  "x": "**DATA PREPARATION** Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_10",
  "x": "In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it. Both corpora include edge labels above all phrases and words. However the flatter structures in Negra result in 39 different edge labels on words while T\u00fcBa-D/Z has only 5. Unlike <cite>K\u00fcbler et al. (2006)</cite> , which ignored edge labels on words, we incorporate all edge labels present in both corpora. As a consequence of this, providing a parser with perfect lexical tags would also provide the edge label for that word.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_11",
  "x": "Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of <cite>K\u00fcbler et al. (2006)</cite> . For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb. form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects. 9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label.",
  "y": "similarities"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_12",
  "x": "---------------------------------- **GRAMMATICAL FUNCTION LABEL EVALUATION** <cite>K\u00fcbler et al. (2006)</cite> present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) . In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_14",
  "x": "In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. <cite>K\u00fcbler et al. (2006)</cite> Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing. This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_15",
  "x": "The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra. As discussed in the data preparation section, Negra was modified for our experiment so as not to provide the parser with the grammatical function labels for single word phrases as part of the perfect tags provided. This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_16",
  "x": "The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments. These results also form a clear contrast to the PARSEVAL results we saw in Table 2 . Contrary to the finding in <cite>K\u00fcbler et al. (2006)</cite> , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation. In keeping with the results from Rehbein and van Genabith (2007a) , we find that PARSEVAL is not an adequate predictor of performance in an evaluation targeting the functorargument structure of the sentence for comparisons between PCFG parsers trained on corpora with different annotation schemes. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_17",
  "x": "11 The relative numbers of instances where a lexical head is not found are comparable for Negra and T\u00fcBa-D/Z. Heads are not found for approximately 4% of subjects, 1% of accusative objects, and 1% of dative objects. These instances are frequently due to elision of the verb in headlines and coordinated clauses. Table 5 : Labeled Dependency Evaluation tion, we confirm that the PARSEVAL scores do not correlate with the scores in the other two evaluations, which given their closeness to the semantic functor argument structure make meaningful targets for evaluating parsers. Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_18",
  "x": "Table 5 : Labeled Dependency Evaluation tion, we confirm that the PARSEVAL scores do not correlate with the scores in the other two evaluations, which given their closeness to the semantic functor argument structure make meaningful targets for evaluating parsers. Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments. The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> . When the grammatical function label evaluation is extended into a labeled dependency evaluation by finding the verbal head to complete the labeled dependency triple, the parser trained on Negra outperforms that trained on T\u00fcBa-D/Z. The more significant drop in results for T\u00fcBa-D/Z compared to the grammatical function label evaluation may be due to the fact that a verbal lexical head in T\u00fcBa-D/Z is not in the same local tree as its dependents, whereas it is in Negra.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_19",
  "x": "---------------------------------- **CONCLUSION** Addressing the general question of how to compare parsing results for different annotation schemes, we revisited the comparison of PCFG parsing results for the Negra and T\u00fcBa-D/Z corpora. We show that these different annotation schemes lead to very significant differences in PARSEVAL scores for unlexicalized PCFG parsing models, but grammatical function label and labeled dependency evaluations for arguments of verbs show that this difference does not carry over to measures which are relevant to the semantic functor-argument structure. In contrast to <cite>K\u00fcbler et al. (2006)</cite> a grammatical function evaluation on subjects, accusative objects, and dative objects establishes that Negra and T\u00fcBa-D/Z perform similarly when all types of words and phrases appearing as arguments are taken into consideration.",
  "y": "differences"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_0",
  "x": "This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms ). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by . The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing.",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_1",
  "x": "---------------------------------- **BACKGROUND** In this section, we introduce a grammar conversion ) and CFG filtering<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000) . ---------------------------------- **GRAMMAR CONVERSION**",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_2",
  "x": "In CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) , every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1 . ---------------------------------- **CFG RULES** Figure 2: Extraction of CFG from HPSG Because the obtained CFG can reflect only local constraints given in each local structure of the elementary trees, it generates invalid parse trees that connect local trees in different elementary trees. In order to eliminate such parse trees, a link between branchings is preserved as a node number which records a unique node address (a subscript attached to each node in Figure 1 ).",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_3",
  "x": "As described in Section 2.1, the grammar conversion preserves the whole structure of each elementary tree (precisely, a canonical elementary tree) in a stack, and grammar rules manipulate a head element of the stack. A generated feature structure in the approximation process thus corresponds to the whole unprocessed portion of a canonical elementary tree. This implies that successful context-free derivations obtained by CFG TNT basically involve elementary trees in which all substitution and adjunction have succeeded. However, CFG PB (also a CFG produced by the other work<cite> (Harbusch, 1990)</cite> ) cannot avoid generating invalid parse trees that connect two lo-cal structures where adjunction takes place between them. We measured with G 2-21 the proportion of the number of ok-prop between two node numbers of nodes that take adjunction and its success rate.",
  "y": "motivation uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_0",
  "x": "**INTRODUCTION** Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_1",
  "x": "Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> . One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself-games are puzzleswhich results in inefficient training.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_2",
  "x": "Next, we explore the transfer of control policies in deep Q-learning (DQN) by pre-training portions of a deep Q-network using question-answering and by DQN-to-DQN parameter transfer between games. We evaluate these techniques on two different sets of human authored and computer generated games, demonstrating that our transfer learning methods enable us to learn a higher-quality control policy faster. ---------------------------------- **BACKGROUND AND RELATED WORK** Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> : (1) The agent must act based only on potentially incomplete textual descriptions of the world around it.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_3",
  "x": "This can be represented as a 7-tuple of S, T, A, \u2126, O, R, \u03b3 : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively . Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> . Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_4",
  "x": "Previous work <cite>(Ammanabrolu and Riedl, 2019)</cite> introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld that uses grammars to generate a series of similar (but not exact same) games. An oracle was used to play perfect games and the traces were used to pre-train portions of the agent's network responsible for encoding the observations, graph, and actions. Their results show that this form of pre-training improves the quality of the policy at convergence it does not show a significant improvement in the training time required to reach convergence. Further, it is generally unrealistic to have a corpus of very similar games to draw from.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_5",
  "x": "The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in in a map. The graph also makes a distinction with respect to items that are in the agent's possession or in their immediate surrounding environment. We make minor modifications to the rules used in<cite> Ammanabrolu and Riedl (2019)</cite> to better achieve such a graph in general interactive fiction environments. The agent also has access to all actions accepted by the game's parser, following Narasimhan et al. (2015) . For general interactive fiction environments, we develop our own method to extract this information.",
  "y": "extends"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_6",
  "x": "An example of such a template is \"place OBJ in OBJ\". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of A = O(|V | \u00d7 |O| 2 ) where V is the number of action verbs, and O is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <cite>(Ammanabrolu and Riedl, 2019)</cite> The architecture for the deep Q-network consists of two separate neural networks-encoding state and action separately-with the final Q-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1 ). We train with a standard DQN training loop; the policy is determined by the Q-value of a particular state-action pair, which is updated using the Bellman equation (Sutton and Barto, 2018) :",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_7",
  "x": "In this approach, decisions made during the training of the target task are jointly made using the frozen parameters of the transferred policy network as well as the current policy network. Our system first trains a question-answering system (Chen et al., 2017) using traces given by an oracle, as in Section 4. For commercial textadventure games, these traces take the form of state-action pairs generated using perfect walkthrough descriptions of the game found online as described in Section 4. We use the parameters of the questionanswering system to pre-train portions of the deep Q-network for a different game within in the same domain. The portions that are pre-trained are the same parts of the architecture as in<cite> Ammanabrolu and Riedl (2019)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_8",
  "x": "We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and sourceto-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017) : average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_9",
  "x": "We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ---------------------------------- **SLICE OF LIFE EXPERIMENTS** TextWorld uses a grammar to generate similar games. Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system.",
  "y": "differences similarities"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_10",
  "x": "Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state. We do not let our agent access this additional solution information or admissible actions list. Given the relatively small quest length for TextWorld games-games can be completed in as little as 5 steps-we generate 50 such games and partition them into train and test sets in a 4:1 ratio.",
  "y": "uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_0",
  "x": "Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the stateof-the-art approach. ---------------------------------- **INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_1",
  "x": "**INTRODUCTION** In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_2",
  "x": "We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: Reuters-21578 (Lewis, 1997) , RT-2k (Pang and Lee, 2004) , MR (Pang and Lee, 2005) , TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004) . We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 , demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of Cheng et al. (2018) by almost 10%. The rest of the paper is organized as follows.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_3",
  "x": "**RELATED WORK** There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_4",
  "x": "There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and Clinchant and Perronnin (2013) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_5",
  "x": "We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k \u00b7 d = 10 \u00b7 300 = 3000 components. Similar to J\u00e9gou et al. (2012) , we set \u03b1 = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets. The top three results on each data set are highlighted in red, green and blue, respectively. Best viewed in color.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_6",
  "x": "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS**",
  "y": "similarities uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_7",
  "x": "As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art. ---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_8",
  "x": "---------------------------------- **RESULTS** We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "3ebfa05038431571701a7199163832_0",
  "x": "More recently, [17] has trained fully connected neural networks to classify the severity of the disease, using TORGO and the UASPEECH [18] database. All these models are trained on standard low-level features. In this work we show that dysarthria detection benefits significantly from learning directly from the raw waveform. Previous work has explored learnable alternatives to speech features that rely on a similar computation to spectral representations [19, 20, 21, <cite>22,</cite> 5] . These approaches learn convolutions that are then passed through a non-linearity, eventually a pooling operator and then a log compression to replicate the dynamic range compression typically performed on spectrograms or mel-filterbanks.",
  "y": "background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_1",
  "x": "In this work, we start from an attention-based model on mel-filterbanks, which already outperforms an equivalent model trained on low-level descriptors (LLDs). Our experiments show that by training a PCEN block on top of mel-filterbanks or replacing them by learnable time-domain filterbanks from<cite> [22]</cite> , we get a gain in accuracy around 10% in absolute when training an identical neural network for dysarthria detection. Finally, by combining time-domain filterbanks and PCEN we propose the first audio frontend that can learn features, compression and normalization jointly with a neural network using backpropagation. ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_2",
  "x": "---------------------------------- **TIME-DOMAIN FILTERBANKS** As the first step of our computational pipeline, we use TimeDomain filterbanks from<cite> [22]</cite> . Time-Domain filterbanks are neural network layers that take the raw waveform as input. They can be initialized to replicate mel-filterbanks, and then learnt for the task at hand.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_3",
  "x": "The standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters. More formally, the n th melfilterbank of a signal in t is: where is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks).",
  "y": "background uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_4",
  "x": "The waveform goes through a complex-valued convolution, a modulus operator and the a convolution with a lowpass-filter (the squared hanning window) that performs the decimation. When not combined with PCEN, a log-compression is added on top of TD-filterbanks after adding 1 to their absolute value to avoid numerical issues. Table 1 shows the detailed layers. Following<cite> [22]</cite> , the first 1D convolution filters are initialized with Gabor wavelets, to replicate mel-filterbanks, and are then learnt at the same time as the rest of the model. The second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering.",
  "y": "uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_0",
  "x": "We describe a computational verb lexicon called VerbNet which utilizes Levin verb classes (Levin, 1993) to systematically construct lexical entries. We have used Lexicalized Tree Adjoining Grammar (LTAG) (Joshi, 1985; Schabes, 1990) to capture the syntax associated with each verb class, and have added semantic predicates. We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts <cite>(Dang et al., 1998)</cite> . Whereas previous research on tying semantics to Levin classes (Dorr, 1997) has not explicitly implemented the close relation between syntax and semantics hypothesized by Levin, our lexical resource combines traditional lexical semantic information, such as thematic roles and semantic predicates, with syntactic frames and selectional restrictions.",
  "y": "background uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_1",
  "x": "The core meaning of this verb class is exertion of force. Adjunction of a path PP implying motion modifies membership of these verbs to the Carry class. Push/Pull verbs can appear in the conative construction, which emphasizes their forceful semantic component and ability to express an attempted action where any result that might be associated with the verb is not necessarily achieved; Carry verbs (used with a goal or directional phrase) cannot take the conative alternation because this would conflict with the causation of motion which is the intrinsic meaning of the class <cite>(Dang et al., 1998)</cite> . Palmer et al. (1999) and Bleam et al. (1998) also defined compositional semantics for classes of verbs implemented in FB-LTAG, but they represented general semantic components (e.g., motion, manner) as features on the nodes of the trees. Our use of separate logical forms gives a more detailed semantics for the sentence, so that for an event involving motion, it is possible to know not only that the event has a motion semantic component, but also which entity is actually in motion.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_0",
  "x": "Semantic role labeling, which is a sentence-level semantic task aimed at identifying \"Who did What to Whom, and How, When and Where?\" (Palmer et al., 2010) , has strengthened this focus. Recently, several neural mechanisms have been used to train end-to-end SRL models that do not require task-specific feature engineering as the traditional SRL models do. Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_1",
  "x": "DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics. by w p . Here, words outside argument spans have the tag O, and words at the beginning and inside of argument spans with role r have the tags B r and I r , respectively.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_2",
  "x": "**MODEL CONSTRUCTION MODULES** As can be seen in Fig. 1 , the framework divides model construction in four phases: (I) word representation, (II) sentence representation, (III) output modeling, and (IV) inference. Phase I: The word representation of a word w i consist of three optional concatenated components: a word-embedding, a Boolean indicating if w i is the predicate of the semantic frame (w p ), and a character representation. DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) .",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_3",
  "x": "As can be seen in Fig. 1 , the framework divides model construction in four phases: (I) word representation, (II) sentence representation, (III) output modeling, and (IV) inference. Phase I: The word representation of a word w i consist of three optional concatenated components: a word-embedding, a Boolean indicating if w i is the predicate of the semantic frame (w p ), and a character representation. DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) .",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_0",
  "x": "**ABSTRACT** We describe the submission from the Columbia Arabic & Dialect Modeling group (CADIM) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL'2013). We participate in the Arabic Dependency parsing task for predicted POS tags and features. Our system is based on <cite>Marton et al. (2013)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_1",
  "x": "---------------------------------- **APPROACH** In this section, we summarize <cite>Marton et al. (2013)</cite> . We first present some background information on Arabic morphology and then discuss our methodology and main results. We present our best performing set of features, which we also use in our SPMRL'2013 submission.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_2",
  "x": "It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ----------------------------------",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_3",
  "x": "It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999) : CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_5",
  "x": "In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_6",
  "x": "In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_7",
  "x": "Additionally, almost all work to date in MSA morphological analysis and part-of-speech (POS) tagging has concentrated on the morphemic form of the words. However, often the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the \"surface\" (form-based) morphology; a well-known example of this are the \"broken\" (irregular) plurals of nominals, which often have singular-form morphemes but are in fact plurals and show plural agreement if the referent is rational. In <cite>Marton et al. (2013)</cite> , we showed that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (2013) test (old split) 81.0 84.0 92.7 Table 2 : Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for reference also on the data splits used in our previous work <cite>(Marton et al., 2013)</cite> ; \"\u2264 70\" refers to the test sentences with 70 or fewer words. Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score 5K (SPMRL'2013) test \u2264 70 86.4 89.9 All (SPMRL'2013) test \u2264 70 87.8 90.8 Table 3 : Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags; \"\u2264 70\" refers to the test sentences with 70 or fewer words (again, both when using gold and when using predicted POS and morphological features). We also showed that for parsing with predicted POS and morphological features, training on a combination of gold and predicted POS and morphological feature values outperforms the alternative training scenarios.",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_8",
  "x": "---------------------------------- **BEST PERFORMING FEATURE SET** The best performing set of features on non-gold input, obtained in <cite>Marton et al. (2013)</cite> , are shown in Table 1 . The features are clustered into three types. \u2022 First is part-of-speech, represented using a \"core\" 12-tag set.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_9",
  "x": "**DATA PREPARATION** The data split used in the shared task is different from the data split we used in <cite>(Marton et al., 2013)</cite> , so we retrained our models on the new splits (Diab et al., 2013) . The data released for the Shared Task showed inconsistent availability of lemmas across gold and predicted input, so we used the ALMOR analyzer (Habash, 2007) with the SAMA databases (Graff et al., 2009 ) to determine a lemma given the word form and the provided (gold or predicted) POS tags. In addition to the lemmas, the ALMOR analyzer also provides morphological features in the feature-value representation our approach requires. Finally, we ran our existing converter (Alkuhlani and Habash, 2012) over this representation to obtain functional number and gender, as well as the rationality feature.",
  "y": "differences"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_10",
  "x": "Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_11",
  "x": "Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted Tags, is shown in Table 2 . Our performance in the Shared Task for Arabic Dependency, Predicted Tokenization, Predicted Tags, is shown in Table 3 . For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_1",
  "x": "knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_0",
  "x": "**INTRODUCTION** PredPatt 1 <cite>(White et al., 2016</cite> ) is a pattern-based framework for predicate-argument extraction. It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de Marneffe et al., 2014) , and extracts predicates and arguments through these manual patterns. Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 .",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_1",
  "x": "White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics. Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) .",
  "y": "background motivation"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_2",
  "x": "However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems.",
  "y": "motivation extends"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_3",
  "x": "From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT. We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_4",
  "x": "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below. Fixed-MWE-pred: The UD version 2.0 introduces a new dependency relation fixed for identifying fixed function-word \"multiword expressions\" (MWEs).",
  "y": "background extends"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_0",
  "x": "Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) . One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention <cite>(Vaswani et al., 2017)</cite> , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_1",
  "x": "In addition, the performance of SANs can be improved by multi-head attention <cite>(Vaswani et al., 2017)</cite> , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns Guo et al., 2019) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_2",
  "x": "We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model <cite>(Vaswani et al., 2017)</cite> across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Sperber et al., 2018) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks",
  "y": "differences"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_3",
  "x": "where ATT(\u00b7) is an attention model (Bahdanau et al., 2015;<cite> Vaswani et al., 2017)</cite> that retrieves the keys K h with the query q h i . The final output representation O is the concatenation of outputs generated by multiple attention models: 3 Approach As shown in Figure 1 (a), the vanilla SANs use the query q h i to compute a categorical distribution over all elements from K h (Equation 2). It may inherit the attention to neighboring information (Yu et al., 2018; Guo et al., 2019) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_4",
  "x": "Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism <cite>(Vaswani et al., 2017)</cite> employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) . Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018 , proposed to aggregate information captured by different attention heads.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_5",
  "x": "**EXPERIMENTS** We conducted experiments with the Transformer model <cite>(Vaswani et al., 2017)</cite> on English\u21d2German (En\u21d2De), Chinese\u21d2English (Zh\u21d2En) and Japanese\u21d2English (Ja\u21d2En) translation tasks. For the En\u21d2De and Zh\u21d2En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations.",
  "y": "uses"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_6",
  "x": "Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018) , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_0",
  "x": "Vaswani et al. (2017) introduce Transformer networks that do not use any convolution or recurrent connections while obtaining the best translation performance. These non-recurrent models are appealing due to their highly parallelizable computations on modern GPUs. But do they have the same ability to exploit hierarchical structures implicitly in comparison to RNNs? In this work, we provide a first answer to this question. Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations <cite>(Bowman et al., 2015b</cite>; Tran et al., 2016; Linzen et al., 2016) . We choose Transformer as a non-recurrent model to study in this paper.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_1",
  "x": "**TASKS** We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by<cite> Bowman et al. (2015b)</cite> to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017) .",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_2",
  "x": "Figure 3 shows the results for all cases where the model made the correct prediction. While it is hard to interpret the exact role of attention for different heads and at different layers, we find that some of the attention heads at the higher layers ( 2 h1, 3 h0) frequently point to the subject with an accuracy that decreases linearly with the distance between subject and verb. ---------------------------------- **LOGICAL INFERENCE** In this task, we choose the artificial language introduced by<cite> Bowman et al. (2015b)</cite> .",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_3",
  "x": "**MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors. These two vectors are then concatenated and fed to a 3-layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation. The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_4",
  "x": "---------------------------------- **MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors. These two vectors are then concatenated and fed to a 3-layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation. The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_5",
  "x": "We find the best hyperparameters for each model by running a grid search as explained in \u00a74. ---------------------------------- **RESULTS** Following the experimental protocol of<cite> Bowman et al. (2015b)</cite> , the data is divided into 13 bins based on the number of logical operators. Both FANs and LSTMs are trained on samples with at most n logical operators and tested on all bins.",
  "y": "uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_0",
  "x": "To that end, researchers have investigated the linguistic knowledge that these models learn by analyzing BERT (Goldberg, 2018; Lin et al., 2019) directly or training probing classifiers on the contextualized embeddings or attention heads of BERT (Tenney et al., 2019b,a; Hewitt and Manning, 2019) . BERT and RoBERTa, as Transformer models (Vaswani et al., 2017) , compute the hidden representation of all the attention heads at each layer for each token by attending to all the token representations in the preceding layer. In this work, we investigate the hypothesis that BERTstyle models use at least some of their attention heads to track syntactic dependency relationships between words. We use two dependency relation extraction methods to extract dependency relations from each self-attention heads of BERT and RoBERTa. The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> .",
  "y": "background uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_1",
  "x": "In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general. We are able to replicate their results on BERT using our MAX method. In addition, we also perform a similar analysis on BERT models fine-tuned on natural language understanding tasks as well as RoBERTa. Our experiments suggest that there are particular attention heads of BERT and RoBERTa that encode certain dependency relation types such as nsubj, obj with substantially higher accuracy than our baselines-a randomly initialized Transformer and relative positional baselines. We find that fine-tuning BERT on the syntax-oriented CoLA does not significantly impact the accuracy of extracted dependency relations.",
  "y": "background motivation extends"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_2",
  "x": "Using this matrix, they propose a method to extract constituency and (undirected) dependency trees by recursively splitting and constructing the maximum spanning trees respectively. In contrast, Raganato and Tiedemann (2018) train a Transformer-based machine translation model on different language pairs and extract the maximum spanning tree algorithm from the attention weights of the encoder for each layer and head individually. They find that the best dependency score is not significantly higher than a right-branching tree baseline. Voita et al. (2019) find the most confident attention heads of the Transformer NMT encoder based on a heuristic of the concentration of attention weights on a single token, and find that these heads mostly attend to relative positions, syntactic relations, and rare words. Additionally, researchers have investigated the syntactic knowledge that BERT learns by analyzing the contextualized embeddings (Warstadt et al., 2019a) and attention heads of BERT<cite> (Clark et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_3",
  "x": "Method 1: Maximum Attention Weights (MAX) Given a token A in a sentence, a selfattention mechanism is designed to assign high attention weights on tokens that have some kind of relationship with token A (Vaswani et al., 2017) . Therefore, for a given token A, a token B that has the highest attention weight with respect to the token A should be related to token A. Our aim is to investigate whether this relation maps to a universal dependency relation. We assign a relation (w i , w j ) between word w i and w j if j = argmax W [i] for each row (that corresponds to a word in attention matrix) i in attention matrix W . Based on this simple strategy, we extract relations for all sentences in our evaluation datasets. This method is similar to<cite> Clark et al. (2019)</cite> , and attempts to recover individual arcs between words; the relations extracted using this method need not form a valid tree, or even be fully connected, and the resulting edge directions may or may not match the canonical directions. Hence, we evaluate the resulting arcs individually and ignore their direction. After extracting dependency relations from all heads at all layers, we take the maximum UUAS over all relations types.",
  "y": "similarities extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_0",
  "x": "<cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations?",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_1",
  "x": "This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.",
  "y": "motivation"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_2",
  "x": "The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_3",
  "x": "(2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) .",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_4",
  "x": "' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_5",
  "x": "'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_6",
  "x": "In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_7",
  "x": "---------------------------------- **EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> . As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_8",
  "x": "<cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_9",
  "x": "<cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word. Concretely, we consider a correct prediction to be one in which the average probability of all possible correct words is higher than that of all incorrect words.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_10",
  "x": "By \"incorrect words\", we mean words of the same part of speech as the masked word but that differ from the masked word with respect to at least one feature value. We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_11",
  "x": "Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_12",
  "x": "We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_13",
  "x": "Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_14",
  "x": "Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_15",
  "x": [
   "We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target."
  ],
  "y": "differences"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_0",
  "x": "Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) . For example, in order to make diagnoses based on the medical report in Table 1 , the highlighted symptoms may be extracted as evidence. Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation.",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_1",
  "x": "Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) . For example, in order to make diagnoses based on the medical report in Table 1 , the highlighted symptoms may be extracted as evidence. Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation. She was begun on heparin anticoagulated carefully secondary to the petechial bleed .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_2",
  "x": [
   "She had some bleeding after nasogastric tube insertion . Diagnoses: Cerebral artery occlusion; Unspecified essential hypertension; Atrial fibrillation; Diabetes mellitus. nism to show the importance of basic units (words or ngrams) through their attention weights. However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases."
  ],
  "y": "background motivation"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_3",
  "x": "However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1 , such as \"bleeding after nasogastric tube insertion\", are larger than a single word. Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear.",
  "y": "background motivation"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_4",
  "x": "Our work leverages the attention-based selfexplaining method<cite> (Lin et al., 2017)</cite> , as shown in Figure 1 . First, our text encoder ( \u00a73) formulates an input text into a list of basic units, learning a vector representation for each, where the basic units can be words, phrases, or arbitrary ngrams. Then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights {\u03b1 1 , ..., \u03b1 n }. Eventually, the attention weight \u03b1 i will be used to reveal how important a basic unit h i is. The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction.",
  "y": "uses"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_5",
  "x": "The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction. ---------------------------------- **BASELINES:** We compare two types of baseline text encoders in Figure 1 . (1)<cite> Lin et al. (2017)</cite> (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) . The original model in (Mullenbach et al., 2018) only utilizes 4-grams.",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_0",
  "x": "**RELATED WORK** Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , <cite>[11]</cite> ) and maximum entropy [10] . Among these leaning methods, the most important issue is what features will be used to construct the classifier.",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_1",
  "x": "In Dang's [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_2",
  "x": "Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. In our system, we do not rely on co-occurrence information.",
  "y": "background differences"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_3",
  "x": "Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature. Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet. ----------------------------------",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_4",
  "x": "These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system.",
  "y": "background uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_5",
  "x": "The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_6",
  "x": "The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
  "y": "background differences"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_0",
  "x": "In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017) . SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017;<cite> Ortega and Vu, 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_1",
  "x": "Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in (Lee and Dernoncourt, 2016;<cite> Ortega and Vu, 2017)</cite> . ---------------------------------- **EXPERIMENTAL SETUP** We setup our experimental evaluation, as follows: given a classification task and a dataset, we generate an on-device model.",
  "y": "uses"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_2",
  "x": "**COMPARISON AGAINST STATE-OF-ART METHODS** We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN (Lee and Dernoncourt, 2016) , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
  "y": "background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_3",
  "x": "According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications.",
  "y": "differences"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_5",
  "x": "**CONCLUSION** We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and Dernoncourt, 2016; Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly.",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_0",
  "x": "We show how the theoretical computations of Coecke et al. (2010) instantiate in this concrete setting, and how the Frobenius Algebras, originating from group theory (Frobenius, 1903) and later extended to vector spaces (Coecke et al., 2008) , allow us to not only represent meanings of words with complex roles, such as verbs, adjectives, and prepositions, in an intuitive relational manner, but also to stay faithful to their original linguistic types. Equally as importantly, this model enables us to realize the concrete computations in lower dimensional spaces, thus reduce the space complexity of the implementation. We experiment in two different tasks with promising results: First, we repeat the disambiguation experiment of <cite>Grefenstette and Sadrzadeh (2011a)</cite> for transitive verbs. Then we proceed to a novel task: We use The Oxford Junior Dictionary (Sansome et al., 2000) , Oxford Concise School Dictionary (Hawkins et al., 2004) , and WordNet in order to derive a set of term/definition pairs, measure the similarity of each term with every definition, and use this measurement to classify the definitions to specific terms. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_1",
  "x": "**STIPULATING S = N \u2297 N** The work of <cite>Grefenstette and Sadrzadeh (2011a)</cite> was the first large-scale practical implementation of this framework for intransitive and transitive sentences, and thus a first step towards providing some concrete answers to these questions. Following ideas from formal semantics that verbs are actually relations, the authors argue that the distributional meaning of a verb is a weighted relation representing the extent according to which the verb is related to its subjects and objects. In vector spaces, these relations are represented by linear maps, equivalent to matrices for the case of binary relations and to tensors for relations of arity n. Hence transitive verbs can be represented by matrices created by structurally mixing and summing up all the contexts (subject and object pairs) in which the verb appears. More precisely, we have:",
  "y": "background"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_2",
  "x": "Take \u2212 \u2212 \u2192 do g and \u2212\u2192 cat be the context vectors for the subject and the object, both living in N as prescribed by their types. As any vector, these can be expressed as weighted sums of their basis vectors, that is, . By putting everything together, the meaning of the sentence is calculated as follows; this result lives in N , since it is a weighted sum over \u2212 \u2192 n j : An important consequence of our design decision is that it enables us to reduce the space complexity of the implementation from \u0398(d n )<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> to \u0398(d), making the problem much more tractable. What remains to be solved is a theoretical issue, that in practice the meaning of relational words such as 'chase' as calculated by Equation 5 is a matrix living in N 2 -however, the mathematical framework above prescribes that it should be a rank-3 tensor in N 3 .",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_3",
  "x": "The tensor product is depicted as juxtaposition of triangles. We also remind to the reader that the relational method for constructing a tensor for the meaning of a verb<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> provides us with a matrix in N 2 . In order to embed this in N 3 , as required by the categorical framework, we apply a \u03c3 : N 2 \u2192 N 3 map to it. Now the Frobenius operation \u03c3 gives us some options for the form of the resulting tensor, which are presented below: CPSBJ The first option is to copy the \"row\" dimension of the matrix which, according to Equation 5, corresponds to the subject.",
  "y": "similarities"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_4",
  "x": "We should bring to the reader's attention the fact that equipped with the above closed forms we do not need to create or manipulate rank-3 tensors at any point of the computation, something that would cause high computational overhead. Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS** We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> .",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_5",
  "x": "We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> . Specifically, we use the 2000 most frequent words as the basis for our vector space; this single space will serve as a semantic space for both nouns and sentences. The weights of the vectors are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. As our similarity measure we use the cosine distance. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_6",
  "x": "The weights of the vectors are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. As our similarity measure we use the cosine distance. ---------------------------------- **DISAMBIGUATION** We first test our models against the disambiguation task for transitive sentences described in <cite>Grefenstette and Sadrzadeh (2011a)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_7",
  "x": "The dataset contains 200 such entries with verbs from CELEX, hence 400 sentences. The evaluation of this experiment is performed by calculating Spearman's \u03c1 correlation against the judgements of 25 human evaluators. As our baselines we use an additive (ADDTV) and a multiplicative (MULTP) model, where the meaning of a sentence is computed by adding and point-wise multiplying, respectively, the context vectors of its words. The results are shown in Table 1 . The most successful S = N model for this task is the copyobject model, which is performing really close to the original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> , with the difference to be statistically insignificant.",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_8",
  "x": "1 The original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> with S = N 2 , provided a \u03c1 of 0.21. When computed with our program with the exact same parameters (without embedding them in the S = N model), we obtained a \u03c1 of 0.195. The differences between both of these and our best model are statistically insignificant. In Grefenstette and Sadrzadeh (2011b) , a direct non-relational model was used to compute verb matrices; this provided a \u03c1 of 0.28. However, as explained by the authors themselves, this method is not general and for instance cannot be used for intransitive verbs.",
  "y": "extends differences"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_0",
  "x": "They also introduced a loss clipping strategy in order to make the model more robust. Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution. With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_1",
  "x": "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_2",
  "x": "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "differences uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_3",
  "x": "Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing. Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder. However, one should note that our architecture can be readily applied to other types of RNN such as GRU.",
  "y": "motivation"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_4",
  "x": "**BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_5",
  "x": "**BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.",
  "y": "background uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_0",
  "x": "Consequently, the attention weight is usually concentrated on a single source symbol, being more a pointer than a distributed probability mass. Moreover, this pointer traverses the source word from left to right in order to generate the inflected form. All that motivated the hard attention model of (Aharoni and Goldberg, 2017) , which outperformed the soft attention approaches. The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol. This model was further improved by<cite> (Makarov et al., 2017)</cite> , whose system was the winner of Sigmorphon 2017 evaluation campaign (Cotterell et al., 2017) .",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_1",
  "x": "However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics. For example, to detect the vowel in the suffix of the Turkish verb one do not need to observe any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. A natural way to capture the phonetic patterns are character language models. They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_2",
  "x": "They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages.",
  "y": "extends"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_3",
  "x": "Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component. We followed the architecture of (Gulcehre et al., 2017) , whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.",
  "y": "differences"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_4",
  "x": "**BASELINE MODEL** As the state-of-the-art baseline we choose the model of Makarov et al.<cite> (Makarov et al., 2017)</cite> , the winner of previous Sigmorphon Shared Task. This system is based on earlier work of Aharoni and Goldberg (Aharoni and Goldberg, 2017) . We briefly describe the structure of baseline model (we call it AGM-model further) and refer the reader to these two papers for more information. AGMmodel consists of encoder and decoder, where an encoder is just a bidirectional LSTM.",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_5",
  "x": "On each step the decoder produces a vector z i as output and propagates updated hidden state vector h i to the next timestep. z i is then passed to a two-layer perceptron with ReLU activation on the intermediate layer and softmax activation on the output layer, which produces the output distribution p i over output letters, formally: If y i is the index of step symbol, we move the pointer to the next input letter. We also use the copy gate from<cite> (Makarov et al., 2017)</cite> : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above. The weight \u03c3 i is the output of another one-layer perceptron:",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_6",
  "x": "The dataset also contained a development set containing 1000 instances most of the time, for all languages we used this subset as validation data. Overall, there were 86 languages in the high setting, 102 in medium and 103 in low. ---------------------------------- **RESULTS AND DISCUSSION** We submitted three systems, one replicating the algorithm of<cite> (Makarov et al., 2017)</cite> , the second equipped with language models.",
  "y": "uses"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_0",
  "x": "This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation<cite> (Bolukbasi et al., 2016)</cite> . This can affect downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017) . The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems. An example of this gender bias is the word \"friend\" in the English sentence \"She works in a hospital, my friend is a nurse\" would be correctly translated to \"amiga\" (feminine of friend) in Spanish, while \"She works in a hospital, my friend is a doctor\" would be incorrectly translated to \"amigo\" (masculine of friend) in Spanish. We consider that this translation contains gender bias since it ignores the fact that, for both cases, \"friend\" is a female and translates by focusing on the occupational stereotypes, i.e. translating doctor as male and nurse as female.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_1",
  "x": "The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_2",
  "x": "The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> . Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_3",
  "x": "GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings. It consists of two main parts: First the direction of the embeddings where the bias is present is identified. Second, the gender neutral words in this direction are neutralized to zero and also equalizes the sets by making the neutral word equidistant to the remaining ones in the set.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_4",
  "x": "**MODELS** The word embeddings are trained from the same corpus, using GloVe (Pennington et al., 2014) and GN-GloVe (Zhao et al., 2018b) . The dimension of the vectors is settled to 512 as standard and kept through all the experiments in this study. The parameter values for training the word embedding models with GloVe and GN-GloVe methods are listed in Table 3 . Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a debiasing post-process performed on trained embeddings.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_5",
  "x": "Biases learned from human generated corpora in natural language processing applications is a topic that has been gaining relevance over the last few years. Specifically, for machine translation, studies quantifying gender bias present in news corpora and proposing debiasing approaches for word embedding models have shown improvements on this matter. We studied the impact of gender debiasing on neural machine translation. We trained sets of word embeddings with the standard GloVe algorithm. Then, we debiased the embeddings using Debiaswe<cite> (Bolukbasi et al., 2016)</cite> and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) .",
  "y": "uses"
 },
 {
  "id": "4bc5fc3bccb704b9978b294ffb07de_0",
  "x": "---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; <cite>Li et al., 2016a</cite>) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
  "y": "background uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_0",
  "x": "For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_1",
  "x": "For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ---------------------------------- **MODEL**",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_2",
  "x": "where N f is the number of morphological features active on the token indexed by f . In other words, we embed a bag of features into a shared embedding space by averaging the individual feature embeddings. k-best Tags. The non-linear network models of Weiss et al. (2015) and Chen and Manning (2014) embed the 1-best tag, according to a first-stage tagger, for a select set of tokens for any configuration. Inspired by the work of<cite> Bohnet and Nivre (2012)</cite> , we embed the set of top tags according to a first-stage tagger.",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_3",
  "x": "---------------------------------- **INTEGRATING PARSING AND TAGGING** While past work on neural network transitionbased parsing has focused exclusively on the arcstandard transition system, it is known that better results can often be obtained with more sophisticated transition systems that have a larger set of possible actions. The integrated arc-standard transition system of<cite> Bohnet and Nivre (2012)</cite> allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc-standard system. It does this by replacing the shift action in the arc-standard system with an action shift p , which, aside from shifting the top token on the buffer also assigns it one of the k best POS tags from a first-stage tagger.",
  "y": "uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_4",
  "x": "For example, LAS improvements can be as high as 0.98% in CoNLL'09 German when increasing the size of the two hidden layers from 200 to 1024. We use B = 16 or B = 32 based on the development set performance per language. For ease of experimentation, we deviate from<cite> Bohnet and Nivre (2012)</cite> and use a single unstructured beam, rather than separate beams for POS tag and parse differences. We train our neural networks on the standard training sets only, except for initializing with word embeddings generated by word2vec and using cluster features in our POS tagger. Unlike Weiss et al. (2015) we train our model only on the treebank training set and do not use tri-training, which can likely further improve the results.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_5",
  "x": "In Table 3 , we compare our models to the winners of the CoNLL '09 shared task, Gesmundo et al. (2009 ), Bohnet (2009 ), Che et al. (2009 ), Ren et al. (2009 , as well as to more recent results on the same datasets. It is worth pointing out that Gesmundo et al. (2009) is itself a neural net parser. Our models achieve higher labeled accuracy than the winning systems in the shared task in all languages. Additionally, our pipelined neural network parser always outperforms its linear counterpart, an in-house reimplementation of the system of Zhang and Nivre (2011) , as well as the more recent and highly accurate parsers of Zhang and McDonald (2014) and Lei et al. (2014 again outperforms its linear counterpart<cite> (Bohnet and Nivre, 2012)</cite> , however, in some cases the addition of graph-based and cluster features<cite> (Bohnet and Nivre, 2012</cite> )+G+C can lead to even better results. The improvements in POS tagging (Table  2 ) range from 0.3% for English to 1.4% absolute for Chinese and are always higher for the neural network models compared to the linear models.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_6",
  "x": "The results shown in Table 4 , we find that our full model surpasses, to our knowledge, all previously reported supervised parsing models for the Stanford dependency conversions. It surpasses its linear analog, the work of<cite> Bohnet and Nivre (2012)</cite> on Stanford Dependencies UAS by 0.9% UAS and by 1.14% LAS. It also outperforms the pipeline neural net model of Weiss et al. (2015) by a considerable margin and matches the semisupervised variant of Weiss et al. (2015) . ---------------------------------- **ENGLISH TREEBANK UNION**",
  "y": "differences"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_0",
  "x": "<cite>Zhang et al. (2017)</cite> utilize adversarial training to obtain cross-lingual word embeddings without any parallel data. However, their performance is still significantly worse than supervised methods. <cite>Zhang et al. (2017)</cite> apply adversarial training to align monolingual word vector spaces with no supervision.",
  "y": "background"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_1",
  "x": "It should be noted that there is no variance once completing training. Our experiments could be divided into two parts. In the first part, we conduct experiments on smallscale datasets and our main baseline is <cite>Zhang et al. (2017)</cite> . In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_2",
  "x": "---------------------------------- **SMALL-SCALE DATASETS** In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space.",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_3",
  "x": "Performance is measured by top-1 accuracy. ---------------------------------- **EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_4",
  "x": "The statistics of the final training data is given in Table 1 . We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_5",
  "x": "The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models. ----------------------------------",
  "y": "differences uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_6",
  "x": "---------------------------------- **EXPERIMENTS ON OTHER LANGUAGE PAIRS DATASETS** We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 . Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset.",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_7",
  "x": "---------------------------------- **LARGE-SCALE DATASETS** In this section, we integrate our method with Conneau et al. (2018) , whose method improves <cite>Zhang et al. (2017)</cite> by more sophiscated refinement procedure and validation criterion. We replace their first step, namely the adversarial training step, with our model. Basically, we first map the source and target embeddings into the latent space using our algorithm, and then fine-tune the identity mapping in the latent space with the closed-form Procrustes solution.",
  "y": "differences extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_0",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_1",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_2",
  "x": "Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions. This strategy boosts decoding speed by an order of magnitude over stochastic search in our experiments. Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_3",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_4",
  "x": "**RELATED WORK** Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application<cite> (MacCartney et al., 2008)</cite> . Outside of NLI, prior research has also explored the task of monolingual word align-ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002) . ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009 ). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments.",
  "y": "background motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_5",
  "x": "The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES** A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_6",
  "x": "It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. ---------------------------------- **THE MANLI ALIGNER** Our alignment system is structured identically to MANLI<cite> (MacCartney et al., 2008)</cite> and uses the same phrase-based alignment representation. An alignment E between two fragments of text T 1 and T 2 is represented by a set of edits {e 1 , e 2 , . . .}, each belonging to one of the following types:",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_7",
  "x": "Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. ---------------------------------- **PARAMETER INFERENCE**",
  "y": "uses extends"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_8",
  "x": "We deviate from <cite>MacCartney et al. (2008)</cite> and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. ---------------------------------- **DECODING** The decoding problem is that of finding the highestscoring alignment under some parameter values for the model.",
  "y": "extends differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_9",
  "x": "**ALIGNMENT EXPERIMENTS** For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in \u00a73. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. \u00a73.1) using the training parameters specified in <cite>MacCartney et al. (2008)</cite> . Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems.",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_10",
  "x": "We first observe that our reimplemented version of MANLI improves over the results reported in <cite>MacCartney et al. (2008)</cite> , gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations. We attribute at least some part of this gain to our modified parameter inference (cf. \u00a73.3) which avoids normalizing the structured perceptron weights and instead adheres closely to the algorithm of Collins (2002) . Although exact decoding improves alignment performance over the approximate search approach, the gain is marginal and not significant. This seems to indicate that the simulated annealing search strategy is fairly effective at avoiding local maxima and finding the highest-scoring alignments.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_11",
  "x": "---------------------------------- **CONCLUSION** We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in <cite>MacCartney et al. (2008)</cite> by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_12",
  "x": "An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by <cite>MacCartney et al. (2008)</cite> with regard to the original MANLI system. In addition, stricter constraints that enforce the alignment of syntactically-related tokens (rather than just their inclusion in the solution) may also yield performance gains. Although MANLI's structured prediction approach to the alignment problem allows us to encode preferences as features and learn their weights via the structured perceptron, the decoding constraints used here can be used to establish dynamic links between alignment edits which cannot be determined a priori. The interaction between the selection of soft features for structured prediction and hard constraints for decoding is an interesting avenue for further research on this task. Initial experiments with a feature that considers the similarity of dependency heads of tokens in an edit (similar to MANLI's contextual features that look at preceding and following words) yielded some improvement over the baseline models; however, this did not perform as well as the simple constraints described above.",
  "y": "similarities future_work"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_0",
  "x": "The initial state of this decoder is a weighted combination of the final states of the two encoders. Intuitively, such an integration of sourcelanguage information in APE should be useful in conveying the context information to improve the APE performance. To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe). The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations.",
  "y": "background"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_1",
  "x": "To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe). The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_2",
  "x": "Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> . The entire model is optimized as a single end-to-end transformer network. 2 Transformer-Based Multi-Source APE MT errors originating from the input source sentences suggest that APE systems should leverage information from both the src and mt, instead of considering mt in isolation. This can help the model to disambiguate corrections applied at every time step.",
  "y": "similarities uses"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_3",
  "x": "---------------------------------- **SINGLE-SOURCE TRANSFORMER FOR APE** (mt \u2192 pe) Our single-source model (SS) is based on an encoder-decoder-based transformer architecture <cite>(Vaswani et al., 2017)</cite> . Transformer models can replace sequence-aligned recurrence entirely and follow three types of multi-head attention: encoder-decoder attention (also known as vanilla Figure 1 : Multi-source transformer-based APE attention), encoder self-attention, and masked decoder self-attention.",
  "y": "extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_4",
  "x": "For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling. One self-attended encoder for src maps s = (s 1 , s 2 , ..., s k ) into a sequence of continuous representations, enc src = (e 1 , e 2 , ..., e k ), and a second encoder for mt, m = (m 1 , m 2 , ..., m l ), returns another sequence of continuous representations, enc mt = (e \u2032 1 , e \u2032 2 , ..., e \u2032 l ).",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_5",
  "x": "---------------------------------- **MULTI-SOURCE TRANSFORMER FOR APE ({SRC, MT} \u2192 PE)** For our multi-source model (MS), we propose a novel joint transformer model (cf. Figure 1) , which combines the encodings of src and mt and attends over a combination of both sequences while generating the post-edited sentence. Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_6",
  "x": "Each encoder and decoder contains a fully connected feed-forward network having dimensionality d model = 256 for the input and output and dimensionality d f f = 1024 for the inner layer. This is a similar setting to<cite> Vaswani et al. (2017)</cite> 's C \u2212 model 1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional-",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_7",
  "x": "For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensional- For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_8",
  "x": "For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since<cite> Vaswani et al. (2017)</cite> reported nearly identical results to sinusoidal encodings. After finishing training, we save the 5 best checkpoints saved at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints. During decoding, we perform greedy-search-based decoding. We follow a similar hyper-parameter setup for mt \u2192 pe.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_9",
  "x": "An ensemble of both variants increases the performance further. For the PBSMT task, the baseline MT system was outperformed by 3.2 BLEU points, while the NMT baseline remains 0.51 BLEU points better than our APE approach on the 2018 test set. In the future, we will investigate if the performance of each system can be improved by using a different hyper-parameter setup. Unfortunately, we could not test either the 'big' or the 'base' hyper-parameter configuration in<cite> Vaswani et al. (2017)</cite> due to unavailable computing resources at the time of submission. As additional future work, we would like to explore whether using re-ranking and ensembling of different neural APEs helps to improve the performance further.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_0",
  "x": "The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning. The study in Song et al. (2018) In this paper, we propose a new method to solve the multi-hop RC problem across multiple documents.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_1",
  "x": "Our proposed HDE graph has the following advantages: \u2022 Instead of graphs with single type of nodes (Song et al., 2018; De Cao et al., 2018) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information. Specifically, instead of only entity nodes as in (Song et al., 2018; De Cao et al., 2018) , we include nodes corresponding to candidates, documents and entities. In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; \u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_2",
  "x": "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; \u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates. Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task. Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_3",
  "x": "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK** The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_4",
  "x": "The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_5",
  "x": "Entity extraction: entities play an import role in bridging multiple documents and connecting a query and the corresponding answer as shown in figure 1. For example, the entity \"get ready\" in query and two entities \"Mase\" and \"Sean Combs\" co-occur in the 2nd support document, and both \"Mase\" and \"Sean Combs\" can lead to the correct answer \"bad boy records\". Based on this observation, we propose to extract mentions of both query subject s and candidates C q from documents. We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_6",
  "x": "We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_7",
  "x": "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration. Same operations can be applied to other documents, or between the query and extracted entities.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_8",
  "x": "sof tmax(\u00b7) denotes column-wise normalization. We further encode the co-attended document context using a bidirectional RNN f with GRU: The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by <cite>Zhong et al. (2019)</cite> . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query.",
  "y": "uses differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_9",
  "x": "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> . Self-attentive pooling summarizes the information presented in the coattention output by calculating a score for each word in the sequence. The scores are normalized and a weighted sum based pooling is applied to the sequence to get a single feature vector as the summarization of the input sequence.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_10",
  "x": "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_11",
  "x": "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query. ---------------------------------- **REASONING OVER HDE GRAPH** Graph building: let a HDE graph be denoted as G = {V, E}, where V stands for node representations and E represents edges between nodes.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_13",
  "x": "**RESULTS** In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results. We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6%<cite> (Zhong et al., 2019)</cite> to 70.9%. Compared to two previous studies using GNN for multi-hop reading comprehension (Song et al., 2018; De Cao et al., 2018) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (Peters et al., 2018) . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_0",
  "x": "Of the many design features which make human language unique, cultural transmission is important partially because it allows language itself to change over time via cultural evolution (Tomasello, 1999; Christiansen & Kirby, 2003a) . This helps explain how a modern language like English emerged from some proto-language, an \"almost language\" precursor lacking the functionality of modern languages. Figure 1 . We introduce cultural transmission into language emergence between neural agents. The starting point of our study is the goal oriented dialogue task of <cite>Kottur et al. (2017)</cite> , summarized in Fig. 2 .",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_1",
  "x": "For example, an agent who understands blue square and purple triangle should also understand purple square without directly experiencing it; we use this sort of generalization to measure compositionality. Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> , but it only investigates how language changes within a generation. Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, 2001 ) and with human subjects (Kirby et al., 2008) . In this model, language is directly but incompletely transmitted (taught) to one generation of agents from the previous generation. Because learning is incomplete and biased, the student language may differ from the teacher language.",
  "y": "background motivation"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_2",
  "x": "This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Choi et al., 2018) . In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_3",
  "x": "In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically. The resulting knowledge gap makes it easier for the new agent to learn from the older agents than to create a new language.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_4",
  "x": "As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) . Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task. ---------------------------------- **1**",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_5",
  "x": "---------------------------------- **1** This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_6",
  "x": "---------------------------------- **1** This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t.",
  "y": "background uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_7",
  "x": "One open choice in this approach is how to select which agents to re-initialize -we explore different options in this section. Every E epochs, replacement policy \u03c0 is called and returns a list of agents to be re-initialized, as seen at the blue lines (10-12) of Algorithm 1 3 . This process creates generations of agents such that each generation learns languages that are slightly different but eventually improve upon those of previous generations. Single Agent. In <cite>Kottur et al. (2017)</cite> there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_8",
  "x": "Task Description. As in <cite>Kottur et al. (2017)</cite> , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values. Objects are represented 'symbolically' as 3-hot vectors and not rendered as RGB images. Evaluation with Compositional Dataset. The explicit annotation of independent properties like shape and color allows compositionality to be tested, a necessarily domain specific evaluation.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_9",
  "x": "This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) . Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_10",
  "x": "Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like \"red triangle\" and \"filled star\" instead of words for \"red,\" \"triangle,\" \"filled,\" and \"star.\" The drop in accuracy 5 between test and validation (which does not hold out attribute pairs) is roughly 20 points. Architecture and Training.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_11",
  "x": "Architecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated.",
  "y": "differences similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_12",
  "x": "Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000) .",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_13",
  "x": "In the Multi Agent setting we use N A = N Q = 5. To decide when to stop we measure validation set accuracy averaged over all Q-bot-Abot pairs and choose the first population whose validation accuracy did not improve for 200k epochs. 7 This differs from <cite>Kottur et al. (2017)</cite> , which stopped once train accuracy reached 100%. Furthermore, we do not mine negatives for each training batch. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_14",
  "x": "Always replace every agent (i.e., with B = all agents at line 11 of Algorithm 1). Comparing to the No Replacement baseline establishes the main result by measuring the difference replacement makes. However, each time lines 11 and 12 of Algorithm 1 are executed there is one more chance of getting a lucky random initialization. Since the No Replacement baseline never does this it has a smaller chance of running in to one such lucky agent. Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in<cite> (Kottur et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_15",
  "x": "Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_16",
  "x": "Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_17",
  "x": "Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> . Our variations on cultural transmission outperform the baselines (lighter two green and lighter two blue bars) where language does not change over generations. Cultural transmission is complementary with other factors that encourage compositionality. The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_18",
  "x": "Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) . Removing memory sometimes hurts. Removing memory always makes a significant difference (p \u2264 0.05) to Small Vocab models and only sometimes makes a difference for Overcomplete models.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_19",
  "x": "This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al., 2008; Cornish et al., 2009; Scott-Phillips & Kirby, 2010) experiments. Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge. While existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents. We consider such a setting because of its potential to complement our existing understanding. Language Emergence in Deep Learning.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_20",
  "x": "Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> . Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_21",
  "x": "Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> . Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality. Cultural Evolution and Neural Nets.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_0",
  "x": "**ABSTRACT** Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. <cite>Koppel and Ordan (2011)</cite> performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_1",
  "x": "Recently, scholars in this area identified several properties of the translation process with the aid of corpora (Baker, 1993; Baker, 1996; Olohan, 2001; Laviosa, 2002; Hansen, 2003; Pym, 2005) . These properties are subsumed under four keywords: explicitation, simplification, normalization and levelling out. They focus on the general effects of the translation process. Toury (1995) has a different theory from these. That is, a translated text will carry some fingerprints of its source language. Recently, Pastor et al. (2008) and Ilisei et al. (2009; have provided empirical evidence of simplification translation properties using a comparable corpus of Spanish. <cite>Koppel and Ordan (2011)</cite> perform empirical studies to validate both theories, using a subcorpus extracted from the Europarl (Koehn, 2005) and IHT corpora<cite> (Koppel and Ordan, 2011)</cite> . They used a comparable corpus of original English and English translated from five other European languages. In addition, original English and English translated from Greek and Korean was also used in their experiment. They have found that a translated text contains both source language dependent and independent features. Obviously, corpora of this sort, which focus on a single language (e.g., English), are not adequate for claiming the universal validity of translation properties. Different languages (and language families) have different linguistic properties.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_2",
  "x": "<cite>Koppel and Ordan (2011)</cite> have built a classifier that can identify the correct source of the translated text (given different possible source languages). They have built another classifier which can identify source text and translated text. Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_3",
  "x": "The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_4",
  "x": "The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_5",
  "x": "We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their ----------------------------------",
  "y": "uses"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_6",
  "x": "We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_7",
  "x": "---------------------------------- **SOURCE LANGUAGE IDENTIFICATION** In this experiment, our goal is to validate the translation properties postulated by Toury (1995) . He stated that a translated text inherits some fingerprints from the source language. The experimental result of <cite>Koppel and Ordan (2011)</cite> shows that text translated into English holds this property.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_8",
  "x": "Each chunk contains at least seven sentences. Our hypothesis is again similar to <cite>Koppel and Ordan (2011)</cite> , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text. If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text. Table 3 and Table 4 show the evaluation results. Table 4 : Source language identification evaluation (Accuracy)",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_9",
  "x": "In the past, researchers have used comparable corpora to validate these translation properties (Baroni and Bernardini, 2006; Pastor et al., 2008; Ilisei et al., 2009; Ilisei et al., 2010;<cite> Koppel and Ordan, 2011)</cite> . Most of them used comparable corpora for two-class classification, distinguishing translated texts from the original texts. Only<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> used English texts translated from multiple source languages. We perform similar experiments only for six European languages as shown in Table 1 . In this experiment, the translated text in our training and test set will be a combination of all languages other than the target language.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_10",
  "x": "For example: when the original class contains original texts (source) in German, then the translation class contains texts that are translated German texts, translated from French, Dutch, Spanish, Polish, and Czech texts. Each class contains 200 chunks of texts, where as the translated class has 40 chunks from each of the source languages. The source language texts are extracted for the corresponding languages in a similar way from the Europarl corpus. <cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above. The training and test data are generated in similar ways as in our previous experiment.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_11",
  "x": "The training and test data are generated in similar ways as in our previous experiment. That is, 80% of the data is randomly extracted for training and the rest of the data is used for testing. Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_12",
  "x": "---------------------------------- **DISCUSSION** The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results. We find our results to be compatible with <cite>Koppel and Ordan (2011)</cite> who used 300 function words. A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.",
  "y": "similarities"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_0",
  "x": "Scottish Gaelic (usually hereafter Gaelic) is a Celtic language, rather closely related to Irish, with around 59,000 speakers as of the last UK census in 2011. As opposed to the situation for Irish Gaelic (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available from Sabhal M\u00f2r Ostaig. A single paper in the ACL Anthology (Kessler, 1995) mentions Scottish Gaelic in the context of computational dialectology of Irish. There is also an LREC workshop paper (Scannell, 2006 ) on machine translation between Irish and Scottish Gaelic. Elsewhere in the Celtic languages, Welsh has an LFG grammar (Mittendorf and Sadler, 2005) but no treebanks.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_1",
  "x": "The initial inspiration was provided by the C&C parser (Curran et al., 2007) , which in addition to providing categorial grammar derivations for sentences provides a dependency structure in the GR (Grammatical Representation) scheme due to (Briscoe and Carroll, 2000; Briscoe and Carroll, 2002) . This contains 23 types and was developed originally for parser evaluation. Another popular scheme is the Stanford Dependency scheme (de Marneffe and Manning, 2008; de Marneffe and Manning, 2013) , which is more finely-grained with over twice the number of dependency types to deal specifically with noisy data and to make it more accessible to non-linguists building information extraction applications. A very important scheme is the Dublin scheme for Irish (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013) , which is of a similar size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large number of dependencies intended to handle grammatical features found in Irish but not in English. Lastly we mention the Universal Dependency Scheme developed in (McDonald et al., 2013) , which we have adopted, despite its being coarser-grained than the Dublin scheme, on account of its simplicity and utility for cross-lingual comparisons and cross-training (Lynn et al., 2014) .",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_2",
  "x": "Fig. 1 shows our dependency tree for this. Note that this is different from the scheme in <cite>Lynn et al. (2012b)</cite> because of a difference between the two languages. They treat the analogous sentence Is tusa an m\u00fainteoir \"You are the teacher\" as having a subject, \"the teacher\", and a clausal predicate, tusa, \"you indeed\". The most straightforward way of expressing a preference is the assertive is followed by an adjective or noun, a PP marking the preferrer, and then the object. If you dislike music, you might say Is beag orm ce\u00f2l.",
  "y": "differences"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_0",
  "x": "Ni and Wang (2017) have proposed a task of generating a definition for a phrase given its local context. However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by Gadetsky et al. (2018) that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases.",
  "y": "background"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_1",
  "x": "The model therefore combines both pieces of information to generate a natural language description. Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities. Our contributions are as follows: \u2022 We propose a general task of defining unknown phrases given their contexts. This task is a generalization of three related tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) and involves various situations where we need definitions of unknown phrases ( \u00a7 2).",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_2",
  "x": "To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016) , it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to <cite>Noraset et al. (2017)</cite> to dynamically control how the global and local contexts influence the description.",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_3",
  "x": "d t is computed with an attention mechanism (Luong and Manning, 2016) as where U h and U s are matrices that map the encoder and decoder hidden states into a common space, respectively. ---------------------------------- **USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_4",
  "x": "**USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . ---------------------------------- **GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS**",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_5",
  "x": "**GATE FUNCTION TO CONTROL LOCAL & GLOBAL CONTEXTS** In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively.",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_6",
  "x": "**EXPERIMENTS** We evaluate our method by applying it to describe words in WordNet 5 (Miller, 1995) and Oxford Dictionary, 6 phrases in Urban Dictionary 7 and Wikipedia/Wikidata. 8 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as groundtruth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet using the dict-definition 9 toolkit.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_7",
  "x": "The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of <cite>Noraset et al. (2017)</cite> . In addition to WordNet, we use the Oxford Dictionary following Gadetsky et al. (2018) , the Urban Dictionary following Ni and Wang (2017) and our Wikipedia dataset described in the previous section. Table 1 and Table 2 show the properties and statistics of the four datasets, respectively. To simulate a situation in a real application where we might not have access to global context for the target phrases, we did not train domainspecific word embeddings on each dataset.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_8",
  "x": "Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS WE IMPLEMENTED FOUR METHODS: (1)** Global<cite> (Noraset et al., 2017)</cite> , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in <cite>Noraset et al. (2017)</cite> . It can access the global context of a phrase to be described, but has no ability to read the local context.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_9",
  "x": "However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_0",
  "x": "Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets. Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_1",
  "x": "This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets. Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_2",
  "x": "In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training.",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_3",
  "x": "In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence [13] under the condition of language information being known during training.",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_4",
  "x": "The ASR Transformer architecture used in this work is the same as our work<cite> [9,</cite> 12] which is shown in Figure 1 . It stacks multihead attention (MHA) [17] and position-wise, fully connected layers for both the encode and decoder. The encoder is composed of a stack of N identical layers. Each layer has two sublayers. The first is a MHA, and the second is a position-wise fully connected feed-forward network.",
  "y": "similarities uses"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_5",
  "x": "Similar to [20, 21] , at the current frame t, these features are stacked with 3 frames to the left and downsampled to a 30ms frame rate. We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ---------------------------------- **MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_6",
  "x": "---------------------------------- **MODEL AND TRAINING DETAILS** We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters. The Adam algorithm [23] with gradient clipping and warmup is used for optimization.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_7",
  "x": "At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good. We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work. To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work <cite>[9]</cite> . Its softmax layer is replaced by the language-specific softmax layer which is initialized randomly. Through this initialization method, the ASR Transformer can converge very well.",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_0",
  "x": "We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see <cite>Adel and Sch\u00fctze (2017)</cite> ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; Bekoulis et al. (2018a) ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2). To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5).",
  "y": "differences"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_1",
  "x": "<cite>Adel and Sch\u00fctze (2017)</cite> solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, Bekoulis et al. (2018a) use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training.",
  "y": "background"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_2",
  "x": "We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> . We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in Bekoulis et al. (2018a) .",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_3",
  "x": "On the other hand, Li et al. (2017) use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_0",
  "x": "This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_1",
  "x": "Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; , * Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_2",
  "x": "This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages. Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data. This permits effective relexicalization and target language adaptation of the transfer parser. Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3).",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_3",
  "x": "Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ---------------------------------- **MULTI-SOURCE DELEXICALIZED TRANSFER** The methods proposed in this paper fall into the delexicalized transfer approach to multilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; S\u00f8gaard, 2011) .",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_4",
  "x": "However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_5",
  "x": "In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_6",
  "x": "To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP**",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_7",
  "x": "---------------------------------- **BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout. We then delve into details on how to incorporate selective sharing in this model in \u00a74.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_8",
  "x": "To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as <cite>Naseem et al. (2012)</cite> . Notably, we use the mapping proposed by Naseem et al. (2010) to map from fine-grained treebank specific part-of-speech tags to coarse-grained \"universal\" tags, rather than the more recent mapping proposed by Petrov et al. (2012) . For Figure 2: Arc-factored feature templates for graph-based parsing. Direction: d \u2208 {LEFT, RIGHT}; dependency length: l \u2208 {1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p \u2208 {NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z \u22121 / z +1 ; WALS features: w.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_9",
  "x": "p, so that the template also falls back on its undirectional variant. each target language evaluated, the treebanks of the remaining languages are used as labeled training data, while the target language treebank is used for testing only (in \u00a75 a different portion of the target language treebank is additionally used as unlabeled training data). We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_10",
  "x": "We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation. ---------------------------------- **BASELINE MODELS**",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_11",
  "x": "The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_12",
  "x": "The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) . The feature templates used by this model are shown to the left in Figure 2 . Note that there is no selective sharing in this model.",
  "y": "differences extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_13",
  "x": "The second and third columns of Table 2 show the unlabeled attachment scores of the baseline models for each target language. We see that Delex performs well on target languages that are related to the majority of the source languages. However, for languages 3 Model \"D-,To\" in Table 2 from <cite>Naseem et al. (2012)</cite> . that diverge from the Indo-European majority family, the selective sharing model, <cite>NBG</cite>, achieves substantially higher accuracies. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_14",
  "x": "In order to verify that these issues are the cause of the poor performance of the Delex model, we remove all directional features and all features that model local word order from Delex. The feature templates of the resulting Bare model are shown in the center of Figure 2 . These features only model selectional preferences and dependency length, analogously to the selection component of <cite>NBG</cite>. The performance of Bare is shown in the fourth column of Table 2 . The removal of most of the features results in a performance drop on average.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_15",
  "x": "---------------------------------- **SHARING BASED ON TYPOLOGICAL FEATURES** After removing all directional features, we now carefully reintroduce them. Inspired by <cite>Naseem et al.</cite> Table 2 from <cite>Naseem et al. (2012)</cite> . (2012), we make use of the typological features from WALS (Dryer and Haspelmath, 2011), listed in Table 1, to selectively share directional parameters between languages.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_16",
  "x": "85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives. This problem applies to any method for parameter mixing that treats all the parameters as equal. Like <cite>Naseem et al. (2012)</cite> , we instead share parameters more selectively. Our strategy is to use the relevant part-of-speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge. The resulting features are shown to the right in Figure 2 .",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_17",
  "x": "Although this model still performs worse than <cite>NBG</cite>, it is an improvement over the Delex baseline and actually outperforms the former on 5 out of the 16 languages. ---------------------------------- **SHARING BASED ON LANGUAGE GROUPS** While Share models selectional preferences and arc directions for a subset of dependency relations, it does not capture the rich local word order information captured by Delex. We now consider two ways of selectively including such information based on language similarity.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_18",
  "x": "The remaining languages are again treated as isolates and revert to Similar. The results for these models are given in the last two columns of Table 2 . We see that by adding these rich features back into the fold, but having them fire only for languages in the same group, we can significantly increase the performance -from 57.4% to 62.0% on average when considering Family. If we consider our original Delex baseline, we see an absolute improvement of 6.9% on average and a relative error reduction of 15%. Particular gains are seen for non-Indo-European languages; e.g., Japanese increases from 38.9% to 65.9%. Furthermore, Family achieves a 7% relative error reduction over the <cite>NBG</cite> baseline and outperforms it on 12 of the 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_19",
  "x": "Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) . Similarly T\u00e4ckstr\u00f6m (2012) used self-training to adapt a multi-source direct transfer named-entity recognizer to different target languages, \"relexicalizing\" the model with word cluster features. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation. ----------------------------------",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_20",
  "x": "While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_21",
  "x": "When arc-marginals are used to construct the ambiguity set, |A k (x, m)| \u2265 1, but when the Viterbiparse is used, A k (x, m) is a singleton. We next form , m) as the ensemble arc ambiguity set from which\u1ef9(x) is assembled. In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( \u00a75.2); and second, the Viterbi arc set from the <cite>NBG</cite> parser of <cite>Naseem et al. (2012)</cite> in Table 2 . 4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_22",
  "x": "4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ---------------------------------- **ADAPTATION EXPERIMENTS** We now study the different approaches to target language adaptation empirically. As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_23",
  "x": "The final model is then trained on this data using standard lexicalized features (McDonald et al., 2005) . Since labeled training data is unavailable in the target language, we cannot tune any hyper-parameters and simply set \u03bb = 1 and \u03c3 = 0.95 throughout. Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_24",
  "x": "First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text. Second, Family is the best discriminative model from the previous section. Third, Viterbi is the basic Viterbi self-training model. The results of each of these models are shown in Table 3 . There are a number of things that can be observed.",
  "y": "extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_25",
  "x": "First, Viterbi self-training helps slightly on average, but the gains are not consistent and there are even drops in accuracy for some languages. Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_26",
  "x": "Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_27",
  "x": "AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages. Furthermore, ambiguity-aware self-training appears to help more than expectation-maximization for generative (unlexicalized) models. <cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_28",
  "x": "Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%).",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_29",
  "x": "While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%). One promising statistic along these lines is that the oracle accuracy for the ambiguous labelings of AAST is 75.7%, averaged across languages, which suggests that other training algorithms, priors or constraints could improve the accuracy substantially. Second, relexicalization is a key component of self-training. If we use delexicalized features during self-training, we only observe a small average improvement from 62.0% to 62.1%. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_30",
  "x": [
   "First, we showed how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based model of dependency parsing. We then showed how ambiguous labelings can be used to integrate heterogenous knowledge sources in parser training. Two instantiations of this framework were explored. First, an ambiguity-aware self-training method that can be used to effectively relexicalize and adapt a delexicalized transfer parser using unlabeled target language data. Second, an ambiguityaware ensemble-training method, in which predictions from different parsers can be incorporated and further adapted."
  ],
  "y": "differences"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_0",
  "x": "Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_1",
  "x": "One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances that challenge the models' ability to identify relations between subject and object.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_2",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_3",
  "x": "---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_4",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers. <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_5",
  "x": "**PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_6",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_7",
  "x": "Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_8",
  "x": "We first investigate the zeroshot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_9",
  "x": "Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple. The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_10",
  "x": "In other words, we are left with the original question and a paragraph relevant to the topic of that question, but which typically no longer contains sentences answering it. Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_11",
  "x": "Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_12",
  "x": "We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_13",
  "x": "Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_14",
  "x": "Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_15",
  "x": "Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset. Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_16",
  "x": "Figure 2 plots how performance improves as more data becomes available about the relations in the entity split test set. We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available. In this case, the dedicated relation extraction model is able to achieve an F1 of around 90%, with or without augmentation with SQuAD.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_17",
  "x": "---------------------------------- **GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_18",
  "x": "**GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_19",
  "x": "We then build new train, dev and test sets (UWRE+) from the original entity split datasets in which half the original negative instances have been replaced with these more challenging instances. As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances.",
  "y": "uses extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_20",
  "x": "Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_21",
  "x": "Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set. Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_22",
  "x": "Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases. In contrast, as the quantity of UWRE+ data grows, performance improves, peaking at around 100,000 instances, which is around the same size as SQuAD. ---------------------------------- **DISCUSSION** The results on our challenge test set suggest that the model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_23",
  "x": "In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_24",
  "x": "Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above. Models We train both BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017 ) models on the modified SQuAD training data, using their standard architectures and hyperparameters. Evaluation We evaluate F1 on the same zeroshot evaluation considered in Section 2 and also accuracy on the challenge test set from Section 3. Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original <cite>UWRE</cite> test set. In contrast, FastQA's performance is substantially worse.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_0",
  "x": "State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the grammar of the target programming language (Yin and Neubig, 2017; Rabinovich et al., 2017; <cite>Iyer et al., 2018</cite>) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code. For example, Figure 1 shows an intermediate parse tree for a generic if-then-else code snippet, for which the decoder requires as many as eleven decoding steps before ultimately filling in the slots for the if condition, the then expression and the else expression. However, the if-then-else block can be seen as a higher level structure such as shown in Figure 1 (b) that can be applied in one decoding step and reused in many different programs. In this paper, we refer to frequently recurring subtrees of programmatic parse trees as code idioms, and we equip semantic parsers with the ability to learn and directly generate idiomatic structures as in Figure 1 (b).",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_1",
  "x": "Analogous to the byte pair encoding (BPE) method (Gage, 1994; Sennrich et al., 2016 ) that creates new subtokens of words by repeatedly combining frequently occurring adjacent pairs of subtokens, our method takes a depth-2 syntax subtree and replaces it with a tree of depth-1 by removing all the internal nodes. This method is in contrast with the approach using probabilistic tree substitution grammars (pTSG) taken by Allamanis and Sutton (2014) , who use the explanation quality of an idiom to prioritize idioms that are more interesting, with an end goal of suggesting useful idioms to programmers using IDEs. Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_2",
  "x": "Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours. Taking advantage of this reduced training time, we further push the state of the art on CONCODE to an EM of 13.4 and a BLEU score of 28.9 by training on an extended version of the training set (with 5x the amount of training examples). ---------------------------------- **RELATED WORK** Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> .",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_3",
  "x": "Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) . Iy<cite>er et al. (2018</cite>) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation. We augment these neural encoder-decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_4",
  "x": "More concretely, we first obtain the parse tree t i (or produc- tion rule set p i ) for each training example program y i under grammar G (Step 3) and then greedily collapse each depth-2 subtree in t i corresponding to every idiom in I (Step 5). Once t i cannot be further collapsed, we translate t i into production rules r i based on the collapsed tree, with |r i | \u2264 |p i | (Step 7). This process is illustrated in Figure 3 where we perform two applications of the first idiom from Figure 2 (b), followed by one application of the second idiom from Figure 2 (d) , after which, the tree cannot be further compressed using those two idioms. The final tree can be represented using |r i | = 2 rules instead of the original |p i | = 5 rules. The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; <cite>Iyer et al., 2018</cite>) using the compressed set of rules.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_5",
  "x": "In later experiments, we find that this results in a rule set compression of more than 50% (see Section 7). ---------------------------------- **EXPERIMENTAL SETUP** We apply our approach to the context dependent encoder-decoder model of <cite>Iyer et al. (2018</cite>) on the CONCODE dataset, and compare performance to a better tuned instance of their best model. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_6",
  "x": "Formally, their task is: Given a NL utterance q, a set of context variables {v i } with types {t i }, and a set of context methods {m i } with return types {r i }, predict a set of parsing rules {a i } of the target program. Their best performing model is an encoder-decoder model with a context aware encoder and a decoder that produces production rules from the grammar of the target programming language. ---------------------------------- **BASELINE MODEL** We follow the approach of <cite>Iyer et al. (2018</cite>) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) .",
  "y": "extends"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_7",
  "x": "Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_8",
  "x": "First, all these elements are embedded using a BPE token embedding matrix B to give us q i , t ij , v ij , r ij and m ij . Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp).",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_9",
  "x": "We evaluate all our models on the CONCODE dataset which was created using Java class files from github.com. It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_10",
  "x": "Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_12",
  "x": "It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_13",
  "x": "Compared to the model of <cite>Iyer et al. (2018)</cite> , our significantly reduced training time enables us to train on their extended training set. We run IyerSimp using 400 idioms (taking advantage of even lower training time) on up to 5 times the amount of data, making sure that we do not include in training any NL from the validation or the test sets. Since the original set of idioms learned from the original training set are quite general, we directly use them rather than relearn the idioms from scratch. We report EM and BLEU scores for different amounts of training data on the same validation and test sets as CONCODE in Table 3 . In general, accuracies increase with the amount of data with the best model achieving a BLEU score of 28.9 and EM of 13.4.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_0",
  "x": "Detecting bias is very important to help users to acquire balanced information. Moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience, which may have large social implications (especially in very controversial topics such as terror attacks and religion issues). In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links.",
  "y": "background"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_2",
  "x": "In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links. <cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations. We build upon <cite>previous work</cite> and use the dataset from <cite>[5]</cite> : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_3",
  "x": "This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias. The rest of the paper is structured as follows. In Section 2 we describe our method to hyperpartisan news detection based on masking. Section 3 presents details on the dataset, experimental results and a discussion of our results.",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_4",
  "x": "We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively). Additionally, we removed duplicates (33) and files with the same text but inconsistent labels (2) .",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_5",
  "x": "**EXPERIMENTS** We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016. Therefore, the content of all the articles is related to the same topic. During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively).",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_6",
  "x": "As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS**",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_7",
  "x": "As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ---------------------------------- **MASKING CONTENT VS. STYLE IN HYPERPARTISAN NEWS**",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_8",
  "x": "Machine (SVM) and Random Forest (RF); for the three classifiers we used the versions implemented in sklearn with the parameters set by default. Evaluation: We performed 3-fold cross-validation with the same configuration used in <cite>[5]</cite> . Therefore, each fold comprised one publisher from each orientation (the classifiers did not learn a publisher's style). We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_9",
  "x": "We used macro F 1 as the evaluation measure since the test set is unbalanced with respect to the three classes. In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_10",
  "x": "Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_11",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3).",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_12",
  "x": "We also include the macro F 1 score because of the unbalance test set. For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_13",
  "x": "Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model). This could be due to the fact that all the articles are about the same political event in a very limited period of time. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_14",
  "x": "We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> . The highest scores were consistently achieved using the SVM classifier and masking the style-related information (i.e., the topic-related model).",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_15",
  "x": "**RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_16",
  "x": "Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams. In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM. Figures 1 shows the results of the variation of k \u2208 {100, 200, ..., 5000}. When k > 5000, we clearly can see that the topic-related model, in which the k most frequent terms are masked, is decreasing the performance.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_17",
  "x": "This confirms that for the used dataset, taking into account only style-related information is not good, and observing also topic-related information benefits the classification. When k tends to the vocabulary size, the style-related model tends to behave like the baseline model, which we already saw in Table 3 that achieves the best results. From this experiment, we conclude that: (i) the topic-related model is less sensitive than the style-related model when k < 500, i.e. the k most frequent terms are stylerelated ones; and (ii) when we vary the value of k, both models achieve worse results than our baseline. On the other hand, the results of extracting character 5-grams are higher than extracting smaller n-grams, as can be seen in Figures 2. These results confirm that perhaps the performance of our approach overcomes the models proposed in <cite>[5]</cite> because of the length of the n-grams 7 .",
  "y": "differences"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_0",
  "x": "English: Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of<cite> Isozaki et al. (2010b)</cite> , and its reference Japanese translation. Semi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; Currey et al., 2017) . However, both methods have problems with lowresource and syntactically divergent language pairs. Back translation assumes enough data to create a functional NMT system, an unrealistic requirement in low-resource scenarios, while autoencoding target sentences by definition will not be able to learn source-target word reordering.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_1",
  "x": "Prior to NMT, word reordering was a major challenge for statistical machine translation (SMT), and many techniques have emerged over the years to address this challenge (Xia and McCord, 2004; . Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ;<cite> Isozaki et al. (2010b)</cite> ; Fig. 1 ). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) . Hence we focus on semi-supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools.",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_2",
  "x": "tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. However, similar pre-ordering methods have not proven useful in NMT (Du and Way, 2017) , largely because high-resource scenarios NMT is much more effective at learning reordering than previous SMT methods were (Bentivogli et al., 2016) . However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask \"how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?\" We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1 , (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and add them as the source side of a pseudo-parallel corpus. Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_3",
  "x": "**THE PROPOSED METHOD** Training Framework We assume that there are two types of available resources: a small parallel corpus P = {(s, t)} and a large monolingual target corpus Q. The goal of our method is to create a pseudo-parallel corpusQ = {(\u015d, t)}, where\u015d is a pseudo-parallel sentence automatically created in two steps of (1) word reordering, and (2) word-byword translation. Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_4",
  "x": "Word Reordering The first step reorders monolingual target sentences t \u2208 Q into the source order t s . Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well. Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> .",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_5",
  "x": "In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of t s into pseudo-source sentence\u015d using a bilingual dictionary (Xie et al., 2018) . 3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available. In addition, we can induce word translations for more words in target language using methods for bilingual lexicon induction over pre-trained word embeddings (e.g. Grave et al. (2018) ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_6",
  "x": "As noted above, we use HF<cite> (Isozaki et al., 2010b)</cite> as our re-ordering rule. HF was designed for transforming English into Japanese order, but we use it as-is for the Uyghur-English pair as well to demonstrate that simple, linguistically motivated rules can generalize across pairs with similar syntax with little or no modification. Further details regarding the experimental settings are in the supplementary material. ---------------------------------- **SIMULATED JAPANESE TO ENGLISH EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_0",
  "x": "We show that the proposed method outperforms the baseline of <cite>Richardson et al. (2013)</cite> , and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_1",
  "x": "To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) .",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_2",
  "x": "To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (Stern and Dagan, 2011) .",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_3",
  "x": "These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> . Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by Narasimhan and Barzilay (2015) , Wang et al. (2015) and Sachan et al. (2015) . Furthermore, we examine the types of questions and answers in the two datasets. We argue that some types are relatively simple to answer, partly due to the limited vocabulary used, which explains why simple lexical matching methods can perform well.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_4",
  "x": "All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest overlap score between a story text window and the question-answer pair is taken as the score for the answer.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_5",
  "x": "Both datasets are divided into training, development, and test sets. All development was conducted on the training and development sets; the test sets were used only to report the final results. 3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_6",
  "x": "Similar to <cite>Richardson et al. (2013)</cite> , we use a linear combination of this score with their distancebased scoring function, and we weigh tokens with their inverse document frequencies in each individual story. By itself, this simple enhancement gives substantial improvements over the MSR baseline as shown in Table 1 (Enhanced SW+D), as it measures the overlap of the question-answer pair with multiple portions of the story text. ---------------------------------- **INCORPORATING LINGUISTIC ANALYSES** We build upon our enhanced scoring function using stemming, rules taking into account the type of the question, and coreference.",
  "y": "similarities uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_7",
  "x": "We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2 . Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_8",
  "x": "Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> . Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_9",
  "x": "These questions are difficult to identify automatically by the form of the question alone, thus we cannot provide accuracies for them. Our results confirm that it is easier to achieve better performance on MC160 with simple lexical techniques, while the MC500 has proved more resilient to the same improvements. We also observed that the MC500 registers smaller improvements in accuracy when adding components such as co-reference. This is a consequence of the design and curation process of the MC500 dataset, which stipulated that answers must not be contained directly within the story text, or if they are, that two or more misleading choices included. <cite>Richardson et al. (2013)</cite> demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both.",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_0",
  "x": "Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix<cite> (Liu et al., 2009)</cite> . Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model.",
  "y": "background motivation"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_1",
  "x": "First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1 . Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a). Since<cite> Liu et al. (2009)</cite> show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (Chiang, 2005) and the tree-to-string model Huang et al., 2006) . While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_2",
  "x": "Another challenge is how to achieve a balance between performance and rule table size. Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices<cite> (Liu et al., 2009</cite> ). If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases.",
  "y": "background motivation differences"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_3",
  "x": "A weighted alignment matrix<cite> (Liu et al., 2009)</cite> m is a J \u00d7 I matrix to encode the probabilities of n-best alignments of the same sentence pair. Each element in the matrix stores a link probability p m (j, i), which is estimated from an n-best list by calculating relative frequencies: where Here N is an n-best list, p(a) is the probability of an alignment a in the n-best list. The numbers in the cells in Figure 2 (c) are the corresponding p m .",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_4",
  "x": [
   "Liu et al. (2009) use the product of inside and outside probabilities as the fractional count of a phrase pair. Liu et al. (2009) define that inside probability indicates the probability that at least one word in source phrase is aligned to a word in target phrase, and outside probability indicates the chance that no words in one phrase are aligned to a word outside the other phrase. The fractional count is calculated: where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column."
  ],
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_5",
  "x": "Figure 3 shows an example. The light shading area is the outside area of phrase pair and the area inside the pane with bold lines is the inside area. To calculate the lexical weights,<cite> Liu et al. (2009)</cite> adapt p m (j, i) as the fractional count count(f j , e i ). The fractional counts of NULL words can be calculated as: Then the lexical weight can be calculated as: where",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_6",
  "x": "The bulk of syntax grammars consists of two parts: phrase pairs and variable rules. The difference between them is containing NTs or not. Since we can calculate relative frequencies and lexical weights of phrase pairs as in<cite> Liu et al. (2009)</cite> , we only focus on the calculation of variable rules. ---------------------------------- **EXTRACTION ALGORITHM**",
  "y": "background uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_7",
  "x": "---------------------------------- **CALCULATING RELATIVE FREQUENCIES** We follow<cite> Liu et al. (2009)</cite> to calculate relative frequencies using the product of inside and outside probabilities. We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs. Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree).",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_8",
  "x": "For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4. We also use Equation 5 to calculate the fractional counts of hierarchical rules. We follow<cite> Liu et al. (2009)</cite> to prune rule table using a threshold of frequency. Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 . If the threshold is 0.2, we retain all the rules in Table 2 .",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_9",
  "x": "We train a 4-gram language model on the Xinhua portion of GIGA-WORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995 To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (Koehn et al., 2003) to all 20 \u00d7 20 bidirectional alignment pairs. We follow<cite> Liu et al. (2009)</cite> to use p s2t \u00d7 p t2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair.",
  "y": "uses"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_0",
  "x": "Named entity recognition (NER) is a subtask of information extraction that seeks to locate and classify predefined entities, such as names of persons, locations, organizations, etc. in unstructured texts. It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption<cite> (Krishnan and Manning, 2006)</cite> . But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task).",
  "y": "background motivation"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_1",
  "x": "However, in the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005) . Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference,<cite> Krishnan and Manning (2006)</cite> propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes.",
  "y": "background motivation"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_2",
  "x": "In this paper, we employ non-local information to recall the missed entities. Similar to<cite> Krishnan and Manning (2006)</cite> , we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification.",
  "y": "differences similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_3",
  "x": "To validate the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to<cite> (Krishnan and Manning, 2006)</cite> , we employ two-stage architecture under conditional random fields (CRFs) framework. In the first stage, we build the baseline with local features only, and then we build the second NER system with non-local features. We will introduce them step by step. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_4",
  "x": "Entity-majority features (F3): These refer to the majority label assigned to the token sequence which is matched with the entity list exactly. These features enable us to capture the dependencies between the identical entities and their classes, so that the same candidate entities of different occurrences can be recalled favorably, and their label consistencies can be considered too. Token-position & entity-majority features (F4): These features capture non-local information from F2 and F3 simultaneously. They take into account the entity boundary and semantic class information at the same time. These non-local features are applied in English NER in one-step approach<cite> (Krishnan and Manning, 2006</cite>; Wong and Ng, 2007) , they employ these features to improve entity consistence among their different occurrences. These features are assigned to token sequences that are matched exactly with the (entity, majority-type) list in forward maximum matching (FMM) way.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_0",
  "x": "Recently, high quality and easy to train Skip-gram shallow architectures were presented in <cite>[10]</cite> and considerably improved in [11] with the introduction of negative sampling and subsampling of frequent words. The \"magical\" ability of word embeddings to capture syntactic and semantic regularities on text words is applicable in various applications like machine translations, error correcting systems, sentiment analyzers etc. This ability has been tested in [12] and other studies with analogy question tests of the form \"A is to B as C is to \" or male/female relations. A recent improved method for generating word embeddings is Glove [15] which makes efficient use of global statistics of text words and preserves the linear substructure of Skip-gram word2vec, the other popular method. Authors report that Glove outperforms other methods such as Skip-gram in several tasks like word similarity, word analogy etc.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_1",
  "x": "Google News is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases <cite>[10]</cite> . It was trained using Skipgram word2vec with negative sampling, windows size 5 and 300 dimensions. Even bigger is Common Crawl 840, a huge corpus of 840 billion tokens and 2.2 million word vectors also used at [15] . It contains data of Common Crawl (http://commoncrawl.org), a nonprofit organization that creates and maintains public datasets by crawling the web. Common Crawl 42 is a reduced version made up of 42 billion tokens and a vocabulary of 1.9 million words.",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_0",
  "x": "Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation.",
  "y": "extends motivation"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_1",
  "x": "A er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation. ---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] . Representations are learned from a document collection and domain-speci c associations between documents and entities.",
  "y": "background motivation"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_2",
  "x": "Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] . In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] .",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_3",
  "x": "Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] . In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] . Users can easily extend existing models or implement their own models within the uni ed framework.",
  "y": "extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_4",
  "x": "To support short documents, a special-purpose padding token can be used to ll up word sequences that are longer than a particular document. A er word sequence extraction, a weight can be assigned to each word sequence/entity pair that can be used to re-weight the training objective. For example, in the case of expert nding [9] , this weight is the reciprocal of the document length of the document where the sequence was extracted from. is avoids a bias in the objective towards long documents. An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [<cite>8</cite>] . Code snippet 1: Illustrative example of the SERT model interface.",
  "y": "extends similarities"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_5",
  "x": "e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [<cite>8</cite>] . Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework. Code snippet 1 shows an example of a model implemented in the SERT toolkit where users can de ne a symbolic cost function that will be optimized using eano [6] . Due to the component-wise organization of the toolkit (Fig. 1) , modeling and text processing are separated from each other.",
  "y": "background extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_7",
  "x": "A er ranking, SERT outputs the entity rankings as a TREC-compatible le that can be used as input to the trec eval 1 evaluation utility. In this paper we described the Semantic Entity Retrieval Toolkit, a toolkit that learns latent representations of words and entities. e toolkit contains implementations of state-of-the-art entity representations algorithms [<cite>8</cite>, 9] and consists of three components: text processing, representation learning and inference. Users of the toolkit can easily make changes to existing model implementations or contribute their own models by extending an interface provided by the SERT framework. Future work includes integration with Pyndri [11] such that document collections indexed with Indri can transparently be used to train entity representations.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_0",
  "x": "The recent advances in image captioning as well as the release of large-scale movie description datasets such as <cite>MPII-MD</cite> <cite>[28]</cite> allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_2",
  "x": "In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_3",
  "x": "Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_4",
  "x": "<cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_5",
  "x": "This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task. This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_6",
  "x": "---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_7",
  "x": "[38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model. ---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_8",
  "x": "---------------------------------- **MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_9",
  "x": "**MOVIE DESCRIPTION.** Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_10",
  "x": "To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach. We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36] ), LSDA (large scale object detector [14] ) and PLACES (Places-CNN [41] ). Next, we concatenate the scores from a subset of selected robust classifiers and use them as input to our LSTM.",
  "y": "differences uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_11",
  "x": "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15] . However, we analyze different aspects and variants of this architecture for movie description. To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach.",
  "y": "uses differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_12",
  "x": "**VISUAL LABELS FOR ROBUST VISUAL CLASSIFIERS** For training we rely on a parallel corpus of videos and weak sentence annotations. As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_13",
  "x": "As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure. Not all sentences can be parsed successfully, as e.g. some sentences are incomplete or grammatically incorrect. To avoid loosing the potential labels in these sentences, we match our set of initial labels to the sentences which the parser failed to process.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_14",
  "x": "In order to find the verbs among the labels we rely on the semantic parser of <cite>[28]</cite> . Next, we look up the list of \"places\" used in [41] and search for corresponding words among our labels. We look up the object classes used in [14] and search for these \"objects\", as well as their base forms (e.g. \"domestic cat\" and \"cat\"). We discard all the labels that do not belong to any of our three groups of interest as we assume that they are likely not visual and thus are difficult to recognize. Finally, we discard labels which the classifiers could not learn, as these are likely to be noisy or not visual.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_15",
  "x": "The polynomial learning strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio Guadarrama in Caffe [15] . ---------------------------------- **EVALUATION** In this section we first analyze our approach on the <cite>MPII-MD</cite> <cite>[28]</cite> dataset and explore different design choices. Then, we compare our best system to <cite>prior work</cite>.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_17",
  "x": "---------------------------------- **ANALYSIS OF OUR APPROACH** Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_18",
  "x": "Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_19",
  "x": "To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips).",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_21",
  "x": "In our experiments we combine three in an ensemble, averaging the resulting word predictions. In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4 ). To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_22",
  "x": "---------------------------------- **COMPARISON TO RELATED WORK** Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_23",
  "x": "Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] . We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_24",
  "x": "Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_25",
  "x": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_26",
  "x": "A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_27",
  "x": "A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_28",
  "x": "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set. ---------------------------------- **APPROACH SENTENCE**",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_30",
  "x": "In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set. ---------------------------------- **APPROACH SENTENCE** <cite>SMT</cite> <cite>[28]</cite> Someone is a man, someone is a man.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_35",
  "x": "We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_36",
  "x": "We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> .",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_37",
  "x": "We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> .",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_38",
  "x": "To handle the weak sentence annotations we rely on three main ingredients. First, we distinguish three semantic groups of labels (verbs, objects and places), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers. For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations. As the result we obtain the highest performance on the <cite>MPII-MD</cite> dataset as shown by all automatic evaluation measures and extensive human evaluation. We analyze the challenges in the movie description task using our and <cite>two prior works</cite>.",
  "y": "uses"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_0",
  "x": "**INTRODUCTION** Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure.",
  "y": "background"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_1",
  "x": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information.",
  "y": "motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_0",
  "x": "Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube. The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016;<cite> Waseem and Hovy, 2016</cite>; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter. We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (Blodgett et al., 2016) .",
  "y": "background motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_1",
  "x": "We use all available datasets where tweets are labeled as various types of abuse and are written in English. We now briefly describe each of these datasets in chronological order. <cite>Waseem and Hovy (2016)</cite> collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful. They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory. These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_2",
  "x": "The results of Experiment 1 are shown in Table 2. We observe substantial racial disparities in the performance of all classifiers. In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the <cite>Waseem and Hovy (2016)</cite> classifier. Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_3",
  "x": "Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive. Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 .",
  "y": "similarities uses"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_4",
  "x": "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
  "y": "similarities"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_5",
  "x": "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_6",
  "x": "While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes. We now discuss the results as they pertain to each of the datasets used. Classifiers trained on data from <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_7",
  "x": "We have not evaluated these datasets for bias related to other ethnic and racial groups, nor other protected categories like gender and sexuality, but expect that such bias is also likely to exist. We recommend that efforts to measure and mitigate bias should start by focusing on how bias enters into datasets as they are collected and labeled. In particular, future work should focus on the following three areas. First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors<cite> (Waseem and Hovy, 2016</cite>; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_8",
  "x": "In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories. For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled. Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017;<cite> Waseem and Hovy, 2016)</cite> and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) .",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_0",
  "x": "Sentiment terms present in such sentences may not necessarily contribute to the actual sentiment of the sentence, for example 'I wish it tasted as amazing as it looked' is not positive. While this is considered as a challenge for sentiment analysis, we adopt a different perspective, and discover benefits of the presence of subjunctive mood in opinionated text. Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements<cite> (Ramanand et al., 2010)</cite> (Table 1) . Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_1",
  "x": "Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) . To the best of our knowledge, subjunctive mood has never been analysed in the context of wish and suggestion detection. We collect a sample dataset comprising of example sentences of subjunctive mood, and identify features of subjunctive mood. We then employ a state of the art statistical classifier, and use subjunctive features in order to perform two kind of tasks on a given set of sentences: 1. Detect wish expressing sentences, and 2.",
  "y": "background motivation"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_2",
  "x": "Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews. They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: <cite>Ramanand et al. (2010)</cite> pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion detection, where they focussed only on suggestion bearing wishes, and used manually formulated syntactic patterns for their detection.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_3",
  "x": "<cite>Ramanand et al. (2010)</cite> worked on product review dataset of the wish corpus, with an objective to extract suggestions for improvements. They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes. They also annotated additional product reviews, but their data is not available for open research. \u2022 Suggestion Detection Product reviews (new): We re-annotated the product review dataset from Goldberg et al. (2009) , for suggestions. This also includes wishes for improvements and new features.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_4",
  "x": "Out of 1235 sentences, 6% are annotated as suggestions. Table 1 presents some examples from this dataset. Annotation Details: We had 2 annotators annotate each sentence with a suggestion or non-suggestion tag. We support the observation of <cite>Ramanand et al. (2010)</cite> that wishes for improvements and new features are implicit expression of suggestions. Therefore, annotators were also asked to annotate suggestions which were expressed as wishes.",
  "y": "motivation similarities"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_5",
  "x": "We then expand this set of verbs by using VerbNet 3.2 (Schuler, 2005) . VerbNet is a wide coverage verb lexicon, which places verbs into classes whose members have common syntactic and semantic properties. We collect all members of the VerbNet verb classes advice, wish, want, urge, require; 28 different verbs were obtained. <cite>Ramanand et al. (2010)</cite> also used a similar but much smaller subset {love, like, prefer and suggest} in their rules. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_0",
  "x": "Both models assume that word frequency, predictability, and length affect eye movements in reading by affecting word recognition, yet neither one models the process of identifying words from visual information. Rather, each of these models directly specifies the effects of these variables on exogenous word processing functions, and the eye movements the models produce are sensitive to these functions' output. Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_1",
  "x": "Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_2",
  "x": "Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_3",
  "x": "So while E-Z Reader and SWIFT produce reasonable effects of word length on eye movement measures (in which longer words receive more and longer fixations) by assuming a particular effect of visual acuity, it is less clear whether a visual acuity account can yield reasonable word length effects in a model that also includes the two opposing effects mentioned above. Determining how these different factors should interact to produce word length effects requires a model of eye movements in reading that models the process of word identification from disambiguating visual input (Bicknell & Levy, in press ). The model presented by <cite>Bicknell and Levy (2010)</cite> fits this description, and includes visual acuity limitations (in fact, identical to the visual acuity function in SWIFT). As already mentioned, how-22 ever, Bicknell (2011) showed that the model did not yield a humanlike length effect. Instead, while longer words were skipped less often and refixated more (as for humans), fixation durations generally fell with word length -the opposite of the pattern shown by humans.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_4",
  "x": "For these reasons, some recent models of isolated word recognition (Gomez, Ratcliff, & Perea, 2008; Norris, Kinoshita, & van Casteren, 2010) have suggested that readers have some uncertainty about the number of letters in a word early in processing. If readers have uncertainty about the length of words, we may expect that the amount of uncertainty would grow proportionally to length, as uncertainty is proportional to set size in other tasks of number estimation (Dehaene, 1997) . This would agree with the intuition that an 8-character word should be more easily confused with a 9-character word than a 3-character word with a 4-character word. Including uncertainty about word length that is larger for longer words would have the effect of increasing the number of visual neighbors for longer words more than for shorter words, providing another reason (in addition to visual acuity limitations) that longer words may require more and longer fixations. In the remainder of this paper, we describe an extension of<cite> Bicknell and Levy's (2010)</cite> model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_5",
  "x": "We then present two sets of simulations with this extended model demonstrating that it produces more humanlike effects of word length, suggesting that uncertainty about word length may be an important component of a full understanding of the effects of word length in reading. ---------------------------------- **A RATIONAL MODEL OF READING** In this section, we describe our extension of<cite> Bicknell and Levy's (2010)</cite> rational model of eye movement control in reading. Except for the visual input system, and a small change to the behavior policy to allow for uncertainty about word length, the model is identical to that described by Bicknell and Levy.",
  "y": "extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_6",
  "x": "1 ---------------------------------- **LANGUAGE KNOWLEDGE** Following <cite>Bicknell and Levy (2010)</cite>, we use very simple probabilistic models of language knowledge: word n-gram models (Jurafsky & Martin, 2009) , which encode the probability of each word conditional on the n \u2212 1 previous words. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_7",
  "x": "---------------------------------- **SIMULATION 1: FULL MODEL** We now assess the effects of word length produced by the extended version of the model. Following Bicknell (2011), we use the model to simulate reading of a modified version of the Schilling, Rayner, and Chumbley (1998) corpus of typical sentences used in reading experiments. We compare three levels of length uncertainty: \u03b4 \u2208 {0, .05, .1}. The first of these (\u03b4 = 0) corresponds to<cite> Bicknell and Levy's (2010)</cite> model, which has no uncertainty about word length.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_8",
  "x": "**TEST CORPUS** We test the model on a corpus of 33 sentences from the Schilling corpus slightly modified by <cite>Bicknell and Levy (2010)</cite> so that every bigram occurred in the BNC, ensuring that the results do not depend on smoothing. ---------------------------------- **ANALYSIS** With each model, we performed 50 stochastic simulations of the reading of the corpus.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_9",
  "x": "**CONCLUSION** In this paper, we argued that the success of major models of eye movements in reading to reproduce the (positive) human effect of word length via acuity limitations may be a result of not including opposing factors such as the negative correlation between visual neighborhood size and word length. We described the failure of the rational model presented in <cite>Bicknell and Levy (2010)</cite> to obtain humanlike effects of word length, despite including all of these factors, suggesting that our understanding of word length effects in reading is incomplete. We proposed a new reason for word length effects -uncertainty about word length that is larger for longer wordsand noted that this reason was not implemented in Bicknell and Levy's model because of a simplifying assumption. We presented an extension of the model relaxing this assumption, in which readers obtain noisy information about word length, and showed through two sets of simulations that the new model produces effects of word length that look more like those of human readers.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_0",
  "x": "By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task. Our results show that this approach is effective in the zero-shot and low-resource cases, and is more robust on a set of test instances designed to challenge the models' ability to identify relations between subject and object. Figure 1 gives an overview of using QA on the slot-filling task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_1",
  "x": "**INTRODUCTION** Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_2",
  "x": "When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_3",
  "x": "In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_4",
  "x": "In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_5",
  "x": "In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task. This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_6",
  "x": "This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The UWRE data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_7",
  "x": "The negative examples also contain the subject entity of the relation, but express a different relation. <cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set. The latter tests the ability to generalise from one set of entities to another for the complete set of relations.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_8",
  "x": "We also construct a series of datasets that combine increasing quantities of the UWRE entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall.",
  "y": "similarities uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_9",
  "x": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision.",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_10",
  "x": "Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation. Our approach Model Acc BiDAF 0.82 FastQA 0.99 merely requires prefixing all texts with a dummy token that stands in for the answer when no real answer is present. Data We train our models on a modified version of SQuAD, which has been augmented with negative examples by removing answer spans, as described in Section 2, and then had the token NoAnswerFound inserted into every text and as the answer for the negative examples, as described above.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_11",
  "x": "Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original UWRE test set. In contrast, FastQA's performance is substantially worse. However, Table 4 reveals that FastQA is extremely accurate on the challenge test set, while BiDAF's performance is comparable to the modified model trained on SQuAD. The unmodified BiDAF and FastQA architectures have complementary strengths on the two evaluations. FastQA's strong performance on the challenge instances may be related to its use of binary features indicating whether a word was present in the question.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_0",
  "x": "****MEASURING SYNTACTIC DIFFERENCE IN BRITISH ENGLISH**** **ABSTRACT** Recent work by <cite>Nerbonne and Wiersma (2006)</cite> has provided a foundation for measuring syntactic differences between corpora. It uses part-of-speech trigrams as an approximation to syntactic structure, comparing the trigrams of two corpora for statistically significant differences. This paper extends the method and its application.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_1",
  "x": "For example, a method for determining phonetic distance is given by Heeringa (2004) . Heeringa and others have also done related work on phonological distance in Nerbonne and Heeringa (1997) and Gooskens and Heeringa (2004) . A measure of syntactic distance is the obvious next step: <cite>Nerbonne and Wiersma (2006)</cite> provide one such method. This method approximates internal syntactic structure using vectors of part-of-speech trigrams. The trigram types can then be compared for statistically significant differences using a permutation test.",
  "y": "background motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_2",
  "x": "Second, the experiments did not test data for geographical dialect variation, but compared two generations of Norwegian L2 learners of English, with differences between ages of initial acquisition. We address these areas by using the syntactically annotated speech section of the International Corpus of English, Great Britain (ICE-GB) (Nelson et al., 2002) , which provides a corpus with full syntactic annotations, one that can be divided into groups for comparison. The sentences of the corpus, being represented as parse trees rather than a vector of POS tags, are converted into a vector of leafancestor paths, which were developed by Sampson (2000) to aid in parser evaluation by providing a way to compare gold-standard trees with parser output trees. In this way, each sentence produces its own vec-tor of leaf-ancestor paths. Fortunately, the permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is already designed to normalize the effects of differing sentence length when combining POS trigrams into a single vector per region.",
  "y": "motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_3",
  "x": "---------------------------------- **METHODS** The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) . Their permutation test collects POS trigrams from a random subcorpus of sentences sampled from the combined corpora.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_4",
  "x": "The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_5",
  "x": "Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_6",
  "x": "Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols. Therefore, leaf-ancestor paths should do just as well as trigrams as long as they do not introduce any additional constraints on how they are generated from the corpus. Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_7",
  "x": "However, to find out if the value of R is significant, we must use a permutation test with a Monte Carlo technique described by Good (1995) , following closely the same usage by <cite>Nerbonne and Wiersma (2006)</cite> . The intuition behind the technique is to compare the R of the two corpora with the R of two random subsets of the combined corpora. For comparison to the experiment conducted by <cite>Nerbonne and Wiersma (2006)</cite> , the experiment was also run with POS trigrams.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_8",
  "x": "As a simple, objective partitioning, the speakers were divided into 11 geographical regions based on the 9 Government Office Regions of England with Wales and Scotland added as single regions. Some speakers had to be thrown out at this point because they lacked brithplace information or were born outside the UK. Each region varied in size; however, the average number of sentences per corpus was 4682, with an average of 44,726 words per corpus (see table 1). Thus, the average sentence length was 9.55 words. The average corpus was smaller than the Norwegian L2 English corpora of <cite>Nerbonne and Wiersma (2006)</cite> , which had two groups, one with 221,000 words and the other with 84,000.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_9",
  "x": "On the other hand, the total size of Southeast England compared to Scotland is only half of the other significantly different comparisons; this difference may be a result of more extreme syntactic differences than the other areas. Finally, it is interesting to note that the summed Norwegian corpus size is around 305,000 words, which is about three times the size needed for significance as estimated from the ICE-GB data. ---------------------------------- **DISCUSSION** Our work extends that of <cite>Nerbonne and Wiersma (2006)</cite> in a number of ways.",
  "y": "extends"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_10",
  "x": "This is very likely, because corpus divisions that better reflect reality have a better chance of achieving a significant difference. In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in <cite>Nerbonne and Wiersma (2006)</cite> by up to a factor of 10. This raises the question of a lower limit on corpus size. Our experiment suggests that the two corpora must have at least 250,000 words, although we suspect that better divisions will allow smaller corpus sizes. While we are reducing corpus size, we might as well compare the increasing numbers of smaller and smaller corpora in an advantageous order.",
  "y": "differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_0",
  "x": "**ABSTRACT** Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by <cite>Cherry and Guo (2015)</cite> , who use a discriminative, semi-Markov tagger, augmented with multiple word representations. We enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase. Our system achieves a typed F1 of 44.7, resulting in a third-place finish, despite training only on the official training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_1",
  "x": "The text to be analyzed is unedited and noisy, and covers a much more diverse set of topics than one might expect in newswire. As such, we are quite interested in the W-NUT Named Entity Recognition in Twitter task (Baldwin et al., 2015) as a platform to benchmark and drive forward work on NER in social media. Our submission to this competition closely follows <cite>Cherry and Guo (2015)</cite> , who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings. We augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership. Our novel infusion technique allows us to adapt existing vectors to NER regardless of their source, by training a typelevel auto-encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_2",
  "x": "We update and augment these lists with our own Freebase queries in Section 3.2. We use unannotated tweets to build various word representations (see Section 3.1). Our unannotated corpus collects 98M tweets (1,995M tokens) from between May 2011 and April 2012. The same corpus is used by <cite>Cherry and Guo (2015)</cite> . These tweets have been tokenized and post-processed to remove many special Unicode characters; they closely resemble those that appear in the provided training and development sets.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_3",
  "x": "**BASE TAGGER** We first summarize the approach of <cite>Cherry and Guo (2015)</cite> , which we build upon for our system. Tagger: We tag each tweet independently using a semi-Markov tagger (Sarawagi and Cohen, 2004) , which tags phrasal entities using a single operation, as opposed to traditional word-based entity tagging schemes. An example tag sequence, drawn from the 2010 development data, is shown in Figure 1 . Semi-Markov tagging gives us the freedom to design features at either the phrase or the word level, while also simplifying our tag set.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_4",
  "x": "Note that we do not include part-of-speech tags as features, as they were not found to be useful by <cite>Cherry and Guo (2015)</cite> . ---------------------------------- **UPDATED GAZETTEERS** During development, we found that features involving gazetteers were having a larger impact on dev than on dev 2015. Therefore, we enhanced the gazetteers provided with the W-NUT baseline system with our own Freebase queries, issued between May 6-7, 2015.",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_5",
  "x": "We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010;<cite> Cherry and Guo, 2015)</cite> . Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014) . A similar strategy for enhancing word embeddings has also been demonstrated for sentiment analysis (Tang et al., 2014) . Following this line of research, we aim to tailor (post-process) the unsupervised phrase embeddings, created in Section 3.3, for our NER task, using an auto-encoder.",
  "y": "background"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_6",
  "x": "Our baseline is our attempt to re-implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers. C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_7",
  "x": "Our baseline is our attempt to re-implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers. C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_8",
  "x": "We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_9",
  "x": "We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings. Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "extends differences"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_0",
  "x": "To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014) . The performance of the state-of-the-art systems has improved significantly<cite> (Horn et al., 2014</cite>; Siddharthan and Angrosh, 2014) . Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output.",
  "y": "background"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_1",
  "x": "Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list. Following <cite>Horn et al. (2014)</cite> , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (Bird et al., 2009) , nor words in our stoplist, which are already simple. In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step. We trained the model with all sentences from Wikipedia. For each target word, the model returns a list of the most similar words; we extract the top 20 in this list that are included in the user-supplied vocabulary list.",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_2",
  "x": "**EVALUATION** We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> . This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators. To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) . To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list.",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_3",
  "x": "Often, this step also requires generation of referring expressions, determiners, conjunctions and sentence re-ordering. Since most of these tasks require real-world knowledge, the editor currently leaves it to the user for post-editing. ---------------------------------- **EVALUATION** We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_0",
  "x": "In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_1",
  "x": "In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ---------------------------------- **BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation.",
  "y": "background extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_2",
  "x": "**BACKGROUND** In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes. \u2022 if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_3",
  "x": "On the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize. Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_4",
  "x": "The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results. This clear gap in our previous work represents an opportunity for improvement discussed in the next section. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_5",
  "x": "Because this feature distinguishes the very kinds of instruction given to each participant to complete the TUNA task, the information provided by -/+ LOC is likely to have a significant impact on the overall results. This clear gap in our previous work represents an opportunity for improvement discussed in the next section. ---------------------------------- **ALGORITHM** The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 <cite>(Lucena & Paraboni, 2008)</cite> , now taking also the trial condition (+/-LOC) into account.",
  "y": "extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_6",
  "x": "For ease of comparison with our previous work, we also present Dice and MASI scores computed as in the previous TUNA Challenge, although these scores were not required for the current competition. The most relevant comparison with our previous work is observed in the overall string-edit distance values in Figure 1 : considering that in<cite> Lucena & Paraboni (2008)</cite> we reported 6.12 editdistance for Furniture and 7.38 for People, the overall improvement (driven by the descriptions in the Furniture domain) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the TUNA trials (the trial condition +/-LOC. ) On the other hand, the divide between +LOC and -LOC strategies does not have a significant effect on the results based on the semantics of the description (i.e., Dice and MASI scores), which remain the same as those obtained previously. This may be explained by the fact that using location information inappropriately counts as one single error in Dice/MASI calculations, but it may have a much greater impact on the wording of the surface string (e.g., one single use of the X-DIMENSION attribute may be realized as \"on the far left\", adding four words to the descriptions.) ----------------------------------",
  "y": "differences"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_0",
  "x": "Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_1",
  "x": "Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_2",
  "x": "It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_3",
  "x": "Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_4",
  "x": "Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations. Empirically, we evaluate on lifelong versions of the datasets by Bordes et al. (2015) and Han et al. (2018) and demonstrate con-siderable performance improvements over prior state-of-the-art approaches. ---------------------------------- **BACKGROUND**",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_5",
  "x": "Update \u03b8 \u2190 \u03b8t 20: end while ---------------------------------- **RELATION CLASSIFICATION MODEL** In principle the learner model f \u03b8 could be any gradient-optimized relation extraction model. In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_6",
  "x": "Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task. Note that the meta-learning algorithm uses SGD as the update rule (U), and does not add any additional trainable parameters to the learner model. ---------------------------------- **EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_7",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation. The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset. Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_8",
  "x": "---------------------------------- **DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation. The 8 relations in each cluster were obtained by clustering the averaged Glove word embeddings of the relation names in the FewRel dataset.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_10",
  "x": "Each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations. <cite>Lifelong SimpleQuestions</cite> was similarly obtained from the SimpleQuestions (Bordes et al., 2015) dataset, and is made up of 20 clusters of relations, with each cluster serving as a task. ---------------------------------- **EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> .",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_11",
  "x": "We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ---------------------------------- **RESULTS AND DISCUSSION**",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_12",
  "x": "**EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments. We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_13",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_14",
  "x": "Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations. ---------------------------------- **LIMITED SUPERVISION RESULTS** The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by <cite>Wang et al. (2019)</cite> to using our approach when only limited supervision is available for all tasks.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_15",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_0",
  "x": "Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature <cite>(Elson et al., 2010</cite>; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015) . These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010) . This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works.",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_1",
  "x": "First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by<cite> Elson et al. (2010)</cite> . ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_2",
  "x": "Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net-works without relying on dialogue, unlike the methods of<cite> Elson et al. (2010)</cite> and He et al. (2013) . Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation).",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_3",
  "x": "---------------------------------- **COMPARISON SYSTEMS** The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in<cite> Elson et al. (2010)</cite> , BookNLP (Bamman et al., 2014) , and the method described in Ardanuy and Sporleder (2014) . For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_4",
  "x": "For the Sherlock Holmes corpus, stage 7 is slightly detrimental to overall performance, which as we stipulated earlier is caused by missing co-referent links. Finally, returning to the initially-posed question of how many characters are present, we find that despite the detection error in our method, the overall predicted number of characters is quite close to the actual: for Sherlock Holmes stories, the number of characters was estimated within 2.4 on average, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories<cite> Elson et al. (2010)</cite> analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) . Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_5",
  "x": "Character detection was run on the same novels from<cite> Elson et al. (2010)</cite> and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size. Thus, our work raises questions about how these character interact and whether the setting influences the structure of the social network, despite similar numbers of characters. Experiment 3: Historical Trends As a second application of our technique, we examine historical trends in how many characters appear in a novel. All fiction novels listed on Project Gutenberg were compiled and publication dates were automatically extracted for 1066 and manually entered for an additional 637. This set was combined with a corpus of 6333 novels, including works such as To The Lighthouse by Virginia Woolf, not available on Project Gutenberg.",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_0",
  "x": "The domain mismatch between the entailment (captions) and summarization (news) datasets suggests that the model is learning some domain-agnostic inference skills. ---------------------------------- **INTRODUCTION** Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_1",
  "x": "Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation. This is because the task of abstractive summarization involves two subtasks: salient (important) event detection as well as logical compression, i.e., the summary should not contain any information that is contradictory or unrelated to the original document.",
  "y": "motivation"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_2",
  "x": "Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006) . The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization.",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_3",
  "x": "---------------------------------- **EVALUATION** Following previous work<cite> (Nallapati et al., 2016</cite>; Chopra et al., 2016; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_5",
  "x": "**SUMMARIZATION RESULTS: DUC** Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information.",
  "y": "differences"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_6",
  "x": [
   "**ANALYSIS EXAMPLES** ---------------------------------- **CONCLUSION AND NEXT STEPS** We presented a multi-task learning approach to incorporate entailment generation knowledge into summarization models. We demonstrated promising initial improvements based on multiple datasets and metrics, even when the entailment knowledge was extracted from a domain different from the summarization domain."
  ],
  "y": "future_work"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_0",
  "x": "Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language. However, our goal is slightly different: We are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages. Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_1",
  "x": "Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup. In Section 4, we give a detailed error analysis and discuss the results of our experiment. The conclusions and the avenues for future research are presented in Section 5.",
  "y": "extends differences"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_2",
  "x": "---------------------------------- **RELATED WORK** Annotation projection is a method that allows for automatically transferring annotations from a well-studied (source) language to a low-resource (target) language in a parallel corpus in order to automatically obtain annotated data. It was first introduced in the work of (Yarowsky et al., 2001) Rahman and Ng, 2012; Martins, 2015; Grishina and Stede, 2015) . Thereafter, <cite>(Grishina and Stede, 2017)</cite> proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively).",
  "y": "background"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_3",
  "x": "Although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large-scale due to the complexity of the annotation task. ---------------------------------- **ANNOTATION PROJECTION EXPERIMENT** In our experiment, we propose a fully automatic projection setup: First, we perform coreference resolution on the source language data and then we implement the single-and multi-source approaches to transfer the automatically produced annotations. We use the English-German-Russian unannotated corpus of <cite>(Grishina and Stede, 2017)</cite> as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language).",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_4",
  "x": "For our experiment, we implement a direct projection method for coreference as described in (Grishina and Stede, 2015) . Our method works as follows: For each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one. Since the corpus was already sentence-and word-aligned 2 , we use the available alignments to transfer the annotations. Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 .",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_5",
  "x": "Our method works as follows: For each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one. Since the corpus was already sentence-and word-aligned 2 , we use the available alignments to transfer the annotations. Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>:",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_6",
  "x": "**RESULTS** To evaluate the projection results, we computed the standard coreference metrics -MUC (Vilain et al., 1995) , B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) -and their average for each of the approaches (Table 3) . As one can see from the table, the quality of projections from English to Russian outperforms the quality of projections from German to Russian by 6.5 points F1. Moreover, while Precision number are quite similar, projections from English exhibit higher Recall numbers. As for the multi-source settings, we were able to achieve the highest F1 of 36.2 by combining disjoint chains (Setting 1), which is 1.9 point higher than the best single-source projection scores and constitutes almost 62% of the quality of the projection of gold standard annotations reported in <cite>(Grishina and Stede, 2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_7",
  "x": "Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names. Overall, for all the markables, the projection accuracy for English-Russian is around 10% better than projection accuracy for GermanRussian. en-ru de-ru Nc 64.5 60.7 Np 70.5 66.6 P 83.6 76.5 All 65.1 55.6 Table 5 : Projection accuracy for common nouns, proper nouns and pronouns (%) Moreover, we compare the projected annotations across the two genres. Interestingly, the results for the two languages vary: While the average coreference scores for English-Russian are quite comparable (news: 34.2 F1, stories: 33.3 F1), the scores for German-Russian differ considerably (news: 30.8 F1, stories: 20.8 F1).",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_8",
  "x": "However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type. Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names. Overall, for all the markables, the projection accuracy for English-Russian is around 10% better than projection accuracy for GermanRussian. en-ru de-ru Nc 64.5 60.7 Np 70.5 66.6 P 83.6 76.5 All 65.1 55.6 Table 5 : Projection accuracy for common nouns, proper nouns and pronouns (%) Moreover, we compare the projected annotations across the two genres.",
  "y": "differences"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_0",
  "x": "For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information. Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction. Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1,<cite> 2]</cite> . There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] .",
  "y": "background"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_1",
  "x": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 . While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_2",
  "x": "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_3",
  "x": "Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_4",
  "x": "We train using stochastic gradient descent with Nesterov momentum and a batch size of 128. Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_5",
  "x": "Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
  "y": "similarities uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_6",
  "x": "Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_7",
  "x": "Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers.",
  "y": "similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_8",
  "x": "Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers.",
  "y": "differences"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_0",
  "x": "Several research work have been reported since 2010 in this research field of hate speech detection (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; <cite>Davidson et al., 2017</cite>; Malmasi and Zampieri, 2018; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; ElSherief et al., 2018; Gamb\u00e4ck and Sikdar, 2017; Zhang et al., 2018; Mathur et al., 2018) . Schmidt and Wiegand (2017) & Fortuna and Nunes (2018) reviewed the approaches used for hate speech detection. Kwok and Wang (2013) used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\". Burnap and Williams (2015) developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier. Djuric et al. (2015) learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection.",
  "y": "background"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_1",
  "x": [
   "The given instances are preprocessed and vectorized using word embeddings in deep learning models. We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks. The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two. The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively. Our models outperform the base line for all the three tasks."
  ],
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_0",
  "x": "FHMMs to supertagging for the categories defined in CCGbank for English. Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007) . Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging<cite> (Baldridge, 2008)</cite> separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_1",
  "x": "(1) ---------------------------------- **SUPERTAGGING WITH VARYING AMOUNTS OF TRAINING DATA** In this experiment, we use the training and test sets used by <cite>Baldridge (2008)</cite> from CCGbank. We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences.",
  "y": "similarities uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_2",
  "x": "State-of-the-art POS taggers report accuracies in the range of 96\u221297%; our model FHMMB was comparable (95.35% for \u03b1 = 0.05 and 94.41 for \u03b1 = 1.0). The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively. The accuracy of our HMM is lower than the performance of <cite>Baldridge (2008)</cite> for supertags. We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags. We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_3",
  "x": "Since annotation is costly, we are interested in automatic annotation of unlabeled sentences with minimal supervision. In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words. We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization<cite> (Baldridge, 2008)</cite> . We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity: where complexity(c i ) is defined as the number of sub-categories contained in category c i .",
  "y": "uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_4",
  "x": "Results of this experiment for \u03b1 = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a). The results for \u03b1 = 0.05 is shown in Table 6 (a). We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
  "y": "similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_5",
  "x": "Results of this experiment for \u03b1 = 1.0, on ambiguous CCG categories, are tabulated in Table  5 (a). The results for \u03b1 = 0.05 is shown in Table 6 (a). We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_6",
  "x": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_7",
  "x": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_8",
  "x": "However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs.",
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_9",
  "x": "This paper follows the work of Duh (2005) , <cite>Baldridge (2008)</cite> and Goldwater and Griffiths (2007) . Duh (2005) uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (Sang et al., 2000) . His is a fully supervised model for a simpler task. We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs. Goldwater and Griffiths (2007) uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters \u03b1 and \u03b2, (2) hyper parameter inference (learning the optimal values for \u03b1 and \u03b2) and (3) hyper parameter inference with varying corpus size and dictionary knowledge.",
  "y": "extends"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_10",
  "x": "To the best of our knowledge, this is the first work on joint inference in the Bayesian framework for supertagging. There is plenty of scope for further improvements. Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings. Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in <cite>Baldridge (2008)</cite> . This may make them more appropriate for developing CCGbanks for other languages and domains.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_0",
  "x": "While it has, historically, mainly been considered a challenging problem in (concept-to-text) language generation tasks, more recently, the issue has also generated interest within summarization research (Barzilay, 2003; Ji and Pulman, 2006) . In the spirit of the latter, this paper investigates the following questions: (1) Does the topic of the text influence the factors that are important to sentence ordering? (2) Which factors are most important for determining coherent sentence orderings? (3) How much performance is gained when using deeper knowledge resources? Past research has investigated a wide range of aspects pertaining to the ordering of sentences in text. The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (<cite>Barzilay and Lapata, 2008</cite>) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_1",
  "x": "The sentence ordering task used in this paper can easily be transformed into a ranking problem. Hence, paralleling <cite>Barzilay and Lapata (2008)</cite> , our model has the following structure. The data consists of alternative orderings (x ij , x ik ) of the sentences of the same document d i . In the training data, the preference ranking of the alternative orderings is known. As a result, training consists of determining a parameter vector w that minimizes the number of violations of pairwise rankings in the training set, a problem which can be solved using SVM constraint optimization (Joachims, 2002) .",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_2",
  "x": "The features in this category are inspired by discourse entity-based accounts of local coherence. Yet, in contrast to <cite>Barzilay and Lapata (2008)</cite> , <cite>who</cite> employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences (similar to Karamanis (2004) 's NOCB metric). We also investigate whether using only the information from the head of the noun group (cf. <cite>Barzilay and Lapata (2008)</cite> ) suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity. Moreover, as indicated above, some of the noun group measures make use of WordNet synonym, hypernym, hyponym, antonym relationships.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_3",
  "x": "This set of features uses information on the temporal ordering of sentences, although it currently only includes the \"happens-before\" relations in VerbOcean. ---------------------------------- **LONGER RANGE RELATIONS** The group similarity features only capture the relation between a sentence and its immediate successor. However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., <cite>Barzilay and Lapata (2008)</cite> ).",
  "y": "motivation"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_4",
  "x": "This section introduces the datasets used for the experiments, describes the experiments, and discusses our main findings. ---------------------------------- **EVALUATION DATASETS** The three datasets used for the automatic evaluation in this paper are based on human-generated texts (Table 1 ). The first two are the earthquake and accident datasets used by <cite>Barzilay and Lapata (2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_5",
  "x": "It comprises 300 human summaries on 50 document sets, resulting in a total of 6,000 pairwise rankings split into training and test sets. The source furthermore differs from <cite>Barzilay and Lapata (2008)</cite> 's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it). Since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic-independent properties of our model. ---------------------------------- **EXPERIMENT 1**",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_6",
  "x": "Given the indications from the foregoing experiments, the results in Table 6 are disappointing. In particular, the performance on the earthquake dataset is below standard. However, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well. (<cite>Barzilay and Lapata (2008)</cite> only perform well when using <cite>their</cite> coreference module, which determines antecedents based on the identified coreferences in the original sentence ordering, thereby biasing <cite>their</cite> orderings towards the correct ordering.) Longer range and WordNet relations together (Chunk+Temp-WN+LongRange+) achieve the best performance. The corresponding configuration is also the only one that achieves reasonable performance when compared with other systems.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_7",
  "x": "It is, in this regard, important to distinguish coherence as studied in Experiment 1 and coherence in the context of automatic summarization. Namely, for newswire summarization systems, the topics of the documents are Table 7 : Cross-Training between Accident and Earthquake datasets. The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from <cite>Barzilay and Lapata (2008)</cite>. unknown at the time of training. As a result, model performance on out-of-domain texts is important for summarization.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_8",
  "x": "When compared to the results obtained by <cite>Barzilay and Lapata (2008)</cite> and Barzilay and Lee (2004) , it would appear that direct sentenceto-sentence similarity (as suggested by the <cite>Barzilay and Lapata</cite> baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. The final experimental setup applies the best Table 8 : Accuracy on 20 test topics (2,700 pairs) with respect to the number of topics used for training using the model Chunk+Temporal-WordNet+LongRange+ model (Chunk+Temporal-WordNet+LongRange+) to the summarization dataset and evaluates how well the model generalises as the number of topics in the training dataset increases. The results -provided in Table 8 -indicate that very little training data (both regarding the number of pairs and the number of different topics) is needed. Unfortunately, they also suggest that the DUC summaries are more similar to the earthquake than to the accident dataset. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_0",
  "x": "**INTRODUCTION** Multi-hop QA requires finding multiple supporting evidence, and reasoning over them in order to answer a question (Welbl et al., 2018; Talmor and Berant, 2018; <cite>Yang et al., 2018)</cite> . For example, to answer the question shown in figure 1 , the QA system has to retrieve two different paragraphs and reason over them. Moreover, the paragraph containing the answer to the question has very little lexical overlap with the question, making it difficult for search engines to retrieve them from a large corpus. For instance, the accuracy of a BM25 retriever for finding all supporting evidence for a question decreases from 53.7% to 25.9% on the 'easy' and 'hard' subsets of the HOTPOTQA training dataset.",
  "y": "background"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_1",
  "x": "To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in<cite> Yang et al. (2018)</cite> , it improved the QA performance on the hidden test set by 10.59 F1 points. 2 ---------------------------------- **METHODOLOGY**",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_2",
  "x": "**EXPERIMENTS** For all our experiment, unless specified otherwise, we use the open domain corpus 4 released by<cite> Yang et al. (2018)</cite> which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018) . 5 For the BERT encoder, we use the BERT-BASE-UNCASED model. 6 We use the implementation of widely-used BM25 retrieval available in Lucene.",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_4",
  "x": "This evaluation is a bit tricky to do in HOTPOTQA, since the evaluataion set only contains questions from 'hard' subset<cite> (Yang et al., 2018)</cite> . However, within that hard subset, we find the set of question, that has the answer span present in all the supporting passages (SINGLE-HOP (HARD)) and only in one of the supporting passages (MULTI-HOP (HARD)) 11 . The intuition is that if there are multiple evidence containing the answer spans then it might be a little easier for a downstream QA model to identify the answer span. Figure 3 shows that our model performs equally well on both type of queries and hence can be applied in a practical setting. Baseline Reader<cite> (Yang et al., 2018)</cite> Table 2 shows the performance on the QA task.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_0",
  "x": "Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_1",
  "x": "Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.",
  "y": "background motivation"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_2",
  "x": "Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance.",
  "y": "background differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_3",
  "x": "One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation. 1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_4",
  "x": "1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_5",
  "x": "Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance.",
  "y": "background differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_6",
  "x": "**DISCO OBJECTIVE** Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, <cite>Ghazvininejad et al. (2019)</cite> ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs . Namely, for every sentence pair in bitext X and Y , where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) .",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_7",
  "x": "where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right. To address this limitation, we propose a Disentangled Context (DisCo) objective. The objective involves prediction of every token given an arbitrary (thus disentangled) subset of the other tokens.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_8",
  "x": "---------------------------------- **DISCO TRANSFORMER ARCHITECTURE** Simply computing conditional probabilities P (Y n |X, Y n obs ) with a vanilla transformer decoder will necessitate N separate transformer passes for each Y n obs . We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs<cite> (Ghazvininejad et al., 2019)</cite> sample the number of masked tokens uniformly from [1, N ]. DisCo transformer to compute these N contexts in one shot:",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_9",
  "x": "To avoid this cyclic leakage, we make keys and values independent of the previous layer's output: k j n , v j n = Proj(w n + p n ) q j n = Proj(h j\u22121 n ) h n = Attention(K n,j obs , V n,j obs , q j n ) In other words, we decontextualize keys and values in stacked DisCo transformer layers. ---------------------------------- **TRAINING LOSS** We use a standard transformer as an encoder and stacked DisCo layers as a decoder. For each Y n in Y where |Y | = N , we uniformly sample the number of visible tokens from [0, N \u2212 1], and then we randomly choose that number of tokens from Y \\ Y n as Y n obs , similarly to CMLMs<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_10",
  "x": "**MASK-PREDICT** Mask-predict is an iterative inference algorithm introduced in <cite>Ghazvininejad et al. (2019)</cite> to decode a conditional masked language model (CMLM). The target length N is first predicted, and then the algorithm iterates over two steps: mask where i t tokens with lowest probability are masked and predict where those masked tokens are updated given the other N \u2212 i t tokens. The number of masked tokens i t decays from N with a constant rate over a fixed number of iterations T . Specifically, at iteration t,",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_11",
  "x": "obs Namely, we update each position given previous predictions on the easier positions. In a later section, we will explore several variants of choosing Y n,t obs and show that this easyfirst strategy performs best despite its simplicity. ---------------------------------- **LENGTH BEAM** Following <cite>Ghazvininejad et al. (2019)</cite> , we apply length beam.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_12",
  "x": "1 for full pseudo-code. Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5<cite> (Ghazvininejad et al., 2019)</cite> unless otherwise noted. In Sec. 5.2, we Algorithm 1 Parallel Easy-First with Length Beam Source sentence: X Predicted lengths: N1, \u00b7 \u00b7 \u00b7 , NK Max number of iterations: T for k \u2208 {1, 2, ..., K} do for n \u2208 {1, 2, ..., N k } do Y 1,k n , p k n = (arg)max w P (yn = w|X) end for Get the easy-first order z k by sorting p k and let z k (i) be the rank of the ith position. end for will illustrate that length beam facilitates decoding both the CMLM and DisCo transformer.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_13",
  "x": "Benchmark datasets We evaluate on 7 directions from four standard datasets with various training data sizes: WMT'14 EN-DE (4.5M pairs), WMT'16 EN-RO (610K pairs), WMT'17 EN-ZH (20M pairs), and WMT'14 EN-FR (36M pairs, en\u2192fr only). These datasets are all encoded into subword units by BPE (Sennrich et al., 2016) . 4 We use the same preprocessed data and train/dev/test splits as prior work for fair comparisons (EN-DE: Vaswani et al. 2017); 4 We run joint BPE on all language pairs except EN-ZH. Ott et al. (2018) ). We evaluate performance with BLEU scores (Papineni et al., 2002) for all directions except that we use SacreBLEU (Post, 2018) 5 in en\u2192zh again for fair comparison with prior work<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_14",
  "x": "Further, many of prior NAT approaches generate varying numbers of translation candidates and rescore them using an autoregressive model. The rescoring process typically costs overhead of one parallel pass of a transformer encoder followed by a decoder. Given this complexity in latency comparison, we highlight two state-of-the-art iteration-based NAT models whose latency is comparable to our DisCo transformer due to the similar model structure. See Sec. 6 for descriptions of more work on NAT. CMLM As discussed earlier, we can generate a translation with mask-predict from a CMLM<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_15",
  "x": "See Gu et al. (2019) (Luong et al., 2015; Vaswani et al., 2017) . Unfortunately, we lack consensus in evaluation (Post, 2018) . Hyperparameters We generally follow the hyperparameters for a transformer base (Vaswani et al., 2017;<cite> Ghazvininejad et al., 2019)</cite> : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions. We sample weights from N (0, 0.02), initialize biases to zero, and set layer normalization parameters to \u03b2 = 0, \u03b3 = 1 (Devlin et al., 2019) . For regularization, we tune the dropout rate from [0.1, 0.2, 0.3] based on dev performance in each direction, and use 0.01 L 2 weight decay and label smoothing with \u03b5 = 0.1.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_16",
  "x": "**RESULTS AND DISCUSSION** Seen in Table 1 are the results in the four directions from the WMT'14 EN-DE and WMT'16 EN-RO datasets. First, our re-implementations of CMLM + Mask-Predict outperform <cite>Ghazvininejad et al. (2019)</cite> (e.g. 31.24 vs. 30.53 in de\u2192en with 10 steps). This is probably due to our tuning on the dropout rate and weight averaging of the 5 best epochs based on the validation BLEU performance (Sec. 4.1). Our DisCo transformer with the parallel easy-first inference achieves at least comparable performance to the CMLM with 10 steps despite the significantly fewer steps on average (e.g. 4.82 steps in en\u2192de).",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_17",
  "x": "**RELATED AND FUTURE WORK** Recent work on non-autoregressive translation developed ways to mitigate the trade-off between decoding parallelism and performance. As in this work, several prior work proposed methods to iteratively refine output predictions (Lee et al., 2018;<cite> Ghazvininejad et al., 2019</cite>; Gu et al., 2019; Mansimov et al., 2019) . Other approaches include adding a lite autoregressive module to parallel decoding (Kaiser et al., 2018; Sun et al., 2019; Ran et al., 2019) , partially decoding autoregressively (Stern et al., 2018; , rescoring output candidates autoregressively (e.g. Gu et al. (2018) ), mimicking hidden states of an autoregressive teacher , training with different objectives than vanilla negative log likelihood (Libovick\u00fd & Helcl, 2018; Wang et al., 2019; Shao et al., 2020) , reordering input sentences (Ran et al., 2019) , and modeling with latent variables (Ma et al., 2019; Shu et al., 2020) . While this work took iterative decoding methods, our DisCo transformer can be combined with other approaches for efficient training.",
  "y": "similarities"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_0",
  "x": "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence. The domain prediction was based only on the source as the extra token was added at target-side and there was no need for apriori domain information. This approach allowed the model to improve the quality for each domain.",
  "y": "uses"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_1",
  "x": "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012) . Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_0",
  "x": "**INTRODUCTION** Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) .",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_2",
  "x": "The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) . McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_3",
  "x": "The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_4",
  "x": "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_5",
  "x": "In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> .",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_6",
  "x": "However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences.",
  "y": "differences uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_7",
  "x": "**SELF-TRAINING EXPERIMENTS** Charniak and Johnson's reranking parser (June 2006 version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 . The f-score of 83.7% is lower than the f-score of 85.2% reported by<cite> McClosky et al. (2006b)</cite> for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21.",
  "y": "differences"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_0",
  "x": "Commercial software is available to partially automated online support by suggesting responses to a human agent, which may then be accepted or overwritten. The research presented in this paper aims to provide a degree of natural language understanding to assist in automating task-oriented dialogue, such as support services, by suggesting utterances during the dialogue. We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> . The service is designed to assist potential customers with finding various items for sale on the MSN Shopping web site.",
  "y": "background uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_1",
  "x": "A sample from one of the dialogues in this corpus is shown in Table 1 . The research presented here advances previous work which examined various models and tech-niques to predict dialogue acts in task-oriented instant messaging. In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts. Probabilistic models were then trained to predict dialogue acts given a sequence of utterances. Ivanovic (2005a) examined probabilistic and linguistic methods to automatically segment messages from the same corpus into utterances.",
  "y": "background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_2",
  "x": "**BACKGROUND** Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) . The MSN Shopping corpus we use comprises approximately 550 utterances and 6,500 words. <cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data.",
  "y": "uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_3",
  "x": "<cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data. Kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement (Ivanovic, 2005a ). ---------------------------------- **EVALUATION AND RESULTS** As part of a high-level, end-to-end evaluation of dialogue act prediction and their usefulness in semiautomated dialogue systems, we developed a program that simulates a live conversation while suggesting responses.",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_0",
  "x": "About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] . The latest involvement using Wikipedia is the portable cross lingual NER for low resource languages using translation of an annotated NER corpus from English [12, 19] . Another approach to cross lingual and language independent corpora is to learn a model on language independent features of a source language and test the model on other languages using same features [13] .",
  "y": "background uses"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_1",
  "x": "Wikipedia is an open and collaborative multilingual encyclopedia contributed by several collaborators currently having 5.6 million English articles [21] . Constantly, the articles are updated and new articles are added by its collaborators. About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] .",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_2",
  "x": "**CORPUS ACQUISITION** Wikipedia being a huge source of information, its articles comprise of: topic and its comprehensive summary in paragraphs and images; reference to reliable resources; and hyperlinks, also called wikilinks to other articles. Our method takes the advantage of wikilinks between the articles from which linktext is extracted. Since wikilinks are links to articles, it may be considered as a named entity. This approach is similar to Nothman et al (2008) <cite>[4]</cite> to generate the NER data from wikilinks.",
  "y": "similarities"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_3",
  "x": "The MISC F-score is expectedly low, in agreement with the results of Nothman et al (2008) <cite>[4]</cite> . The variation reflected in F-score among all may be the effect of diversity in linguistic attributes. An increase in accuracy from 89% to 92% is observed when the model is trained without MISC tag which reflects that the confusion is created in data by the inclusion of training examples that belong to MISC tag. The Fig.1 illustrates the effect of varying size of the Hi-En-WP training data. The improving trend of accuracy score on increasing training data sufficiently produces scope to scale the size of corpus in future.",
  "y": "similarities"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_0",
  "x": "Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a) . They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; or supervised<cite> (Mohammad et al., 2013</cite>; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011) .",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_1",
  "x": "Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011) . One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> . Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_2",
  "x": "Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_3",
  "x": "Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_4",
  "x": "In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_5",
  "x": "The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013) . In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_6",
  "x": "Mohammad et al. (2013) improve the method by computing sentiment scores using distance-supervised data from emoticon-baring tweets instead of seed words. This approach can be used to automatically extract multilingual sentiment lexicons Mohammad et al., 2015) without using manual resources, which makes it more flexible compared to the first two methods. We consider it as our baseline. We use the same data source as<cite> Mohammad et al. (2013)</cite> to train lexicons. However, rather than relying on PMI, we take a machine-learning method in optimizing the prediction accuracy of emoticons using the lexicons.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_7",
  "x": "In order to evaluate both unsupervised and supervised methods, we follow Tang et al. (2014b) and , removing neutral tweets. The statistics is shown in Table 2 . We compare our lexicon with the lexicons of NRC 4<cite> (Mohammad et al., 2013)</cite> , HIT 5 (Tang et al., 2014a) and WEKA 6 (Bravo-Marquez et al., 2015) . As shown in Table  3 , using the unsupervised sentiment classification method (unsup) in Section 5, our lexicon gives significantly better result in comparison with countbased lexicons of NRC. Under both settings, our lexicon yields the best results compared to other methods.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_8",
  "x": "We employ the standard Arabic Twitter dataset ASTD (Nabil et al., 2015) , which consists of about 10,000 tweets with 4 labels: objective (obj), negative (neg), positive (pos) and mixed subjective (mix). The standard splits of ASTD are shown in Table 4 . We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_9",
  "x": "We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_10",
  "x": "We follow Nabil et al. (2015) by merging training and validating data for learning model. We compare our lexicon with only the lexicons of NRC 7 , because the methods of Tang et al. (2014a) Table 5 , our lexicon consistently gives the best performance on both the balanced and unbalanced datasets, showing the advantage of \"predicting\" over \"counting\". Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_11",
  "x": "The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0). ---------------------------------- **ANALYSIS** ----------------------------------",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_0",
  "x": "---------------------------------- **INTRODUCTION** To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, <cite>18,</cite> 9] have considered a structural re-ranking strategy. The idea is to re-rank the top N documents that some initial search engine produces, where the re-ordering utilizes information about inter-document relationships within that set. Promising results have been previously obtained by using document centrality within the initially retrieved list to perform structural re-ranking, on the premise that if the quality of this list is reasonable to begin with, then the documents that are most related to most of the docuPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_1",
  "x": "To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'06, August 6-11, 2006 , Seattle, Washington, USA. Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_2",
  "x": "The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents?",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_3",
  "x": "The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents. We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents?",
  "y": "background motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_4",
  "x": "Also, clusters have long been considered a promising source of information. The well-known cluster hypothesis [35] encapsulates the intuition that clusters can reveal groups of relevant documents; in practice, the potential utility of clustering for this purpose has been demonstrated for both the case wherein clusters were created in a query-independent fashion [14, 4] and the re-ranking setting [13, 22, 34] . In this paper, we show through an array of experiments that consideration of the mutual reinforcement of clusters and documents in determining centrality can lead to highly effective algorithms for re-ranking an initially retrieved list. Specifically, our experimental results show that the centralityinduction methods that we previously studied solely in the context of document-only graphs<cite> [18]</cite> result in much better re-ranking performance if implemented over bipartite graphs of documents (on one side) and clusters (on the other side). For example, ranking documents by their \"authoritativeness\" as computed by HITS upon these cluster-document graphs yields better performance than that of a previously proposed PageRank implementation applied to documentonly graphs.",
  "y": "extends"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_5",
  "x": "3 4 But identifying such clusters is facilitated by knowledge of which documents are most likely to be relevant -exactly the mutual reinforcement property that HITS was designed to leverage. ---------------------------------- **ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_6",
  "x": "**ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed. Thus, writing \"PR\" for both auth and hub, we conceptually have the (single) equation",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_7",
  "x": "Equation 4 is recursive, but there are iterative algorithms that provably converge to the unique positive solution PR * satisfying the sum-normalization constraint P v\u2208V PR(v) = 1 [21] . Moreover, a (non-trivial) closed-form -and quite easily computed -solution exists for one-way bipartite graphs: is an affine transformation (with respect to positive constants) of, and therefore equivalent for ranking purposes to, the unique positive sum-normalized solution to Equation 4. (Proof omitted due to space constraints.) Interestingly, this result shows that while one might have thought that clusters and documents would \"compete\" for PageRank score when placed within the same graph, in our document-as-authority and document-as-hub graphs this is not the case. Earlier work<cite> [18]</cite> also considered scoring a node v by its influx, P u\u2208V w t(u \u2192 v).",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_8",
  "x": "Examples include document (re-)ranking [7, 24, 9, <cite>18,</cite> 39 ], text summarization [11, 26] , sentence retrieval [28] , and document representation [10] . In contrast to our methods, links connect entities of the same type, and clusters of entities are not modeled within the graphs. While ideas similar to ours by virtue of leveraging the mutual reinforcement of entities of different types, or using bipartite graphs of such entities for clustering (rather than using clusters), are abundant (e.g., [15, 8, 2] ), we focus here on exploiting mutual reinforcement in ad hoc retrieval. Random walks (with early stopping) over bipartite graphs of terms and documents were used for query expansion [20] , but in contrast to our work, no stationary solution was sought. A similar \"short chain\" approach utilizing bipartite graphs of clusters and documents for ranking an entire corpus was recently proposed [19] , thereby constituting the work most resembling ours.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_9",
  "x": "Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION**",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_10",
  "x": "Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ---------------------------------- **GRAPH CONSTRUCTION** Relevance flow based on language models (LMs).",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_11",
  "x": "---------------------------------- **EVALUATION FRAMEWORK** Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters.",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_12",
  "x": "**GRAPH CONSTRUCTION** Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space:",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_13",
  "x": "To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_14",
  "x": "To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] . The relevance-flow measure we use is essentially a directed similarity in language-model space: where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_15",
  "x": "For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters).",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_16",
  "x": "7 Concatenation order is irrelevant for unigram LMs. is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_17",
  "x": "Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> . ---------------------------------- **GRAPHS USED IN EXPERIMENTS.**",
  "y": "background similarities"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_18",
  "x": "Parameter selection for graph-based methods. There are two motivations underlying our approach to choosing values for our algorithms' parameters<cite> [18]</cite> . First, we hope to show that structural re-ranking can provide better results than the optimized baselines even when initialized with a sub-optimal (yet reasonable) ranking. Hence, let the initial ranking be the document ordering induced on the entire corpus by p (q), where \u00b51000 is the smoothing-parameter value optimizing the average noninterpolated precision of the top 1000 documents.",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_19",
  "x": "In what follows, when we say that results or the difference between results are \"significant\", we mean according to the two-sided Wilcoxon test at a confidence level of 95%. ---------------------------------- **RE-RANKING BY DOCUMENT CENTRALITY** Main result. We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work<cite> [18]</cite> ?",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_20",
  "x": "Full suite of comparisons. We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.) To focus our discussion, it is useful to first point out that in almost all of our nine evaluation settings (3 corpora \u00d7 3 evaluation measures), all three of the re-ranking algorithms perform better when applied to c\u2192d graphs than to d\u2194d graphs, as the number of dark bars in Figure 2 indicates. Since it is thus clearly useful to incorporate cluster-based information, we will now mainly concentrate on c\u2192d-based algorithms.",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_21",
  "x": "**FURTHER ANALYSIS** HITS on PageRank-style graphs. Consider our comparison of doc-Auth[d\u2194d] against doc-PageRank[d\u2194d]. As the notation suggests, this corresponds to running HITS and PageRank on the same graph, d\u2194d. But an alternative interpretation<cite> [18]</cite> is that non-smoothed (or no-random-jump) PageRank, as expressed by Equation (3), is applied to a different version of d\u2194d wherein the original edge weights w t(u \u2192 v) have been smoothed as follows: (we ignore nodes with no positive-weight out-edges to simplify discussion, and omit the d\u2194d superscripts for clarity).",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_22",
  "x": "Only in two of the nine evaluation settings did this change cause an increase in performance of docAuth[c\u2192d] over the results attained under the original edgeweighting scheme, despite the fact that the re-weighting involves an extra free parameter. Thus, while we have already demonstrated in previous sections of this paper that information about document-cluster similarity relationships is very valuable, the results just mentioned suggest that such information is more useful in \"raw\" form. Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_23",
  "x": "Re-anchoring to the query. In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_24",
  "x": "While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ----------------------------------",
  "y": "background uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_25",
  "x": "Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively. These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "74471d4e333ce76fd62b968045eba5_0",
  "x": "To the best of our knowledge, there is no similar tutorial presented in previous ACL/COLING/EMNLP/NAACL. This three-hour tutorial will concentrate on a wide range of theories and applications and systematically present the recent advances in deep Bayesian and sequential learning which are impacting the communities of computational linguistics, human language technology and machine learning for natural language processing. ---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; <cite>Zhang et al., 2015</cite>) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few.",
  "y": "background uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_0",
  "x": "In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models -for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-based sequence labeling model that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head 2 information, a subtask found to be useful for many information extraction applications. Our model significantly outperforms the previously mentioned hypergraph model of<cite> Lu and Roth (2015)</cite> and Muis and Lu (2017) on entity mention recognition for the ACE2004 and ACE2005 corpora.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_1",
  "x": "As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> . Directed hypergraphs were also introduced for parsing by Klein and Manning (2001) . While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_2",
  "x": "As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> . Directed hypergraphs were also introduced for parsing by Klein and Manning (2001) . While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure.",
  "y": "background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_3",
  "x": "Also, our model learns the hypergraph greedily and significantly outperforms their approach. Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and<cite> Lu and Roth (2015)</cite> adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection. How-ever, we will show that our model also outperforms their approach on all tasks.",
  "y": "uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_4",
  "x": "Thus at any time step, the representation size is bounded by the number of possible output states instead of the potentially exponential number of output sequences. We then also adjust the directed edges such that they have the same type of head node and the same type of tail node as before in Figure 1 . If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_5",
  "x": "The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> . Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_6",
  "x": "**EVALUATION METRICS** We use a strict evaluation metric similar to<cite> Lu and Roth (2015)</cite> : an entity mention is considered correct if both the mention span and the mention type are exactly correct. Similarly, for the task of joint extraction of entity mentions and mention heads, the mention span, head span and the entity type should all exactly match the gold label. ---------------------------------- **BASELINES AND PREVIOUS MODELS**",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_9",
  "x": "There are 3 hidden layers in our network and the dimensionality of hidden units is 100 in all our experiments. And we set the threshold T as 0.3. ---------------------------------- **RESULTS** We show the performance of our approaches in Table 1 compared to the previous state-of-the-art system<cite> (Lu and Roth, 2015</cite>; Muis and Lu, 2017) on both the ACE2004 and ACE2005 datasets. We find that our LSTM-flat baseline that ignores embedded entity mentions during training performs worse than<cite> Lu and Roth (2015)</cite> ; however, our other neural network-based approaches all outperform the previous feature-based approach.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_10",
  "x": "**DATA** We also evaluate our model on the GENIA dataset (Ohta et al., 2002) for nested named entity recognition. We follow the same dataset split as Finkel and Manning (2009);<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . Thus, the first 90% of the sentences were used in training and the remaining 10% were used for evaluation. We also consider five entity types -DNA, RNA, protein, cell line and cell type.",
  "y": "similarities uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_12",
  "x": "Roth (2015) . We suspect that it is because we use pretrained word embeddings 5 trained on PubMed data (Pyysalo et al., 2013) whereas<cite> Lu and Roth (2015)</cite> did not have access to them. We again find that our neural network model outperforms the previous state-of-the-art (Finkel and Manning, 2009; Muis and Lu, 2017) system. However, we see that both softmax and sparsemax models perform comparably on this dataset. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_13",
  "x": "In this paper, we present a novel recurrent network-based model for nested named entity recognition and nested entity mention detection. We propose a hypergraph representation for this problem and learn the structure using an LSTM network in a greedy manner. We show that our model significantly outperforms a feature based mention hypergraph model<cite> (Lu and Roth, 2015)</cite> and a recent multigraph model (Muis and Lu, 2017) on the ACE dataset. Our model also outperforms the constituency parser-based approach of Finkel and Manning (2009) on the GENIA dataset. In future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally.",
  "y": "differences"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_0",
  "x": "Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics. LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (Brody and Lapata 2009) . However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 . ---------------------------------- **LDA: !(#|%)**",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_1",
  "x": "In the most recent shared task on WSI (Jurgens and Klapaftis 2013) , top models used lexical substitution method (AI-KU) (Baskaya et al. 2013) and Hierarchical Dirichlet Process trained with additional instances (Unimelb) . Latent variable models such as LDA (Blei, Ng, and Jordan 2003) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (Goyal and Hovy 2014). More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and Chen 2014) and that topics and senses should be inferred jointly (STM) (<cite>Wang et al. 2015</cite>) . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs. HC was also extended to a nonparametric model (BNP-HC) (Teh et al. 2004 ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van Durme 2011; Lau et al. 2012; .",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_2",
  "x": "In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective. Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (<cite>Wang et al. 2015</cite>) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; Pelevina et al. 2016; Chang et al. 2018 ). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (Komninos and Manandhar 2016) .",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_3",
  "x": "**PROPOSED MODEL** There are two reasons why Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5).",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_4",
  "x": "First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_5",
  "x": "To solve the problems above, we propose to extend LDA in two parts. First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses. Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and Chen 2014) , or to generate only the neighboring words within a local context, decided by a strict user-specified window (<cite>Wang et al. 2015</cite>) . We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_6",
  "x": "Following (<cite>Wang et al. 2015</cite>), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_7",
  "x": "Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_8",
  "x": "Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; <cite>Wang et al. 2015</cite>) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8 s|d as shown in Equation 1 as the solution of the WSI problem. ---------------------------------- **MODEL F-S V-M AVG \u0394(#S)**",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_9",
  "x": "SemEval 2010 For the SemEval 2010 dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (Manandhar et al. 2010) . In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following (<cite>Wang et al. 2015</cite>) . Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as \u03b4(#S). We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and  Hovy Results are shown in Table 2a , where AutoSense outperforms other competing models on AVG.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_10",
  "x": "The model BNP-HC is an example of such: Though its V-M metric is the highest, it scores the lowest on the F-S metric and greatly overestimates #S, thus having a very high \u03b4(#S). The goal is thus a good balance of V-M and F-S (i.e. highest AVG), and a close estimation of #S (i.e. lowest \u03b4(#S), which is successfully achieved by our models. SemEval 2013 Two metrics are used for the SemEval 2013 dataset: fuzzy B-cubed (F-BC) and fuzzy normalized mutual information (F-NMI). F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_11",
  "x": "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_12",
  "x": "The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model. We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed. This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value. For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (<cite>Wang et al. 2015</cite>) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_13",
  "x": "We similarly apply the actual additional contexts to AutoSense and find that we achieve state-of-the-art performance on AVG. ---------------------------------- **SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_14",
  "x": "---------------------------------- **SENSE GRANULARITY PROBLEM** The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_15",
  "x": "We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA (Blei, Ng, and Jordan 2003) , STM (<cite>Wang et al. 2015</cite>) , HC (Chang, Pei, and Chen 2014) , and a nonparametric model HDP (Teh et al. 2004 ), with AutoSense. We report the results in Figure 4 . Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses. HC and STM also throw garbage senses since they also introduce in some way a new sense variable, however the cluster errors of both models still increase when S is set beyond the maximum number of senses.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_16",
  "x": "It includes the PubMed ID of the papers authored by the given author name. We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website. We use LDA (Blei, Ng, and Jordan 2003) , HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) as baselines. We do not compare with non-text feature-based models (Tang et al. 2012; Cen et al. 2013 ) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_0",
  "x": "**INTRODUCTION** Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective.",
  "y": "background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_1",
  "x": "<cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) argues, most Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate g q and g p ; an attention encoder is applied on post embeddings to generate variable-length queryaware features h i . These features are further aggregated to yield v, which feeds into the final prediction.",
  "y": "background similarities"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_2",
  "x": "Each dataset contains around 50 queries and the more detailed statistics are shown in Table 1 . Following <cite>Rao et al. (2018)</cite> , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) . We run four-fold cross-validation test split by year (i.e., train on three year's data, test on one year's data), and we randomly sample 10 queries from each year in the training sets (in total 30 queries) as our validation set. The mean average precision (MAP) and precision at top 30 (P30) are adopted as our evaluation metrics. We also conducted Fisher's two-sided, paired randomization test (Smucker et al., 2007) to test for statistical significance at p < 0.05.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_3",
  "x": "Baselines: QL is a competitive language modeling baseline. RM3 (Lavrenko and Croft, 2001 ) is an interpolation model combining the QL score with a relevance model using pseudo-relevance feedback. MP-HCNN<cite> (Rao et al., 2018)</cite> is the first neural model that captures the characteristics of social media domain. Their method improves current neural IR methods, e.g., K-NRM (Xiong et al., 2017) , DUET (Mitra et al., 2017) , by a signifi- 4 BiCNN+PAtt+QL 0.4728 1-3 5-7 0.4293 1-3 5-7 0.4147 1,2 5,6 0.2621 1-3 5-7 0.5367 1-3 5,6 0.2990 1-3 5 0.6806 1,2 5-8 0.4563 1-3 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_4",
  "x": "---------------------------------- **5,7** Existing Models 5 QL 0.4000 1 0.3576 1 0.3311 1 0.2091 1 0.4450 1 0.2532 1 0.6182 1 0.3924 1 6 RM3 0.4211 1 0.3824 1 0.3452 1 0.2342 1 0.4733 1 0.2766 1 0.6339 1 0.4480 1 7 MP-HCNN(+URL) 0.4306 1 0.3940 1,2 0.3757 1,5 0.2313 1,5 0.5211 1,5 0.2856 1,5 0.6279 1 0.4178 1 8 MP-HCNN(+URL)+QL 0.4435 1-2 5,6 0.4040 1,2 5,6 0.3915 1,5 6 0.2482 1,2 5 0.5250 1,5 6 0.2937 1,2 5 0.6455 1 0.4403 1,5 Table 2 : Results of non-neural and neural models on the TREC Microblog Tracks datasets. Results from 5 -8 are adopted from <cite>Rao et al. (2018)</cite> . Models denoted with (+URL) represents utilizing the URL information.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_5",
  "x": "cant margin. To the best of our knowledge, <cite>Rao et al. (2018)</cite> is the best neural model to date, and there are no neural models from TREC evaluations for further comparison. We also compared to MP-HCNN+QL, which is a linear interpolation to combine the raw MP-HCNN and QL scores. Table 2 shows our experiment results of all settings and the results of existing models. Model 1 is the effectiveness of BiCNN model with kernel window size 2.",
  "y": "background"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_0",
  "x": "In this paper, we propose to use language as a guide for representation learning, building few-shot classification models that learn visual representations while jointly predicting task-specific language during training. Crucially, our models can operate without language at test time: a more practical setting, since it is often unrealistic to assume that linguistic supervision is available for unseen classes encountered in the wild. Compared to meta-learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model <cite>[1]</cite> , we find this simple auxiliary training objective results in learned representations that generalize better to new concepts. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_3",
  "x": "[13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations. [28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_5",
  "x": "Our work is most similar to <cite>[1]</cite> , which we describe and compare to later. ---------------------------------- **LANGUAGE-SHAPED LEARNING** We are interested in N -way, K-shot learning, where a task t consists of N support classes {S The goal is to predict each y The approach we propose is applicable to any meta-learning framework that learns an embedding of its input.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_9",
  "x": "where \u03bb NL is a tunable parameter controlling the weight of the natural language loss. At test, we simply discard g \u03c6 and use f \u03b8 to classify. With this component, we call our approach language-shaped learning (LSL) (Figure<cite> 1</cite> ). Relation to L3. LSL is similar to another recent model for this setting: Learning with Latent Language (L3) <cite>[1]</cite> , which proposes to use language not only as a supervision source, but as a bottleneck for classification ( Figure<cite> 1</cite> ).",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_10",
  "x": "ShapeWorld. First, we use the ShapeWorld [20] dataset devised by <cite>[1]</cite> , which consists of 9000 training,<cite> 1</cite>000 validation, and 4000 test tasks ( Figure 2 ). 3 Each task contains a single support set of K = 4 images representing a visual concept with an associated (artificial) English language description, generated with a minimal recursion semantics representation of the concept [7] . Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_11",
  "x": "Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept. Model details are identical to <cite>[1]</cite> for easy comparison. f \u03b8 is the final convolutional layer of a fixed ImageNet-pretrained VGG-16 [25] fed through two fully-connected layers: Since this is a binary classification task with only<cite> 1</cite> (positive) support class S and prototype c, we define the similarity function s(a, b) = \u03c3(a \u00b7 b) and the prediction P (\u0177 =<cite> 1</cite> | x) = s (f \u03b8 (x), c).",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_17",
  "x": "**A TASK/MODEL TRAINING DETAILS** Our code is publicly available at https://github.com/jayelm/lsl. A.1 ShapeWorld f \u03b8 . Like <cite>[1]</cite> , f \u03b8 starts with features extracted from the last convolutional layer of a fixed ImageNetpretrained VGG-19 network [25] . These 4608-d embeddings are then fed into two fully connected layers \u2208 R 4608\u00d7512 , R 512\u00d7512 with one ReLU nonlinearity in between. LSL.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_19",
  "x": "f \u03b8 and g \u03c6 are the same as in LSL and Meta. h \u03b7 is a unidirectional<cite> 1</cite>-layer GRU with hidden size 512 sharing the same word embeddings as g \u03c6 . The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_21",
  "x": "Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time. Training. We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training. This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_22",
  "x": "We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping. A.2 Birds f \u03b8 . The 4-layer convolutional backbone f \u03b8 is the same as the one used in much of the few-shot literature [4, 26] .",
  "y": "similarities differences uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_23",
  "x": "This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same. Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> . This is likely due to (1) differences in model initialization due to our PyTorch reimplementation, (2) recreation of the dataset, and (3) our use of early stopping.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_0",
  "x": "Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. ---------------------------------- **INTRODUCTION** The semantics and pragmatics of discourse structure has been a central theme in linguistic research for quite some time.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_1",
  "x": "3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast. ----------------------------------",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_2",
  "x": "It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast. ---------------------------------- **THE DATA** The British National Corpus (BNC; Burnage and Baguley (1996) ) served as the data source for the present investigation.",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_3",
  "x": "This section will focus on the discourse function of the adverbial phrase in contrast. Following <cite>Webber et al. (2003)</cite>, we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora. Discourse anaphora involves a relation between an anaphor, such as a pronoun or a temporal adverbial, and an antecedent that is present in the previous discourse or that can be inferred from it. In the case of pronouns, antecedents are typically NPs, while the antecedents of temporal adverbials can be time-denoting expressions, such as dates, events or states of affairs. For pronouns, discourse anaphora can either involve coreference or more indirect referential relations which do not involve identity of reference with a previous discourse entity, but where the anaphor is merely associated with a previously mentioned discourse entity.",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_4",
  "x": "(8) Sue grabbed one phone, as Tom darted to the other phone.<cite> (Webber et al. (2003)</cite>, p. 555) Here the referent of the other phone can be inferred from the antecedent one phone. The referential relation between the anaphor and the antecedent is not one of identity of reference. Rather, the referents of the antecedent and the anaphor together constitute the set of phones owned by Sue and Tom. It is indicative of the anaphoric character of in contrast that it licenses other-anaphora in the same way, as shown in (9). (9) He retired to Hampshire and died in 1832 at the age of 76.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_5",
  "x": "In contrast, extensive use of pesticides in Europe, North America and Japan is backed by government legislation or voluntary schemes that provide farmers with detailed advice. B7G (0726) Note that the domain of the set of countries in the quantified NP few countries is subsequently narrowed so as to not include countries in Europe, North America and Japan. It is precisely the explicitly mentioned contrast that leads to this effect. <cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial. (12) a. Stack five blocks on top of one another.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_6",
  "x": "(12) a. Stack five blocks on top of one another. Now close your eyes and try knocking the tower, this tower\u00a1 over with your nose.<cite> (Webber et al. (2003)</cite> , p. 552) b. Do you want an apple? Otherwise you can have a pear. (Webber et al. (2003), p. 552) Notice that the same kind of inference is required for contrast in example (13), providing further evidence for the anaphoric nature of this discourse connective. (13) Jack's heart lurched as he saw the ambulances and the busy, functional building and he immediately forgot everything they had been saying. \"I'll ask where he is,\" said Jamie Shepherd as they walked towards the reception desk. In contrast to the outside, the area was softly carpeted, softly lit, as if illness and death had to be cushioned away, made to look as if they didn't exist.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_7",
  "x": "Like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intrasententially, as in example (15). (15) In contrast to his predecessors who worked at all hours of the day Macmillan tended to keep office hours. B0H (0476) Another property that distinguishes anaphoric discourse adverbials from structural connectives in the sense of <cite>Webber et al. (2003)</cite> , i.e. coordinating and subordinating conjunctions, concerns the type of dependencies that the arguments of the types of connectives can enter into. While structural connectives only allow non-crossing adjacent material as their arguments, discourse adverbials may involve crossing dependencies among non-adjacent material -just like other anaphoric expressions. e.g. pronouns and definite descriptions.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_8",
  "x": "The discourse properties of in contrast are not just of interest from a purely theoretical perspective. Teufel and Moens (2002) and Siddharthan and Teufel (2007) In the previous section we established at some length that in contrast shares with other discourse adverbials its anaphoric behavior. This naturally raises the question whether the semantics that has been proposed for this class of expressions can be naturally generalized to the semantics of in contrast. Following earlier proposals by Hinrichs (1986) and Kamp and Reyle (1993) , <cite>Webber et al. (2003)</cite> assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events. For example, the two clauses in (16) refer to individual events, which are put in the sequence-relation by the adverbial then.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_9",
  "x": "It is this highly structured character of the in-contrast relation that distinguishes this discourse connective from the much simpler two-place relations denoted by coordinating and subordinating conjunctions. The latter simply denote relations between events and/or states of affairs, namely those denoted by the two conjunct clauses. The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ---------------------------------- **CONCLUSION AND FUTURE WORK**",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_10",
  "x": "**CONCLUSION AND FUTURE WORK** This paper has presented a corpus-based study of the discourse connective in contrast. The corpus data were drawn from the British National Corpus (BNC) and were analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_0",
  "x": "Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016) ; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint. Vuli\u0107 and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_1",
  "x": "<cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction.",
  "y": "motivation"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_2",
  "x": "The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_3",
  "x": "In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ---------------------------------- **BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_4",
  "x": "**BACKGROUND** This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_5",
  "x": "Also let D \u2208 {0, 1} |Vs|\u00d7|Vt| be a binary matrix representing a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings. By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_6",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_7",
  "x": "The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_8",
  "x": "The optimal solution to this equation is W * = U V T , where X T DZ = U \u03a3V T is the singular value decomposition of X T DZ. This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_9",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_10",
  "x": "We propose two methods for extending this system using orthographic information, described in the following two sections. ---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> . To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_11",
  "x": "**ORTHOGRAPHIC SIMILARITY ADJUSTMENT** This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_12",
  "x": "This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normalized edit distance function is denoted as NL(\u00b7, \u00b7).",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_13",
  "x": "We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value. ---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_14",
  "x": "---------------------------------- **EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_15",
  "x": "Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in <cite>Artetxe et al. (2017)</cite> . 1 However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel- Table 1 : Comparison of methods on test data. Scaling constants c e and c s were selected based on performance on development data over all three language pairs. The last two rows report the results of using both methods together.",
  "y": "similarities uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_16",
  "x": "The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them. Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_17",
  "x": "Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_0",
  "x": "For example, when asked to identify a dog's owner among a group of people, the human visual system adaptively allocates greater computational resources to processing visual information associated with the dog and potential owners, versus other aspects of the scene. The perceptual effects can be so dramatic that prominent entities may not even rise to the level of awareness when the viewer is attending to other things in the scene [3, 4, 5] . Yet attention has not been a transformative force in computer vision, possibly because many standard computer vision tasks like detection, segmentation, and classification do not involve the sort of complex reasoning which attention is thought to facilitate. Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> . Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_1",
  "x": "Successful Visual QA architectures must be able Given a natural image and a textual question as input, our Visual QA architecture outputs an answer. It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_2",
  "x": "It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] . We recognize a broad distinction between types of attention in computer vision and machine learning -soft versus hard attention. Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_3",
  "x": "In particular, selecting those feature vectors with the greatest L2-norm values proves to be a heuristic that can facilitate hard attention -and provide the performance and efficiency benefits associated with -without requiring specialized learning procedures (see Figure 1 ). This attentional signal results indirectly from a standard supervised task loss, and does not require explicit supervision to incentivize norms to be proportional to object presence, salience, or other potentially meaningful measures [20, 21] . We rely on a canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] augmented with a hard attention mechanism that uses the L2-norms of the feature vectors to select subsets of the information for further processing. The first version, called the Hard Attention Network (HAN), selects a fixed number of feature vectors by choosing those with the top norms. The second version, called the Adaptive Hard Attention Network (AdaHAN), selects a variable number of feature vectors that depends on the input.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_4",
  "x": "This algorithm computes features over pairs of input features and thus scale quadratically with number of vectors in the feature map, highlighting the importance of feature selection. ---------------------------------- **RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_5",
  "x": "**RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] . Thus, we focus on the recently-introduced VQA-CP <cite>[7]</cite> and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning. One of the core challenges of Visual QA is the problem of grounding language: that is, associating the meaning of a language term with a specific perceptual input [36] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_6",
  "x": "The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] . In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells. The visual representation is then the weighted-sum of the input tensor across space. The alternative is to select CNN grid cells in a discrete way, but due to many challenges in training non-differentiable architectures, such hard attention alternatives are severely under-explored.",
  "y": "background motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_7",
  "x": "**METHOD** Answering questions about images is often formulated in terms of predictive models [24] . These architectures maximize a conditional distribution over answers a, given questions q and images x: where A is a countable set of all possible answers. As is common in question answering<cite> [7,</cite> 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice).",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_8",
  "x": "We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] . Questions and images are encoded into their vector representations. Next, the spatial encoding of the visual features is unraveled, and the question embedding is broadcasted and concatenated (or added) accordingly to form a multimodal representation of the inputs. Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_10",
  "x": "Questions and images are encoded into their vector representations. Next, the spatial encoding of the visual features is unraveled, and the question embedding is broadcasted and concatenated (or added) accordingly to form a multimodal representation of the inputs. Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module. work<cite> [7,</cite> 9, 22, 23, 24, 25] . After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms<cite> [7,</cite> 8, 9, 10] at this point in the architecture.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_11",
  "x": "**DATASETS** VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_12",
  "x": "VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture.",
  "y": "motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_13",
  "x": "We therefore repeat the above experiment with the non-local pairwise aggregation mechanism described in section 3, which computes activations for every pair of attended cells, and therefore scales quadratically with the number of at-tended cells. These results are shown in the middle of Table 1 , where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention. Finally, we compare standard soft attention baselines in the bottom of Table 1. In particular, we include previous results using a basic soft attention network<cite> [7,</cite> 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in<cite> [7,</cite> 9] with the same features used in other experiments. Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_14",
  "x": "Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. The nonlocal pairwise aggregation performs better than SAN on its own, although the best result includes hard attention. Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor. Table 2 : Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation.",
  "y": "differences"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_15",
  "x": "We trained our models until we notice a saturation on the training set. Then we evaluate these models on the test set. Our tables show the performance of all the methods wrt. the second digits precision obtained by rounding. Table 1 shows SAN's [9] results reported by <cite>[7]</cite> together with our in-house implementation (denoted as \"ours\").",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_16",
  "x": "Analysis. In our experiments, simple-SAN achieves about 21% performance on the test set. Surprisingly, simple-HAN+sum achieves about 24% performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture [67] ; the results are reported by <cite>[7]</cite> . This result shows that the hard attention mechanism can indeed be tightly coupled within the training process, and that the whole procedure does not rely heavily on the properties of the ImageNet pre-trained networks. In a sense, we see that a discrete notion of entities also \"emerges\" through the learning process, leading to efficient training.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_0",
  "x": "Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000) , more recent methods (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013 ) and a measure of semantic relatedness of short texts (\u0160ari\u0107 et al., 2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences. We then derive segments using the maximal cliques of such similarity graphs.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_1",
  "x": "---------------------------------- **RELATED WORK** Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing. Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 ).",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_2",
  "x": "The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_3",
  "x": "Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_4",
  "x": "More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_5",
  "x": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg-2008, and 2012 U.S. elections ments contain. For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_6",
  "x": "For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset. 4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_7",
  "x": "Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) .",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_8",
  "x": "On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments. WindowDiff is a stricter version of P k as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_9",
  "x": "In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_10",
  "x": "cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation. ---------------------------------- **RESULTS AND DISCUSSION** In Table 2 we report the performance of GRAPH-SEG and prominent TS methods on the synthetic Choi dataset. GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_11",
  "x": "GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself. Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents -some of which belong to the the training set and others to the test set, as admitted by <cite>Riedl and Biemann (2012)</cite> and this is why their reported performance on this dataset is overestimated. In Table 3 we report the results on the Manifesto dataset.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_12",
  "x": "In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not significantly improve the performance for neither of the two models. This result contrasts previous findings (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods' with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset. ---------------------------------- **CONCLUSION** In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation.",
  "y": "differences"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_0",
  "x": "Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin.",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_1",
  "x": "Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin.",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_2",
  "x": "Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin.",
  "y": "background motivation"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_3",
  "x": "The training of the in-domain MT system is done with a two-step process: (1) We use the target-side English texts as the pivot, and train an unsupervised NMT (model unsup ) directly between in-domain English text and the available monolingual Pidgin corpus. (2) Next, we employ self-training (He et al., 2019) to create augmented parallel pairs to continue updating the system (model self ). ---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT).",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_4",
  "x": "First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input. We augment this dataset to the existing monolingual corpus.",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_5",
  "x": "First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup . Then we further utilize model unsup to construct pseudo parallel corpus by predicting target Pidgin text given the English input. We augment this dataset to the existing monolingual corpus.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_0",
  "x": "Researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction. Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999) , Turney (2000; , Hulth (2003) , Medelyan et al. (2009)) . A disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains. Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ).",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_1",
  "x": "Supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not (e.g., Frank et al. (1999) , Turney (2000; , Hulth (2003) , Medelyan et al. (2009)) . A disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains. Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue.",
  "y": "motivation"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_2",
  "x": "How would these systems perform on a different dataset with their original configuration? What could be the underlying reasons in case they perform poorly? Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) .",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_3",
  "x": "Is there any system that can generalize fairly well across various domains? We seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions. More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_4",
  "x": "More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_5",
  "x": "The DUC-2001 dataset (Over, 2001) , which is a collection of 308 news articles, is annotated by Wan and Xiao (2008) . We report results on all 308 articles in our evaluation. The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. Each document has two sets of keyphrases assigned by the indexers: the controlled keyphrases, which are keyphrases that appear in the Inspec thesaurus; and the uncontrolled keyphrases, which do not necessarily appear in the thesaurus. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by Hulth (2003) and later by<cite> Mihalcea and Tarau (2004)</cite> and Liu et al. (2009b) .",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_6",
  "x": "A generic unsupervised keyphrase extraction system typically operates in three steps (Section 3.1), which will help understand the unsupervised systems explained in Section 3.2. ---------------------------------- **GENERIC KEYPHRASE EXTRACTOR** Step 1: Candidate lexical unit selection The first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. Commonly used heuristics include (1) using a stop word list to remove non-keywords (e.g., Liu et al. (2009b) ) and (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be considered candidate keywords<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009a) , Wan and Xiao (2008) ).",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_7",
  "x": "Depending on the underlying approach, each candidate word is represented by its syntactic and/or semantic relationship with other candidate words. The relationship can be defined using co-occurrence statistics, external resources (e.g., neighborhood documents, Wikipedia), or other syntactic clues. Step 3: Keyphrase formation In the final step, the ranked list of candidate words is used to form keyphrases. A candidate phrase, typically a sequence of nouns and adjectives, is selected as a keyphrase if (1) it includes one or more of the top-ranked candidate words<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009b) ), or (2) the sum of the ranking scores of its constituent words makes it a top scoring phrase (Wan and Xiao, 2008) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_8",
  "x": "---------------------------------- **TEXTRANK** In the TextRank algorithm <cite>(Mihalcea and Tarau, 2004)</cite> , a text is represented by a graph. Each vertex corresponds to a word type. A weight, w ij , is assigned to the edge connecting the two vertices, v i and v j , and its value is the number of times the corresponding word types co-occur within a window of W words in the associated text.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_9",
  "x": "As noted before, after convergence, the T % top-scored vertices are selected as keywords. Adjacent keywords are then collapsed and output as a keyphrase. According to<cite> Mihalcea and Tarau (2004)</cite> , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words. Hence, our implementation of TextRank follows this configuration. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_10",
  "x": "---------------------------------- **EVALUATION** ---------------------------------- **EXPERIMENTAL SETUP** TextRank and SingleRank setup Following<cite> Mihalcea and Tarau (2004)</cite> and Wan and Xiao (2008) , we set the co-occurrence window size for TextRank and SingleRank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_11",
  "x": "We generate the curves for each system as follows. For Tf-Idf, SingleRank, and ExpandRank, we vary the number of keyphrases, N , predicted by each system. On average, TextRank performs much worse compared to Tf-Idf. This certainly gives more insight into TextRank since it was evaluated on Inspec only for T=33% by<cite> Mihalcea and Tarau (2004)</cite> .",
  "y": "differences"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_12",
  "x": "An examination of Liu et al.'s (2009b) results reveals a subtle caveat in keyphrase extraction evaluations. In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec. We score 2 and 5 points less than Wan and Xiao's (2008) implementations of SingleRank and ExpandRank, respectively.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_13",
  "x": "In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec. We score 2 and 5 points less than Wan and Xiao's (2008) implementations of SingleRank and ExpandRank, respectively. We speculate that document pre-processing (e.g., stemming) has contributed to the discrepancy, but additional experiments are needed to determine the reason.",
  "y": "similarities"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_0",
  "x": "---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_1",
  "x": "---------------------------------- **INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_2",
  "x": "**INTRODUCTION** Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_3",
  "x": "---------------------------------- **UNIVERSAL SCHEMA** A universal schema is defined as the union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base<cite> (Riedel et al., 2013)</cite> . The task here is to complete this schema by jointly reasoning over surface form patterns and relations. A successful approach to this joint reasoning is to embed both kinds of relations into the same low-dimensional embedding space, which can be achieved by matrix or tensor factorization methods.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_4",
  "x": "We will study such representations for universal schema in this paper. ---------------------------------- **MATRIX FACTORIZATION WITH FACTORS OVER ENTITY-PAIRS** In matrix factorization for universal schema,<cite> Riedel et al. (2013)</cite> construct a sparse binary matrix of size |P| \u00d7 |R| whose rows are indexed by entity-pairs (a, b) \u2208 P and columns by surface form and Freebase relations s \u2208 R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r \u2208 R |R|\u00d7k and entity-pair factors p \u2208 R |P|\u00d7k , the probability of a relation s and two entities a and b is: where \u03c3 is the sigmoid function.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_5",
  "x": "In this work we use a variant of TransE in which different embeddings are learned for an entity for each argument position. ---------------------------------- **MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e.,",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_6",
  "x": "**MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_7",
  "x": "Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations. It is worth noting that tensor factorization for universal schema has been proposed by , who also observed that tensor factorization by itself performs poorly (even with additional type constraints), and the predictions need to be combined with matrix factorization to be accurate. In this section we will present the fundamental differences between matrix and tensor factorization, and examine a few hybrid models that can address these concerns. Black is a sparsely observed relation between any pair of entities.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_8",
  "x": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other. For instance, even if the type constraints of a relation are violated, a high score by the pairwise model score might still yield a high prediction for that triplet.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_9",
  "x": "**COMBINED MODEL (FE)** As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)** A problem with combining the two models additively, as in FE, is that one model can easily override the other.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_10",
  "x": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. A problem with combining the two models additively, as in FE, is that one model can easily override the other. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet.",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_11",
  "x": "To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet. Specifically, we use the smooth approximation of a rectifier \u2295(x) = log(1 + e x ) and define the probability for a triplet as follows: ---------------------------------- **PARAMETER ESTIMATION** As by<cite> Riedel et al. (2013)</cite> , we use a Bayesian personalized ranking objective (Rendle et al., 2009 ) to estimate parameters, i.e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using AdaGrad.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_12",
  "x": "Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview). In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in .",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_13",
  "x": "**UNIVERSAL SCHEMA RELATION EXTRACTION** With the promising results shown on synthetic data, we now turn to evaluation on real-world information extraction. In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview).",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_14",
  "x": [
   "In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in . Models that use the matrix factorization (F, FE, R13-F and RFE) are significantly better, but more importantly, the hybrid appraoch FE achieves the highest accuracy. It is unclear why RFE fails to provide similar gains, in particular, performing slightly worse than matrix factorization."
  ],
  "y": "differences"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_0",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation.",
  "y": "background"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_1",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation. Their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents. Even though CCG is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon. Moreover, these annotation tools are not open-source and cannot easily be deployed to support other annotation efforts. In this paper, we present an open-source, lightweight, easy-to-use graphical annotation tool that employs a statistical parser to create initial CCG derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.",
  "y": "motivation"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_2",
  "x": "**A QUADRILINGUAL PILOT CCG TREEBANK** To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019) , each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank <cite>(Hockenmaier and Steedman, 2007)</cite> , which is in turn based on the Penn Treebank (Marcus et al., 1993) . Since CCGrebank only covers English and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed. We initially annotated ten sentences in four languages and discussed disagreements.",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_0",
  "x": "Using semantic information to improve parsing performance has been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994) . Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_1",
  "x": "In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_2",
  "x": "We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_3",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized.",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_4",
  "x": "As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_5",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_6",
  "x": "---------------------------------- **DATASET** We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing.",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_7",
  "x": "We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_8",
  "x": "**SEMANTIC REPRESENTATION AND DISAMBIGUATION METHODS** We will experiment with the range of semantic representations used in <cite>Agirre et al. (2008)</cite> , all of which are based on WordNet 2.1. Words in WordNet (Fellbaum, 1998) are organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_9",
  "x": "\u2022 Feature combinations give an improvement over using a single feature. <cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 . Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_10",
  "x": "MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 . Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. \u2022 The present work presents a statistically significant improvement for the full treebank using WordNet-based semantic information for the first time. Our results extend those of <cite>Agirre et al. (2008)</cite> , which showed improvements on a subset of the PTB.",
  "y": "extends differences"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_0",
  "x": "**INTRODUCTION** The advent of neural networks in natural language processing (NLP) has significantly improved state-of-the-art results within the field. While recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so-called transformer models (Vaswani et al., 2017) . This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 (Radford et al., 2018 (Radford et al., , 2019 and ELMo (Peters et al., 2018) . BERT <cite>(Devlin et al., 2019)</cite> improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_1",
  "x": "The second phase only needs a relatively small annotated data set for fine-tuning to outperform previous popular approaches in one of a large number of possible language tasks. While language models are usually trained on English data, some multilingual models also exist. These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) . However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_2",
  "x": "**RELATED WORK** Transformer models have been successfully used for a wide range of language tasks. Initially, transformers were introduced for use in machine translation, where they vastly improved state-of-the-art results for English to German in an efficient manner (Vaswani et al., 2017) . This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <cite>(Devlin et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_3",
  "x": "These tasks allowed the model to create internal representations about a language, which could thereafter be reused for different language tasks. This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures <cite>(Devlin et al., 2019)</cite> . Transformer models are also capable of generating contextualized word embeddings. These contextualized embeddings were presented by Peters et al. (2018) and addressed the well known issue with a word's meaning being defined by its context (e.g. \"a stick\" versus \"let's stick to\"). This lack of context is something that traditional word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) lack, whereas BERT automatically incorporates the context a word occurs in.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_4",
  "x": "That<cite> Devlin et al. (2019)</cite> trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every input. Other research improved the NSP task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order (Lan et al., 2019) . Devlin et al. (2019) also presented a multilingual model (mBERT) with the same architecture as BERT, but trained on Wikipedia corpora in 104 languages. Unfortunately, the quality of these multilingual embeddings is often considered worse than their monolingual counterparts.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_5",
  "x": "---------------------------------- **TRAINING** RobBERT shares its architecture with RoBERTa's base model, which itself is a replication and improvement over BERT . The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads <cite>(Devlin et al., 2019)</cite> . One difference with the original BERT is due to the different pre-training task specified by RoBERTa, using only the MLM task and not the NSP task.",
  "y": "similarities"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_6",
  "x": "ZeroR (majority class) 66.70 mBERT <cite>(Devlin et al., 2019)</cite> 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03 RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table 1 and Table 2 ). It is also able to reach similar performance using less data. The fact that zero-shot RobBERT outperforms other zero-shot BERT models is also an indication that the base model has internalised more knowledge about Dutch than the other two have. The reason RobBERT and other BERT models outperform the previous RNN-based approach is likely the transformers ability to deal better with coreference resolution , and by extension better in deciding which word the \"die\" or \"dat\" belongs to.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_0",
  "x": "To deal with phonological variability alternate pronunciations are included in the lexicon, and optional phonological rules are applied during training and recognition. A trigram LM is used in a second acoustic decoding pass which makes use of the word graph generated using the bigram LM [6] . Experimental results are reported on the ARPA Wall Street Journal (WSJ) <cite>[19]</cite> and BREF [14] corpora, using for both corpora over 37k utterances for acoustic training and more than 37 million words of newspaper text for language model training. It is shown that for both corpora increasing the amount of training utterances by an order of magnitude reduces the word error by about 30%. The use of a trigram LM in a second pass also gives an error reduction of 20% to 30%.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_1",
  "x": "Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:. In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts <cite>[19]</cite> .",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_2",
  "x": "Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests. ing pass to reduce computational requirements. The trigram langage model is used in the second pass of the decoding process:. In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts <cite>[19]</cite> .",
  "y": "uses differences"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_3",
  "x": "**WSJ:** The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies. Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information. The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> .",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_4",
  "x": "The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies. Text materials were selected to provide training and test data for 5k and 20k word, closed and open vocabularies, and with both verbalized (VP) and non-verbalized (NVP) punctuation. 41n our implementation, a word lattice differs from a word graph only because it includes word endpoint information. The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> . Two sets of standard training material have been used for these experiments: The standard WSJ0 SI84 training data which include 7240 sentences from 84 speakers, and the standard set of 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_5",
  "x": "The Nov92 5k and 20k nvp test sets were used to assess progress during this development phase. The WSJ system was evaluated in the Nov92 ARPA evaluation test [17] for the 5k-closed vocabulary and in the Nov93 ARPA evaluation test [18] for the 5k and 64k hubs. Except when explicitly stated otherwise, all of the results reported for WSJ use the standard language models <cite>[19]</cite> . Using a set of 1084 CD models trained with the WSJ0 si84 training data, the word error is 6.6% on the Nov92 5k test data and 9.4% on the Nov93 test data. Using the combined WSJ0]WSJ1 si284 training data reduces the error by about 27% for both tests.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_6",
  "x": "---------------------------------- **DISCUSSION AND SUMMARY** The recognizer has been evaluated on 5k and 20k test data for the English and French languages using similar style corpora. It should be pointed out however, that although the Nov92 5k WSJ test data and the BREF 5k test data were closed-vocabulary, the conditions are not quite the same. For WSJ, paragraphs were selected ensuring not more than one word was out of the 5.6k most frequent words<cite> [ 19]</cite> , and these additional words were then included as part of the vocabulary.",
  "y": "uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_0",
  "x": "Antol et al. [9] propose an element-wise multiplication between image and question embeddings to generate spatial attention map. Fukui et al.<cite> [6]</cite> propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. [26] extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilinear operator. They achieve this by adding an initial step that efficiently expands the textual and visual embeddings to a high-dimensional space. In terms of structural innovations, Noh et al. [16] embed the textual question as an intermediate dynamic bilinear layer of a ConvNet that processes the visual information.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_1",
  "x": "The common approach is to use a one-way attention scheme, where the embedding of the question is used to generate a set of attention coefficients over a set of predefined image regions. These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21,<cite> 6,</cite> 25, 26] . More elaborated forms of attention has also been proposed. Xu and Saenko [23] suggest use wordlevel embedding to generate attention. Yang et al. [24] iterates the application of a soft-attention mechanism over the visual input as a way to progressively refine the location of relevant cues to answer the question.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_2",
  "x": "Specifically, we take the mined grounding labels as weakly-supervised signals and denote two types of attention supervision, namely region-level and object-level labels. Figure 2 shows the main pipeline of our VQA model. We mostly build upon the MCB model in<cite> [6]</cite> , which exemplifies current state-of-the-art techniques for this problem. Our main innovation to this model is the addition of an Attention Supervision Module that incorporates visual grounding as an auxiliary task. Next we describe the main modules behind this model.",
  "y": "extends differences"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_3",
  "x": "Image Attention Module: Images are passed through an embedding layer consisting of a pre-trained ConvNet model, such as Resnet pretrained with the ImageNet dataset [10] . This generates image features I f \u2208 R C\u00d7H\u00d7W , where C, H and W are depth, height and width of the extracted feature maps. Fusion Module I is then used to generate a set of image attention coefficients. First, question features Q w are tiled as the same spatial shape of I f . Afterwards, the fusion module models the joint relationship J attn \u2208 R O\u00d7H\u00d7W between questions and images, mapping them to a common space of dimension O. In the simplest case, one can implement the fusion module using either concatenation or Hadamard product [1] , but more effective pooling schemes can be applied <cite>[6,</cite> 11, 25, 26] .",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_4",
  "x": "Afterwards, it computes the logits over a set of predefined candidate answers. Following previous work<cite> [6]</cite> , we use as candidate outputs the top 3000 most frequent answers in the VQA dataset. At the end of this process, we obtain the highest scoring answer\u00c2. ---------------------------------- **ATTENTION SUPERVISION MODULE:**",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_5",
  "x": "Sections 4 and 5 provide details about our proposed method to obtain the attention labels and to train the resulting model, respectively. J a n s Figure 2 . Schematic diagram of the main parts of the VQA model. It is mostly based on the model presented in<cite> [6]</cite> . Main innovation is the Attention Supervision Module that incorporates visual grounding as an auxiliary task.",
  "y": "extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_6",
  "x": "**IMPLEMENTATION DETAILS** We build the attention supervision on top of the opensourced implementation of MCB<cite> [6]</cite> and MFB [25] . Similar to them, We extract the image feature from res5c layer of Resnet-152, resulting in 14 \u00d7 14 spatial grid (H = 14, W = 14, C = 2048). We construct our ground-truth visual grounding labels to be G v = 2 glimpse maps per QA pair, where the first map is object-level grounding and the second map is region-level grounding, as discussed in Section 4. Let (x i min , y i min , x i max , y i max ) be the coordinate of i th selected object bounding box in the grounding labels, then the mined object-level attention maps C 0 gt are: where I[\u00b7] is the indicator function.",
  "y": "differences extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_7",
  "x": "The dataset is split into training (82K images and 443K questions), validation (40K images and 214K questions), and testing (81K images and 448K questions) sets. The task is to predict a correct answer A given a corresponding image-question pair (I, Q). As a main advantage with respect to version 1.0 [9] , for every question VQA-2.0 includes complementary images that lead to different answers, reducing language bias by forcing the model to use the visual information. Visual Genome: The Visual Genome (VG) dataset [12] contains 108077 images, with an average of 17 QA pairs per image. We follow the processing scheme from<cite> [6]</cite> , where non-informative words in the questions and answers such as \"a\" and \"is\" are removed.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_8",
  "x": "**RANK CORRELATION** Accuracy/% VQA-HAT VQA-X VQA-2.0 Human [5] 0.623 -80.62 PJ-X [17] 0.396 0.342 -MCB<cite> [6]</cite> 0 authors also collect 1374 \u00d7 3 = 4122 HAT maps for VQA-1.0 validation sets, where each of the 1374 (I, Q, A) were labeled by three different annotators, so one can compare the level of agreement among labels. We use VQA-HAT to evaluate visual grounding performance, by comparing the rank-correlation between human attention and model attention, as in [5, 17] . VQA-X: VQA-X dataset [17] contains 2000 labeled attention maps in VQA-2.0 validation sets. In contrast to VQA-HAT, VQA-X attention maps are in the form of instance segmentations, where annotators were asked to segment objects and/or regions that most prominently justify the answer.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_0",
  "x": "---------------------------------- **INTRODUCTION** In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_1",
  "x": "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources). We seek to understand under which conditions a low-resource neural tagger benefits from external lexical knowledge. In particular: a) we evaluate the neural tagger across a total of 20+ languages, proposing a novel baseline which uses retrofitting; b) we investigate the reliance on dictionary size and properties; c) we analyze model-internal representations via a probing task to investigate to what extent model-internal representations capture morphosyntactic information.",
  "y": "motivation"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_2",
  "x": "We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon. A property here is a possible PoS tag (for Wiktionary) or a morphological feature (for Unimorph).",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_3",
  "x": "1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon.",
  "y": "extends"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_4",
  "x": "In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_5",
  "x": "Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_6",
  "x": "Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_7",
  "x": "The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings. Language-specific hyperparameters could lead to optimized models for each language.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_8",
  "x": "Language-specific hyperparameters could lead to optimized models for each language. However, we use identical settings for each language which worked well and is less expensive, following Bohnet et al. (2018) . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_9",
  "x": "We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ---------------------------------- **RESULTS**",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_10",
  "x": "There are several take-aways. ---------------------------------- **INCLUSION OF LEXICAL INFORMATION** Combining the best of two worlds results in the overall best tagging accuracy, confirming<cite> Plank and Agi\u0107 (2018)</cite> : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages). On 15 out of 21 languages, DSDS is the best performing model.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_11",
  "x": "---------------------------------- **LEARNING CURVES** The lexicons we use so far are of different sizes (shown in Table 1 of<cite> Plank and Agi\u0107 (2018)</cite> ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries. In a low-resource setup, large dictionaries might not be available. It is thus interesting to examine how tagging accuracy is affected by dictionary size.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_12",
  "x": "One of the first works that combines neural representations with semantic symbolic lexicons is the work on retrofitting (Faruqui et al., 2015) . The main idea is to use the relations defined in semantic lexicons to refine word embedding representations, such that words linked in the lexical resource are encouraged to be closer to each other in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_13",
  "x": "We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner. We replicated the results of<cite> Plank and Agi\u0107 (2018)</cite> , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting. By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size. Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data. Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task.",
  "y": "uses extends"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_0",
  "x": "**ABSTRACT** In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model -the <cite>relaxed hybrid tree</cite> model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/.",
  "y": "differences motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_1",
  "x": "One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_2",
  "x": "One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations. Thus, <cite>the model</cite> is unable to identify joint representations for certain sentence-semantics pairs during the training process, and is unable to produce desired outputs for certain inputs during the evaluation process. In this work, we propose a solution addressing the above limitation, which makes our model more robust.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_3",
  "x": "The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_4",
  "x": "On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms of supervision is also possible.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_6",
  "x": "**<cite>RELAXED HYBRID TREES</cite>** We briefly discuss our previously proposed <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> in this section. <cite>The model</cite> is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008) . Both systems are publicly available 1 . Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. <cite>The model</cite> defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach:",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_7",
  "x": "its corresponding semantic representation. Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_8",
  "x": "Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se- In the hybrid tree structure, each word is strictly associated with a semantic unit. For example the word w 3 is associated with the semantic unit m b . In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_9",
  "x": "In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . These indirect associations allow the longdistance dependencies to be captured. Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level. For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ----------------------------------",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_10",
  "x": "For example, within the semantic unit m a , we have a pattern wXw which states that we first have words that are directly associated with m a , followed by some words covered by its first child semantic unit, then another sequence of words directly associated with m a . ---------------------------------- **LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_11",
  "x": "This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid.",
  "y": "motivation differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_13",
  "x": "In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation. This may lead to possible <cite>relaxed hybrid tree</cite> representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) . When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_14",
  "x": "This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h . To eliminate <cite>relaxed hybrid trees</cite> consisting of an infinite number of nodes, pattern X is disallowed in the <cite>relaxed hybrid trees</cite> model <cite>(Lu, 2014)</cite> . However, disallowing pattern X has led to other issues. Specifically, for certain semanticssentence pairs, it is not possible to find <cite>relaxed hybrid trees</cite> that jointly represent them. In the example semantics-sentence pair given in Figure 3 (a) , it is not possible to find any <cite>relaxed hybrid tree</cite> that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_17",
  "x": "---------------------------------- **CONSTRAINED SEMANTIC FORESTS** To address this limitation, we allow pattern X to be included when building our new discriminative semantic parsing model. However, as mentioned above, doing so will lead to latent structures (<cite>relaxed hybrid tree</cite> representations) of infinite heights. To resolve such an issue, we instead add an additional constraint -limiting the height of a semantic representation to a fixed constant c, where c is larger than the maximum height of all the trees appearing in the training set.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_18",
  "x": "Table 1 summarizes the list of patterns that our model considers. This is essentially the same as those considered by the hybrid tree model. Our new objective function is as follows: where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible <cite>relaxed hybrid tree</cite> representations where the pattern X is allowed. The main challenge now becomes the computation of the denominator term in Equation 2, as the set M is still very large.",
  "y": "uses motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_19",
  "x": "To properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest (CSF) representation of M here. Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_20",
  "x": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_21",
  "x": "Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm. ---------------------------------- **EXPERIMENTS**",
  "y": "similarities"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_22",
  "x": "To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 .",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_23",
  "x": "To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) . The results of our system as well as those of several previous systems are given in Table 2 .",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_24",
  "x": "UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_25",
  "x": "Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_26",
  "x": "<cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_27",
  "x": "Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1. Second, our model considers the additional pattern X and therefore has the capability to capture more accurate dependencies between the words and semantic units. We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_28",
  "x": "Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages.",
  "y": "differences extends"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_29",
  "x": "As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_30",
  "x": "While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ).",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_31",
  "x": "It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_0",
  "x": "In subsequent work, we showed that we could augment the E2E training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training<cite> [5]</cite> . Here, instead, we explore whether a model that is trained to achieve a single stylistic personality target can produce outputs that combine stylistic targets, to yield a novel style that is significantly different than what was seen in training, while still maintaining high semantic correctness. We first train each stylistic model with a single latent variable for supervision, for five different personality models, or voices, based on the Big Five theory of personality, namely the personality trait styles of EXTRAVERT, AGREEABLE, DISAGREEABLE, CONSCI-ENTIOUS, and UNCONSCIENTIOUS. Then, at generation time, we provide the model with combinations of the stylistic variables, i.e. we instruct the NNLG to generate multivoice outputs that combine EXTRAVERT with DISAGREEABLE, where such combined outputs never occurred in the training data. We first describe how we set up our dataset and neural models in Section 2, and then present our results in Section 3.",
  "y": "background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_1",
  "x": "There is a long tradition in AI of using slightly synthetic tasks and datasets in order to test the ability of particular models to achieve these tasks [7, 8] . The PERSONAGE corpus<cite> [5]</cite> provides a controlled environment for testing different models of neural generation and style generation. It consists of 88,500 restaurant domain utterances whose style varies according to models of personality, which were generated by an existing statistical NLG engine that has the capability of manipulating 67 different stylistic parameters [9] . Table 2 shows sample utterances that are output for the singlevoice models and for each of our multi- Table 2 : MultiVoice generation output and comparable singlevoice outputs for DISAGREEABLE, EXTRAVERT and CONSCIENTIOUS for the meaning representation in Figure 1 . We count the frequency of periods (Period Agg.) and expletives (Explet. Prag) for multivoice models that utilize DISAGREEABLE).",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_2",
  "x": "The frequencies of longer utterances (more attribute MRs) vary across train and test with test MRs not seen during training. The training data has more smaller MRs, while the test set is more challenging, with more larger MRs. Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation. This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description.",
  "y": "background differences"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_3",
  "x": "The training data has more smaller MRs, while the test set is more challenging, with more larger MRs. Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation. This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> .",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_4",
  "x": "For every input MR and a personality, we train the model with the corresponding PER-SONAGE generated sentence. Our model differs from the TO-KEN model used in our previous work<cite> [5]</cite> because it is trained on unsorted inputs to allow us to add multiple CONVERT tags to the MR at generation time. Note that we do not train on multiple personalities, instead, we train one model that uses all the data, where each distinct single personality has a corresponding CONVERT(PERSONALITY = X) in the training instance. At generation time, we generate singlevoice data for all the test MRs (1,390 total realizations, 278 unique MRs, realized for each of 5 personalities). For the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test MRs, since the order of the CONVERT tags matters.",
  "y": "differences"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_5",
  "x": "Natural language generators for task-oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics. The use of neural natural language generation (NNLG) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance. However, there has been little investigation of NNLGs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation. Instead, work on stylistic transfer has focused on tasks where only coarse-grained semantic fidelity is needed, such as controlling the sentiment of the utterance (positive or negative), or the topic or entity under discussion [1, 2, 3] . Consider for example a training instance for the restaurant domain consisting of a meaning representation (MR) from the End-to-End (E2E) Generation Challenge 1 and a sample output from one of our neural generation models in Figure 1 [4, <cite>5]</cite> .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_0",
  "x": "Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by<cite> McDonald and Nivre (2007)</cite> . On the one hand, global beam-search parsing is more similar to global, exhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_1",
  "x": "We follow<cite> McDonald and Nivre (2007)</cite> and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data. Our results show that beam-search im-proves the precision on long sentences and dependencies compared to greedy search, while the advantage of transition-based parsing on short dependencies is preserved. Under particular measures, such as precision for arcs at different levels of the trees, ZPar shows characteristics surprisingly similar to MSTParser. ---------------------------------- **ANALYZING THE EFFECT OF GLOBAL LEARNING AND BEAM-SEARCH**",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_2",
  "x": "---------------------------------- **THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_3",
  "x": "In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_4",
  "x": "We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005 ) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_5",
  "x": "Figure 4 shows the precision and recall of each system for arcs of varying distances to the root. Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root. Although the vast majority of sentences have only one root-modifier, there is no global control for the number of root-modifiers in the greedy shift-reduce process, and each action is made locally and independently.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_6",
  "x": "Figure 4 shows the precision and recall of each system for arcs of varying distances to the root. Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root. Although the vast majority of sentences have only one root-modifier, there is no global control for the number of root-modifiers in the greedy shift-reduce process, and each action is made locally and independently.",
  "y": "background similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_0",
  "x": "However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006) . Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010) . However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V\u00e9ronis, 1998; Murray and Green, 2004; <cite>Erk et al., 2009</cite>; Passonneau et al., 2012b) .",
  "y": "background motivation"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_1",
  "x": "Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. This paper provides the following contributions. First, we demonstrate that the choice in annotation setup can significantly improve IAA and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_3",
  "x": "Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of<cite> Erk et al. (2009)</cite> which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012) . We propose a two-stage strategy that aims to reduce the complexity of the annotation task, referred to as Select and Rate (S+R). First, Turkers are presented with all the senses and asked to make a binary choice of which senses apply.",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_4",
  "x": "The Select task can potentially remove such noise and therefore improve both IAA and rating quality in the subsequent Rate task. Second, while the present study analyzes words with 4-8 senses, we are ultimately interested in annotating highly polysemous words with tens of senses, which could present a significant cognitive burden for an annotator to rate concurrently. Here, the Select stage can potentially reduce the number of senses presented, leading to less cognitive burden in the Rate stage. Furthermore, as a pragmatic benefit, removing inapplicable senses reduces the visual space required for displaying the questions on the MTurk platform, which can improve annotation throughput. MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word's senses and then asked to select (1) the sense option that best matches the mean-add.v ask.v win.v argument.n interest.n paper.n different.a important.a<cite> Erk et al. (2009)</cite> ing in the example context and (2) the sense option that least matches (Louviere, 1991) .",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_5",
  "x": "IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of<cite> Erk et al. (2009)</cite> , where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3.",
  "y": "uses"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_0",
  "x": "Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_1",
  "x": "More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
  "y": "differences"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_2",
  "x": "**SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_3",
  "x": "We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_4",
  "x": "For each token, we convert the distance to the two arguments of the example to two position embeddings. We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by Zeng et al. (2014) . The entity embedding (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i \u2208 [0, T ), T is the length of the sentence.",
  "y": "similarities"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_5",
  "x": "Then we obtain the high level summarization \u03c6(x) for the relation example. The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014; Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_6",
  "x": "It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) . It also has more data than ACE05, which we expect to be helpful in the multi-task learning.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_7",
  "x": "We tune \u03bb linearly from 0 to 1, and \u03b2 logarithmically from 5 \u00b7 10 \u22121 to 10 \u22124 For all scores, we run experiments 10 times and take the average. ---------------------------------- **AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> did, it could be further improved.",
  "y": "future_work"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_0",
  "x": "We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset<cite> (Hermann et al., 2015)</cite> , our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_1",
  "x": "**INTRODUCTION** Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_2",
  "x": "Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_3",
  "x": ". . \" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori. Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2).",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_4",
  "x": ". . \" can only understand \"Robert Downey Jr.\" as something that \"may be Iron Man\" at this stage, given that it does not know Robert Downey Jr. a priori. Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2).",
  "y": "motivation"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_5",
  "x": "Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as: in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>: in which a vector u(D, q) is learned to represent the status of a reader after reading a document and a query, and this vector is used to retrieve an answer by coupling with the answer vector v(a).",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_6",
  "x": "in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>: in which a vector u(D, q) is learned to represent the status of a reader after reading a document and a query, and this vector is used to retrieve an answer by coupling with the answer vector v(a). 1 Factorization (2) relies on the hypothesis that there exists a fixed vector for each candidate answer representing its meaning. However, as we argued in Section 1, an entity surface does not possess meaning; rather, it serves as an anchor to link pieces of information about it. Therefore, we hypothesize that the meaning representation v(e; D, q) of an entity e should be dynamically constructed from its surrounding contexts, and the meanings are \"accumulated\" through the reader reading the document D. We explain the construction of v(e; D, q) in Section 2.1, and propose a max-pooling process for modeling information accumulation in Section 2.2.",
  "y": "differences"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_7",
  "x": "This vector x c,\u03c4 draws information from preceding contexts, and is regarded as the meaning of the entity e that the reader understands so far, before reading the sentence c. It is used in place of a vector previously randomly initialized as a notion of e, in the construction of the new dynamic entity representation d e,c . ---------------------------------- **3 EVALUATION** We use the CNN-QA dataset<cite> (Hermann et al., 2015)</cite> for evaluating our model's ability to answer questions about named entities. The dataset consists of (D, q, e)-triples, where the document D is taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document (Figure 1) .",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_8",
  "x": "Finally, we note that our model, full DER Network, shows the best results compared to several previous reader models<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) , endorsing our approach as promising. The 99% confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [0.700, 0.740] and [0.708, 0.749], respectively (measured by bootstrap tests). Analysis In the example shown in Figure 4 , our basic model missed by paying little attention to the second and third sentences, probably because it does not mention @entity0 (Downey). In contrast, maxpooling of @entity2 (Iron Man) draws attention to the second and third sentences because Iron Man is said related to Downey in the first sentence. This helps Iron Man surpass @entity26 (Transformers), which is the name of a different movie series in which robots appear but Downey doesn't.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_0",
  "x": "---------------------------------- **INTRODUCTION** Task-oriented dialog systems, such as hotel booking or technical support service, help users to achieve specific goals with natural language. Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017) , end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018) , because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge- * Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; <cite>Madotto et al., 2018</cite>; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019) .",
  "y": "background"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_1",
  "x": "---------------------------------- **MODEL DESCRIPTION** The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_2",
  "x": "**MODEL DESCRIPTION** The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set. P T RE supervised information for copying words in dialog history P T RS = {ptrS,1, \u00b7 \u00b7 \u00b7 , ptrS,m}, KB pointer index set.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_3",
  "x": "---------------------------------- **WORKING MEMORY DECODER** Inspired by the studies on the working memory, we design our decoder as an attentional control system for dialog generation which consists of the working memory and two long-term memories. As shown in Figure 1 , we adopt the E-MemNN to memorize the dialog history X as described in Section 2.1, and then store KB tuples into the S-MemNN without TRANS(\u00b7). We also incorporate additional temporal information and speaker information into dialog utterances as <cite>(Madotto et al., 2018)</cite> and adopt a (subject, relation, object) representation of KB information as (Eric and Manning, 2017b) .",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_4",
  "x": "When querying S-MemNN, we consider the dialog history by using query q t = q E + q t and then obtain the copy distribution for KB entities P S\u00b7ptr . The two copy distributions are obtained by augmenting MemNNs with copy mechanism that is P E\u00b7ptr = p K E,t and P S\u00b7ptr = p K S,t . Now, three distributions, P vocab , P E\u00b7ptr and P S\u00b7ptr , are activated and moved into the STS, and then a proper word is generated from the activated distributions. We here use a rule-based word selection strategy by extending the sentinel idea in <cite>(Madotto et al., 2018)</cite> , which is shown in Figure 1 . If the expected word is not appearing either in the episodic memory or the semantic memory, the two copy pointers are trained to produce the sentinel token and our WMM2Seq generates the token from P vocab ; otherwise, the token is generated by copying from either the dialog history or KB tuples and this is done by comparing the two copy distributions.",
  "y": "extends"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_5",
  "x": "**RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 . We can see that our model does much better on the OOV situation and is on par with the best results on T5.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_6",
  "x": "The hyper-parameters for best models are given in the supplementary material. ---------------------------------- **RESULTS AND ANALYSIS** We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ).",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_7",
  "x": "This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) .",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_8",
  "x": "Ablation Study: To better understand the components used in our model, we report our ablation studies from three aspects. First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5.",
  "y": "extends differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_9",
  "x": "Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory. The separation of memories and stacking S-MemNN on E-MemNN also help a lot, because the whole external memory, consisting of the episodic and semantic memories, can be seen as a multi-hop (two-level) structure (the first level is the episode memory and the second level is the semantic memory). Attention Visualization: As an intuitive way to show the model's dynamics, attention weight visualization is also used to understand how the Central-EXE controls the access to the two long-term memories (E-MemNN and S-MemNN).",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_10",
  "x": "Firstly, our model generates a different but still correct response as the customer wants a moderately priced restaurant in the west and does not care about the type of food. Secondly, the generated response has tokens from the vocabulary (e.g. \"is\" and \"a\"), dialog history (e.g. \"west\" and \"food\") and KB information (e.g. \"saint johns chop house\" and \"british\"), indicating that our model learns to interact well with the two long-term memories by two sentinels. Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_11",
  "x": "Secondly, the generated response has tokens from the vocabulary (e.g. \"is\" and \"a\"), dialog history (e.g. \"west\" and \"food\") and KB information (e.g. \"saint johns chop house\" and \"british\"), indicating that our model learns to interact well with the two long-term memories by two sentinels. Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_12",
  "x": "As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material. ---------------------------------- **CONCLUSION** We leverage the knowledge from the psychological studies and propose our WMM2Seq for dialog response generation.",
  "y": "differences"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_0",
  "x": "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_1",
  "x": "**INTRODUCTION** BLEU (Papineni et al., 2002 ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_2",
  "x": "For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010) , respectively (Narayan and Gardent, 2016) . Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018) , in which the automatic metric used is BLEU. For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by<cite> Xu et al. (2016)</cite> , evaluating a variety of system outputs, as well as HSplit.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_3",
  "x": "tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b) . ---------------------------------- **BLEU IN TS.** While BLEU is standardly used for TS evaluation (e.g.,<cite> Xu et al., 2016</cite>; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017 ), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy.",
  "y": "motivation"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_4",
  "x": "**GOLD-STANDARD SPLITTING CORPUS** In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of<cite> Xu et al. (2016)</cite> . 3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_5",
  "x": "3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) . We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence's gram-maticality, fluency and meaning. The guidelines include two sentence splitting examples.",
  "y": "extends"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_6",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_7",
  "x": "Metrics. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_8",
  "x": "Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017) , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (Koehn et al., 2007) and SBMT-SARI<cite> (Xu et al., 2016)</cite> , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (Sulem et al., 2018b) . Human Evaluation.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_9",
  "x": "For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism. 9 We explore two settings. In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", \u00a74.3), we use HSplit as the reference set. Systems.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_10",
  "x": "The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (Sulem et al., 2018b) . Human Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of<cite> Xu et al. (2016)</cite> , and extend it to apply to HSplit as well. The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS).",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_11",
  "x": "12 The high scores obtained for Identity, also observed by<cite> Xu et al. (2016)</cite> , indicate that BLEU is a not a good predictor for relative simplicity to the input. The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited. For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores. They are described in the following paragraph. Correlation with Human Evaluation.",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_0",
  "x": "Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts. 2<cite> Wulczyn et al. (2017)</cite> estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions. News portals also suffer from abusive user comments, which damage their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions. They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments. 3 Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_1",
  "x": "1.6M manually moderated (accepted or rejected) user comments from a Greek sports news portal (called Gazzetta), which we make publicly available. 4 This is one of the largest publicly available datasets of moderated user comments. We also provide word embeddings pre-trained on 5.2M comments from the same portal. Furthermore, we experiment on the 'attacks' dataset of<cite> Wulczyn et al. (2017)</cite> , approx. 115K English Wikipedia talk page comments labeled as containing personal attacks or not.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_3",
  "x": "To get a more accurate view of performance in normal situtations, we manually re-moderated (labeled as 'accept' or 'reject') the comments of G-TEST-S, producing G-TEST-S-R. The reject ratio is approx. 30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R. Each G-TEST-S-R comment was re-moderated by five annotators. Krippendorff's (2004) alpha was 0.4762, close to the value (0.45) reported by<cite> Wulczyn et al. (2017)</cite> for the Wikipedia 'attacks' dataset. Using Cohen's Kappa (Cohen, 1960) , the mean pairwise agreement was 0.4749. The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_4",
  "x": "The Wikipedia 'attacks' dataset <cite>(Wulczyn et al., 2017)</cite> contains approx. 115K English Wikipedia talk page comments, which were labeled as containing personal attacks or not. Each comment was labeled by at least 10 annotators. Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff's (2004) alpha, was 0.45. The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject). Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels. 7 The dataset is split in three parts (Table 1) : training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_5",
  "x": [
   "Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not."
  ],
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_6",
  "x": "Experiments we reported elsewhere (Pavlopoulos et al., 2017) show that results on the 'attacks' and 'toxicity' datasets are very similar; we do not include results on the latter in this paper to save space. ---------------------------------- **METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_7",
  "x": "---------------------------------- **DETOX** DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_8",
  "x": "We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_9",
  "x": "Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of<cite> Wulczyn et al. (2017)</cite> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it. An API for Perspective is available at https://www.perspectiveapi. com/, but we did not have access to the API at the time the experiments of this paper were carried out.",
  "y": "differences"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_12",
  "x": "Table 2 : Comment classification results. Scores reported by<cite> Wulczyn et al. (2017)</cite> are shown in brackets. always better than CNN and DETOX; there is no clear winner between CNN and DETOX. Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to Table 2 . Also, da-CENT is always worse than a-RNN and RNN, confirming that the hidden states (intuitively, context-aware word embeddings) of the RNN chain are important, even with the attention mechanism.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_13",
  "x": "We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> .",
  "y": "background uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_14",
  "x": "We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10. Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_15",
  "x": [
   "Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx. 17K tweets annotated for hate speech. Their best results were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender."
  ],
  "y": "similarities uses"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_0",
  "x": "Nonetheless, the low accuracy of such formulas and their language dependency made way for more advanced and accurate readability assessment methods, which involve machine learning techniques. These models are highly accurate for their use of sophisticated NLP features and machine intelligence to associate the extracted features to a proper readability level. Models proposed by Vajjala and Meurers [17] , Xia et al. <cite>[18]</cite> , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience. These models are using Support Vector Machines trained on complex and proper feature sets extracted from related datasets. Still, their use of complicated and language-specific NLP features makes these models challenging to implement and heavily language-dependent.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_1",
  "x": "These features consist of different traditional, lexical, and syntactical features totaling a set of 46 distinct features. The second language text readability assessment can be considered as a subfield of text readability assessment. Despite the widespread use of English as a second language, this field has seen a few thorough studies. Different models are required to assess the readability of English texts for the second language readers as a different set of characteristics of text is influential on its readability level for second language readers [35] . Xia et al. <cite>[18]</cite> has published a thorough study on second language text readability assessment.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_2",
  "x": "To examine the proposed DRL model regarding this ability, it is applied to the Cambridge Exams dataset <cite>[18]</cite> . This dataset contains texts from the reading section of Cambridge English Exams, which is targeted for students at five readability levels (A2 to C2) of the Common European Framework of Reference (CEFR). This dataset contains 331 texts, which makes it a small dataset in comparison to the Weebit dataset. The automated feature extraction ability of the proposed model has also given the model the ability to be readily applied to other languages. As GloVe and language models are language-specific, the only necessary change is the use of the GloVe and frequency language model of the target language.",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_0",
  "x": "For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning.",
  "y": "background motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_1",
  "x": "For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] . Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_2",
  "x": "al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_3",
  "x": "Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text. We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_4",
  "x": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory. There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory.",
  "y": "extends similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_5",
  "x": "We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:**",
  "y": "extends similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_6",
  "x": "We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question. ---------------------------------- **INPUT ENCODER:** The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] :",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_7",
  "x": "The memory thus consists of D memory slots {m 1 , . . . , m D } (each is a vector of dimension K) and associated keys {k 1 , . . . , k D } (again vectors of dimension K). At time t, after reading the sentence t into a vector representation s t , a gating mechanism decides the set of memories to be updated (< \u00b7, \u00b7 > denotes inner product): Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_8",
  "x": "al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities. The update in (1) essentially does a soft selection of memory slots based on cosine distance in the embedding space. Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> . After selecting the set of memories, there is an update step which stores information in the corresponding memory slots: where PReLU is a parametric Rectified linear unit [21] , and U , V and W are k \u00d7 k parameter matrices.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_9",
  "x": "where g m i , g m j select the relational memory slot based on the active entity slots and the last sigmoid gate decides whether the corresponding relational memory needs to be updated based on the current input sentence. After selecting the set of active relational memory, we update the contents of the relational memory:r ij \u2190 P ReLU (Ar ij + Bs t ) r ij \u2190 r ij + g r ij \u2299r ij (4) where again A, B are k \u00d7 k parameter matrices. Note that for updates (3)- (4) we use a different encoding mask to obtain the sentence representation for relations. Similar to [<cite>18</cite>] , we normalize the memories after each update step (that is after reading each sentence). This acts as a forget step and does not cause the memory to explode.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_10",
  "x": "The full memory consists of the entity memory slots {h j } and the relational memory slots {r ij }. Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] . The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension. Then attention on these projected memories, conditioned on the vector q, yields the final answer:",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_11",
  "x": "**RELATED WORK** There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] . As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_12",
  "x": "Our model is also a memory network based model and is also related to the neural turing machine [26] . As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations. Other approaches to question answering use external knowledge, for instance external knowledge bases [27, 12, 28, 29, 10] or external text like Wikipedia [30, 31] . Very recently, and in parallel to this work, a method for relational reasoning called relation networks [32] was proposed. They demonstrated that simple neural network modules are not as effective at relational reasoning and their proposed module is similar to our model.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_14",
  "x": "Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16. The document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130. The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_15",
  "x": "The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_16",
  "x": "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ---------------------------------- **CONCLUSION** We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering.",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_17",
  "x": "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ---------------------------------- **CONCLUSION** We demonstrated an end-to-end trained neural network augmented with a structured memory representation which can reason about entities and relations for question answering.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_0",
  "x": "Such framework has been widely studied by Thrun (1996) ; Caruana (1997) ; Evgeniou & Pontil (2004) ; Ando & Zhang (2005) ; Argyriou et al. (2007) ; Kumar & III (2012) , among many others. In the context of deep neural networks, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015) , to vision (Donahue et al., 2014) , and speech (Heigold et al., 2013; Huang et al., 2013) . Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> .",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_1",
  "x": "Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages.",
  "y": "motivation"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_2",
  "x": "Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach -for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach -useful for multisource translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach -which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_3",
  "x": "Recently, Bahdanau et al. (2015) proposes an attention mechanism, a way to provide seq2seq models with a random access memory, to handle long input sequences. This is accomplished by setting s in Eq. (1) to be the set of encoder hidden states already computed. On the decoder side, at each time step, the attention mechanism will decide how much information to retrieve from that memory by learning where to focus, i.e., computing the alignment weights for all input positions. Recent work such as Jean et al., 2015a; Luong et al., 2015a; <cite>Vinyals et al., 2015a)</cite> has found that it is crucial to empower seq2seq models with the attention mechanism.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_5",
  "x": "For testing, to be comparable with existing results in (Luong et al., 2015a) For the unsupervised tasks, we use the English and German monolingual corpora from WMT'15. 4 Since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the same validation and test sets as the accompanied translation tasks. For constituency parsing, we experiment with two types of corpora: 1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by<cite> Vinyals et al. (2015a)</cite> . The two parsing tasks, however, are evaluated on the same validation (section 22) and test (section 23) sets from the PTB data.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_6",
  "x": "Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> . Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b) . ---------------------------------- **TRAINING DETAILS** In all experiments, following Sutskever et al. (2014) and Luong et al. (2015b) , we train deep LSTM models as follows: (a) we use 4 LSTM layers each of which has 1000-dimensional cells and embeddings, 5 (b) parameters are uniformly initialized in [-0.06, 0.06] , (c) we use a mini-batch size of 128, (d) dropout is applied with probability of 0.2 over vertical connections (Pham et al., 2014) , (e) we use SGD with a fixed learning rate of 0.7, (f) input sequences are reversed, and lastly, (g) we use a simple finetuning schedule -after x epochs, we halve the learning rate every y epochs.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_7",
  "x": "Since the parsing task maps from a sequence of English words to a sequence of parsing tags<cite> (Vinyals et al., 2015a)</cite> , only the encoder can be shared with an English\u2192German translation task. As a result, this is a one-to-many MTL scenario ( \u00a73.1). To our surprise, the results in Table 2 suggest that by adding a very small number of parsing minibatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we can improve the translation quality substantially. More concretely, our best multi-task model yields a gain of +1.5 BLEU points over the single-task baseline. It is worth pointing out that as shown in Table 2 , our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a) .",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_8",
  "x": "More concretely, our best multi-task model yields a gain of +1.5 BLEU points over the single-task baseline. It is worth pointing out that as shown in Table 2 , our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a) . Larger mixing coefficients, however, overfit the small PTB corpus; hence, achieve smaller gains in translation quality. For parsing, as<cite> Vinyals et al. (2015a)</cite> have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneficial for parsing, yielding an improvement of up to +8.9 F 1 points over the baseline.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_9",
  "x": "The average results of 2 runs are in mean (stddev) format. data. Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-confidence (HC) corpus, which is provided by<cite> Vinyals et al. (2015a)</cite> . As highlighted in Table 4 , the trend is consistent; MTL helps boost translation quality by up to +0.9 BLEU points. Table 4 : English\u2192German WMT'14 translation -shown are perplexities (ppl) and BLEU scores of various translation models.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_10",
  "x": "Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_11",
  "x": "We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 . Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_12",
  "x": "Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data. 7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_13",
  "x": "At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score. ---------------------------------- **MULTI-TASKS AND UNSUPERVISED LEARNING** Our main focus in this section is to determine whether unsupervised learning can help improve translation.",
  "y": "similarities"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_0",
  "x": "Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data.",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_1",
  "x": "SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification. The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_2",
  "x": "In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification.",
  "y": "motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_3",
  "x": "More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res.",
  "y": "background motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_4",
  "x": "Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ).",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_5",
  "x": "These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between a word (the headword of the argument to be classified) and a set of words (the headwords of arguments of a given role). WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible. Distributional similarity: Following (<cite>Zapirain et al., 2009</cite>) we considered both first order and second order similarity.",
  "y": "uses"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_6",
  "x": "When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP).",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_7",
  "x": "When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP).",
  "y": "extends differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_0",
  "x": "<cite>Dyer et al. (2013)</cite> present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4. While this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power. For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely. However, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences. For example, when aligning to Dutch, it may be common for one verb to be aligned near the end of the sentence that would be at the beginning in English.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_1",
  "x": "Furthermore, it is common in word alignment to take word classes into account. This is commonly implemented for the HMM alignment model as well as Models 4 and 5. Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is. However, using the reparameterization in<cite> (Dyer et al., 2013)</cite> would leave the model simple enough even with a relatively large amount of word classes.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_2",
  "x": "**METHODS** We make use of a modified version of Model 2, from <cite>Dyer et al. (2013)</cite> , which has an alignment model that is parameterised in its original form solely on the variable \u03bb. Specifically, the probability of a sentence e given a sentence f is given as: here, m is the length of the target sentence e, n the same for source sentence f , \u03b4 is the alignment model and \u03b8 is the translation model. In this paper we are mainly concerned with the alignment model \u03b4.",
  "y": "extends"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_3",
  "x": "Further, the offset is denoted as \u03c9. we change the definition of h(\u00b7) to the following instead: + \u03c9 otherwise j \u2193 is the point closest to or on the diagonal here, calculated as: Here, \u03c9 can range from \u22121 to 1, and thus the calculation for the diagonal j \u2193 is clamped to be in a valid range for alignments. As the partition function (Z(\u00b7)) used in<cite> (Dyer et al., 2013)</cite> consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute \u03b3 for the geometric series calculations in order to use different parameters for each:",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_4",
  "x": "For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in<cite> (Dyer et al., 2013)</cite> . Conditioning on the POS of the target words then becomes as simple as using a different \u03bb, \u03b3, and \u03c9 for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag. A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_5",
  "x": "---------------------------------- **EMPIRICAL RESULTS** The above described model is evaluated with experiments on a set of 3 language pairs, on which AER scores and BLEU scores are computed. We use similar corpora as used in<cite> (Dyer et al., 2013)</cite> : a French-English corpus made up of Europarl version 7 and news-commentary corpora, the ArabicEnglish parallel data consisting of the non-UN portions of the NIST training corpora, and the FBIS Chinese-English corpora. The models that are compared are the original reparameterization of Model 2, a version where \u03bb is split around the diagonal (split), one where pos tags are used, but \u03bb is not split around the diagonal (pos), one where an offset is used, but parameters aren't split about the diagonal (offset), one that's split about the diagonal and uses pos tags used as in<cite> (Dyer et al., 2013)</cite> , with stepsize for updates to \u03bb and \u03b3 during gradient ascent is 1000, and that for \u03c9 is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration.",
  "y": "similarities"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_0",
  "x": "---------------------------------- **DETERMINING THE AFFECTIVE VALENCE OF WORDS FROM SMALL CORPORA** To my knowledge, the first researchers to propose an automatic procedure to determine the valence of words on the basis of corpora are Hatzivassiloglou and McKeown (1997) . Their algorithm aims to infer the semantic orientation of adjectives on the basis of an analysis of their co-occurrences with conjunctions. The main limitation of their algorithm is that it was developed specifically for adjectives and that the question of its application to other grammatical categories has not been solved <cite>(Turney & Littman, 2003)</cite> .",
  "y": "motivation"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_1",
  "x": "Two techniques that fulfil this condition are described below. ---------------------------------- **SO-LSA** The technique proposed by<cite> Turney and Littman (2003)</cite> tries to infer semantic orientation from semantic association in a corpus. It is based on the semantic proximity between a target word and fourteen benchmarks: seven with positive valence and seven with negative valence (see Table 1 ).",
  "y": "background"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_3",
  "x": "The two techniques described above were used in this experiment. The fourteen SO-LSA benchmarks chosen by<cite> Turney and Littman (2003)</cite> were translated into 2 Each sentence was automatically modified so as to replace the name and the description of the function of every individual by a generic first name of adequate sex (Mary, John, etc.) in order to prevent the judges being influenced by their prior positive or negative opinion about these people. French (bon, gentil, excellent, positif, heureux, correct et sup\u00e9rieur: mauvais, m\u00e9chant, m\u00e9diocre, n\u00e9gatif, malheureux, faux et inf\u00e9rieur) . For DI-LSA, a French lexicon made up of 3000 words evaluated on the pleasant-unpleasant scale was used (Hogenraad et al., 1995) . A minimum of thirty judges rated the words on a seven-point scale from 'very unpleasant' (1) Table 3 : Emotional valences of several sentences.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_0",
  "x": "**ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_1",
  "x": "**ABSTRACT** LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_2",
  "x": "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_3",
  "x": "Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_4",
  "x": "Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.",
  "y": "motivation"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_5",
  "x": "To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique. This investigation aimed at understanding how sensitive the WSD approach is w.r.t. the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets. The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community.",
  "y": "extends"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_6",
  "x": "The contribution of this paper is thus two-fold: On the one hand, we present a reproduction study whose results are publicly available and hence can be freely used by the community. Notice that the lack of available models has been explicitly mentioned, in a recent work, as the cause for the missing comparison of this technique with other competitors (Raganato et al., 2017b, footnote 10) . On the other hand, we present other experiments to shed more light on the value of this and similar methods. We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_7",
  "x": "First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings. Finally, we show that the limited sense coverage in the annotated datasets is a major limitation, as shown by the fact that resulting model does not have a representation for more than 30% of the meanings which should have been considered for disambiguating the test sets.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_8",
  "x": "The work by <cite>Yuan et al. (2016)</cite> , which we consider in this paper, belongs to this last category. Different from Melamud et al. (2016) , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated. We describe this approach in more detail in the following section. ---------------------------------- **WSD WITH LANGUAGE MODELS**",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_9",
  "x": "Different from Melamud et al. (2016) , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated. We describe this approach in more detail in the following section. ---------------------------------- **WSD WITH LANGUAGE MODELS** The method proposed by <cite>Yuan et al. (2016)</cite> performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_10",
  "x": "Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 \u2264 k \u2264 n) by a special token $. The model takes this new sentence as input and produces a context vector c of dimensionality p (see Figure 1 ).",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_11",
  "x": ". , s n computed in the previous step. 3. The procedure invokes a subroutine to choose one of the n senses for the context vector c t . It selects the sense whose vector is closest to c t using cosine as the similarity function. Label Propagation. <cite>Yuan et al. (2016)</cite> argue that the averaging procedure is suboptimal because of two reasons.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_12",
  "x": "For the training of the sense embeddings, we use the same two corpora used by <cite>Yuan et al. (2016)</cite>: 1. SemCor (Miller et al., 1993 ) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (Francis and Kucera, 1979) and cover various genres. 2. OMSTI (Taghipour and Ng, 2015) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (Eisele and Chen, 2010) . A list of English translations were manually created for each WordNet sense. If the Chinese translation of an English word matches one of the manually curated translations for a WordNet sense, that sense is selected.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_13",
  "x": "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by <cite>Yuan et al. (2016)</cite> while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_14",
  "x": "In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_15",
  "x": "**RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_16",
  "x": "Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared. However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how <cite>Yuan et al. (2016)</cite> handled these cases. In the WSD evaluation framework (Moro et al., 2014 ) that we selected for evaluation, these cases were either re-annotated or removed.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_17",
  "x": "We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_19",
  "x": "n represents the number of considered instances. al., 2016; Melamud et al., 2016) . However, the gap with the graph-based approach of Weissenborn et al. (2015) is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation).",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_20",
  "x": "When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets. Most-vs. less-frequent-sense instances.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_21",
  "x": "Table 2 shows that the method by <cite>Yuan et al. (2016)</cite> does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them). On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI. For comparison, the default system IMS (Zhong and Ng, 2010) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (Postma et al., 2016) and only reaches 0.33 with a large amount of annotated data. Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS. When we compare the results of the averaging technique using SemCor and OMSTI versus when we use label propagation, we notice an increase in the MFS recall (from 0.85 to 0.91), whereas the LFS recall drops from 0.40 to 0.32.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_23",
  "x": "Figure 2a shows the effect of unannotated data volume on WSD performance. The data points at 100 billion (10 11 ) tokens correspond to <cite>Yuan et al. (2016)</cite> 's reported results. As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD. However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale). Extrapolating from this graph, to get a performance of 0.8 F 1 by adding more unannotated data, one would need a corpus of 10 12 tokens.",
  "y": "similarities uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_24",
  "x": "---------------------------------- **CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_25",
  "x": "This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_0",
  "x": "Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model. We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present. Furthermore, this formulation precludes results dependent on a random document generation procedure. In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style. We demonstrate a general technique that can accurately discern multiple authors contained within the Books of Ezekiel and Jeremiah.",
  "y": "background"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_1",
  "x": "Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model. We consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text (a 'document'), for which it can be reliably asserted that only a single author is present. Furthermore, this formulation precludes results dependent on a random document generation procedure. In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style. We demonstrate a general technique that can accurately discern multiple authors contained within the Books of Ezekiel and Jeremiah.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_2",
  "x": "Both Zheng et al. (2006) and Layton et al. (2013) propose that syntactic feature sets are reliable predictors for authorial attribution, and Tschuggnall and Specht (2014) demonstrates, with modest success, authorial decomposition using pq-grams extracted from sentences' syntax trees. We found that by combining the feature set of POS n-grams with a clustering approach similar to the one presented by Akiva (2013) , our method of decomposition attains higher accuracy than Tschuggnall's method, which also considers grammatical style. Additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by Akiva (2013) and <cite>Aldebei (2015)</cite> , which both rely on word occurrences as features. This paper is organized as follows: section 2 outlines our proposed framework, section 3 clari-fies our method in detail through an example, section 4 contains results, section 5 tests an explanation of our results, and section 6 concludes our findings and discusses future work. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_3",
  "x": "The NYT cor- pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> . The corpus is comprised of texts from four columnists: Gail Collins (274 documents), Maureen Dowd (298 documents), Thomas Friedman (279 documents), and Paul Krugman (331 documents). Each document is approximately the same length and the columnists discuss a variety of topics. Here we consider the binary (k = 2) case of clustering the set of 629 Dowd and Krugman documents into two groups. In step one, the documents are converted into their 'POS-translated' form as previously outlined.",
  "y": "similarities"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_4",
  "x": "Indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> -by replacing POS n-grams with ordinary word occurrences in step one -our framework performed very well, clustering at 95.3%. Similarly, our framework performed poorly on the Books of Ezekiel and Jeremiah from the Hebrew Bible. Using the English-translated King James Version, and considering each chapter as an individual document, our framework clusters the 48 chapters of Ezekiel and the 52 chapters of Jeremiah at 54.7%. Aldebei et al. (2015) reports 98.0% on this dataset, and when considering the original English text instead of the POS-translated text, our framework achieves 99.0%. The simultaneous success of word features and failure of POS features on these two datasets seemed to completely contradict our previous results.",
  "y": "similarities"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_0",
  "x": "<cite>[9]</cite> in <cite>their work</cite> pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language. Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from [10] data set demonstrating online abuse They find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive. [4] introduced a large, hand-coded corpus of online harassment data for studying the nature of harassing comments and the culture of trolling. Keeping these motivations in mind, we make the following salient contributions: \u2022 We build a deep context-aware attention-based model for abusive behavior detection on Twitter .",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_1",
  "x": "Existing approaches to abusive text detection can be broadly divided into two categories: 1) Feature intensive machine learning algorithms such as Logistic Regression (LR), Multilayer Perceptron (MLP) and etc. 2) Deep Learning models which learn feature representations on their own. [10] released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus. <cite>[9]</cite> use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech. [2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by [10] . Their best-reported F1-score is achieved using Long Short Term Memory Networks (LSTM) + Gradient Boosting.",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_3",
  "x": "---------------------------------- **DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither.",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_4",
  "x": "We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter.",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_5",
  "x": "**RESULTS** The network is trained at a learning rate of 0.001 for 10 epochs, with a dropout of 0.2 to prevent over-fitting. The results are averaged over 10-fold cross-validations for D1 and D3 and 5 fold cross-validations for <cite>D2</cite> because <cite>[9]</cite> reported results using 5 fold CV. Because of class imbalance in all our data sets, we report weighted F1 scores. Table 3 shows our results in detail.",
  "y": "motivation"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_6",
  "x": "On closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle. The first tweet is a tweet from [10] , the second tweet is a tweet from from <cite>[9]</cite> data set and the third from the [4] ---------------------------------- **WHY CONTEXTUAL ATTENTION?** Attention mechanism enables our neural network to focus on the relevant parts of the input more than the irrelevant parts while performing a prediction task.",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_0",
  "x": "****STRUCTURAL CORRESPONDENCE LEARNING FOR PARSE DISAMBIGUATION**** **ABSTRACT** The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_1",
  "x": "The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_2",
  "x": "The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain (Gildea, 2001; Daum\u00e9 III, 2007) , we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001 ). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III and Marcu, 2006; Daum\u00e9 III, 2007;<cite> Blitzer et al., 2006</cite>; McClosky et al., 2006; .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_3",
  "x": "In contrast, semi-supervised domain adaptation<cite> (Blitzer et al., 2006</cite>; McClosky et al., 2006; is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult. Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are \"surprisingly difficult to beat\" (Daum\u00e9 III, 2007) . Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daum\u00e9 III, 2007; Plank and van Noord, 2008) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_5",
  "x": "While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006;<cite> Blitzer et al., 2006</cite>; . Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification.",
  "y": "background motivation"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_6",
  "x": "Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\". In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007 ). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets and a bug in their system (Shimizu and Nakagawa, 2007) , their results are inconclusive.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_7",
  "x": "This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006) .",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_8",
  "x": "---------------------------------- **STRUCTURAL CORRESPONDENCE LEARNING** ---------------------------------- **SCL** (Structural Correspondence Learning)<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_9",
  "x": "Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as \"boring\" and \"repetitive\" are common ways to express negative sentiment in A, while \"not working\" or \"defective\" are specific to B. If there are features across the domains, e.g. \"don't buy\", with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_10",
  "x": "Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available)<cite> (Blitzer et al., 2006)</cite> . The outline of the algorithm is given in Figure 1 . The first step is to identify m pivot features occurring frequently in the unlabeled data of both 4. Apply SVD to W : :h,:] are the h top left singular vectors of W .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_11",
  "x": "Applying the projection W T x (where x is a training instance) would give us m new features, however, for \"both computational and statistical reasons\"<cite> (Blitzer et al., 2006</cite>; Ando and Zhang, 2005 ) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let \u03b8 = U T h\u00d7n be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features). The resulting \u03b8 is a projection onto a lower dimensional space R h , parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x, \u03b8x . In more detail, the original feature space x is augmented with h new features obtained by applying the projection \u03b8x.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_12",
  "x": "A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input. So far, pivot features on the word level were used<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) , e.g. \"Does the bigram not buy occur in this document?\" (Blitzer, 2008) . Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand. For the task of parse disambiguation based on a conditional model this is not the case.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_13",
  "x": "We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold. In all our experiments, we set t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes).",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_14",
  "x": "In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W , we disregard all negative entries in W and compute the SVD (W = U DV T ) on the resulting non-negative sparse matrix. This sparse representation saves both time and space.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_15",
  "x": "**MATRIX AND SVD** ---------------------------------- **FURTHER PRACTICAL ISSUES OF SCL** In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006;<cite> Blitzer et al., 2006</cite>; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_16",
  "x": "They then rescaled the features by a factor \u03b1 found on heldout data: \u03b1\u03b8x. Restricted Regularization. When training the supervised model on the augmented feature space x, \u03b8x ,<cite> Blitzer et al. (2006)</cite> only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008) . Dimensionality reduction by feature type.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_17",
  "x": "Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ---------------------------------- **EXPERIMENTS AND RESULTS** ---------------------------------- **EXPERIMENTAL DESIGN**",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_18",
  "x": "<wikipage id=\"6677\"> <cat t=\"direct\" n=\"Categorie:Paus\"/> <cat t=\"direct\" n=\"Categorie:Pools_theoloog\"/> <cat t=\"super\" n=\"Categorie:Religieus leider\"/> <cat t=\"super\" n=\"Categorie:Rooms-katholiek persoon\"/> <cat t=\"super\" n=\"Categorie:Vaticaanstad\"/> <cat t=\"super\" n=\"Categorie:Bisschop\"/> <cat t=\"super\" n=\"Categorie:Kerkgeschiedenis\"/> <cat t=\"sub\" n=\"Categorie:Tegenpaus\"/> <cat t=\"super\" n=\"Categorie:Pools persoon\"/> </wikipage> To create the set of related pages for a given article p, we proceed as follows: 1. Find sub-and supercategories of p 2. Extract all pages that are related to p (through sharing a direct, sub or super category) 3. Optionally, filter out certain pages In our empirical setup, we followed<cite> Blitzer et al. (2006)</cite> and tried to balance the size of source and target data.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_19",
  "x": "**SCL RESULTS** The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in \u03c6 measure (absolute error reduction). This corresponds to a relative error reduction of up to 7.29%. Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4) , which is in line with previous findings (Ando and Zhang, 2005;<cite> Blitzer et al., 2006)</cite> .",
  "y": "similarities"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_20",
  "x": "We also tested feature normalization (as described in Section 4.2). While<cite> Blitzer et al. (2006)</cite> found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. A look at \u03b8 To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of \u03b8. Recall that applying a row of the projection matrix \u03b8 i to a training instance x gives us a new real-valued feature.",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_21",
  "x": "The paper presents an application of Structural Correspondence Learning (SCL) to parse disam- Figure 5 : Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed. biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> . We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation.",
  "y": "background motivation"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_22",
  "x": "While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> . We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation. On the three examined datasets, SCL slightly but constantly outperformed the baseline. Applying SCL involves many design choices and practical issues, which we tried to depict here in detail.",
  "y": "differences"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_0",
  "x": "Relation extraction can also benefit from incorporating paraphrase generation into its processing pipeline (Romano et al., 2006) . Manually annotating translation references is expensive, and automatically generating references through paraphrasing has been shown to be effective for evaluation of machine translation (Zhou et al., 2006; Kauchak and Barzilay, 2006) . Datasets used for paraphrase generation include QUORA 1 , TWITTER (Lan et al., 2017) and MSCOCO (Lin et al., 2014) . Previous work on paraphrase generation that used these datasets (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics. In this paper, we find that simply using the input sentence as output in an unsupervised manner (i.e. fully parroting the input) significantly outperforms the state-of-the-art on two metrics for TWITTER, and on one metric for QUORA.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_1",
  "x": "After processing the dataset, there are 149,650 unique sentences that have reference paraphrases. <cite>Gupta et al. (2018)</cite> sampled 4K sentences as their test set, but did not specify which sentences they used. Li et al.(2018) sampled 30K sentences as their test set, also not specifying which sentences they used. To avoid selecting a subset of data that is biased in favor of our method, we perform evaluation on the entire QUORA dataset. Although we evaluate on the entire dataset, the size of our training set is zero due to the fully unsupervised nature of full and partial parroting.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_2",
  "x": "As with QUORA, prior paraphrase generation work on this dataset (Li et al., 2018) did not provide their sampled test set sentences, so we evaluate parroting on the entire dataset to avoid bias. We follow the same data processing steps as QUORA, and plot the number of reference paraphrases in Appendix A. MSCOCO. This is an image captioning dataset, with multiple captions provided for a single image (Lin et al., 2014) . There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) .",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_3",
  "x": "However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed. This makes the use of MSCOCO as a paraphrase generation dataset questionable. We plot the number of reference paraphrases in Appendix A. ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_4",
  "x": "Furthermore, the score deviation between different samples is small. Consequently, although the exact test sets used by<cite> (Gupta et al., 2018)</cite> and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets. Partial parroting. We also introduce lexical variation into our parroting method by replacing or cutting words of the input sentence. For replacement, we substitute input words with an outof-vocabulary word not found in any of the input sentence's reference paraphrases.",
  "y": "future_work"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_0",
  "x": "**INTRODUCTION** State-of-the-art approaches to extractive summarization are based on the notion of coverage maximization (Berg-Kirkpatrick et al., 2011) . The assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i , yielding the following coverage maximization objective, subject to the appropriate constraints: In proposing bigrams as concepts for their system,<cite> Gillick and Favre (2009)</cite> explain that:",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_1",
  "x": "[c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example. While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these. In this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_2",
  "x": "Moreover, due the NP-hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints. Bigrams.<cite> Gillick and Favre (2009)</cite> proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) by the frequency with which they occur in the document. Some pre-processing is first carried out to these bigrams: all bigrams consisting uniquely of stop-words are removed from consideration, and each word is stemmed. They also require bigrams to occur with a minimal frequency (cf. Section 3.2). Named entities.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_4",
  "x": "Our baseline is the bigram-based extraction summarization system of<cite> Gillick and Favre (2009)</cite> , icsisumm 7 . Their system was originally intended for multi-document update summarization, and summaries are extracted from document sentences that share more than k content words with some query. We follow this approach for the TAC08 data. For ECHR and WIKIPEDIA, the task is single document summarization, and the now irrelevant topic-document intersection preprocessing step is eliminated. The original system uses the GNU linear programming kit 8 with a time limit of 100 seconds.",
  "y": "uses"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_5",
  "x": "2. Concept count cut-off is the minimum frequency of concepts from the document (set) that qualifies them for consideration in coverage maximization. For bigrams of the original system on TAC08, there are two types of document sets: 'A' and 'B'. For 'A' type documents,<cite> Gillick and Favre (2009)</cite> set this threshold to 3 and for 'B' type documents, they set this to 4. For WIKIPEDIA and ECHR, we take the bigram threshold to be 4.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_6",
  "x": "We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in<cite> (Gillick and Favre, 2009</cite> ) (see Table 1 , BIGRAMS): 0.081 compared to 0.110 respectively. On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here. Hence,<cite> Gillick and Favre (2009)</cite> were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset. However, when we change domain to the legal judgments or Wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the ROUGE metrics. For ECHR, replacing bigrams by frame names (FRAME) results in an increase of +0.1 in ROUGE-1, +0.031 in ROUGE-2 and +0.046 in ROUGE-SU4.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_0",
  "x": "The purpose of cross-genre experiment is to see whether the model can work robustly across genres. Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system<cite> (Friedrich et al., 2016)</cite> and the baseline model by a large margin. As shown in Table 5, benefited from modeling wider contexts and common SE label patterns, our full paragraphlevel model improves performance across almost all the genres. The high performance in the crossgenre setting demonstrates the robustness of our paragraph-level model across genres.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_1",
  "x": "---------------------------------- **INTRODUCTION** Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_2",
  "x": "Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_3",
  "x": "We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_4",
  "x": "Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions. For example, we distinguish GENERIC statements from GENER-ALIZING statements depending on if a clause expresses general information over classes or kinds instead of specific individuals.",
  "y": "background motivation"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_5",
  "x": "In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_6",
  "x": "In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_7",
  "x": "---------------------------------- **RELATED WORK** ---------------------------------- **LINGUISTIC CATEGORIES OF SE TYPES** The situation entity types annotated in the MASC+Wiki corpus<cite> (Friedrich et al., 2016)</cite> were initially introduced by Smith (2003) , which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b) .",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_8",
  "x": "To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) .",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_9",
  "x": "Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_10",
  "x": "To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause. The feature sets include POS tags, Brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause. <cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) .",
  "y": "background differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_11",
  "x": "Situation Entity Type Classification: Finally, the prediction layer will predict the situation entity type for each clause by applying the softmax function to its clause representation: ---------------------------------- **FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_12",
  "x": "---------------------------------- **FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph. Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_13",
  "x": "**DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses. In figure 4 , the horizontal axis shows the distribution of paragraphs based on the number of clauses in a paragraph.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_14",
  "x": "---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_17",
  "x": "**EXPERIMENTAL RESULTS** Following the previous work<cite> (Friedrich et al., 2016)</cite> on the same task and dataset, we report accuracy and macro-average F1-score across SE types on the test set of MASC+Wiki. The first section of Table 3 shows the results of the previous works. The second section shows the result of our implemented clause-level Bi-LSTM baseline, which already outperforms the previous best model. This result proves the effectiveness of the Bi-LSTM + max pooling approach in clause representation learning (Conneau et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_18",
  "x": "---------------------------------- **10-FOLD CROSS-VALIDATION** We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_19",
  "x": "We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results. Consistently, our clause-level baseline model already outperforms the previous best model. By exploiting paragraph-wide contexts, the basic paragraph-level model obtains consistent performance improvements across all the classes compared with the baseline clause-level prediction model, especially for the classes GENERIC and GENERALIZING, where the improvements are significant.",
  "y": "uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_0",
  "x": "Importantly, since our article is about an American guitarist, we would explicitly want to start with the English (original) version of the name, and treat other languages as extra data, rather than vice versa. In order to effectively incorporate the otherlanguage data, we apply SVM re-ranking in a manner that has previously been shown to provide significant improvement for grapheme-to-phoneme conversion (Bhargava and Kondrak, 2011) . This method is flexible enough to incorporate multiple languages; it employs features based on character alignments between potential outputs and existing transliterations from other languages, as well as scores of these alignments, which serve as a measure of similarity. We apply this approach on top of the same DIRECTL+ system as submitted last year <cite>(Jiampojamarn et al., 2010b)</cite> for English-to-Hindi machine transliteration. Compared to the base DI-RECTL+ performance, we are able to achieve significantly better results, with a relative performance increase of over 10%.",
  "y": "background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_1",
  "x": "**BASE SYSTEMS** Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration <cite>(Jiampojamarn et al., 2010b)</cite> . For re-ranking, note that training a re-ranker requires training data where the base system scores are representative of unseen data so that the re-ranker does not simply learn to follow the base system; we therefore split the training data into ten folds and perform a sort-of cross validation with DIRECTL+. This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> .",
  "y": "background uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_2",
  "x": "This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system. We also repeat the cross-validation process for training the re-ranker. We also apply the SVM re-ranking approach to system combination.",
  "y": "extends"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_4",
  "x": "Finally, we observe very good overall accuracy in the English-to-Japanese results (which also only use base DIRECTL+), which further confirm the effectiveness of DIRECTL+ when applied to machine transliteration. ---------------------------------- **PREVIOUS WORK** There are three lines of research that are relevant to the work we have presented in this paper: (1) DI-RECTL+ and SEQUITUR for machine transliteration; (2) applying multiple languages; and (3) system combination. For the NEWS 2009 and 2010 Shared Tasks, the discriminative DIRECTL+ system that incorporates many-to-many alignments, online maxmargin training and a phrasal decoder was shown to function well as a general string transduction tool; while originally designed for grapheme-tophoneme conversion, it produced excellent results for machine transliteration (Jiampojamarn et al., 2009;<cite> Jiampojamarn et al., 2010b)</cite> , leading us to re-use it here.",
  "y": "background uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_0",
  "x": "**RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) .",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_1",
  "x": "Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) .",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_2",
  "x": "Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009 ) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) . The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_3",
  "x": "**MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction.",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_4",
  "x": "**MWE.** The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_5",
  "x": "However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ---------------------------------- **METHODOLOGY** Our basic method relies on analysis of lexical overlap between the component words and the definitions of the MWE in Wiktionary, in the manner of Lesk (1986) .",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_6",
  "x": "A third information source in Wiktionary that can be used to predict compositionality is sense-level translation data. Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_7",
  "x": "Due to the user-generated nature of Wiktionary, the set of languages for which 1 Although the recall of these tags is low (Muzny and Zettlemoyer, 2013 translations are provided varies greatly across lexical entries. Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation. Unlike the definition and synonym-based approach, the translation-based approach will produce real rather than binary values.",
  "y": "differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_8",
  "x": "**DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE. In our case, the task setup is a binary classification task relative to each of the two components of the MWE. The ENC dataset was originally constructed by Reddy et al. (2011) , and annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun.",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_9",
  "x": "**DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE. In our case, the task setup is a binary classification task relative to each of the two components of the MWE. The ENC dataset was originally constructed by Reddy et al. (2011) , and annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun.",
  "y": "differences similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_10",
  "x": "Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are compositional (e.g. from the Wiktionary guidelines for idiom inclusion, 3 computer chess, basketball player, telephone box). We also compare our method with: (1) \"LCS\", the string similarity-based method of<cite> Salehi and Cook (2013)</cite> , in which 54 languages are used; (2) \"DS\", the monolingual distributional similarity method of Salehi et al. (2014) ; (3) \"DS+DSL2\", the multilingual distributional similarity method of Salehi et al. (2014) , including supervised language selection for a given dataset, based on crossvalidation; and (4) \"LCS+DS+DSL2\", whereby the first three methods are combined using a supervised support vector regression model. In each case, the continuous output of the model is equal-width discretised to generate a binary classification. We additionally present results for the combination of each of the six methods proposed in this paper with LCS, DS and DSL2, using a linear-kernel support vector machine (represented with the suffix \" COMB(LCS+DS+DSL2) \" for a given method). The results are based on cross-3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD validation, and for direct comparability, the partitions are exactly the same as Salehi et al. (2014) .",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_11",
  "x": "Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods. It is important to reinforce that the proposed methods make no language-specific assumptions and are therefore applicable to any type of MWE and any language, with the only requirement being that the MWE of interest be listed in the Wiktionary for ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_12",
  "x": "The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods. For reasons of space, results without translation data are therefore omitted from the paper. Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_0",
  "x": "We have tested four learning algorithms: Naive Bayes (NB), C4.5, PART and k-nearest neighbor (kNN), all implemented in the Weka package (Witten and Frank, 1999) . The version of Weka used in this work is Weka 3.0.1. The algorithms used can be biased to prefer the mistake of classify a UCE message as not UCE to the opposite, assigning a penalty to the second kind of errors. Following<cite> (Androutsopoulos et al., 2000)</cite> , we have assigned 9 and 999 (9 and 999 times more important) penalties to the missclassification of legitimate messages as UCE. This means that every instance of a legitimate message has been replaced by 9 and 999 instances of the same message respectively for NB, C4.5 and PART.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_1",
  "x": "The kNN algorithm was tested with values of k equal to 1, 2, 5 and 8, being 5 the optimal number of neighbors. We present the weighted accuracy (wacc), and also the recall (rec) and precision (pre) for the class UCE. Weighted accuracy is a measure that weights higher the hits and misses for the preferred class. Recall and precision for the UCE class show how effective the filter is blocking UCE, and what is its effectiveness letting legitimate messages pass the filter, respectively<cite> (Androutsopoulos et al., 2000)</cite> . In Table 1 , no costs were used.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_2",
  "x": "This is probably due to the sampling method, which only slightly affects to the estimation of probabilities (done by approximation to a normal distribution). In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words. The improvement shown in our experiments is modest, due to the heuristics used.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_3",
  "x": "With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words. The improvement shown in our experiments is modest, due to the heuristics used. We are not able to add other heuristics in this case because the Spambase collection comes in a preprocessed fashion. For future experiments, we will use the collection from<cite> (Androutsopoulos et al., 2000)</cite> , which is in raw form. This fact will enable us to search for more powerful heuristics.",
  "y": "future_work"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_0",
  "x": "**INTRODUCTION** Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data<cite> (Bolukbasi et al., 2016</cite>; Caliskan et al., 2017) . In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways.",
  "y": "background motivation"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_1",
  "x": "For word representations,<cite> Bolukbasi et al. (2016)</cite> and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task.",
  "y": "background motivation"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_2",
  "x": "In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.",
  "y": "background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_3",
  "x": "Next, we analyze the gender subspace in ELMo. We first sample 400 sentences with at least one gendered word (e.g., he or she from the OntoNotes 5.0 dataset (Weischedel et al., 2012) and generate the corresponding gender-swapped variants (changing he to she and vice-versa). We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one<cite> (Bolukbasi et al., 2016)</cite> . The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender).",
  "y": "differences"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_4",
  "x": "Data augmentation is performed by replacing gender revealing entities in the OntoNotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data. In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from<cite> Bolukbasi et al. (2016)</cite> . We evaluate the performance of both aspects of this approach. ---------------------------------- **DATA AUGMENTATION**",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_0",
  "x": "We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier <cite>(Socher et al., 2013)</cite> to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets. In particular, we address the following research questions: 1. How does off-the-shelf MT on Arabic social data influence SA performance? 2. Can MT-based approaches be a viable alternative to improve sentiment classification performance on Arabic tweets? 3.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_1",
  "x": "Hence, they conclude that it is possible to obtain high quality training data using MT, which is an encouraging result to motivate our approach. Wan (2009) proposes a co-training approach to tackle the lack of Chinese sentiment corpora by employing Google Translate as publicly available machine translation (MT) service to translate a set of annotated English reviews into Chinese. Using a held-out test set, the best reported accuracy score was at 81.3% with SVM on binary classification task: positive vs negative. Our approach differs from the ones described, in that we use automatic MT to translate Arabic tweets into English and then perform SA using a stateof-the-art SA classifier for English <cite>(Socher et al., 2013)</cite> . Most importantly, we empirically benchmark its performance towards previous SA approaches, including lexicon-based, fully supervised and distant supervision SA.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_2",
  "x": "In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments. Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 . The authors observe that the recursive models work well on shorter text while BOW features with NB and SVM perform well only on longer sentences.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_3",
  "x": "Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 . The authors observe that the recursive models work well on shorter text while BOW features with NB and SVM perform well only on longer sentences. Using<cite> Socher et al. (2013)</cite> 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 . ---------------------------------- **BASELINE SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_4",
  "x": "Note that our ML baseline systems as well as the English SA classifier by<cite> Socher et al. (2013)</cite> are trained on balanced data sets, i.e. we can assume no prior bias towards one class. ---------------------------------- **PLANNED CONTRASTS** ---------------------------------- **ERROR ANALYSIS**",
  "y": "similarities"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_5",
  "x": "Note that the Stanford SA system pays particular attention to sentence structure due to its \"deep\" architecture that adds to the model the feature of being sensitive to word ordering <cite>(Socher et al., 2013)</cite> . In future work, we will verify this by comparing these results to other high performing English SA tools (see for example Abbasi et al. (2014) In sum, one of the major challenges of this approach seems to be the use of Arabic dialects in social media, such as Twitter. In order to confirm this hypothesis, we automatically label Dialectal Arabic (DA) vs. Modern Standard Arabic (MSA) using AIDA (Elfardy et al., 2014) and analyse the performance of MT-based SA. The results in Fig. 1 show a significant correlation (Pearson, p<0.05) between language class and SA accuracy, with MSA outperforming DA. This confirms DA as a major source of error in the MT-based approach.",
  "y": "future_work"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_6",
  "x": "In particular, we make use of off-theshelf MT tools, such as Google and Microsoft MT, to translate Arabic Tweets into English. We then use the Stanford Sentiment Classifier <cite>(Socher et al., 2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. In contrast to previous work, we benchmark this approach on a gold-standard test set of 937 manually annotated tweets and compare its performance to standard SA approaches, including lexicon-based, supervised and distant supervision approaches. We find that MT approaches reach a comparable performance or significantly outperform more resourceintense standard approaches. As such, we conclude that using off-the-shelf tools to perform SA for under-resourced languages, such as Arabic, is an effective and efficient alternative to building SA classifiers from scratch.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_0",
  "x": "To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) .",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_1",
  "x": "On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) . We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description.",
  "y": "motivation"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_2",
  "x": "The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows: \u2022 We set up a general task of defining phrases given their contexts.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_3",
  "x": "We present LOG-Cad, a neural network-based description generator (Figure 1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen. Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_4",
  "x": "Local & Global Contexts for Description Generation In this paper, we refer to the explicit contextual information included in a single sentence as \"local context,\" and the implicit contextual information in the word/phrase embedding trained in an unsupervised manner on largescale corpora as \"global context. \" Previous work on the definition generation task<cite> (Noraset et al., 2017)</cite> has shown that global contexts can be useful clues when generating definitions of unknown words. The intuition behind their method is that words with similar meanings tend to have similar definitions in a dictionary. This can be seen as an extension of the Distributional Hypothesis (Harris, 1954; Firth, 1957) , which states words that share semantic meanings tend to appear in similar contexts. Additionally, work on the WSD task (Navigli, 2009) , novel sense detection (Erk, 2006; Lau et al., 2014) , and the non-standard word explanation task (Ni and Wang, 2017) have revealed that local contexts surrounding the word can help disambiguate its sense.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_5",
  "x": "The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a GATE function<cite> (Noraset et al., 2017)</cite> to dynamically control how the global and local contexts influence the generation of the description. We use bi-directional and uni-directional LSTMs (Hochreiter and Schmidhuber, 1997) as our context encoder and description decoder (Figure 1 ), respectively. Given a sentence X and a phrase X trg , the context encoder generates a sequence of continuous vectors where x i denotes the word embedding of word x i .",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_6",
  "x": "In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text. We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> . First, we feed phrase embedding x trg to initialize the decoder as",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_7",
  "x": "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text. We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> . First, we feed phrase embedding x trg to initialize the decoder as Here, phrase embedding x trg is calculated by simply summing up all the embeddings of words that consistute the phrase X trg .",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_8",
  "x": "We evaluate our method by applying it to describe words in WordNet (Miller, 1995) and Oxford Dictionary, 4 phrases in Urban Dictionary 5 and Wiki- data. 6 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as ground-truth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet 7 using the dict-definition 8 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the word.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_9",
  "x": "Note that our pre-trained embeddings only cover 26.79% of the words in the expressions to be described in our Wikipedia dataset, while it covers all words in WordNet dataset (See Table 2 ). Even if no reliable word embeddings are available, all models can capture the character information through character-level CNNs (See Figure 1) . ---------------------------------- **MODELS** We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in<cite> (Noraset et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_10",
  "x": "Although several studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider sub-sentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) have proposed a definition generation method that works with polysemous words in dictionaries. They present a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_11",
  "x": "While their method use local context only for disambiguating the meanings that are mixed up in word embeddings, the information from local contexts cannot be utilized if the pre-trained embeddings are unavailable or unreliable. On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_12",
  "x": "On the other hand, our method can fully utilize the local context through an attentional mechanism, even if the reliable word embeddings are unavailable. Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_0",
  "x": "**INTRODUCTION** Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
  "y": "background"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_1",
  "x": "**INTRODUCTION** Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
  "y": "extends"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_2",
  "x": "As in <cite>(Rosti et al., 2007)</cite> , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models. System weights and language model weights are tuned to optimize the quality of the decoding output on a development set. This paper is organized as follows. The incremental TER alignment algorithm is described in Section 2. Experimental evaluation comparing the incremental and pair-wise alignment methods are presented in Section 3 along with results on the WMT08 Europarl test sets.",
  "y": "similarities uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_3",
  "x": "After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment. each set of two consecutive nodes. Other scores for the word arc are set as in <cite>(Rosti et al., 2007)</cite> . ---------------------------------- **BENEFITS OVER PAIR-WISE TER ALIGNMENT**",
  "y": "similarities uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_0",
  "x": "<cite>Gurevych (2005)</cite> conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965) , but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004) . However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_1",
  "x": "Semantic wordnet based measures Lesk (1986) introduced a measure (Les) based on the number of word overlaps in the textual definitions (or glosses) of two terms, where higher overlap means higher similarity. As GermaNet does not contain glosses, this measure cannot be employed. <cite>Gurevych (2005)</cite> proposed an alternative algorithm (PG) generating surrogate glosses by using a concept's relations within the hierarchy. Following the description in Budanitsky and Hirst (2006) , we further define several measures using the taxonomy structure. PL is the taxonomic path length l(c 1 , c 2 ) between two concepts c1 and c2.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_2",
  "x": "Table 2 gives an overview of our experimental results on three German datasets. Best values for each dataset and knowledge source are in bold. We use the P G measure in optimal configuration as reported by <cite>Gurevych (2005)</cite> . For the Les measure, we give the results for considering: (i) only the first paragraph (+First) and (ii) the full text (+Full). For the path length based measures, we give the values for averaging over all category pairs (+Avg), or taking the best SR value computed among the pairs (+Best).",
  "y": "uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_3",
  "x": "**EXPERIMENTS & RESULTS** Our results on Gur65 using GermaNet are very close to those published by <cite>Gurevych (2005)</cite> , ranging from 0.69-0.75. For Gur350, the performance drops to 0.38-0.50, due to the lower upper bound, and because GermaNet does not model SR well. These findings are endorsed by an even more significant performance drop on ZG222. The measures based on Wikipedia behave less uniformly.",
  "y": "similarities"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_0",
  "x": "---------------------------------- **INTRODUCTION** Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016;<cite> Vaswani et al., 2017)</cite> trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017) . Recent studies (Tiedemann and Scherrer, 2017; Wang et al., 2017; have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017) .",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_1",
  "x": "The HAN encoder helps in the disambiguation of source-word representations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integration is done by (i) re-using the hidden representations from both the encoder and decoder of previous sentence translations and (ii) providing input to both the encoder and decoder for the current translation. This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_2",
  "x": "The integration is done by (i) re-using the hidden representations from both the encoder and decoder of previous sentence translations and (ii) providing input to both the encoder and decoder for the current translation. This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-chitecture<cite> (Vaswani et al., 2017)</cite> and show significant improvement over two strong baselines on multiple data sets.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_3",
  "x": "The function f w is a linear transformation to obtain the query q w . We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: Figure 1 : Integration of HAN during encoding at time step t,h t is the context-aware hidden state of the word x t . Similar architecture is used during decoding.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_4",
  "x": "We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: Figure 1 : Integration of HAN during encoding at time step t,h t is the context-aware hidden state of the word x t . Similar architecture is used during decoding. where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ).",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_5",
  "x": "As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by , with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention.",
  "y": "similarities uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_6",
  "x": "The optimization and regularization methods were the same as proposed by<cite> Vaswani et al. (2017)</cite> . Inspired by we trained the models in two stages. First we optimize the parameters for the NMT without the HAN, then we proceed to optimize the parameters of the whole network. We use k = 3 previous sentences, which gave the best performance on the development set. Table 1 shows the BLEU scores for different models.",
  "y": "similarities uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_0",
  "x": "---------------------------------- **INTRODUCTION** There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b; Levy et al., 2015) . Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory.",
  "y": "background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_1",
  "x": "Moreover, we provide two ways to train existing algorithms<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b ) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16. The truncated word embeddings are available from the papers web page at https://cogcomp.cs.illinois. edu/page/publication_view/790.",
  "y": "extends"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_2",
  "x": "**RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) . However, it has been proven that Brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks (Ratinov and Roth, 2009 ). Guo et al. (Guo et al., 2014) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.",
  "y": "background motivation"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_4",
  "x": "In this case, we can have final n-bit values in word embedding vectors as good as the method presented in Section 3.1. ---------------------------------- **EXPERIMENTS ON WORD/PHRASE SIMILARITY** In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algorithms, word2vec<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) , based on the Oct. 2013 Wikipedia dump.",
  "y": "uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_5",
  "x": "Note that it is also easy to incorporate our truncation methods into existing phrase embedding algorithms. We follow (Wieting et al., 2015) in using cosine similarity to evaluate the correlation between the computed similarity and annotated similarity between paraphrases. ---------------------------------- **ANALYSIS OF BITS NEEDED** We ran both CBOW and skipgram with negative sampling<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) on the Wikipedia dump data, and set the window size of context to be five.",
  "y": "uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_0",
  "x": "These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . ---------------------------------- **1** ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_1",
  "x": "One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation's government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods.",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_2",
  "x": "CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system <cite>(Durrett and Klein, 2014)</cite> . Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_3",
  "x": "Following<cite> Durrett and Klein (2014)</cite> , we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve. Query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. This processes generates on average 9 queries for each mention. Conveniently, this set of queries also defines the set of candidate entities that we consider linking a mention to: each query generates a set of potential entities based on link counts, whose unions are then taken to give on the possible entity targets for each mention (including the null link). In the example shown in Figure 1 , the query phrases are Pink Floyd and Floyd, which generate Pink Floyd and Gavin Floyd as potential link targets (among other options that might be derived from the Floyd query).",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_4",
  "x": "f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> .",
  "y": "similarities extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_5",
  "x": "f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> . f C is as discussed in Section 2.1. Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> .",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_6",
  "x": "f Q only impacts which query is selected and not the disambiguation to a title. It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> . ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_7",
  "x": "This corpus was used in Fahrni and Strube (2014) and<cite> Durrett and Klein (2014)</cite> . \u2022 CoNLL-YAGO (Hoffart et al., 2011) : This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities. \u2022 WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia. \u2022 Table 2 : Performance of the system in this work (Full) compared to two baselines from prior work and two ablations. Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) .",
  "y": "similarities uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_8",
  "x": "Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_9",
  "x": "Table 2 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN features (Full). We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) . We can also compare to two ablations: using just the sparse features (a system which is a direct extension of<cite> Durrett and Klein (2014)</cite> ) or using just the CNNderived features. 5 Our CNN features generally outperform the sparse features and improve even further when stacked with them.",
  "y": "differences extends"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_10",
  "x": "**ACE** In the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. This suggests that the system of<cite> Durrett and Klein (2014)</cite> has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct. By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so. ---------------------------------- **MULTIPLE GRANULARITIES OF CONVOLUTION**",
  "y": "background motivation"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_0",
  "x": "Computational segmentation approaches can be divided into rule-based (Porter, 1980) , supervised (Ruokolainen et al., 2013) , semi-supervised (Gr\u00f6nroos et al., 2014) , and unsupervised (Creutz and Lagus, 2002) . <cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries.",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_1",
  "x": "Computational segmentation approaches can be divided into rule-based (Porter, 1980) , supervised (Ruokolainen et al., 2013) , semi-supervised (Gr\u00f6nroos et al., 2014) , and unsupervised (Creutz and Lagus, 2002) . <cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries.",
  "y": "extends"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_2",
  "x": "We propose an explanation for this surprising result. ---------------------------------- **METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification.",
  "y": "uses"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_3",
  "x": "---------------------------------- **METHODS** In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (Tsochantaridis et al., 2005) .",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_5",
  "x": "---------------------------------- **BASELINE SYLLABIFICATION** As a baseline, we replicate the experiments of <cite>Bartlett et al. (2008)</cite> , and extend them to lowresource settings. Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 . We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples.",
  "y": "uses extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_0",
  "x": "Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) . In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_1",
  "x": "Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981) ) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_2",
  "x": "There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) . In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways.",
  "y": "extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_3",
  "x": "Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001 ). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim-pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_4",
  "x": "Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. In (Wubben et al., 2012; Coster and Kauchak, 2011) , simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments.",
  "y": "background uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_5",
  "x": "To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a traditional phrase based approach. Similarly, Wubben et al. (2012) use Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) systems.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_6",
  "x": "While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2),<cite> Zhu et al. (2010)</cite> fails to copy the shared argument \"The judge\" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (\"he\" or \"she\") for the antecedent \"The judge\". (2) C. The judge ordered that Chapman should receive psychiatric treatment in prison and sentenced him to twenty years to life. S1.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_7",
  "x": "Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_8",
  "x": "Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_9",
  "x": "**ESTIMATING THE PARAMETERS** We use the EM algorithm (Dempster et al., 1977) to estimate our split and deletion model parameters. For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and<cite> Zhu et al. (2010)</cite> ; and build training graphs (Figure 2 ) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). Each deletion candidate creates a deletion O-node marking successful or failed deletion of the candidate and a result M-node.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_10",
  "x": "---------------------------------- **EXPERIMENTS** We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_11",
  "x": "We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments. ---------------------------------- **TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_12",
  "x": "**TRAINING AND TEST DATA** The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs. We tokenize PWKP using Stanford CoreNLP toolkit 8 .",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_13",
  "x": "We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer produces a DRS for 96 of the 100 input sentences. These input are simplified using our simplification system namely, the DRS-SM model and the phrase-based machine translation system (Section 3.2). For the remaining four complex sentences, Boxer fails to produce DRSs. These four sentences are directly sent to the phrase-based machine translation system to produce simplified sentences.",
  "y": "uses"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_0",
  "x": "---------------------------------- **INTRODUCTION** Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) .",
  "y": "background"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_2",
  "x": "from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of <cite>McClosky et al. (2006a</cite>; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_0",
  "x": "It is the reason why opinion mining has recently become a topic of interest in both academia and business institutions. Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values<cite> (Yu et al., 2015)</cite> . The categorical approach aims at classifying the sentiment into polarity classes (such as positive, neutral, and negative,) or Ekman's six basic emotions, i.e., anger, happiness, fear, sadness, disgust, and surprise (Ekman, 1992 ). This approach is extensively studied because it can provide a desirable outcome, which is an overall evaluation of the sentiment in the material that is being analyzed. For instance, a popular form of media in recent years is live webcasting.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_1",
  "x": "On the other hand, the dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valencearousal space (Markus and Kitayama, 1991) , as shown in Fig. 1 gree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement. According to the twodimensional representation, any affective state can be represented as a point in the valence-arousal space by determining the degrees of valence and arousal of given words (Wei et al., 2011; <cite>Yu et al., 2015)</cite> or texts (Kim et al., 2010) . Dimen-sional sentiment analysis is an increasingly active research field with potential applications including antisocial behavior detection (Munezero et al., 2011) and mood analysis (De Choudhury et al., 2012) . In light of this, the objective of the Dimensional Sentiment Analysis for Chinese Words (DSAW) shared task at the 21th International Conference on Asian Language Processing is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases for compiling Chinese valence-arousal lexicons. The expected output of this task is to predict a real-valued score from 1 to 9 for both valence and arousal dimensions of the given 750 test words and phrases.",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_2",
  "x": "At word level, we use EHowNet (Chen et al., 2005) , a system that is designed for the purpose of automatic semantic composition and decomposition, to extract synonyms of the words from CVAW 2.0, and expand it to 19,611 words with valence-arousal ratings, called WVA. Fig. 2 illustrates the proposed framework. In order to cope with the problem of unknown words, we separate words in WVA into 4,184 characters with valence-arousal ratings, called CVA. The valence-arousal score of the unknown word can be obtained by averaging the matched CVA. Moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence-arousal methods<cite> (Yu et al., 2015)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_0",
  "x": "****MORPHO-SYNTACTIC REGULARITIES IN CONTINUOUS WORD REPRESENTATIONS: A MULTILINGUAL STUDY**** **ABSTRACT** We replicate the syntactic experiments of <cite>Mikolov et al. (2013b)</cite> on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language.",
  "y": "uses extends"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_1",
  "x": "We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. ---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_2",
  "x": "In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_3",
  "x": "The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus. <cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_4",
  "x": "---------------------------------- **TRAINING CORPUS FOR WORD VECTORS** The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus.",
  "y": "motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_5",
  "x": "<cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model. 1 Rather than use an RNN model to learn our own vectors, we employ the far simpler skip-gram model. Mikolov et al. (2013a) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train. The skip-gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words.",
  "y": "differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_7",
  "x": "The verbal set includes comparisons between the preterite, the infinitive, and the 3rd person singular present, but not the past and present participles. ---------------------------------- **RESULTS** In Table 1 , we report two numbers for each part of speech. The first, labeled as M13, is the result of applying the vectors of <cite>Mikolov et al. (2013b)</cite> to their test set.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_8",
  "x": "The second column, labeled as Ours, reports the results for our vectors, which were trained using WORD2VEC on the English data described in Section 2.1. Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of <cite>Mikolov et al. (2013b)</cite> , but they are not far off. For nouns, however, we obtain higher accuracy than Mikolov et al. The tokenization method that removes possessives from consideration may produce better vectors for singular and plural forms, as it increases the frequency of these types. ---------------------------------- **MULTILINGUAL EXPERIMENTS**",
  "y": "similarities differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_9",
  "x": "In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of <cite>Mikolov et al. (2013b)</cite> in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by TREETAGGER (Schmid, 1994) . We identify inflections using manually constructed inflection tables from several sources. Spanish and German verbal inflections, as well as German nominal inflections, are from a Wiktionary data set introduced by Durrett and DeNero (2013) .",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_0",
  "x": "The N-gram-based SMT framework is based on tuples. Tuples are minimal translation units composed of source and target cepts 2 . N-gram-based models are Markov models over sequences of tuples or operations encapsulating tuples<cite> (Durrani et al., 2011)</cite> . This mechanism has several useful properties. Firstly, no phrasal independence assumption is made.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_1",
  "x": "In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; , or a higher beam size is used in decoding<cite> (Durrani et al., 2011)</cite> . The ability to memorize and produce larger translation chunks during decoding, on the other hand, gives a distinct advantage to the phrase-based system during search. Phrase-based systems i) have access to uncommon translations, ii) do not require higher beam sizes, iii) have more accurate future-cost estimates because of the availability of phrase-internal language model context before search is started. To illustrate this consider the German-English phrase-pair \"scho\u00df ein Torscored a goal\", composed from the tuples (ceptpairs) \"scho\u00df -scored\", \"ein -a\" and \"Tor -goal\". It is likely that the N-gram system does not have the tuple \"scho\u00df -scored\" in its n-best translation options because \"scored\" is an uncommon translation for \"scho\u00df\" outside the sports domain.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_2",
  "x": "A higher beam is required to prevent it from getting pruned. Phrase-based systems, on the other hand, are likely to have access to the phrasal unit \"scho\u00df ein Tor -scored a goal\" and can generate it in a single step. Moreover, a more accurate future-cost estimate can be computed because of the available context internal to the phrase. In this work, we extend the N-gram model, based on operation sequences<cite> (Durrani et al., 2011)</cite> , to use phrases during decoding. The main idea is to study whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above-mentioned problems.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_3",
  "x": "<cite>Durrani et al. (2011)</cite> recently addressed these problems by proposing an operation sequence Ngram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules. Representing bilingual sentences as a sequence of operations enables them to memorize phrases and lexical reordering triggers like PBSMT. However, using minimal units during decoding and searching over all possible reorderings means that hypotheses can no longer be arranged in 2 m stacks. The problem of inaccurate future-cost estimates resurfaces resulting in more search errors. A higher beam size of 500 is therefore used to produce translation units in comparison to phrase-based systems.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_4",
  "x": ". . , o j\u22121 be a sequence of operations as hypothesized by the translator to generate the bilingual sentence pair F, E with an alignment function A. The translation model is defined as: where n indicates the amount of context used. The translation model is implemented as an N-gram model of operations using SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing. A 9-gram model is used. Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework<cite> (Durrani et al., 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_5",
  "x": "The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations. The model backs-off to the smaller n-grams of operations if the full history is unknown. We use Kneser-Ney smoothing to handle back-off 5 .",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_6",
  "x": "The ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence. The overall process can be roughly divided into the following steps: i) extraction of translation units ii) future-cost estimation, iii) hypothesis extension iv) recombination and pruning. During the hypothesis extension each extracted phrase is translated into a sequence of operations. The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ).",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_7",
  "x": "---------------------------------- **TRAINING** We extended the training steps in <cite>Durrani et al. (2011)</cite> to extract a phrase lexicon from the parallel data. We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments. Only continuous phrases as used in a traditional phrase-based system are extracted thus allowing only inside-out (Wu, 1997) type of alignments.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_8",
  "x": "---------------------------------- **DECODING** We extended the decoder developed by <cite>Durrani et al. (2011)</cite> and tried three ideas. In our primary experiments we enabled the decoder to use phrases instead of cepts. This allows the decoder to i) use phraseinternal context when computing the future-cost estimates, ii) hypothesize translation options not available to the cept-based decoder iii) cover multiple source words in a single step subsequently improving translation coverage and search.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_9",
  "x": "We initially experimented with two language pairs: German-to-English (G-E) and French-to-English (F-E). We trained our system and the baseline systems on most of the data 6 made available for the translation task of the Fourth Workshop on Statistical Machine Translation. 7 We used 1M bilingual sentences, for the estimation of the translation model and 2M sentences from the monolingual corpus (news commentary) which also contains the English part of the bilingual corpus. Word alignments are obtained by running GIZA++ (Och and Ney, 2003) with the grow-diag-final-and (Koehn et al., 2005) symmetrization heuristic. We follow the training steps described in <cite>Durrani et al. (2011)</cite> , consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_10",
  "x": "We mark a result as sig-8 Discontinuous source-side units did not lead to any improvements in<cite> (Durrani et al., 2011)</cite> and increased the decoding times by multiple folds. We also found these to be less useful. 9 Imposing a hard reordering limit significantly reduced the decoding time and also slightly increased the BLEU scores. 10 Higher stack sizes leads to improvement in model scores for both German-English and French-English and slight improvement of BLEU in the case of the former. 11 We used news-dev2009a as dev and news-dev2009b as devtest and tuned the weights with Z-MERT (Zaidan, 2009) .",
  "y": "differences"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_0",
  "x": "However, without explicit feedback on each change, students may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions \"better\". Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings.",
  "y": "background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_1",
  "x": "However, without explicit feedback on each change, students may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions \"better\". Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings.",
  "y": "background extends"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_2",
  "x": "aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. Revision classification. Following the argumentative revision definition in our prior work<cite> (Zhang and Litman, 2015)</cite> , revisions are first categorized to Content (Text-based) and Surface 3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence, and Other content changes (General Content).",
  "y": "uses"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_3",
  "x": "Thus, we have designed an interface that offers multiple views of the revision changes. As demonstrated in Figure 2 , the interface includes a revision overview interface for the overview of the authors' revisions and a revision detail interface that allows the author to access the details of their essays and revisions. Inspired by works on learning analytics (Liu et al., 2013; Verbert et al., 2013) , we design the revision overview interface which displays the statistics of the revisions. Following design principle #1, the revision purposes are color coded and each purpose corresponds to a specific color. Our prior work<cite> (Zhang and Litman, 2015)</cite> demonstrates that only Text-based revisions are significantly correlated with the writing improvement.",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_0",
  "x": "From a synchronic perspective, <cite>Reddy et al. (2011</cite> ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations. However, Schulte im Walde et al. (2013) use German data and do not investigate diachronic changes. They report a Spearman's \u03c1 of 0.65 for predicting the compositionality of compounds based on the features of their semantic space and find that the modifiers mainly influence the compositionality of the whole compound, contrary to their expectation that the head should be the main source of influence. This is true for both the human annotation and their vector space model. Schulte im Walde et al. (2016a) further investigate the role of heads and modifiers on the prediction of compositionality and report \u03c1 values between 0.35 and 0.61 for their models on German and English data.",
  "y": "background similarities"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_1",
  "x": "They also compute the displacement for a single word embedding by calculating the cosine similarity between a point in time t and a later point in time t + \u2206. We adapt this methodology and make use of the same corpus (Google Books Ngram). ---------------------------------- **METHODS AND DATA** Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009;<cite> Reddy et al., 2011</cite>; Schulte im Walde et al., 2016b) . Some of these works have used large corpora to extract vector-based representations of compounds and their parts to automatically determine the compositionality of a given compound.",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_2",
  "x": "We use a sliding window approach, wherein we capture the context of a compound based on its position in the 5-gram. That means that a bigram (say the compound gold mine) could occur in four different positions in the 5-grams (1-2, 2-3, 3-4 and finally 4-5). We then capture the contexts for each of these positions, in order to enrich the representation of a compound and its constituents (which similarly have five such positions, as they are unigrams). Ideally, we would validate our diachronic model on diachronic test data. However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by<cite> Reddy et al. (2011)</cite> (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general.",
  "y": "uses"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_3",
  "x": "Like<cite> Reddy et al. (2011)</cite> and Schulte im Walde et al. (2013), we opt for Spearman's \u03c1. To find the best configuration of a time span and cut-off for the regression models, we use the R 2 metric. Table 1 presents our findings; we will discuss them in the following Section 5. ---------------------------------- **PROGRESSION OF COMPOSITIONALITY OVER TIME**",
  "y": "similarities"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_4",
  "x": "Interestingly, PPMI is always weakly negatively correlated with the ratings. This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_5",
  "x": "Interestingly, PPMI is always weakly negatively correlated with the ratings. This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_6",
  "x": "Although care should be taken given the small data sets (especially for the earlier decades) on which the models were build and tested, the slope of the lines for the three groups of compounds seems to suggest that less compositional compounds go through a more pronounced change in compositionality than compositional compounds, as expected. We also show the graphs for sim-with-head and sim-with-mod (Figures 2 and 3) for the different groups of compounds across time, as these underperformed in our previous experiment. Both figures based on cosine based features largely confound the three groups of compounds across time, reinforcing our previous findings. 6 Future Work Our current work was limited to English compounds from<cite> Reddy et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_0",
  "x": "Abstract Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed<cite> (Moore, 2014)</cite> makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.",
  "y": "background motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_1",
  "x": "2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_2",
  "x": "**** introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl. ----------------------------------",
  "y": "differences motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_3",
  "x": "We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ---------------------------------- **TAG DICTIONARIES AND TAGGING SPEED** A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t 1 , . . . , t n given a sequence of words w 1 , . . . , w n .",
  "y": "differences motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_4",
  "x": "For each word observed in an annotated training set, Ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found <cite>(Moore, 2014</cite> ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_5",
  "x": "This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%. ---------------------------------- **OUR PREVIOUS METHOD** We previously presented<cite> (Moore, 2014)</cite> a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold T .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_6",
  "x": "**THE TAGGING MODEL** The model structure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper<cite> (Moore, 2014)</cite> . The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001 ), using stochastic subgradient descent as described by Zhang (2004) , with early stopping and averaging.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_7",
  "x": "We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Section 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the methods described in our previous work <cite>(Moore, 2014</cite>, Section 3.1) . Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. ---------------------------------- **EXTRACTING THE TAG DICTIONARY**",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_8",
  "x": "Tagging the WSJ development set with these two dictionaries is compared in Table 1 to tagging with our previous pruned KN-smoothed dictionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens-it can simply output the single possible tag. The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described<cite> (Moore, 2014)</cite> , in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_9",
  "x": "For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work<cite> (Moore, 2014)</cite> . The results of these tests are shown in Table 2 Table 2 : WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_10",
  "x": "The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data. ---------------------------------- **CONCLUSIONS** Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art<cite> (Moore, 2014)</cite> for a tag dictionary that does not degrade tagging accuracy.",
  "y": "differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_0",
  "x": "For example, present will still be A1 in sentence \"John gave a nice present to his wonderful wife\", despite different surface forms of the two sentences. We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) .",
  "y": "background differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_1",
  "x": "Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method. <cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by .",
  "y": "background similarities"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_2",
  "x": "<cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation). These linguistically-aware word representations are used within a neural encoder.",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_3",
  "x": "We apply GCNs to the semantic dependency graphs and experiment on the English-German language pair (WMT16). We observe an improvement over the semantics-agnostic baseline (a BiRNN encoder; 23.3 vs 24.5 BLEU). As we use exactly the same modeling approach as in the syntactic method of <cite>Bastings et al. (2017)</cite> , we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics). We observe that when using full WMT data we obtain better results with semantics than with syntax (23.9 BLEU for syntactic GCN). Using syntactic and semantic GCN together, we obtain a further gain (24.9 BLEU) that suggests the complementarity of syntax and semantics.",
  "y": "similarities uses motivation"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_4",
  "x": "Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges.",
  "y": "background differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_5",
  "x": "Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated. In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges. Each node v \u2208 V is represented by a feature vector x v \u2208 R d , where d is the latent space dimensionality.",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_6",
  "x": "The vector b lab(u,v) \u2208 R d is an embedding of a semantic role label of the edge (u, v) (e.g., A0). The functions g u,v are scalar gates which weight the importance of each edge. Gates are particularly useful when the graph is predicted BiRNN Baseline<cite> (Bastings et al., 2017)</cite> 23.3 +Sem 24.5 +Syn<cite> (Bastings et al., 2017)</cite> 23.9 +Syn + Sem 24.9 and thus may contain errors, i.e., wrong edges. In this scenario gates can down weight the influence of such edges. \u03c1 is a non-linearity (ReLU).",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_7",
  "x": "We constructed the English vocabulary by taking all words with frequency higher than three, while for German we used byte-pair encodings (BPE) (Sennrich et al., 2016). All hyperparameter selection was performed on the validation set (see Appendix A). We measured the performance of the models with (cased) BLEU scores (Papineni et al., 2002) . The settings and the framework (Neural Monkey (Helcl and Libovick\u00fd, 2017) ) used for experiments are the ones used in <cite>Bastings et al. (2017)</cite> , which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_8",
  "x": "First, we start with experiments with the smaller News Commentary training set (See Table 1 ). As in <cite>Bastings et al. (2017)</cite> , we used the standard attention-based encoder-decoder model as a baseline. We tested the impact of semantic GCNs when used on top of CNN and BiRNN encoders. As expected, BiRNN results are stronger than CNN ones. In general, for both encoders we observe the same trend: using semantic GCNs leads to an improvement over the baseline model.",
  "y": "uses"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_0",
  "x": "In some cases, the code was initially made available, then removed, and is now back online <cite>(Tang et al., 2016a)</cite> . Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when Chen et al. (2017) used the code and embeddings in Tang et al. (2016b) they observe different results. Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors. Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_1",
  "x": "Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing (Wang et al., 2017) , and RNN <cite>(Tang et al., 2016a)</cite> , as well as having been applied largely to different datasets. At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created.",
  "y": "motivation"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_2",
  "x": "Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) <cite>(Tang et al., 2016a)</cite> , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF . Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_3",
  "x": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN (Dong et al., 2014) , TDLSTM <cite>(Tang et al., 2016a)</cite> , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_4",
  "x": "For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" <cite>(Tang et al., 2016a)</cite> as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs. Within their experiments they used SSWE and Glove Twitter vectors 11 (Pennington et al., 2014) . As the paper being reproduced does not define the number of epochs they trained for, we use early stopping. Thus for early stopping we require to split the training data into train and validation sets to know when to stop.",
  "y": "differences extends"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_5",
  "x": "Our results generally agree with their results on the ranking of the word vectors and the embeddings. Overall, we were able to reproduce the results of all three papers. However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ---------------------------------- **MASS EVALUATION**",
  "y": "extends differences"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_6",
  "x": "---------------------------------- **MASS EVALUATION** For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) .",
  "y": "similarities uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_0",
  "x": "Link prediction methods in knowledge graphs (see (Nickel et al., 2016) for an overview) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities (such as homophily). Lao and Cohen (2010) introduced a new method that predicts direct links based on paths that connect the source and target nodes. Such paths are not only useful for link prediction (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) . These approaches rely on the structure of the knowledge graph, which is inherently incomplete. This incompleteness can affect the process in different ways, e.g. it leads to representations for nodes with few connection that are not very informative, it can miss relevant patterns/paths (or derive misleading patterns/paths).",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_1",
  "x": "We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using<cite> Gardner et al. (2014)</cite> 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL. Our analysis shows that we find different and much fewer paths than the PRA method does (mostly because the abstract paths do not contain back-and-forth sequences of generalizing or type relations). The paths found in the abstract graphs lead to better performance on NELL than the PRA paths, which could be explained by the fact that NELL's relation inventory was designed to capture interdependencies (Carlson et al., 2010a) . On Freebase the results we obtain are lower, but this could be due to a different negative sampling process. Inspection of the paths produced reveal that they seem to capture legitimate dependencies.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_2",
  "x": "A class of embedding models that aim to factorize the graph are termed as latent factor models. Neural network based models such as ER-MLP (Dong et al., 2014) , NTN (Socher et al., 2013) , RNNs (Neelakantan et al., 2015; Das et al., 2016) and Graph CNNs (Schlichtkrull et al., 2018) are examples of embedding models while RESCAL (Nickel et al., 2012) , DistMult (Yang et al., 2015) , TransE (Bordes et al., 2013) , ComplEx (Trouillon et al., 2017) are examples of latent factor models. Lao and Cohen (2010) introduced a novel way to exploit information in knowledge graphs: using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_3",
  "x": "Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into \"meta-paths\". The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) . Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_4",
  "x": "To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to<cite> (Gardner et al., 2014)</cite> . ---------------------------------- **ABSTRACT GRAPHS AND ABSTRACT PATHS** Knowledge graphs are incomplete in an imbalanced way. Figures 1a-1b show how much the relation and node frequencies for Freebase 15k and NELL vary, and the fact that numerous nodes and edges have very low frequency (each data point corresponds to a node/relation, and the value is the degree of the node/frequency of the relation respectively).",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_5",
  "x": "the target node of relation r i in the abstract graph is the set of target nodes (the range) of r i in KG: R is the set of relation types of KG, R set = {intersection, subset, superset} 1 . weighted edges where the weight of a set relation between KG A 's nodes quantifies the overlap between the two sets: Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by<cite> (Gardner et al., 2014)</cite> and in this paper). Every data point is the degree of a node (top plots), or frequency of a relation (bottom plots).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_6",
  "x": "**ABSTRACT PATHS** The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by<cite> (Gardner et al., 2014)</cite> , and their abstract versions associating a value for each added path. Obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees. Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_7",
  "x": "Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_8",
  "x": "Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search. We constrain this path to contain at most k \"proper\" relations 2 , and we do not allow consecutive set relations, thus forcing the algorithm to move from one \"proper\" relation to another through a set relation that connects the range of one with the domain of the next. An abstract path, just like a meta-path extracted by previous work, is a sequence of relation types: \u03c0 j =< r j,1 , r j,2 , ...r j,m >, some of which are \"proper\" relations, some are set relations.",
  "y": "background differences"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_9",
  "x": "We use this weight to rank abstract relations for potential filtering, and to compute the weight of its grounding for specific node pairs. ---------------------------------- **GROUNDED PATHS** The abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple. They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_10",
  "x": "They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph). Set relations have no counterpart in the extensional graph, as at this level nodes themselves make the connection between successive relations (ii) starting at the source node, we follow again a breadth first traversal, constraining at each step the type of relation to follow based on the \"cleaned up\" abstract path. We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t):",
  "y": "background uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_11",
  "x": "We compute the weight of a grounded path gp =< v 0 , r x 1 , v 1 , ..., v l\u22121 , r x l , v l > (where v 0 = s and v l = t) as a combination of the weight of the corresponding abstract path \u03c0 =< r 1 , ..., r m > (r x i \u2208 \u03c0) and specific information for the current node pair (s, t): where the weights of the relations on the grounded path reflect the specificity of the relation to its source node: ---------------------------------- **EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_12",
  "x": "**EXPERIMENTS** Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP).",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_13",
  "x": "The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP). ---------------------------------- **DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_14",
  "x": "We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_15",
  "x": "**DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction. The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph.",
  "y": "background uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_16",
  "x": "The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types. We will skip this graph variation in the rest of the experiments presented here. Gardner et al. (2014) use these graphs to generate paths for augmenting the representation of node pairs, for link prediction, for a subset of 24 relation types from Freebase's inventory, and 10 relations from NELL. Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_17",
  "x": "Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 . Negative sampling The number of negative instances used in<cite> (Gardner et al., 2014)</cite> is not clearly stated. Both the number and methods of generating the negative samples can impact the results (Kotnis and Nastase, 2018) . We use (up to) 200 negative samples for each positive pair: for a pair (s, t) in the provided training or test sets for each relation r, we make 100 negative samples by corrupting the source s, and 100 negative samples by corrupting the target t. The corrupted s and t are chosen from r's domain V r,s and range V r,t respectively, such that these corrupted triples are not part of the training, test or graph. If 100 instances do not exist, we extract as many as possible.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_18",
  "x": "We produce two features for each abstract path: one that is the weight of this path, and one that is the weight of its grounding for a given relation instance. If a relation instance does not have a grounding for an abstract path, the values of these features will be 0. ---------------------------------- **RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_19",
  "x": "Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples). Freebase has 4200+ relation types, and NELL 500+. More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_20",
  "x": "More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> . few times will not help the system find a robust model. For the purpose of the presented experiments we filter the Freebase abstract graph to use only relation types that have at least 10 instances (Table 1 shows the statistics for this configuration).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_21",
  "x": "There is no consistent trend -for some relations using the paths extracted with this approach leads to better results, for others it does not (although, as we frequently mentioned, the fact that we used different negative sampling methods, the results are not directly comparable). A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973). For NELL the number varies more, between 58 and 5509, 6 of the relations have more than 1000 metapaths. With the abstract graphs the numbers are much lower.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_22",
  "x": "The overlap between the sets of paths discovered with the two methods is very small: for Freebase the average overlap with respect to PRA is around 0.004 (for the different graph configurations), and with respect to the abstract paths around 0.2; for NELL around 0.003 relative to PRA and 0.27 relative to the abstract paths. We note that overall, the system found more paths than what could be grounded for the given training instances for both Freebase and NELL. Another general observation is that relations for which we found the most patterns (AthletePlaysForTeam and StateHasLake for NELL, /medicine/disease/symptoms and /film/film/rating for Freebase) do not necessarily perform the best. NELL The results for each relation in terms of average precision are presented in Table 4 . We include the best result on PRA (on any variation of the graph), as reported by<cite> (Gardner et al., 2014)</cite> , although since we used different negative instances the results are not directly comparable.",
  "y": "differences uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_0",
  "x": "More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001;<cite> Sudo et al., 2003</cite>; Yangarber, 2003) . In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_1",
  "x": "A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models. Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_2",
  "x": "Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj).",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_3",
  "x": "These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of node X.",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_4",
  "x": "Linked Chains: The linked chains model represents extraction patterns as a pair of chains which share the same verb but no direct descendants. Example linked chains are shown in Figure 2 . This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 . Subtrees: The final model to be considered is the subtree model<cite> (Sudo et al., 2003)</cite> . In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_5",
  "x": "---------------------------------- **PREVIOUS COMPARISONS** There have been few direct comparisons of the various pattern models. <cite>Sudo et al. (2003)</cite> compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_6",
  "x": "They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns. There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_7",
  "x": "However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task. ---------------------------------- **EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> .",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_8",
  "x": "Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred. <cite>Sudo et al. (2003)</cite> found that it was important to find the appropriate balance between these two factors. They introduced the \u03b2 parameter as a way of controlling the relative contribution of the inverse document frequency. \u03b2 is tuned for each extraction task and pattern model combination. Although simple, this approach has the advantage that it can be applied to each of the four pattern models to provide a direct comparison.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_9",
  "x": "Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\". This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since <cite>Sudo et al. (2003)</cite> showed that adequate knowledge about document relevance could be obtained automatically using an IR system.",
  "y": "differences"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_10",
  "x": "The rightmost extension algorithm is most suited to finding subtrees which occur multiple times and, even using this efficient approach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_11",
  "x": "They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_12",
  "x": "The value of \u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by <cite>Sudo et al. (2003)</cite> . To generate this additional text we used the Reuters Corpus (Rose et al., 2002 ) which consists of a year's worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus.",
  "y": "uses"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_0",
  "x": "Our proposed approach additionally enables to benefit from deep decoders compared to previous works which focus on deep encoders. ---------------------------------- **INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently.",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_1",
  "x": "**INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently. The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; .",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_2",
  "x": "The multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; . In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016) . However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers. show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which weighted combines outputs of all encoder layers as encoded representation.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_3",
  "x": "The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_4",
  "x": "We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k. We conducted our experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer. Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.",
  "y": "extends differences"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_5",
  "x": "Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach.",
  "y": "similarities"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_6",
  "x": "Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> . Results of two different computation order are shown in Table 1 . v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_7",
  "x": "In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization. Our experiments demonstrate the effectiveness of our simple approach on the convergence of deep Transformers, and brings significant improvements on the WMT 14 English to German and the WMT 15 Czech to English news translation tasks.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_8",
  "x": "In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them. Then we conjecture the training problem of deep Transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with Lipschitz restricted parameter initialization. Our experiments demonstrate the effectiveness of our simple approach on the convergence of deep Transformers, and brings significant improvements on the WMT 14 English to German and the WMT 15 Czech to English news translation tasks.",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_0",
  "x": "When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the<cite> Weeds et al. (2014)</cite> datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015) . The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_1",
  "x": "Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_2",
  "x": "Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) .",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_3",
  "x": "Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_4",
  "x": "Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features.",
  "y": "background extends"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_5",
  "x": "On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_6",
  "x": "Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) . The test consisted in providing to the trained model switched hypernyms (e.g. from \"dog HYPER animal\" to \"dog RANDOM fruit\"), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_7",
  "x": "Roller et al. (2014) used the vectors' difference, while<cite> Weeds et al. (2014)</cite> implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014) 's observation was further investigated by Levy et al. (2015) , who claimed that supervised methods learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_8",
  "x": "---------------------------------- **EVALUATION** ---------------------------------- **TASKS** We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and -in particularagainst the best performant models in<cite> Weeds et al. (2014)</cite> ; iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_9",
  "x": "Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_10",
  "x": "These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_11",
  "x": "In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation).",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_12",
  "x": "**WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_13",
  "x": "In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation).",
  "y": "background uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_15",
  "x": "where is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_16",
  "x": "The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech. Also, the performance of three main unsupervised methods is reported as a reference: cosine (see above in this section), balAPinc (Kotlerman et al., 2010) and invCL (Lenci and Benotto, 2012) . A threshold p empirically found in a training set was used in these methods for the decision, Table 2 .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_17",
  "x": "However, it is worth noticing here that such difference disappears with the WN datasets proposed by<cite> Weeds et al. (2014)</cite> . See section 6, and -in particular - Table 3 . F1 scores on a 10-fold cross validation for binary classification tasks. Scores are in percent. Table 3 describes the results of ROOT9 and the baseline in the binary classification tasks.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_18",
  "x": "These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_19",
  "x": "These results confirm the analysis suggested above. ---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_20",
  "x": "**TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms.",
  "y": "background differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_21",
  "x": [
   "However, only 576 out of 6,400 randoms (most of which are likely to be the switched pairs) were misclassified as hypernyms. ---------------------------------- **CONCLUSIONS** In this paper, we have described ROOT9, a classifier for hypernyms, co-hyponyms and random words that is derived from an optimization of ROOT13 (Santus et al., 2016b) . The classifier, based on the Random Forest algorithm, uses only nine unsupervised corpus-based features, which have been described, and their contribution assessed."
  ],
  "y": "uses future_work"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_0",
  "x": "It's better to give other people you're hand out in help then you holding your own hand.\" In the study of Habernal and Gurevych (2016b) , annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory<cite> (Wachsmuth et al., 2017a)</cite> and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a) . In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_2",
  "x": "Dialectical quality dimensions resemble those of cogency, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004) . <cite>Wachsmuth et al. (2017a)</cite> point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. 5-1 B is attacking / abusive. 5-2 B has language/grammar issues, or uses humour or sarcasm.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_3",
  "x": "Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_4",
  "x": "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . These pairs represent the practical view in our experiments.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_5",
  "x": "**CORRELATIONS OF DIMENSIONS AND REASONS** For Hypotheses 1 and 2, we consider all 736 pairs of arguments from Habernal and Gurevych (2016a) where both have been annotated by <cite>Wachsmuth et al. (2017a)</cite> . For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie. This way, we can correlate each dimension with all reason labels in Table 2 including Conv. In particular, we compute Kendall's \u03c4 based on all argument pairs given for each label.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_6",
  "x": "Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from <cite>Wachsmuth et al. (2017a)</cite> given for each reason label (Habernal and Gurevych, 2016a) . The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties. relate more with global relevance and sufficiency respectively. Similarly, 7-3 (off-topic) correlates strongly with local and global relevance (both .95). So, these dimensions seem hard to separate.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_7",
  "x": "**ABSOLUTE RATINGS FOR RELATIVE DIFFERENCES** The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from <cite>Wachsmuth et al. (2017a)</cite> with a particular reason label from Habernal and Gurevych (2016a) . As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings. Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_8",
  "x": "---------------------------------- **ABSOLUTE QUALITY RATINGS BY THE CROWD** We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment. In particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 (or choose \"cannot judge\"). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total).",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_9",
  "x": "First, we checked to what extent lay annotators and experts agree in terms of Krippendorff's \u03b1. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of <cite>Wachsmuth et al. (2017a)</cite> . On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. Table 5 : Mean and MACE Krippendorff's \u03b1 agreement between (a) the crowd and the experts, (b) two independent crowd groups and the experts, (c) group 1 and the experts, and (d) group 2 and the experts. Table 5 (a) presents the results.",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_0",
  "x": "Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990) ). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance <cite>(Downey et al., 2007)</cite> . Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be-to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate. In a large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques.",
  "y": "differences"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_1",
  "x": "We begin by formally defining the extraction and typechecking tasks we consider, then discuss statistical language models and their utilization for extraction assessment. The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances S R , and a list of candidate extractions U R , the task is to order elements of U R such that correct instances for R are ranked above extraction errors. Let U Ri denote the set of the ith arguments of the extractions in U R , and let S Ri be defined similarly for the seed set S R . For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction <cite>(Downey et al., 2007)</cite> . The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors.",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_2",
  "x": "The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990) . Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS <cite>(Downey et al., 2007)</cite> . An HMM models a sentence w as a sequence of observations w i each generated by a hidden state variable t i . Here, hidden states take values from {1, . . . , T }, and each hidden state variable is itself generated by some number k of previous hidden states.",
  "y": "background"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_3",
  "x": "---------------------------------- **PERFORMING ADS WITH SLMS** The Assessment by Distributional Similarity (ADS) technique is to rank extractions in U R in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs <cite>(Downey et al., 2007)</cite> and adapt it to also apply to n-gram models, as detailed below. Define a context of an extraction argument e i to be a string containing the m words preceding and m words following an occurrence of e i in the corpus.",
  "y": "extends"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_4",
  "x": "All language models were trained on a corpus of 2.8M sentences of Web text (about 60 million tokens). SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions. We evaluated performance on three distinct data sets. The first two data sets evaluate ADS for unsupervised information extraction, and were taken from <cite>(Downey et al., 2007)</cite> . The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged).",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_5",
  "x": "We evaluate on the Unary and Binary data sets, since they have been employed in previous work on our corpora. Figure 2 shows that for HMMs, ADS performance increases as perplexity decreases across various model configurations (a similar relationship holds for n-gram models). A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results. As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in <cite>(Downey et al., 2007)</cite> by 26%, on average. The experiments also reveal an important difference between the HMM and n-gram approaches.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_0",
  "x": "However, na\u00efvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_1",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_2",
  "x": "We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_3",
  "x": "We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_4",
  "x": "This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_5",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_6",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_7",
  "x": "Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_8",
  "x": "Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_10",
  "x": "**PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_11",
  "x": "where \u03b8 r \u03c0 is the weight learned by the logistic regression classifier during training specially for relation r and path type \u03c0. During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. ---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_12",
  "x": "---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_13",
  "x": "Low quality bridging entities connect source target pairs from both positive and negative training sets, and hence are eliminated by the sparse logistic regression classifier. The negative dataset is generated using the closed world assumption by performing a random walk. After augmenting the KB, we run the training phase of the PRA algorithm to obtain the feature (path) weights computed by the logistic regression Table 2 : Comparison of Mean Reciprocal Rank (MRR) metric for 10 relations from NELL (higher is better). PRA-SVO, <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_14",
  "x": "**EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_15",
  "x": "For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> . The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_16",
  "x": "The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_17",
  "x": "The augmentation resulted in the addition of 1086 paths during training and 1430 paths during test time. We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_18",
  "x": "Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_19",
  "x": "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8).",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_20",
  "x": "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8).",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_21",
  "x": "The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_22",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs.",
  "y": "similarities"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_23",
  "x": "In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_24",
  "x": "In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_25",
  "x": "We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_0",
  "x": "Specifically, in the Indian subcontinent, number of Internet users has crossed 500 mi 1 , and is rising rapidly due to inexpensive data 2 . With this rise, comes the problem of hate speech, offensive and abusive posts on social media. Although there are many previous works which deal with Hindi and English hate speech (the top two languages in India), but very few on the code-switched version (Hinglish) of the two (<cite>Mathur et al. 2018</cite>) . This is partially due to the following reasons: (i) Hinglish consists of no-fixed grammar and vocabulary. It derives a part of its semantics from Devnagari and another part from the Roman script.",
  "y": "motivation"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_1",
  "x": "Moreover, the word yatra here, can have phonetic variations, which would result in multiple spellings of the word as yatra, yaatra, yaatraa, etc. Also, the problem of hate speech has been rising in India, and according to the policies of the government and the various social networks, one is not allowed to misuse his right to speech to abuse some other community or religion. Due to the various difficulties associated with the Hinglish language, it is challenging to automatically detect and ban such kind of speech. Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (<cite>Mathur et al. 2018</cite>) . We also release the embeddings and the model trained.",
  "y": "differences"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_2",
  "x": "**PRE-PROCESSING** In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (<cite>Mathur et al. 2018</cite>) . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary 3 and translation of each word to English using Hindi to English dictionary 4 . To deal with the spelling variations, we manually added some common variations of popular Hinglish words.",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_3",
  "x": "The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search. Results Table 3 shows the performance of our model (after getting trained on (Davidson et al. 2017) ) with two types of embeddings in comparison to the models by (<cite>Mathur et al. 2018</cite>) and (Davidson et al. 2017 ) on the HEOT dataset averaged over three runs. We also compare results on pre-trained embeddings. As shown in the table, our model when given Glove embeddings performs better than all other models. For comparison purposes, in Table 4 we have also evaluated our results on the dataset by (Davidson et al. 2017 ).",
  "y": "differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_0",
  "x": "1 Recognizing emotions from text has several applications: first, it helps companies and businesses in shaping their marketing strategies based on consumers' emotions (Bougie et al., 2003) ; second, it allows improving typical collaborative filtering based recommender systems (Badaro et al., 2013 (Badaro et al., , 2014c ) in terms of products or advertisements recommendations (Mohammad and Yang, 2011) ; third, politicians can learn how to adapt their political speech based on people emotions (Pang et al., 2008) and last but not least emotion classification helps in stock market predictions (Bollen et al., 2011) . While plenty of works exist for sentiment analysis for different languages including analysis of social media data for sentiment characteristics (Al Sallab et al., 2015; Baly et al., , 2017b , few works focused on emotion recognition from text. Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_1",
  "x": "For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective. In this paper, we focus on expanding coverage of existing emotion lexicon, namely DepecheMood, using the synonymy semantic relation available in English WordNet. We decide to expand DepecheMood since it is one of the largest emotion lexicon publicly available, and since its terms are aligned with EWN, thus allowing us to benefit from powerful semantic relations in EWN. The paper is organized as follows. In section 2, we conduct a brief literature survey on existing emotion lexicons.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_2",
  "x": "While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood<cite> (Staiano and Guerini, 2014)</cite> include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. Affect database extends SentiFul and covers around 2.5K words presented in their lemma form along with the corresponding part of speech (POS) tag. DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions. <cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com. The articles are accompanied by Rappler's Mood Meter, which allows readers to express their emotions about the article they are reading.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_3",
  "x": "They extended WordNet Affect using the concepts in ConceptNet. While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood<cite> (Staiano and Guerini, 2014)</cite> include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. Affect database extends SentiFul and covers around 2.5K words presented in their lemma form along with the corresponding part of speech (POS) tag. DepecheMood was automatically built by harvesting social media data that were implicitly annotated with emotions. <cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_4",
  "x": "**EMOWORDNET** In this section, we describe the approach we followed in order to expand DepecheMood and build EmoWordNet. DepecheMood consists of 37,771 lemmas along with their corresponding POS tags where each entry is appended with scores for 8 emotion labels: afraid, amused, angry, annoyed, don't care, happy, inspired and sad. Three variations of score representations exist for DepecheMood. We select to expand the DepecheMood variation with normalized scores since this variation performed best according to the presented results in<cite> (Staiano and Guerini, 2014)</cite> .",
  "y": "differences extends"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_5",
  "x": "For the regression task, a score between 0 and 1 is provided for each emotion. For the classification task, a threshold is applied on the emotion scores to get a binary representation of the emotions: if the score of a certain emotion is greater than 0.5, the corresponding emotion label is set to 1, otherwise it is 0. The emotion labels used in the dataset correspond to the six emotions of the Ekman model (Ekman, 1992) while those in EmoWordNet, as well as DepecheMood, follow the ones provided by Rappler Mood Meter. We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_6",
  "x": "Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> . One important aspect of the extrinsic evaluation was checking the coverage of EmoWordNet against SemEval dataset. In order to compute coverage, we performed lemmatization of the news headlines using WordNet lemmatizer available through Python NLTK package. We excluded all words with POS tags different than noun, verb, adjective and adverb. EmoWordNet achieved a coverage of 68.6% while DepecheMood had a coverage of 67.1%.",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_7",
  "x": "We then removed all terms that did not belong to any of the four POS tags: noun, verb, adjective and adverbs. For features computation, we considered two variations: the sum and the average of the emotion scores for the five emotion labels that overlapped between EmoWordNet and SemEval dataset. Using average turned out to perform better than when using sum for both lexicons. As stated in<cite> (Staiano and Guerini, 2014)</cite> paper, 'Disgust' emotion was excluded since there was no corresponding mapping in EmoWordNet/DepecheMood. The first evaluation consisted of measuring Pearson Correlation between the scores computed using the lexicons and those provided in SemEval.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_0",
  "x": "The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Malouf and van Noord, 2004; Kaplan et al., 2004; . Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; . An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) . Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_1",
  "x": "Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. <cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) .",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_2",
  "x": "This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) . However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> . In the case of supertagging of Weighted CDG , parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_4",
  "x": "Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in <cite>(Ninomiya et al., 2006)</cite> . The probabilities of <cite>their model</cite> are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but <cite>their model</cite> was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions. ---------------------------------- **HPSG AND PROBABILISTIC MODELS**",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_8",
  "x": "Our model is formally defined as follows: combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution.",
  "y": "uses"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_9",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_10",
  "x": "In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_11",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_12",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_13",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_14",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_15",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_16",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "differences similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_17",
  "x": "Our models increased the parsing accuracy. 'our model 1' was around 2.6 times faster and had around 2.65 points higher F-score than 's model. 'our model 2' was around 2.3 times slower but had around 2.9 points higher F-score than 's model. We must admit that the difference between our models and <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite> was not as great as the difference from 's model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_18",
  "x": "posed a technique for efficient HPSG parsing with supertagging and CFG filtering. Their results with the same grammar and servers are also listed in the lower half of Table 4 . They achieved drastic improvement in efficiency. Their parser ran around 6 times faster than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.' Instead, our models achieved better accuracy. 'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_19",
  "x": "The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module. Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser. On the other hand, <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite> uses the supertagger as an external module. Once the parser acquires the supertagger's outputs, the n-gram information never goes in and out of the kernel. This advantage of <cite>Ninomiya et al. (2006)</cite><cite>'s model</cite> can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_23",
  "x": "We proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for HPSG. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_24",
  "x": "In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS ngram as defined in the CCG/HPSG/CDG supertagging. We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_25",
  "x": "We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser. In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_0",
  "x": "License details: http:// creativecommons.org/licenses/by/4.0/ 2011; Hoffart et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) . (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities. The central idea is that the referent entities of some mentions in a document might in turn introduce useful information to link other mentions in that document due to the semantic relatedness among them. For example, the appearances of \"Manchester\" and \"Chelsea\" as the football clubs in a document would make it more likely that the entity mention \"Liverpool\" in the same document is also a football club.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_1",
  "x": "In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture. This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities.",
  "y": "background motivation"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_2",
  "x": "This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues. Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities. Given a document, we simultaneously perform linking for every entity mention from the beginning to the end of the document. For each entity mention, we utilize convolutional neural networks (CNN) to obtain the distributed representations for the entity mention as well as its target candidates. These distributed representations are then used for two purposes: (i) computing the local similarities for the entity mention and target candidates, and (ii) functioning as the input for the recurrent neural networks (RNN) that runs over the entity mentions in the documents.",
  "y": "background similarities"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_3",
  "x": "In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function<cite> (Francis-Landau et al., 2016)</cite> . Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b) ), we utilize the set L of multiple window sizes to parameterize the convolution operation. Each window size l \u2208 L corresponds to a convolution matrix M l \u2208 R v\u00d7lh of dimensionality v. Eventually, the concatenation vectorx of the resulting vectors for each window size in L would be used as the distributed representation for where is the concatenation operation over the window set L and w i:(i+l\u22121) is the concatenation vector of the given word vectors. For convenience, lets i ,c i ,",
  "y": "background uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_4",
  "x": "We employ the local similarities \u03c6 local (m i , p ij ) from<cite> (Francis-Landau et al., 2016)</cite> , the state-of-the-art neural network model for EL. In particular: In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively. F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL.",
  "y": "background uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_5",
  "x": "In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively. F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> . , on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij .",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_6",
  "x": ", on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij . In particular: The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **GLOBAL SIMILARITIES**",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_7",
  "x": "---------------------------------- **DATASETS** Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST. It is also used in (Fahrni and Strube, 2014) and (Durrett and Klein, 2014) . ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English.",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_8",
  "x": "ii) CoNLL-YAGO (Hoffart et al., 2011 ): This corpus is originally from the CoNLL 2003 shared task of named entity recognition for English. iii) WP (Heath and Bizer, 2011) : This dataset consists of short snippets from Wikipedia. iv) WIKI : This dataset contains 10,000 randomly sampled Wikipedia articles. The task is to disambiguate the links in each article 4 . For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison<cite> (Francis-Landau et al., 2016)</cite>.",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_9",
  "x": "Finally, we pre-train the word embedings on the whole English Wikipedia dump using the word2vec toolkit (Mikolov et al., 2013) . The training parameters are set to the default values in this toolkit. The dimensionality of the word embeddings is 300. Note that every parameter and resource in this work is either taken from the previous work (Nguyen and Grishman, 2016b;<cite> Francis-Landau et al., 2016)</cite> or selected by the development data. ----------------------------------",
  "y": "background similarities"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_10",
  "x": "---------------------------------- **COMPARING TO THE PREVIOUS WORK** This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_11",
  "x": "This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump. We also include the performance of the systems on the December 2014 Wikipedia dump that was used and provided by<cite> (Francis-Landau et al., 2016)</cite> for further and compatible comparison. ----------------------------------",
  "y": "background similarities"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_12",
  "x": "These invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others. For EL, we hypothesize that the global coherence is an effective domain-independent feature that would help to improve the crossdomain performance of the models. The intuition is that the entities mentioned in a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach<cite> (Francis-Landau et al., 2016)</cite> . ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_13",
  "x": "We take half of bc as the development set and use the remaining data for testing. We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013) . Table 3 compares Global-RNN with the neural network EL model in<cite> (Francis-Landau et al., 2016)</cite> , the best reported model on the ACE dataset in the literature 8 . In this table, the models are trained on the source domain news, and evaluated on news itself (in-domain performance) (via 5-fold cross validation) as well as on the 4 target domains bc, cts, wl, un (out-of-domain performance The first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in-domain performance. In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_14",
  "x": "In particular, the performance gap between the in-domain performance and the the worst out-of-domain performance (on the domain wl) is up to 10%, thus indicating the mismatches between the source and the target domains for EL. Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ---------------------------------- **EVALUATION**",
  "y": "background differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_15",
  "x": "Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007) , Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) , Point-wise Mutual Information measures , integer linear programming (Cheng and Roth, 2013) , PageRank (Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) , stacked generalization (He et al., 2013a) , to name a few. The entity linking techniques and systems have been actively evaluated at the NIST-organized Text Analysis Conference (Ji et al., 2014) . Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoising Auto-encoders. Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while <cite>Francis-Landau et al. (2016)</cite> combine CNN-based representations with sparse features to improve the performance.",
  "y": "background differences"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_0",
  "x": "While vocabulary mismatch occurs within the realm of one language, naturally this mismatch occurs across different languages. Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (Mimno et al., 2009;<cite> Platt et al., 2010)</cite> . Aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc. The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to find close matches. Many widely used techniques do not scale efficiently, however, as the size of the document collection grows.",
  "y": "motivation"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_1",
  "x": "Aside from their widespread use on monolingual text, topic models have also been used to model multilingual data (Boyd-Graber and Blei, 2009;<cite> Platt et al., 2010</cite>; Jagarlamudi and Daum\u00e9, 2010; Fukumasu et al., 2012) , to name a few. In this paper, we focus on the Polylingual Topic Model, introduced by Mimno et al. (2009) . Given a multilingual set of aligned documents, the PLTM assumes that across an aligned multilingual document tuple, there exists a single, tuple-specific, distribution across topics. In addition, PLTM assumes that for each language-topic pair, there exists a distribution over words in that language \u03b2 l . As such, PLTM assumes that the multilingual corpus is created through a generative process where first a document tuple is generated by drawing a tuple-specific distribution over topics \u03b8 1 which, as it is the case with LDA, is drawn from a Dirichlet prior \u03b8 \u223c Dir (\u03b1) .",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_2",
  "x": "As stated earlier, the Euclidean based representation is computed using well established approximation approaches and in our case we will use two such approaches: the Exact Euclidean LSH (E2LSH) (Andoni et al., 2005) ---------------------------------- **EFFICIENT APPROXIMATE TRANSLATION DETECTION** Mapping multilingual documents into a common, language-independent vector space for the purpose of improving machine translation (MT) and performing cross-language information retrieval (CLIR) tasks has been explored through various techniques. Mimno et al. (2009) introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently,<cite> Platt et al. (2010)</cite> proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI).",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_3",
  "x": "In this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time. We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection.",
  "y": "differences"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_4",
  "x": "As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_5",
  "x": "For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection. ---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> . That paper used the Europarl (Koehn, 2005) (Mimno et al., 2009) , these performance comparisons are not done on the same training and test sets-a gap that we fill below.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_6",
  "x": "We refer to this code implementation as hash map implementation. ---------------------------------- **EVALUATION TASK AND RESULTS** Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs (out of the whole test set) that were discovered at rank one. This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_7",
  "x": "Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection. Shown in Table 1 are results across the four different measurements for all four PLTM models. When using regular JS divergence, our PLTM model with 200 topics performs the best with 99.42% of the top one ranked candidate translation documents being true translations. When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%. We believe that this is due to the small amount of error in the search introduced by ANN, due to its approximate nature, which for this task yields positive results.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_8",
  "x": "On the same data set, <cite>(Platt et al., 2010)</cite> report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference. Shown in Table 2 are the relative differences in time between all pairs JS divergence, approximate kd-trees and LSH based Hellinger distance with different value of R. Rather than showing absolute speed numbers, which are often influenced by the processor configuration and available memory, we show relative speed improvements where we take the slowest running configuration as a referent value. In our case we assign the referent speed value of 1 to the configuration with T=500 and allpairs JS computation. Results shown are based on comparing running time of E2LSH and ANN against the all-pairs similarity comparison implementation that uses hash tables to store all documents in the bilingual collection which is significantly faster than the other code implementation. For the approximate, LSH based, Hellinger distance with T=100 we obtain a speed improvement of 24.2 times compared to regular all-pairs JS divergence while maintaining the same performance compared to Hellinger distance metric and insignificant loss over all-pairs JS divergence.",
  "y": "extends differences"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_0",
  "x": "However, the DS assumption is too strong and may introduce noise such as false negative samples due to missing facts in the knowledge base. In this paper, we propose relation extraction models and a new dataset to improve RE. We define 'instance' as a sentence containing an entity-pair, and 'instance set' as a set of sentences containing the same entity-pair. It was observed by<cite> [Zeng et al., 2015]</cite> that 50% of the sentences in the Riedel2010 Distant Supervision dataset [Riedel et al., 2010] , a popular DS benchmark dataset, had 40 or more words in them. We note that not all the words in these long sentences contribute towards expressing the given relation.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_1",
  "x": "**BACKGROUND** Relation Extraction: A relation is defined as a semantic property between a set of entities {e k }. In our task, we consider binary relations where k \u2208 [1, 2], such as Born In(Barack Obama, Hawaii). Given a set of sentences S = {s i }; i \u2208 [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN:<cite> [Zeng et al., 2015]</cite> proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction. The Success of the relation extraction task depends on extracting the right structural features from the sentence containing the entity-pair.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_2",
  "x": "For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> . The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows:",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_3",
  "x": "For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> . The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows:",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_4",
  "x": "The u i,j,qk are normalized using a softmax function to generate a i,j,qk , the attention scores for a given word. Similar to the PCNN model in Section 2.1, the attention weighted word embeddings are pooled using piecewise pooling method to generate s ea \u2208 R 1\u00d73g dimensional sentence embeddings. The output from the PCNN layer and the entity attention layers are concatenated and then passed through a linear layer to obtain probabilities for each relation. The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original [Shen and Huang, 2016] model (a) The EA processes a set of sentences. It uses PCNN<cite> [Zeng et al., 2015]</cite> assumption to select the sentence with highest probability of any relation.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_5",
  "x": "The performance of each model is evaluated on a test set using Precision-Recall (PR) curve. Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN)<cite> [Zeng et al., 2015]</cite> and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) [Lin et al., 2016] . Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models. Model Parameters: The parameters used for the various models are summarized in Table 4 . Word embeddings are initialized using the Word2Vec vectors from NYT dataset, similar to [Lin et al., 2016] .",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_6",
  "x": "Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following<cite> [Zeng et al., 2015]</cite> and used author provided results and implementation for NRE baseline. The EA and BGWA models were developed in PyTorch 2 . We use SGD algorithm with dropout [Srivastava et al., 2014] for model learning. The experiments were run on GeForce GTX 1080 Ti using NVIDIA-CUDA.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_7",
  "x": "However, PCNN method used only one sentence in the instance-set to predict the relation label and for backpropagation. [Lin et al., 2016] improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction. [ <cite>Zheng et al., 2016]</cite> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction. Recently, work by [Ye et al., 2016] exploit the connections between relation (class ties) to improve relation extraction performance.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_8",
  "x": "[Weston et al., 2013] proposes a joint-embedding model for text and KB entities where the known part of the KB is utilized as part of the supervision signal. [Han and Sun, 2016] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using Markov logic networks. [Nagarajan et al., 2017] uses inter-instance-set couplings for relation extraction in multi-task setup to improve performance. Attention models learn the importance of a feature in the supervised task through back-propogation. Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction<cite> [Zheng et al., 2016]</cite> etc.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_0",
  "x": "Dependent adjacency pairs of these acts are then identified through \u03c7 2 analysis, and hidden Markov modeling is applied to the observed sequences to induce a descriptive model of the dialogue structure. ---------------------------------- **INTRODUCTION** Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., Bangalore et al., 2006) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) .",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_1",
  "x": "Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al., 2000) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts. This approach operates on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in a dialogue mode that characterizes the nature of the exchanges between tutor and student. In our model, a dialogue mode is defined by a probability distribution over the observed symbols (e.g., dialogue acts and adjacency pairs).",
  "y": "background motivation"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_2",
  "x": "Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study. ---------------------------------- **1** The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & Sacks, 1973) , and adjacency pair analysis has illuminated important phenomena in tutoring as well <cite>(Forbes-Riley et al., 2007)</cite> . For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_0",
  "x": "We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment<cite> (Dong et al., 2014)</cite> . We show a clear state-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task. 2 2 Related Work: Target-dependent Sentiment Classification on Twitter The 2015 Semeval challenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016) , named Tweester, also performs on tweet level sentiment classification.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_1",
  "x": "Recently Vargas et al. (2016) analysed the differences between the overall and target-dependent sentiment of tweets for three events containing 30 targets, showing many significant differences between the corresponding overall and target-dependent sentiment labels, thus confirming that these are distinct tasks. Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work<cite> (Dong et al., 2014)</cite> used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees. Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) .",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_2",
  "x": "Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees. Their manually annotated dataset contains only one target per tweet and has since been used for benchmarking by several subsequent studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_3",
  "x": "On the other hand, they achieved a similar score of \u03ba = 0.341 (z = 77.7, p < 0.0001) (fair agreement) when annotating the sentiment of the resulting targets. It is worth noting that the sentiment annotation for each target also involves choosing among not only positive/negative/neutral but also a fourth category 'doesnotapply'. The resulting dataset contains 4,077 tweets, with an average of 3.09 entity mentions (targets) per tweet. As many as 3,713 tweets have more than a single entity mention (target) per tweet, which makes the task different from 2015 Semeval 10 subtask C (Rosenthal et al., 2015) and a target-dependent benchmarking dataset of<cite> Dong et al. (2014)</cite> where each tweet has only one target annotated and thus one sentiment label assigned. The number of targets in the 4,077 tweets to be annotated originally amounted to 12,874.",
  "y": "differences"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_4",
  "x": "vector as shown in (2), where P (X) presents a list of k different pooling functions on an embedding matrix X. Not only does this proposed framework make the learning process efficient without labor intensive manual feature engineering and heavy architecture engineering for neural models, it has also shown that complex syntactic and semantic information can be effectively drawn by simply concatenating different types of context together without the use of deep learning (other than pretrained word embeddings). Data set: We evaluate and compare our proposed system to the state-of-the-art baselines on a benchmarking corpus<cite> (Dong et al., 2014</cite> ) that has been used by several previous studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) . This corpus contains 6248 training tweets and 692 testing tweets with a sentiment class balance of 25% negative, 50% neutral and 25% positive. Although the original corpus has only annotated one target per tweet, without specifying the location of the target, we expand this notion to consider cases where the target entity may appear more than once at different locations in the tweet, e.g.: \"Nicki Minaj has brought back the female rapper. -really? Nicki Minaj is the biggest parody in popular music since the Lonely Island.\"",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_6",
  "x": "**EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_7",
  "x": "Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well. Among the target-dependent systems neural network baselines have shown varying results. The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ).",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_8",
  "x": "The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ). TD-LSTM and TC-LSTM from Tang et al. (2016a) model left-target-right contexts using two LSTM neural networks and by doing so incorporate target-dependent information. TD-LSTM uses two LSTM neural networks for modeling the left and right contexts respectively. TC-LSTM differs from (and outperforms) TD-LSTM in that it concatenates target word vectors with embedding vectors of each context word. We also test the Gated recurrent neural network models proposed by Zhang et al. (2016) on the same dataset.",
  "y": "background"
 },
 {
  "id": "c57e98c9c07dd5d8653e172136c901_0",
  "x": "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; <cite>Seo et al., 2016)</cite> , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_0",
  "x": "This approach has achieved promising initial results [6] <cite>[7]</cite> [8] [9] 14] , but many questions remain. Two outstanding questions are the best method of learning verb tensors from a corpus, and the best sentence space for a variety of different tasks. This paper presents work in progress which addresses both of these questions. It compares two methods for learning verb representations, the distributional model of <cite>[7]</cite> in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space. A variety of methods for reducing the noun space and composing the verb, subject, and object representations are investigated.",
  "y": "motivation"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_2",
  "x": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] . Below we describe two ways of learning a verb matrix. In the regression method, the learnt matrix consists of parameters from a plausibility classifier.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_3",
  "x": "In the definition of the functional approach to compositional distributional semantics [1] [2] [3] [4] , a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a \"topic space\" where the dimensions correspond to co-occurrence features; we use a reduced space resulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] .",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_4",
  "x": "All of the plausibility data is used for training and validation, as we employ only the resulting verb matrix to produce a sentence space for transitive verb phrases in the test data. The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping. ---------------------------------- **DISTRIBUTIONAL (DIST)** Following <cite>[7]</cite> , we generate a K \u00d7 K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data:",
  "y": "uses"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_6",
  "x": "For the verb disambiguation task we use the GS2011 dataset <cite>[7]</cite> . This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses. For example, the verb draw has senses that correspond to attract and depict. The SVO triple report draw attention has high similarity to report attract attention, but low similarity to report depict attention. Conversely, child draw picture has high similarity to child depict picture, but low similarity to child attract picture.",
  "y": "uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_0",
  "x": "With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017) . Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017;<cite> Xing et al., 2017)</cite> . Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message 1 \"How should I tell my mom?\", as input, to a single-turn ----------------------------------",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_1",
  "x": "\u2022 First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU<cite> (Xing et al., 2017)</cite> or unidirectional GRU (Tian et al., 2017 ). One is the attention-based approach<cite> (Xing et al., 2017)</cite> , the other is the sequential integration approach (Tian et al., 2017) . \u2022 Utterance Representations: Bidirectional GRU vs. Unidirectional GRU<cite> Xing et al. (2017)</cite> utilized a bidirectional GRU and a word-level attention mechanism to transfer word representations to utterance representations. \u2022 Inter-utterance Representations: Attention vs. Sequential Integration<cite> Xing et al. (2017)</cite> proposed a hierarchical attention mechanism to feed the utterance representations to a backward RNN to obtain contextual representation.",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_2",
  "x": "For utterance representation, we consider the advantages of the two state-of-the-art approaches to encoding contextual information for context-sensitive response generation <cite>(Xing et al., 2017</cite>; Tian et al., 2017) . weighing the importance of utterances for generating open-domain conversational responses<cite> (Xing et al., 2017)</cite> , we thus model the inter-utterance representation to obtain the context vector in two measures, namely static and dynamic attention, as shown in Figure 2 .",
  "y": "similarities uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_3",
  "x": "The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows: The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation. Rather than use a hierarchical attention neural network<cite> (Xing et al., 2017)</cite> to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_4",
  "x": "\u2022 VHRED: The augmented HRED model, which incorporates a stochastic latent variable at utterance level for encoding and decoding, is proposed by Serban et al. (2017b) . \u2022 CVAE: The conditional variational autoencoder based approach, which is proposed by , to learn context diversity for conversational responses generation. \u2022 WSI and HRAN are proposed by Tian et al. (2017) and<cite> Xing et al. (2017)</cite> respectively. We detailed describe and compare the two models in Section 2.1 and 2.2 and their frameworks are shown in Figure 1 . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_0",
  "x": "**ABSTRACT** Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce <cite>multisource projection</cite> <cite>(Tyers et al., 2018)</cite> , in which dependency trees of multiple sources are combined.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_1",
  "x": "In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl-edge can be induced from external corpora such as by learning distributed word representations (Mikolov et al., 2013; Al-Rfou' et al., 2013) and more recent contextualized variants Devlin et al., 2019) . This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by:",
  "y": "extends"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_2",
  "x": "1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Vilares et al., 2016) , we investigate whether additional improvements can be made by: 1. using a single polyglot 2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this paper, source language and target language always refer to the projection, not the direction of translation. 2 We adopt the same terminology used in Mulcaire et al. (2019) , who use the term cross-lingual transfer to describe methods involving the use of one or more source languages to process a target language. They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_3",
  "x": "They reserve the term polyglot learning for training a single model on multiple languages and where parameters are shared between languages. For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections. The former differs from the approach of <cite>Tyers et al. (2018)</cite> , who use multiple discrete, monolingual models to parse the translated sentences, whereas in this work we use a single model trained on multiple source treebanks. The latter differs from training on the target treebank produced by multi-source projection in that the information of the individual projections is still available and training data is not reduced to cases where all source languages provide a projection.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_4",
  "x": "When we extended multi-treebank learning to the target side, we experienced additional gains for all cases. Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_5",
  "x": "<cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments. The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_6",
  "x": "The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank. The resulting trees are projected back to the Faroese sentences using word alignments. The four trees for each sentence are combined into a graph with edge scores one to four (the number of trees that support them), from which a single tree per sentence is produced using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) . The resulting trees make up a synthetic treebank for Faroese which is then used to train a Faroese parsing model. The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_7",
  "x": "In contrast to <cite>Tyers et al. (2018)</cite> , they translate a target sentence and project the source parse tree back to the target during test time instead of using this approach to obtain training data for the target language. leverage massively multilingual parallel corpora such as translations of the Bible and web-scraped data from the Watchtower Online Library website 3 for low-resource POS tagging and dependency parsing using annotation projection. They project weight matrices (as opposed to decoded dependency arcs) from multiple source languages and average the matrices weighted by word alignment confidences. They then decode the weight matrices into dependency trees on the target side, which are then used to train a parser. This approach utilizes dense information from multiple source languages, which helps reduce noise from source side predictions but to the best of our knowledge, the source-side parsing models learn information between source languages independently and the cross-lingual interaction only occurs when projecting the edge scores into multi-source weight matrices.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_8",
  "x": "We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences.",
  "y": "similarities uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_9",
  "x": "We outline the process used for creating a synthetic treebank for cross-lingual dependency parsing. We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models. ----------------------------------",
  "y": "differences similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_10",
  "x": "---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_11",
  "x": "---------------------------------- **TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_12",
  "x": "For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l. There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_13",
  "x": "There also exists translation systems from Norwegian Bokm\u00e5l to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_14",
  "x": "We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2. 9 Source Treebanks We use the Universal Dependencies v2.2 treebanks to train our source parsing models. This is the version used for the 2018 CoNLL shared task on Parsing Universal Dependencies . Source Tagging and Parsing Models In order for our parsers to work well with predicted POS tags, we follow the same steps as used in the 2018 CoNLL shared task for creating training and development treebanks with automatically predicted POS tags (henceforth referred to as silver POS).",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_15",
  "x": "In our experiments, we simply 10 We observe slightly lower POS tagging scores on fully annotated test sets than UDPipe, which uses gold lemmas, XPOS and morphological features to predict the UPOS label and therefore cannot be applied to the translated text without also building predictors for these features. 11 https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-1989 use the union of the word embeddings and average the word vector for words that occur in more than one language. Future work should explore crosslingual word embeddings with limited amount of parallel data or use aligned contextual embeddings as in (Schuster et al., 2019) . Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_16",
  "x": "Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_17",
  "x": "Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_18",
  "x": "Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_19",
  "x": "In this section, we describe our experiments, which include a replication of the main findings of <cite>Tyers et al. (2018)</cite> , using AllenNLP for POS tagging and parsing instead of UDPipe (Straka and Strakov\u00e1, 2017 Figure 3 : Multi-source projection. The source language is listed in brackets. ---------------------------------- **DETAILS** The hyper-parameters of our POS tagging and parsing models are given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_20",
  "x": "The multi-source approach was not that effective in our case and some individual better sources were able to surpass this combination approach. One could argue that this may be due to the lower amount of training data when using the multisource treebank. We test this hypothesis by only including those sentences which contributed to multi-source projection in the single-source synthetic treebanks. The results are given in Table 5. Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences.",
  "y": "differences uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_21",
  "x": "However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not. In this way, our monolingual models are stronger and likely do not benefit as much from voting. The second result column (MULTI) of Table 4 shows the effect of training a multi-treebank POS tagger and parser on the Faroese treebanks created by each of the four source languages as well as the treebank which is produced by multi-source projection.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_22",
  "x": "We see improvements over the single treebank setting for all cases. 15 Table 6 places our systems in the context of previous results on the same Faroese test set. The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_23",
  "x": "15 Table 6 places our systems in the context of previous results on the same Faroese test set. The highest scoring system in the 2018 CoNLL shared task was that of Rosa and Mare\u010dek (2018) who achieved a LAS score of 49.4 on the Faroese test set. Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_0",
  "x": "---------------------------------- **INTRODUCTION** The Vision-and-Language Navigation (VLN) task (Anderson et al., 2018) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror). The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b;<cite> Ma et al., 2019)</cite> have demonstrated large gains in accuracy on the VLN task.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_1",
  "x": "A number of methods for the VLN task have been recently proposed. Wang et al. (2018) use model-based and model-free reinforcement learning to learn an environmental model and optimize directly for navigation success. Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_2",
  "x": "Wang et al. (2018) use model-based and model-free reinforcement learning to learn an environmental model and optimize directly for navigation success. Fried et al. (2018b) use a separate instruction generation model to synthesize new instructions as data augmentation during training, and perform pragmatic inference at test time. Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from.",
  "y": "motivation"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_3",
  "x": "Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018 ) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures <cite>(Ma et al., 2019</cite>; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. in a connectivity graph determined by line-of-sight in the physical environment. See the top row of Fig. 1 for a top-down environment illustration.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_4",
  "x": "Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> . We also explore two approaches to make the agents better utilize their visual inputs.",
  "y": "uses differences"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_5",
  "x": "When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe. In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of<cite> Ma et al. (2019)</cite> . These models obtained stateof-the-art results on the R2R dataset. Both models are based on the encoder-decoder approach (Cho et al., 2014 ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context. Compared to the SF model, the SM model introduces an improved visual-textual co-attention mechanism and a progress monitor component.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_6",
  "x": "In our experiments, we use models trained without data augmentation, and during inference predict actions with greedy search (i.e. without beam search, pragmatic, or progress monitorbased inference). For SF, we use the publicly released code. For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_7",
  "x": "We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split. For both the SF and SM models, we train two versions of the agents, using either the studentforcing or teacher-forcing approaches of Anderson et al. (2018) 1 , and select the best training snapshot on the val-seen split. 2 The results are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_8",
  "x": "We then use the same visual attention mechanism as in Fried et al. (2018b) and<cite> Ma et al. (2019)</cite> to obtain an attended object representation x obj,att over these {x obj,j } vectors. We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\"). Then we train the SF model or the SM model using this object representation, with results shown in Table 2 . 3 For SF (lines 1-4), object representations substantially improve generalization ability: using either the object representation (\"Obj\") or the combined representation (\"RN+Obj\") obtains higher success rate on unseen environments than using only the ResNet features (\"RN\"), and the combined representation (\"RN+Obj\") obtains the highest overall performance. For SM (lines 5-8), Table 2 : Success rate (SR) of agents with different visual inputs on the R2R dataset (\"RN\": ResNet CNN, \"Obj\": objects, \"no vis.\": no visual representation). Models: Speaker-Follower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_11",
  "x": "Thus, splitting the prediction task across several models, where each has access to a different input modality, is an effective way to inject an inductive bias that encourages the model to ground into each of the modalities. Supplementary material to \"Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation\" ---------------------------------- **A DETAILS ON THE COMPARED VLN MODELS** The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model<cite> (Ma et al., 2019)</cite> which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent.",
  "y": "uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_0",
  "x": "Our models are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best model yields state-of-the-art performance which advances the BLEU 4 score of existing best models from 16.85 to 21.04. ---------------------------------- **INTRODUCTION** Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_1",
  "x": "So far, the best performing result is reported in (Zhao et al., 2018) , which advances the state-of-the-art results from 13.9 to 16.8 (BLEU 4) . The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_2",
  "x": "However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance. For handling long context, the work (Zhao et al., 2018) introduces a maxout pointer mechanism with gated self-attention encoder for processing paragraphlevel input.",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_3",
  "x": "Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks. Table 1 summarizes some statistics for the compared datasets. \u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set.",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_4",
  "x": "\u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%). ---------------------------------- **IMPLEMENTATION DETAILS**",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_5",
  "x": "The <cite>(Du et al., 2017)</cite> , and SQuAD 81K is the setting of (Zhao et al., 2018) . SQuAD 73K 73240 11877 10570  SQuAD 81K 81577 8964  8964 pre-trained model uses the officially provided BERT base model (12 layers, 768 hidden dimensions, and 12 attention heads.) with a vocab of 30522 words. Dropout probability is set to 0.1 between transformer layers. The Adamax optimizer is applied during the training process, with an initial learning rate of 5e-5. The batch size for the update is set at 28.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_6",
  "x": "---------------------------------- **TRAIN TEST DEV** ---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature.",
  "y": "similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_7",
  "x": "**MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature. The compared models in the experiment are: \u2022 NQG-RC <cite>(Du et al., 2017)</cite> : A seq2seq question generation model based on bidirectional LSTM. \u2022 PLQG (Zhao et al., 2018) : A seq2seq network which contains a gated self-attention encoder and a maxout pointer decoder to enable the capability of handling long text input.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_0",
  "x": "However, Baehrens et al. (2010) and Bach et al. (2015) ; Montavon et al. (2017) explain neural network predictions by sensitivity analysis to different input features and decomposition of decision functions, respectively. Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016) , relation extraction <cite>(Vu et al., 2016a</cite>; Miwa and Bansal, 2016; Gupta et al., 2016 Gupta et al., , 2018c , language modeling (Mikolov et al., 2010; Peters et al., 2018) , slot filling (Mesnil et al., 2015; Vu et al., 2016b) , machine translation (Bahdanau et al., 2014) , sentiment analysis (Wang et al., 2016; Tang et al., 2015) , semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d) . Past works (Zeiler and Fergus, 2014; have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons. Recent studies have investigated visualization of RNN and its variants.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_2",
  "x": "**CONNECTIONIST BI-DIRECTIONAL RNN** We adopt the bi-directional recurrent neural network architecture with ranking loss, proposed by<cite> Vu et al. (2016a)</cite> . The network consists of three parts: a forward pass which processes the original sentence word by word (Equation 1); a backward pass which processes the reversed sentence word by word (Equation 2); and a combination of both (Equation 3). The forward and backward passes are combined by adding their hidden layers. There is also a connection to the previous combined hidden layer with weight W bi with a motivation to include all intermediate hidden layers into the final decision of the network (see Equation 3).",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_3",
  "x": "<e1> demolition </e1> <e1> demolition </e1> was <e1> demolition </e1> was the <e1> demolition </e1> was the cause <e1> demolition </e1> was the cause of C-BRNN C-BRNN <e1> demolition </e1> was the cause of <e2> <e1> demolition </e1> was the cause of <e2> terror D is the hidden unit dimension. U f \u2208 R d\u00d7D and U b \u2208 R d\u00d7D are the weight matrices between hidden units and input w t in forward and backward networks, respectively; W f \u2208 R D\u00d7D and W b \u2208 R D\u00d7D are the weights matrices connecting hidden units in forward and backward networks, respectively. W bi \u2208 R D\u00d7D is the weight matrix connecting the hidden vectors of the combined forward and backward network. Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_4",
  "x": "where s \u03b8 (x) y + and s \u03b8 (x) c \u2212 being the scores for the classes y + and c \u2212 , respectively. The parameter \u03b3 controls the penalization of the prediction errors and m + and m are margins for the correct and incorrect classes. Following<cite> Vu et al. (2016a)</cite> , we set \u03b3 = 2, m + = 2.5 and m \u2212 = 0.5. ---------------------------------- **MODEL TRAINING AND FEATURES:**",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_5",
  "x": "We use word2vec (Mikolov et al., 2013) embeddings, that are updated during model training. As position features in relation classification experiments, we use position indicators (PI) (Zhang and Wang, 2015) in C-BRNN to annotate target entity/nominals in the word sequence, without necessity to change the input vectors, while it increases the length of the input word sequences, as four independent words, as position indicators (<e1>, </ e1>, <e2>, </e2>) around the relation arguments are introduced. In our analysis and interpretation of recurrent neural networks, we use the trained C-BRNN ( Figure 1 )<cite> (Vu et al., 2016a)</cite> model. ---------------------------------- **LISA AND EXAMPLE2PATTERN IN RNN**",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_6",
  "x": "To do so, we first compute N-gram for each word w t in the sentence S. For instance, a 3-gram representation of w t is given by w t\u22121 , w t , w t+1 . Therefore, an N-gram (for N=3) sequence S of words is represented as [[w t\u22121 , w t , w t+1 ] n t=1 ], where w 0 and w n+1 are PADDING (zero) vectors of embedding dimension. Following<cite> Vu et al. (2016a)</cite> , we use N-grams (e.g., tri-grams) representation for each word in each subsequence S \u2264k that is input to C-BRNN to compute P (R|S \u2264k ), where the N-gram (N=3) subsequence S \u2264k is given by, for k \u2208 [1, n]. Observe that the 3-gram tri k con- (d) LISA for S4 < e 1 > c a r < / e 1 > l e f t t h e < e 2 > p l a n t < / e 2 >",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_7",
  "x": "Given a sentence and two annotated nominals, the task of binary relation classification is to predict the semantic relations between the pairs of nominals. In most cases, the context in between the two nominals define the relationship. However,<cite> Vu et al. (2016a)</cite> has shown that the extended context helps. In this work, we focus on the building semantics for a given sentence using relationship contexts between the two nominals. We analyse RNNs for LISA and example2pattern using two relation classification datsets: (1) SemEval10 Shared Task 8 (Hendrickx Input word sequence to C-BRNN pp <e1> 0.10 <e1> demolition 0.25 <e1> demolition </e1> 0.29 <e1> demolition </e1> was 0.30 <e1> demolition </e1> was the 0.35 <e1> demolition </e1> was the cause 0.39 <e1> demolition </e1> was the cause of 0.77 <e1> demolition </e1> was the cause of <e2> 0.98 <e1> demolition </e1> was the cause of <e2> terror 1.00 <e1> demolition </e1> was the cause of <e2> terror </e2> 1.00 Table 2 : Semantic accumulation and sensitivity of C-BRNN over subsequences for sentence S1.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_8",
  "x": "We split the training data into train (6.5k) and development (1.5k) sentences to optimize the C-BRNN Relation 3-gram Patterns 5-gram Patterns 7-gram Patterns </e1> cause <e2> the leading causes of <e2> is one of the leading causes of cause-</e1> caused a the main causes of <e2> is one of the main causes of effect(e1,e2) that cause respiratory </e1> leads to <e2> inspiration </e1> that results in <e2> hardening </e2> which cause acne </e1> that results in <e2> </e1> resulted in the <e2> loss </e2> leading causes of </e1> resulted in the <e2> <e1> sadness </e1> leads to <e2> inspiration caused due to </e1> has been caused by </e1> is caused by a <e2> comet comes from the </e1> are caused by the </e1> however has been caused by the causearose from an </e1> arose from an <e2> </e1> that has been caused by the effect (e2,e1) caused by the </e1> caused due to <e2> that has been caused by the <e2> radiated from a infection </e2> results in an <e1> product </e1> arose from an <e2> in a <e2> </e1> was contained in a </e1> was contained in a <e2> box was inside a </e1> was discovered inside a </e1> was in a <e2> suitcase </e2> contentcontained in a </e1> were in a <e2> </e1> were in a <e2> box </e2> container(e1,e2) hidden in a is hidden in a <e2> </e1> was inside a <e2> box </e2> stored in a </e1> was contained in a </e1> was hidden in an <e2> envelope </e1> released by </e1> issued by the <e2> <e1> products </e1> created by an <e2> product-</e1> issued by </e1> was prepared by <e2> </e1> by an <e2> artist </e2> who produce(e1,e2) </e1> created by was written by a <e2> </e1> written by most of the <e2> by the <e2> </e1> built by the <e2> temple </e1> has been built by <e2> of the <e1> </e1> are made by <e2> </e1> were founded by the <e2> potter </e1> of the </e1> of the <e2> device the <e1> timer </e1> of the <e2> whole(e1, e2) of the <e2> </e1> was a part of </e1> was a part of the romulan componentpart of the </e1> is part of the </e1> was the best part of the </e1> of <e2> is a basic element of </e1> is a basic element of the </e1> on a </e1> is part of a are core components of the <e2> solutions put into a have been moving into the </e1> have been moving back into <e2> released into the was dropped into the <e2> </e1> have been moving into the <e2> entity-</e1> into the </e1> moved into the <e2> </e1> have been dropped into the <e2> destination(e1,e2) moved into the were released into the <e2> </e1> have been released back into the added to the </e1> have been exported to power </e1> is exported to the <e2> </e1> are used </e1> assists the <e2> eye cigarettes </e1> are used by <e2> women used by <e2> </e1> are used by <e2> <e1> telescope </e1> assists the <e2> eye instrument-</e1> is used </e1> were used by some <e1> practices </e1> for <e2> engineers </e2> agency(e1,e2) set by the </e1> with which the <e2> the best <e1> tools </e1> for <e2> </e1> set by readily associated with the <e2> <e1> wire </e1> with which the <e2> The <e1> demolition </e1> was the cause of <e2> terror </e2> and communal divide is just a way of not letting truth prevail. \u2192 cause-effect(e1,e2) The terms demolition and terror are the relation arguments or nominals, where the phrase was the cause of is the relationship context between the two arguments. Table 1 shows the examples sentences (shortened to argument1+relationship context+argument2) drawn from the development and test sets that we employed to analyse the C-BRNN for semantic accumulation in our experiments. We use the similar experimental setup as<cite> Vu et al. (2016a)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_0",
  "x": "Beyond words, several works focused on the representation of sentences [11] , documents [9] , and also knowledge bases (KBs) [3, 18] . Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_1",
  "x": "Beyond words, several works focused on the representation of sentences [11] , documents [9] , and also knowledge bases (KBs) [3, 18] . Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_2",
  "x": "In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively. While the relationalbased regularization function characterizes the word relationships which are interpreted as translations in latent semantic space of word embeddings, the categorical-based one aims at minimizing the weighted distance between words with same attributes. With experiments on text mining and NLP tasks, the authors have reported that combining these two regularization functions allows to significantly improve the quality of word representations.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_3",
  "x": "The first approach that we suggest for integrating KB within a deep neural network focuses on an enhanced representation of documents and queries as illustrated in Figure 2a . While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation <cite>[6,</cite> 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching. Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4] . With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB. On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16] .",
  "y": "differences"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_4",
  "x": "Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4] . With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB. On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16] . On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding<cite> [6]</cite> or a richer embedding representation of a KB sub-graph, as suggested in [2] . The latter presents the advantage to model the compositionality of concepts within the document.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_0",
  "x": "---------------------------------- **INTRODUCTION** Visual Question Answering (VQA) is a challenging and young research field, which can help machines achieve one of the ultimate goals in computer vision, holistic scene understanding (Yao, Fidler, and Urtasun 2012) . VQA is a computer vision task: a system is given an arbitrary textbased question about an image, and then tasked to output the text-based answer of the given question about the image. Currently, most of the state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Chen et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) only focus on how to improve accuracy.",
  "y": "motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_1",
  "x": "In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ. We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_2",
  "x": "We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) . Note that the available pretrained VQA models can be categorized into two main categories, attention-based and non-attention-based VQA models. According to the results of our experiments, we discover that attention-based models not only have the higher accuracy but also the better robustness compared with nonattention-based models. In this work, our main contributions are summarized below:",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_3",
  "x": "Finally, the experimental results are demonstrated in Section 4. ---------------------------------- **RELATED WORK** Recently, there are many papers (<cite>Antol et al. 2015</cite>; Shih, Singh, and Hoiem 2016; Chen et al. 2016; Kafle and Kanan 2016; Ma, Lu, and Li 2016; Ren, Kiros, and Zemel 2015; Zhu et al. 2016; Wu et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) have proposed methods to solve the challenging VQA task. Our VQABQ method involves in different areas in Machine Learning, Natural Language Processing (NLP) and Computer Vision.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_4",
  "x": "The detailed architecture of Module 1 also can be referred to Figure 1 . ---------------------------------- **QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_5",
  "x": "At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_6",
  "x": "At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate. The above step guarantees that our LASSO can work well.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_7",
  "x": "**QUESTION DATA PREPROCESSING** We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ).",
  "y": "similarities uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_8",
  "x": "---------------------------------- **PROBLEM FORMULATION** Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way: (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_9",
  "x": "According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) by Skip-Thought Vectors, and then we have a matrix of basic question candidates. Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027. Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_10",
  "x": "Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns. That is, the dimension of BQ matrix, called A, is 4800 by 186027. Also, we encode the given query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called b. Regarding the selection of the parameter, \u03bb, we will discuss this in Section 4. Now, we can solve the LASSO optimization problem, mentioned in the above subsection of Problem Formulation, to get the solution, x. Here, we consider the elements of the solution vector, x, as the similarity score of the corresponding BQ in the BQ matrix, A. The first element of x corresponds to the first column, i.e. the first BQ, of A. Then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding BQ to be the ranked BQ of the given query question. Intuitively, if a BQ has a larger similarity score to the given MQ, then this BQ can be considered as a question more similar to the given MQ. Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_11",
  "x": "Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET** We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_12",
  "x": "(<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ---------------------------------- **ROBUSTNESS ANALYSIS BY BASIC QUESTIONS**",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_13",
  "x": "Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection. ---------------------------------- **ROBUSTNESS ANALYSIS BY BASIC QUESTIONS** To measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance. The noise can be completely random, have a specific structure and/or be semantically relevant to the final task.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_14",
  "x": "For VQA the input is an image question pair and therefore the noise should be introduced to both. The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017;  Table 3 : MUTAN without Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_15",
  "x": "In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". Carlini and Wagner 2017; Xu, Caramanis, and Mannor 2009) . However, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review. Here we propose a novel robustness measure for VQA by introducing semantically relevant noise to the questions where we can control the strength of noisiness. First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_16",
  "x": "---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_17",
  "x": "<cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing.",
  "y": "background uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_18",
  "x": "---------------------------------- **DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_19",
  "x": "There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_20",
  "x": "Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ. Furthermore, although we rank all of the basic question candidates for each MQ, we only collect top 21 BQ to put into our BQD. The most important reason is that the similarity scores are too small after twentyfirst BQ. Regarding the limit of number of words of question input, for most of available pretrained state-of-the-art VQA models they are trained under the condition maximum number of words of input 26 words.",
  "y": "motivation uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_21",
  "x": "---------------------------------- **EVALUATION METRICS** <cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_22",
  "x": "However, in the multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where N is the total number of examples, I[\u00b7] denotes an indicator function, a i is the predicted answer and T i is an answer set of the i th example.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_23",
  "x": "However, if we compare the quality of BQ of \u03bb equal to 10 \u22125 with \u03bb equal to 10 \u22126 , we think the BQ quality of \u03bb equal to 10 \u22126 is slightly better than \u03bb equal to 10 \u22125 based on our common sense knowledge. We will put some randomly selected BQ examples from our BQD in the supplementary material for references. Note that Figure 2 : The accuracy of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_24",
  "x": "In the next subsection, we will do further analysis on the most robust HieCoAtt VQA model . iv.) Can Basic Questions Directly Help Accuracy? According to Table 6 , we know that HieCoAtt is the robustness VQA model. We want to do more advanced analysis on this model, so we claim that if the quality of Figure 3 : The accuracy decrement of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_26",
  "x": [
   "---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we propose a novel VQABQ method and Basic Question Dataset (BQD) for robustness analysis of VQA models. The VQABQ method has two main modules, Basic Question Generation Module and VQA Module. The former one can generate the basic questions for the query question, and the latter one can take an image, basic and query questions as the input and then output the text-based answer of the query question about the given image."
  ],
  "y": "uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_0",
  "x": "**INTRODUCTION** Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (Lafferty et al., 2001; Ratinov and Roth, 2009) , where conditional random fields (CRF) (Lafferty et al., 2001) has been proven as an effective framework, by taking discrete features as the representation of input sequence (Sha and Pereira, 2003; Keerthi and Sundararajan, 2007) . With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015;<cite> Ma and Hovy, 2016</cite>; Peters et al., 2017) . Features are extracted automatically through network structures including long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and convolution neural network (CNN) (LeCun et al., 1989) with distributed word representations.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_1",
  "x": "Although many authors released their code along with their sequence labeling papers (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding. In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand- crafted features to CRF framework conveniently. We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_2",
  "x": "Figure 1 shows a segment of the configuration file. It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as <cite>Ma and Hovy (2016)</cite> ), plus POS and Cap features, within 10 lines. This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ).",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_3",
  "x": "Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_4",
  "x": "This demonstrates the convenience of designing neural models using NCRF++. \u2022 Flexible with features: human-defined features have been proved useful in neural sequence labeling (Collobert et al., 2011; Chiu and Nichols, 2016) . Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_5",
  "x": "The selection can be configurated through word seq feature in Figure 1 . The input of the word sequence layer is a word representation, which may include word embeddings, character sequence representations and handcrafted neural features (the combination depends on the configuration file). The word sequence layer can be stacked, building a deeper feature extractor. \u2022 Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (Huang et al., 2015; Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017) . Bidirectional RNNs are supported to capture the left and right contexted information of each word.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_6",
  "x": "For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_7",
  "x": "For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_8",
  "x": "As shown in Table 1 , \"WCNN\" based models consistently underperform the \"WLSTM\" based models, showing the advantages of LSTM on capturing global features. Character information can improve model performance significantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to \"CLSTM+WLSTM+CRF\" and \"CCNN+WLSTM+CRF\" of our models) (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017; Peters et al., 2017) . Our implementations can achieve comparable results, with better NER and chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the robustness of our implementation.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_0",
  "x": "Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics). It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance<cite> (Zadeh et al., 2018a)</cite> . While multimodal approaches to sentiment analysis are relatively new in NLP, multimodal emotion recognition has long been a focus of Affective Computing. For example, De Silva and Ng (2000) combined facial expressions and speech acoustics to predict the Big-6 emotion categories (Ekman, 1992) . Emotions and sentiments are closely related concepts in Psychology and Cognitive Science research, and are often used interchangeably.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_1",
  "x": "We use the CMU Multimodal Data Software Development Kit (SDK)<cite> (Zadeh et al., 2018a)</cite> to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets. 1 We implement the sentiment analysis models using the Keras deep learning library (Chollet et al., 2015) . ---------------------------------- **MULTIMODAL FEATURES** For the vocal modality, we use the COVAREP feature set provided by the SDK.",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_2",
  "x": "In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs. In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 . We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion. This is because in previous studies (e.g., <cite>Zadeh et al. (2018a)</cite> ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective. 4",
  "y": "background similarities"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_3",
  "x": "To evaluate the performance of sentiment score prediction, following previous work<cite> (Zadeh et al., 2018a)</cite> , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant. We also include random prediction as a baseline and the human performance reported by . ---------------------------------- **UNIMODAL EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_4",
  "x": "3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ). This suggests that lexical information remains the most effective for sentiment analysis. On each modality, the best performance is achieved by a multi-task learning model. This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning. In multi-task learning, the main task gains additional information from the auxillary tasks.",
  "y": "similarities"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_0",
  "x": "**INTRODUCTION** Word embeddings have revolutionized natural language processing by representing words as dense real-valued vectors in a low dimensional space. Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) .",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_1",
  "x": "In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) . Similarly, understanding the geometry of word embeddings is another area of active research (Mimno and Thompson, 2017) . Researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from (Yin and Shen, 2018) answering the question of optimal dimensionality selection. In contrast to previous work, we explore the dimensional properties of existing pre-trained word embeddings through their principal components. Specifically, our contributions are as follows:",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_2",
  "x": "2. We demonstrate that the amount of variance captured by the principal components is a poor representative for the downstream performance of the embeddings constructed using the very same principal components. 3. We investigate the reasons behind the aforementioned result through syntactic information based dimensional linguistic probing tasks and demonstrate that the syntactic information captured by a principal component is independent of the amount of variance it explains. 4. We point out the limitations of applying variance based post-processing <cite>(Mu and Viswanath, 2018)</cite> and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement. In Section 2 we discuss dimensional properties of word embeddings. In Section we 3 conduct a variance based analysis by measuring performance of word embeddings on downstream tasks.",
  "y": "motivation"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_3",
  "x": "**THE POST PROCESSING ALGORITHM (PPA)** In this section, we first describe and then evaluate the post-processing algorithm (PPA) proposed in <cite>(Mu and Viswanath, 2018)</cite> , which achieves high scores on Word and Semantic textual similarity tasks. The algorithm removes the projections of top principal components from each of the word vectors, making the individual word vectors more discriminative (Refer to Algorithm 1 for details). Algorithm 1: Post Processing Algorithm PPA(X, D) Data: Embedding Matrix X, Threshold Parameter D Result: Post-Processed Word Embedding Matrix X 1 X = X -X ; // Subtract Mean Embedding / * Compute PCA Components * / 2 ui = PCA(X), where i = 1, 2 . .",
  "y": "uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_0",
  "x": "Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) <cite>(Dong et al., 2014)</cite> , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF . Both approaches found that combining the two tasks did not improve results compared to treating the two tasks separately, apart from when considering POS and NEG when the joint task performs better. Finally, created an attention RNN for this task which was evaluated on two very different datasets containing written and spoken (video-based) reviews where the domain adaptation between the two shows some promise.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_1",
  "x": "This has serious implications for generalisability of methods. We correct that limitation in our study. There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset <cite>(Dong et al., 2014)</cite> and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_2",
  "x": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family. A second paper, by Barnes et al. (2017) compares seven approaches to (document and sentence level) sentiment analysis on six benchmark datasets, but does not systematically explore reproduction issues as we do in our paper. ----------------------------------",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_3",
  "x": "Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset. An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans. As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of<cite> Dong et al. (2014)</cite> and Mitchell et al. (2013) . The only dataset that has a small difference between the number of unique sentiments per sentence is the Wang et al. (2017) ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_4",
  "x": "Vo and Zhang (2015) created the first NP method for TDSA. All of their experiments are performed on<cite> Dong et al. (2014)</cite> Twitter data set.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_5",
  "x": "However we agree that combining the two vectors is beneficial and that the rank of methods is the same in our observations. ---------------------------------- **SCALING AND FINAL MODEL COMPARISON** We test all of the methods on the test data set of<cite> Dong et al. (2014)</cite> and show the difference between the original and reproduced models in figure 2 . Finally, we show the effect of scaling using Max Min and not scaling the data.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_6",
  "x": "Wang et al. (2017) extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word. Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2. TDParse the feature of TDParse-and the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the<cite> Dong et al. (2014)</cite> and Wang et al. (2017) Twitter datasets where we train and test on the previously specified train and test splits.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_7",
  "x": "TCLSTM same as the TDLSTM method but each input word vector is concatenated with vector of the target word. All of the methods outputs are fed into a softmax activation function. The experiments are performed on the<cite> Dong et al. (2014)</cite> dataset where we train and test on the specified splits. For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" (Tang et al., 2016a) as we were unsure what this meant. With regards to the number of epochs trained, we used early stopping with a patience of 10 and allowed 300 epochs.",
  "y": "similarities uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_0",
  "x": "**INTRODUCTION** Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_1",
  "x": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_2",
  "x": "While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) . Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014) , feature embeddings , or neural network parsers (Chen and Manning, 2014) . Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies.",
  "y": "motivation differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_3",
  "x": "Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory. Moreover, unlike conjoined features, link embeddings allow a tractable set of accurate per-dimension features, making the feature set even smaller and the featuregeneration process orders of magnitude faster (than hierarchical clustering features). At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly.",
  "y": "differences motivation"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_4",
  "x": "To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec. 2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to <cite>Bansal et al. (2014)</cite> and Levy and Goldberg (2014) . The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc. To this end, we first parse the BLLIP corpus (minus the PTB portion) 3 using the baseline MSTParser (McDonald et al., 2005b) . Next, for each predicted link, we create a tuple, consisting of the parent-child pair p-c (concatenated as a single unit, same as p c) and its various properties such as the N.Y.-Yonkers, Md.-Columbia, N.Y.-Bronx, Va.-Reston, Ky.-Lexington, Mich.-Kalamazoo, Calif.-Calabasas, . .. boost-revenue, tap-markets, take-losses, launch-fight, reduce-holdings, terminate-contract, identify-bidders, ... boosting-bid, meeting-schedules, obtaining-order, having-losses, completing-review, governing-industry, ... says-mean, adds-may, explains-have, contend-has, recalls-had, figures-is, asserted-is, notes-would, ... would-Based, is-Besides, was-Like, is-From, are-Despite, said-Besides, says-Despite, reported-As, ... began-Meanwhile, was-Since, are-Often, would-Now, had-During, were-Over, was-Late, have-Until, ... Catsimatidis-Mr., Swete-Mr., Case-Mr., Montoya-Mr., Byerlein-Mr., Heard-Mr., Leny-Mr., Graham-Mrs., ... only-1.5, about-170, nearly-eight, approximately-10, almost-15, some-80, Only-two, about-23, roughly-50, ... link's dependency relation label l, the grandparent dependency relation label gl, and the signed, binned distance d:",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_5",
  "x": "**FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features.",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_6",
  "x": "**FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_7",
  "x": "We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2. Bit-string features: We first hierarchically cluster the link vectors via MATLAB's linkage function with {method=ward, metric=euclidean} to get 0-1 bit-strings (similar to BROWN).",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_8",
  "x": "For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory.",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_9",
  "x": "For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_10",
  "x": "All the final test improvements, i.e., Bucket (92.3) and Bit-string (92.6) w.r.t. Baseline (91.9), and BROWN + Bucket (93.0) and BROWN + Bit-string (93.1) w.r.t. BROWN (92.7), are statistically significant at p < 0.01. Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_11",
  "x": "BROWN (92.7), are statistically significant at p < 0.01. Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them. We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_12",
  "x": "**RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_13",
  "x": "As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on. Moreover, we hope that similar to word embeddings, these link embeddings will also prove useful when imported into various other NLP tasks as dense, continuous features, but now with additional syntactic information.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_14",
  "x": "In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on. Moreover, we hope that similar to word embeddings, these link embeddings will also prove useful when imported into various other NLP tasks as dense, continuous features, but now with additional syntactic information. There has also been some recent, useful work on reducing the sparsity of features in dependency parsing, e.g., via low-rank tensors (Lei et al., 2014) and via neural network parsers that learn tag and label embeddings (Chen and Manning, 2014) .",
  "y": "differences motivation"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_0",
  "x": "In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_1",
  "x": "Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) . His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) .",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_2",
  "x": "His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. On the other hand, <cite>our past work</cite> on WSD <cite>(Ng and Lee, 1996)</cite> used an exemplar-based (or nearest neighbor) learning approach.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_3",
  "x": "In this paper, we report recent improvements to the exemplar-based learning approach for WSD that have achieved higher disambiguation accuracy. The exemplar-based learning algorithm PEBLS contains a number of parameters that must be set before running the algorithm. These parameters include the number of nearest neighbors to use for determining the class of a test example (i.e., k in a k nearest-neighbor classifier), exemplar weights, feature weights, etc. We found that the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_4",
  "x": "Section 2 gives a brief description of the exemplar-based algorithm PEBLS and the Naive-Bayes algorithm. Section 3 describes the 10-fold cross validation training procedure to determine the best k number of nearest neighbors to use. Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results. Section 6 gives the conclusion.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_5",
  "x": "We only changed the handling of zero probability counts to the method just described. 3 Improvements to Exemplar-Based WSD PEBLS contains a number of parameters that must be set before running the algorithm. These parameters include k (the number of nearest neighbors to use for determining the class of a test example), exemplar weights, feature weights, etc. Each of these parameters has a default value in PEBLS, eg., k = 1, no exemplar weighting, no feature weighting, etc. <cite>We</cite> have used the default values for all parameter settings in <cite>our previous work</cite> on exemplar-based WSD reported in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_6",
  "x": "Experimental Results Mooney (1996) has reported that the Naive-Bayes algorithm gives the best performance on disambiguating six senses of the word \"line\", among seven state-of-the-art learning algorithms tested. However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_7",
  "x": "In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly. For this set of 191 words, the average number of senses per noun is 7.8, while the average number of senses per verb is 12.0. The sentences in this corpus were drawn from the combined corpus of the i million word Brown corpus and the 2.5 million word Wall Street Journal (WSJ) corpus.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_8",
  "x": "We tested both algorithms on two test sets from this corpus. The first test set, named BC50, consists of 7,119 occurrences of the 191 words appearing in 50 text files of the Brown corpus. The second test set, named WSJ6, consists of 14,139 occurrences of the 191 words appearing in 6 text files of the WSJ corpus. Both test sets are identical to the ones reported in <cite>(Ng and Lee, 1996)</cite> . Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation.",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_9",
  "x": "Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> . That local collocation knowledge provides important clues to WSD has also been pointed out previously by Yarowsky (1993) . Let w be the word to be disambiguated, and let 12 ll w rl r2 be the sentence fragment containing w. In the present study, we used seven features in the representation of an example, which are the local collocations of the surrounding 4 words. These seven features are: 12-11, ll-rl, rl-r2, ll, rl, 12 , and r2. The first three features are concatenation of two words.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_10",
  "x": "The first three rows of accuracy fig-1This corpus is available from the Linguistic Data Consortium (LDC). Contact the LDC at ldc@unagi.cis.upenn.edu for details. 2The first five of these seven features were also used in <cite>(Ng and Lee, 1996)</cite> . , 1996) . The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs (Gale et al., 1992b; Miller et al., 1994) . There are two instantiations of this strategy in our current evaluation.",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_11",
  "x": "We call this method \"Sense 1\" in Table 1 . Another assignment method is to determine the most frequently occurring sense in the training examples, and to assign this sense to all test examples. We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_12",
  "x": "We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 . These figures were obtained using all features including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation. However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_13",
  "x": "The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_14",
  "x": "Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance. Furthermore, PEBLS with 10-fold cross validation to select the best k yields results slightly better than the Naive-Bayes algorithm.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_15",
  "x": "The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_16",
  "x": "One potential drawback of an exemplar-based learning approach is the testing time required, since each test example must be compared with every training example, and hence the required testing time grows linearly with the size of the training set. However, more sophisticated indexing methods such as that reported in (Friedman et al., 1977) can reduce this to logarithmic expected time, which will significantly reduce testing time. In the present study, we have focused on the comparison of learning algorithms, but not on feature representation of examples. <cite>Our past work</cite> <cite>(Ng and Lee, 1996)</cite> suggests that multiple sources of knowledge are indeed useful for WSD. Future work will explore the addition of these other features to further improve disambiguation accuracy.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_17",
  "x": "Future work will explore the addition of these other features to further improve disambiguation accuracy. Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights. Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting. Future work can explore the effect of exemplar weighting and feature weighting on disambiguation accuracy.",
  "y": "future_work"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_0",
  "x": "Secondly, the generated questions and answers will be able to augment the training data for QA systems. More importantly, KBQG can improve the ability of machines to actively ask questions on human-machine conversations (Duan et al., 2017; Sun et al., 2018) . Therefore, this task has attracted more attention in recent years (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> . Specifically, KBQG is the task of generating natural language questions according to the input facts from a knowledge base with triplet form, like <subject, predicate, object>. For example, as illustrated in Figure 1 , KBQG aims at generating a question \"Which city is Statue of Liberty located in?\" (Q3) for the input factual triplet Which city is Statue of Liberty located in? Figure 1 : Examples of KBQG. We aims at generating questions like Q3 which expresses (matches) the given predicate and refers to a definitive answer.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_1",
  "x": "In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ---------------------------------- **THE EFFECTIVENESS OF GENERATED QUESTIONS FOR ENHANCING QUESTION ANSWERING OVER KNOWLEDGE BASES** Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_2",
  "x": "More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_3",
  "x": "More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Finally, this model obtained significant improvements. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected.",
  "y": "background motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_4",
  "x": "For example in Figure 1 , Q1 does not express (match) the predicate (fb:location/containedby) while it is expressed in Q2 and Q3. Previous work<cite> (Elsahar et al., 2018)</cite> usually obtained predicate textual contexts through distant supervision. However, the distant supervision is noisy or even wrong (e.g. \"X is the husband of Y\" is the relational pattern for the predicate fb:marriage/spouse, so it is wrong when \"X\" is a woman). Furthermore, many predicates in the KB have no predicate contexts. We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 .",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_5",
  "x": "We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer. A definitive answer means that one question only associates with a determinate answer rather than alternative answers. As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type.",
  "y": "motivation uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_6",
  "x": "As an example in Figure 1 , Q2 may contain ambiguous answers since it does not express the refined answer type. As a result, different answers including \"United State\", \"New York City\", etc. may be correct. In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_7",
  "x": "In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_8",
  "x": "In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers. To address the aforementioned two issues, we exploit more diversified contexts for the given facts as textual contexts in an encoder-decoder model. Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_9",
  "x": "Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously. For the purpose of further making generated questions correspond to definitive answers, we propose the answer-aware loss by optimizing the cross-entropy between the generated question and answer type words, which is beneficial to generate precise questions. We conduct experiments on an open public dataset. Experimental results demonstrate that the proposed model using diversified textual contexts outperforms strong baselines (+4.5 BLEU4 score).",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_10",
  "x": "In contrast to general Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) , the input fact is not a word sequence but instead a structured triplet F = (s, p, o). We employ a fact encoder to transform each atom in the fact into a fixed embedding, and the embedding is obtained from a KB embedding matrix. For example, the subject embedding e s \u2208 R d is looked up from the KB embedding matrix E f \u2208 R k,d , where k represents the size of KB vocabulary, and the size of KB embedding is equal to the number of hidden units (d) in Equation 3. Similarly, the predicate embedding e p and the object embedding e o are mapped from the KB embedding matrix E f , where E f is pre-trained using TransE (Bordes et al., 2013) to capture much more fact information in previous work<cite> (Elsahar et al., 2018)</cite> . In our model, E f can be pre-trained or randomly initiated (Details in Sec. 4.7.1).",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_11",
  "x": "**KB COPY** Previous study found that most questions contain the subject name or its aligns in SimpleQuestion (Petrochuk and Zettlemoyer, 2018) . However, the predicate name and object name hardly appear in the question. Therefore, we only copy the subject name in the KB copy, where P cpkb (y t |s t , f ), the probability of copying the subject name, is calculated by a neural network function with a multilayer perceptron (MLP) projected from s t . <cite>Elsahar et al. (2018)</cite> demonstrated the effectiveness of POS copy for the context.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_12",
  "x": "For the subject and object context, we combine the most frequently mentioned entity type<cite> (Elsahar et al., 2018)</cite> with the type that best describe the entity 3 . The KB copy needs subject names as the copy source, and we map entities with their names similar to those in Mohammed et al. (2018) . The data details are in Appendix A and submitted Supplementary Data. ---------------------------------- **EVALUATION METRICS**",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_13",
  "x": "Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows. (1) Predicate identification: Following Mohammed et al. (2018), we employ annotators to judge whether the generated question expresses the given predicate in the fact or not.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_14",
  "x": "Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . However, such metrics still suffer from some limitations (Novikova et al., 2017) . Crucially, it might be difficult for them to measure whether generated questions that express the given predicate and refer to definitive answers. To better evaluate generated questions, we run two further evaluations as follows. (1) Predicate identification: Following Mohammed et al. (2018), we employ annotators to judge whether the generated question expresses the given predicate in the fact or not.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_15",
  "x": "(3) <cite>Elsahar et al. (2018)</cite> : We compare our methods with the model utilizing copy actions, the best performing model in <cite>Elsahar et al. (2018)</cite> . Although this model is designed to a zero-shot setting (for unseen predicates and entity type), it has good abilities to generate better questions (on known or unknown predicates and entity types) represented in the additional context input and SPO copy mechanism. ---------------------------------- **IMPLEMENTATION DETAILS** To make our model comparable to the comparison methods, we keep most parameter values the same as <cite>Elsahar et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_16",
  "x": "Although this model is designed to a zero-shot setting (for unseen predicates and entity type), it has good abilities to generate better questions (on known or unknown predicates and entity types) represented in the additional context input and SPO copy mechanism. ---------------------------------- **IMPLEMENTATION DETAILS** To make our model comparable to the comparison methods, we keep most parameter values the same as <cite>Elsahar et al. (2018)</cite> . We utilize RMSProp algorithm with a decreasing learning rate (0.001), batch size (200) to optimize the model.",
  "y": "similarities"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_17",
  "x": "We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics. It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> . Especially, incorporating answer-aware loss (the last line in Table 1 ) further improves the performance (+5.16 BLEU4). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_18",
  "x": "Identification Serban et al. (2016) 53.5 <cite>Elsahar et al. (2018)</cite> 71.5 Our Model ans loss 75.5 from each model, and then two annotators are employed to judge whether the generated question expresses the given predicate. The Kappa for inter-annotator statistics is 0.611, and p-value for all scores is less than 0.005. As shown in Table  2 , we can see that our model has a significant improvement in the predicate identification. Table 3 : Performances on answer coverage, where \"Ans cov \" denotes the metric of answer coverage. \"\u03bb\" is the weight of the answer-aware loss in Equation 16. Table 3 reports performances on BLUE4 and answer coverage (Ans cov ).",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_19",
  "x": "This demonstrates that the answer-aware loss does not force all predicates to generate questions with answer type words. Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in <cite>Elsahar et al. (2018)</cite> . ---------------------------------- **ABLATION STUDY** In order to validate the effectiveness of model components, we remove some important components in our model, including context copy, KB copy, answer-aware loss and diversified contexts.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_20",
  "x": "Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ---------------------------------- **PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_21",
  "x": "**PERFORMANCES ON NATURALNESS** ---------------------------------- **MODEL** Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_22",
  "x": "Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_23",
  "x": "Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_24",
  "x": "In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ---------------------------------- **THE EFFECTIVENESS OF GENERATED QUESTIONS FOR ENHANCING QUESTION ANSWERING OVER KNOWLEDGE BASES** Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_25",
  "x": "Our work is inspired by a large number of successful applications using neural encoder-decoder frameworks on NLP tasks such as machine translation (Cho et al., 2014a) and dialog generation (Vinyals and Le, 2015) . Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks. Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions. To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_26",
  "x": "To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions. Moreover, our work also borrows the idea from copy mechanisms. Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating.",
  "y": "background motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_27",
  "x": "<cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we focus on two crucial research issues for the task of question generation over knowledge bases: generating questions that express the given predicate and refer to definitive answers rather than alternative answers.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_28",
  "x": "<cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we focus on two crucial research issues for the task of question generation over knowledge bases: generating questions that express the given predicate and refer to definitive answers rather than alternative answers.",
  "y": "background differences"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_0",
  "x": "**INTRODUCTION** A recent survey by IBM 3 suggests that more than 2.5 quintillion bytes of data are produced on the Web every day. Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data. The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K <cite>[4]</cite> . A large number of challenges has to be addressed while performing a disambiguation.",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_1",
  "x": "A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries. However, MAG (Multilingual AGDISTIS) <cite>[4]</cite> showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data available on more than three languages [6] . The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 . For the sake of server space, we deployed MAG-based web services for 9 languages and offer the other 31 languages for download.",
  "y": "motivation"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_2",
  "x": "During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation. The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention. These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions. MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS. Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention <cite>[4]</cite> .",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_3",
  "x": "-Popularity -The user can set it as popularity=false or popularity=true. It allows MAG to use either the Page Rank or the frequency of a candidate to sort while candidate retrieval. -Graph-based algorithm -The user can choose which graph-based algorithm to use for disambiguating among the candidates per mentions. The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank. -Search by Context -This boolean parameter provides a search of candidates using a context index <cite>[4]</cite> .",
  "y": "background uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_0",
  "x": "Our design most closely resembles the pipeline proposed by the top system last year<cite> (Wang and Lan, 2015)</cite> , in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice. The overall architecture of the system is shown in Figure 1 . Given the input text, the connective classifier identifies explicit discourse connectives. Next, the position classifier is invoked that determines for each explicit relation whether Arg1 is located in the same sentence as Arg2 (SS) or in a previous sentence (PS). The following three modules -SS Arg1/Arg2 Extractor, PS Arg1 Extractor, and PS Arg2 Extractor -extract text spans of the respective arguments.",
  "y": "similarities"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_1",
  "x": "This is a binary classifier that, given a connective word or phrase (e.g. but or if . . . then) determines whether the connective functions as a discourse connective in the specific context. We use the training data to generate a list of 145 connective words and phrases that may function as discourse connectives. Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014;<cite> Wang and Lan, 2015)</cite> . Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006) .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_2",
  "x": "This is a binary classifier with two classes: SS and PS. We employ the features proposed in Lin et al. (2014) and additional features described in last year's top system<cite> (Wang and Lan, 2015)</cite> . The position classifier is trained using the Maximum Entropy algorithm and achieves an F1 score of 99.186% on the development data. In line with prior work<cite> (Wang and Lan, 2015)</cite> , we consider PS to be the sentence that immediately precedes the connective. About 10% of explicit discourse relations have Arg1 occurring in a sentence that does not immediately precede the connective.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_3",
  "x": "---------------------------------- **EXPLICIT RELATIONS: ARGUMENT EXTRACTION** SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_4",
  "x": "SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_5",
  "x": "We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_6",
  "x": "The novel features that we add are shown in Table 1. These features use POS information of tokens in the constituents, punctuation between the constituents, and feature conjunctions. PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features: \u2022 nextFirstW&puncBefore -the first word token of next and the punctuation before next.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_7",
  "x": "\u2022 C parent-category linked context, previous connective and its POS of \"as\"(the connective and its POS of previous relation, if the connective of current relation is \"as\"), previous connective and its POS of \"when\", adopted from<cite> Wang and Lan (2015)</cite> . \u2022 Our new features: first token of C, second token of C (if exists), next word (next), C + next, prev + next, prev + C + next. Table 1 : Novel features used in the PS Arg1 and PS Arg2 extractors. Curr, prev, and next refer to the current, previous, and next constituent in the same sentence, respectively. W denotes word token, and POS denotes the part-of-speech tag of a word.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_8",
  "x": "---------------------------------- **IDENTIFYING NON-EXPLICIT RELATIONS** The first step in identifying non-explicit relations is the generation of sentence pairs that are candidate arguments for a non-explicit relation. Following<cite> Wang and Lan (2015)</cite>, we extract sentence pairs that satisfy the following three criteria: \u2022 Sentences are adjacent \u2022 Sentences occur within the same paragraph \u2022 Neither sentence participates in an explicit relation For all pairs of sentences that meet those criteria, we take the first sentence to be the location of Arg1, and the second sentence -the location of Arg2.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_9",
  "x": "---------------------------------- **IMPLICIT RELATIONS: ARGUMENT EXTRACTION** The argument extractors for implicit relations are implemented in a way similar to explicit relation argument extraction. Candidate sentences are split into constituents based on punctuation symbols and clause boundaries using the SBAR tag. We use features in Lin et al. (2009) and<cite> Wang and Lan (2015)</cite> and augment these with novel features.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_10",
  "x": "This approach to feature generation also avoids sparseness, which was found to be a problem in earlier work. Overall, we generate seven features that use dependency relations. Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and<cite> Wang and Lan (2015)</cite> to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers): \u2022 Lowercased and lemmatized verbs in curr \u2022 The first and last terms of curr",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_12",
  "x": "Table 3 : PS Arg1 extractor, no EP. Baseline denotes taking the entire sentence as argument span. ---------------------------------- **MODEL** Base features refer to features used in<cite> Wang and Lan (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_13",
  "x": "Explicit Sense Classifier: Table 2 evaluates the explicit sense classifier. We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_14",
  "x": "Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 .",
  "y": "differences"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_15",
  "x": "We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 . Results for PS Arg1 extractor are shown in Table 3 . The baseline refers to taking the entire sentence as argument span. Overall, we obtain a 5 point improvement over the baseline method. Similarly, Table 4 shows results for PS Arg2 extractor.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_16",
  "x": "We note that in<cite> Wang and Lan (2015)</cite> the numbers that correspond to the entire sentence baselines are not the same as those that we obtain, so we do not report a direct comparison with their models. However, our base models implement the features they use. Implicit Arg1 Extractor: In Table 5 , we evaluate the Implicit Arg1 extractor. It achieves an improvement of 12 F1 points over the baseline method that considers the entire sentence to be the argument span. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_0",
  "x": "---------------------------------- **INTRODUCTION** Natural language texts are, more often than not, a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments. Text segmentation deals with automatically breaking down the structure of text into such topically contiguous segments, i.e., it aims to identify the points of topic shift (Hearst 1994; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> . Reliable segmentation results with texts that are more readable for humans, but also facilitates downstream tasks like automated text summarization (Angheluta, De Busser, and Moens 2002; Bokaei, Sameti, and Liu 2016) , passage retrieval (Huang et al. 2003; Shtekh et al. 2018) , topical classification (Zirn et al. 2016) , or dialog modeling (Manuvinakurike et al. 2016; Zhao and Kawahara 2017) .",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_1",
  "x": "Given this duality between text segmentation and coherence, it is surprising that the methods for text segmentation capture coherence only implicitly. Unsupervised segmentation models rely either on probabilistic topic modeling (Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013) or semantic similarity between sentences (Glava\u0161, Nanni, and Ponzetto 2016) , both of which only indirectly relate to text coherence. Similarly, a recently proposed state-of-the-art supervised neural segmentation model<cite> (Koshorek et al. 2018</cite> ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence. In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ).",
  "y": "background differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_2",
  "x": "In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ). Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction. However, CATS augments the segmentation objective with an auxiliary coherence-based objec-tive which pushes the model to predict higher coherence for original text snippets than for corrupt (i.e., fake) sentence sequences. We empirically show (1) that even without the auxiliary coherence objective, the Two-Level Transformer model for Text Segmentation (TLT-TS) yields state-of-the-art performance across multiple benchmarks, (2) that the full CATS model, with the auxiliary coherence modeling, further significantly improves the segmentation, and (3) that both TLT-TS and CATS are robust in domain transfer.",
  "y": "differences similarities"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_3",
  "x": [
   "WIKI-727K Corpus. Koshorek et al. (2018) leveraged the manual structuring of Wikipedia pages into sections to automatically create a large segmentation-annotated corpus. WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora."
  ],
  "y": "background uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_4",
  "x": [
   "We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset. Standard Test Corpora. Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow. For years, the synthetic dataset of Choi (2000) was used as a standard becnhmark for text segmentation models. CHOI dataset contains 920 documents, each of which is a concatenation of 10 paragraphs randomly sampled from the Brown corpus."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_5",
  "x": "CHOI dataset is divided into subsets containing only documents with specific variability of segment lengths (e.g., segments with 3-5 or with 9-11 sentences). 7 Finally, we evaluate the performance of our models on two small datasets, CITIES and ELEMENTS, created by Chen et al. (2009) from Wikipedia pages dedicated to the cities of the world and chemical elements, respectively. Other Languages. In order to test the performance of our Transformer-based models in zero-shot language transfer setup, we prepared small evaluation datasets in other languages. Analogous to the WIKI-50 dataset created by<cite> Koshorek et al. (2018)</cite> from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively.",
  "y": "extends similarities"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_6",
  "x": "**COMPARATIVE EVALUATION** Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_7",
  "x": "Evaluation Metric. Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_8",
  "x": "Baseline Models. We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ---------------------------------- **MODEL CONFIGURATION**",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_10",
  "x": "11 We do not tune other transformer hyperparameters, but rather adopt the recommended values from (Vaswani et al. 2017) : filter size of 1024 and dropout probabilities of 0.1 for both attention layers and feed-forward ReLu layers. Table 1 shows models' performance on five EN evaluation datasets. Both our Transformer-based models -TLT-TS and CATS -outperform the competing supervised model of<cite> Koshorek et al. (2018)</cite> , a hierarchical encoder based on recurrent components, across the board. The improved performance that TLT-TS has with respect to the model of<cite> Koshorek et al. (2018)</cite> is consistent with improvements that Transformer-based architectures yield in comparison with models based on recurrent components in other NLP tasks (Vaswani et al. 2017; Devlin et al. 2018) . The gap in performance is particularly wide (>20 P k points) for the EL-EMENTS dataset.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_12",
  "x": "CATS significantly 13 and consistently outperforms TLT-TS. This empirically confirms the usefulness of explicit coherence modeling for text segmentation. Moreover,<cite> Koshorek et al. (2018)</cite> report human performance on the WIKI-50 dataset of 14.97, which is a mere one P k point better than the performance of our coherence-aware CATS model. The unsupervised GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) seems to outperform all supervised models on the synthetic CHOI dataset. We believe that this is primarily because (1) by being synthetic, the CHOI dataset can be accurately segmented based on simple lexical overlaps and word embedding similarities (and GRAPHSEG relies on similarities between averaged word embeddings) and because (2) by being trained on a much more challenging real-world WIKI-727K dataset -on which lexical overlap is insufficient for accurate segmentation -supervised models learn to segment based on deeper natural language understanding (and learn not to encode lexical overlap as reliable segmentation signal).",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_13",
  "x": [
   "Because our CATS model has an auxiliary coherencebased objective, we additionally provide a brief overview of research on modeling text coherence. ---------------------------------- **TEXT SEGMENTATION** Text segmentation tasks come in two main flavors: (1) linear (i.e., sequential) text segmentation and (2) hierarchical segmentation in which top-level segments are further broken down into sub-segments. While the hierarchical segmentation received a non-negligible research attention (Yaari 1997; Eisenstein 2009; Du, Buntine, and John-son 2013) , the vast majority of the proposed models (including this work) focus on linear segmentation (Hearst 1994; Beeferman, Berger, and Lafferty 1999; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Misra et al. 2009; Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; Koshorek et al. 2018, inter alia) ."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_14",
  "x": "Glava\u0161, Nanni, and Ponzetto (2016) propose GRAPHSEG, a graph-based segmentation algorithm similar in nature to (Malioutov and Barzilay 2006) , which uses dense sentence vectors, obtained by aggregating word embeddings, to compute intra-sentence similarities and performs segmentation based on the cliques of the similarity graph. Finally,<cite> Koshorek et al. (2018)</cite> identify Wikipedia as a free large-scale source of manually segmented texts that can be used to train a supervised segmentation model. They train a neural model that hierarchically combines two bidirectional LSTM networks and report massive improvements over unsupervised segmentation on a range of evaluation datasets. The model we presented in this work has a similar hierarchical architecture, but uses Transfomer networks instead of recurrent encoders. Crucially, CATS additionally defines an auxiliary coherence objective, which is coupled with the (primary) segmentation objective in a multi-task learning model.",
  "y": "background differences motivation"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_0",
  "x": "An interesting property of word vectors learned by the log-linear model is that the relations among relevant words seem linear and can be computed by simple vector addition and substraction (Mikolov et al., 2013d) . For example, the following relation approximately holds in the word vector space: ParisFrance + Rome = Italy. In<cite> (Mikolov et al., 2013b)</cite> , the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another. The authors reported a high accuracy on a bilingual word translation task. Although promising, we argue that both the word embedding and the linear transform are ill-posed, due to the inconsistence among the objective function used to learn the word vectors (maximum likelihood based on inner product), the distance measurement for word vectors (cosine distance), and the objective function used to learn the linear transform (mean square error).",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_1",
  "x": "---------------------------------- **RELATED WORK** This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches. In the projection-based approaches, the embedding is performed for each language individually with monolingual data, and then one or several projections are learned using multilingual data to represent the relation between languages.",
  "y": "similarities uses"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_2",
  "x": "Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA). The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding. For example, (Zou et al., 2013) adds an extra term that reflects the distances of some pairs of semantically related words from different languages into the objective funtion. A similar approach is proposed in (Klementiev et al., 2012) , which casts multilingual learning as a multitask learning and encodes the multilingual information in the interaction matrix.",
  "y": "similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_3",
  "x": "1 The consequence of the normalization is that all the word vectors are located on a hypersphere, as illustrated in Figure 1 . In addition, by the normalization, the inner product falls back to the cosine distance, hence solving the inconsistence between the embedding learning and the distance measurement. ---------------------------------- **ORTHOGONAL TRANSFORM** The bilingual word translation provided by <cite>(Mikolov et al., 2013b</cite> ) learns a linear transform from the source language to the target language by the linear regression.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_4",
  "x": "1 For efficiency, this normalization can be conducted every n mini-batches. The performance is expected to be not much impacted, given that n is not too large. where W is the projection matrix to be learned, and x i and z i are word vectors in the source and target language respectively. The bilingual pair (x i , z i ) indicates that x i and z i are identical in semantic meaning. A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation<cite> (Mikolov et al., 2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_5",
  "x": "The monolingual word embedding is conducted with the data published by the EMNLP 2011 SMT workshop (WMT11) 2 . For an easy comparison, we largely follow Mikolov's settings in<cite> (Mikolov et al., 2013b)</cite> and set English and Spanish as the source and target language, respectively. The data preparation involves the following steps. Firstly, the text was tokenized by the standard scripts provided by WMT11 3 , and then duplicated sentences were removed. The numerical expressions were tokenized as 'NUM', and special characters (such as !?,:) were removed.",
  "y": "similarities uses"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_6",
  "x": "These results are comparable with the results reported in <cite>(Mikolov et al., 2013b</cite> ---------------------------------- **RESULTS WITH ORTHOGONAL TRANSFORM** The results with the normalized word vectors and the orthogonal transform are reported in Table 2 . It can be seen that the results with the orthogonal transform are consistently better than those reported in Table1 which are based on the linear transform.",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_0",
  "x": "Given a word f , its distributional profile is: V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_1",
  "x": "V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> . DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i :",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_2",
  "x": "Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase. ---------------------------------- **PARAPHRASES FROM BILINGUAL PIVOTING**",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_3",
  "x": "---------------------------------- **METHODOLOGY** After paraphrase extraction we have paraphrase pairs, (f 1 , f 2 ) and a score S(f 1 , f 2 ) we can induce new translation rules for OOV phrases using the steps in Algo. (1): 1) A graph of source phrases is constructed as in<cite> (Razmara et al., 2013)</cite> ; 2) translations are propagated as labels through the graph as explained in Fig. 2 ; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_4",
  "x": "We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in<cite> (Razmara et al., 2013)</cite> . The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_5",
  "x": "We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to<cite> Razmara et al. (2013)</cite> . In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. ---------------------------------- **GRAPH RANDOM WALKS** Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_6",
  "x": "We use CDEC 1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features 2 . fast align (Dyer et al., 2013 ) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003) . This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with<cite> Razmara et al. (2013)</cite> on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. KenLM (Heafield, 2011 ) is used to train a 5-gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework 3 .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_7",
  "x": "In this experiment we use a setup similar to (Razmara et al., 2013 we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in<cite> (Razmara et al., 2013)</cite> . ACL/WMT 2005 4 is used for dev and test data. We re-implement their paraphrase extraction method (DP) to extract paraphrases from French side of Europarl (2M sentences). We use unigram nodes to construct graphs for both DP and PPDB. In bipartite graphs, each node is connected to at most 20 nodes.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_0",
  "x": "Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible<cite> [5]</cite> . Informal/irregular forms usually do not obey the conventional rules in the languages. For instance, 'khunh' (home) is a frequent form for 'khanh' in Persian conversations or 'goood ' and 'good ' are used interchangeably in English tweets. In this paper, we propose a statistical technique for finding inflectional and derivation formations of words. To this end, we introduce an unsupervised method to cluster all morphological variants of a word.",
  "y": "motivation"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_1",
  "x": "In highly inflected languages, bilingual dictionaries contain only original forms of the words. Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8, 4, 12] , or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_2",
  "x": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i . c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_3",
  "x": "c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> . Our goal is to findc i using the proposed SS4MCT (i.e. set of top-ranked c i ,j according to p t (c i ,j |c i,j )) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part).",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_4",
  "x": "Dirichlet Prior is selected as our document smoothing strategy. Top 30 documents are used for the mixture pseudo-relevance feedback algorithm. Queries are expanded by the top 50 terms generated by the feedback model [14, 6] . We removed Persian stop words from the queries and documents [4, <cite>5]</cite> . We used STeP1 [13] in our stemming process in Persian.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_5",
  "x": "We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries<cite> [5]</cite> . We have exploited 40 Persian POS tags in our experiments. 2 The retrieval results are mainly evaluated by Mean Average Precision (MAP) over top 1000 retrieved documents. Significance tests are computed using two-tailed paired t-test with 95% confidence.",
  "y": "background"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_6",
  "x": "To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11] , rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator 3 , and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system. On the other hand, we run another set of experiments without applying any morphological processing method similar to the Persian state-of-the-art CLIR methods. Iterative translation disambiguation (ITD) [11] , joint cross-lingual topical relevance model (JCLTRLM) [7] , top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in<cite> [5]</cite> (assume |c i | = 0), are the baselines without any morphological processing units. As shown in Table 3 BiCTM outperforms all the baselines when there is no morphological processing unit. Although the improvement compared to JCLTRLM is not statistically significant, for simplicity we assume this model as a base of comparisons in the next set of experiments.",
  "y": "uses"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_0",
  "x": "Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004).",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_1",
  "x": "Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004).",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_2",
  "x": "It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) .",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_3",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_0",
  "x": "These models work by learning per-morpheme representations rather than just per-word ones, and compose the representing vector of each word from those of its morphemes -as derived from a supervised or unsupervised morphological analysis -and (optionally) its surface form (e.g. walking = f (v walk , v ing , v walking )). The works differ in the way they acquire morphological knowledge (from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other) and in the model form (cDSMs (Lazaridou et al., 2013) , RNN (Luong et al., 2013) , LBL (Botha and Blunsom, 2014) , CBOW (Qiu et al., 2014) , SkipGram (Soricut and Och, 2015; <cite>Bojanowski et al., 2016)</cite> , GGM (Cotterell et al., 2016) ). But essentially, they all show that breaking a word into morphological components (base form, affixes and potentially also the complete surface form), learning a vector for each component, and representing a word as a composition of these vectors improves the models semantic performance, especially on rare words. In this work we argue that these models capture two distinct aspects of word similarity, semantic (e.g. sim(walking, hiking) > sim(walking, eating)) and morphological (e.g. sim(walking, hiking) > sim (walking, hiked) ), and that these two aspects are at odds with each other (should sim(walking, hiking) be lower or higher than sim(walking, walked)?). The base form component of the compositional models is mostly responsible for semantic aspects of the similarity, while the affixes are mostly responsible for morphological similarity.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_1",
  "x": "where \u2113 is the log-sigmoid loss function, C t is a set of context words, and N t is a set of negative examples sampled from the vocabulary. s(w t , w c ) is defined as s(w t , w c ) = v \u22a4 wt u wc (where v wt and u wc are the embeddings of the focus and the context words). Bojanowski et al (2016) replace the word representation v wt with the set of character ngrams appearing in it: v wt = g\u2208G(wt) v g where G(w t ) is the set of n-grams appearing in w t . The n-grams are used to approximate the morphemes in the target word. We generalize<cite> Bojanowski et al (2016)</cite> by replacing the set of ngrams G(w) with a set P(w) of explicit linguistic properties.",
  "y": "extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_2",
  "x": "While our experiments focus on Modern Hebrew due to the availability of a reliable semantic similarity dataset, we believe our conclusions hold more generally. ---------------------------------- **MODELS** Our model form is a generalization of the fastText model<cite> (Bojanowski et al., 2016)</cite> , which in turn extends the skip-gram model of Mikolov et al (2013) . The skip-gram model takes a sequence of words w 1 , ..., w T and a function s assigning scores to (word, context) pairs, and maximizes",
  "y": "extends similarities"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_3",
  "x": "Moving from a set of ngrams to a set of explicit linguistic properties, allows finer control of the kinds of information in the word representation. We train models with different subsets of {W, L, M }. ---------------------------------- **EXPERIMENTS AND RESULTS** Our implementation is based on the fastText 2 library<cite> (Bojanowski et al., 2016)</cite> , which we modify as described above.",
  "y": "extends"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_0",
  "x": "The Switchboard and Fisher [1] data collections are large collection of CTS datasets that have been used extensively by researchers working on conversational speech recognition [2, 3, 4, 5, 6] . Recent trends in speech recognition [7, <cite>8,</cite> 9] have demonstrated impressive performance on Switchboard and Fisher data. Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions. Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , <cite>VGG-nets</cite> <cite>[8]</cite> , have significantly improved recognition performance, bringing them closer to human performance [9] .",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_1",
  "x": "The Switchboard and Fisher [1] data collections are large collection of CTS datasets that have been used extensively by researchers working on conversational speech recognition [2, 3, 4, 5, 6] . Recent trends in speech recognition [7, <cite>8,</cite> 9] have demonstrated impressive performance on Switchboard and Fisher data. Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions. Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , <cite>VGG-nets</cite> <cite>[8]</cite> , have significantly improved recognition performance, bringing them closer to human performance [9] .",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_2",
  "x": "These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory. For the sake of simplicity, we used a CNN acoustic model in our experiment, where the baseline system's performance is directly comparable to the state-of-the-art CNN performance reported in <cite>[8]</cite> . We expect our results using the CNN to carry over into other neural network architectures as well. The outline of the paper is as follows. In Section 2 we present the dataset and the recognition task.",
  "y": "uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_3",
  "x": "Table 1 shows that the performance of fMLLR transforms learned from the cepstral version of the features are better than the ones directly from the filterbank features, which is expected, as the cepstral features are uncorrelated, which adheres to the diagonal covariance assumption of the GMM models used to learn those transforms. Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> . Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set. As a next step, we investigated the efficacy of feature combination and focused only on the CNN acoustic models.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_4",
  "x": "The FSH corpus contains speech from quite a diverse set of speakers, helping to reduce the WER of the CH subset more significantly than the SWB subset, a trend reflected in results reported in the literature <cite>[8]</cite> . Table 4 shows the system fusion results after dumping 2000-best lists from the rescored lattices from each individual system of different front-end features with fMLLR, i.e., MFB, DOC, MFB+DOC, MFB+DOC+TV, then conducting M-way combination of the subsystems using N-best ROVER [27] implemented in SRILM [28] . In this system fusion experiment, all subsystems have equal weights for N-best ROVER. As can be seen from the table, N-best ROVER based 2-way and 3-way system fusion produced a further 2% and 4% relative reduction in WER compared to the best single system (MFB+fMLLR + DOC+fMLLR + TV), for SWB and CH evaluation sets respectively. Note that the first row of Table 4 is the last row of Table 3 , i.e., the best single system.",
  "y": "similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_5",
  "x": "Note that only 100 additional neurons were used to accommodate the TV features, hence all the models were of comparable sizes. The benefit of the articulatory features stemmed from the complementary information that they contain (reflecting degree and location of articulatory constrictions in the vocal tract), as demonstrated by earlier studies [22] [23] [24] . Overall the f-CNN-DNN system trained with the combined feature set, MFB+fMLLR + DOC+fMLLR + TV, demonstrated a relative reduction in WER of 7% and 9% compared to the MFB+fMLLR CNN baseline for SWB and CH subsets of the NIST 2000 CTS test set. Table 1 and 2 also demonstrates that sequence training always gave additive performance gain over crossentropy training, supporting the in <cite>[8,</cite> 21] . As a next step, we focused on training the acoustic models using the 2000-hour SWB+FSH CTS data, focusing on the CNN acoustic models and multi-view features.",
  "y": "similarities uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_6",
  "x": [
   "We developed a fused-CNN-DNN architecture, where input convolution was only performed on the acoustic features and the articulatory features were process by a feed-forward layer. We found this architecture effective for combining acoustic features and articulatory features. The robust features and articulatory features capture complementary information, and the addition of them resulted in the best single system performance, with 12% relative reduction of WER on SWB and CH evaluation sets respectively, compared to the MFB+fMLLR CNN baseline. Note that in this study the language model has not been optimized. Future studies should investigate RNN or other neural network-based language modeling techniques that are known to perform better than word n-gram LMs."
  ],
  "y": "future_work"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_7",
  "x": "Table 1 presents the word error rates (WER) from the baseline CNN model trained with the SWB data when evaluated on the NIST 2000 CTS test set, for both cross-entropy (CE) training and sequence training (ST) using MMI. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_0",
  "x": "**ABSTRACT** This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by <cite>Niehues et al. (2011)</cite> .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_1",
  "x": "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models.",
  "y": "background similarities"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_2",
  "x": "Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_3",
  "x": "We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags.",
  "y": "uses extends"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_4",
  "x": "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008) .",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_5",
  "x": "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3). We perform a thorough comparison between different variants of our general model and compare them to the original approach. We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER). Finally, we present a preliminary analysis of the reorderings resulting from the proposed models (Section 4). ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_6",
  "x": "The richer representation allows for a finer distinction between reorderings. For example, Arabic has a morphological marker of definiteness on both nouns and adjectives. If we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model. This kind of intuition underlies the model of <cite>Niehues et al. (2011)</cite> , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows: where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_7",
  "x": "<cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens. Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity. Another disadvantage comes from the fact that we want to compare permutations of the same set of elements.",
  "y": "background similarities"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_8",
  "x": "Figure 1 compares the BiLM and MTU tokenization for a specific example. Since <cite>Niehues et al. (2011)</cite> have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to <cite>Niehues et al. (2011)</cite> as the original BiLM. 4 At the same time, we do not see any specific obstacles for combining our work with MTUs. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_10",
  "x": "By using unnecessarily fine-grained categories we risk running into sparsity issues. <cite>Niehues et al. (2011)</cite> also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part). Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence. Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of \u221210.25 for the incorrect reordering than for the correct one (\u221210.39). Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_11",
  "x": "---------------------------------- **DEPENDENCY-BASED BILM** In this section, we introduce our model which combines the BiLM from <cite>Niehues et al. (2011)</cite> with source dependency information. We further give details on how the proposed models are trained and integrated into a phrase-based decoder. ----------------------------------",
  "y": "background similarities"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_13",
  "x": "We consider two variants of BiLM discussed by <cite>Niehues et al. (2011)</cite> : the standard one, Lex\u2022Lex, and the simplest syntactic one, Pos\u2022Pos. Results for the experiments can be found in Table 2 . In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09. Table 2 .a-b compares the performance of the baseline and original BiLM systems. Lex\u2022Lex yields strongly significant improvements over the baseline for BLEU and weakly significant improvements for TER.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_14",
  "x": "Our method consists of enriching the representation of units of a bilingual language model (BiLM). We argued that the very limited contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units. In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models. The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems. A number of additional evaluations provided an indication for better modeling of reordering phenomena.",
  "y": "background motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_0",
  "x": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_1",
  "x": "The main motivation of multilingual models is the effect of transfer learning that enables machine translation systems to benefit from relationships between languages and training signals that come from different datasets (Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) . Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_2",
  "x": "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_3",
  "x": "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model. This shared layer serves as a fixedsize sentence representation that can be straightforwardly applied to downstream tasks. We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_4",
  "x": "Our architecture follows the standard setup of an encoder-decoder model in machine translation with a traditional attention mechanism (Luong et al., 2015) . However, we augment the network with language specific encoders and decoders to enable multilingual training as in Lu et al. (2018) , plus we introduce an inner-attention layer (Liu et al., 2016; Lin et al., 2017) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> . Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_5",
  "x": "Sentences are encoded using Byte-Pair Encoding (Sennrich et al., 2016) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations.",
  "y": "similarities"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_6",
  "x": "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks. All models perform best with more than one attention head and the general trend is that the accuracies improve with larger representations. The previous claim was that there is the opposite effect and lower numbers of attention heads lead to higher performances in downstream tasks, but we do not see that effect in our setup, at least not in the classification tasks.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_7",
  "x": "Results with \u2020 taken from<cite> C\u00edfka and Bojar (2018).</cite> of multilingual training. We can see that multilingual training objectives are generally helpful for the trainable downstream tasks. Particularly interesting is the fact that the Manyto-Many model performs best on average even though it does not add any further training examples for English (compared to the other multilingual models), which is the target language of the downstream tasks. This suggests that the model is able to improve generalizations even from other language pairs (DE-ES, FR-ES, FR-DE) that are not directly involved in training the representations of English sentences.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_8",
  "x": "Baseline system in the right-most column. model is provided by a bilingual setting with only one attention head. This is in line with the findings of<cite> C\u00edfka and Bojar (2018)</cite> and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training. More surprising is the negative effect of the multilingual models. We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_0",
  "x": "Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance. ---------------------------------- **INTRODUCTION** A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph<cite> (Das et al., 2018</cite>; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) . In these methods the input question is first parsed into a constituent question entity and relation.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_1",
  "x": "We show that this improves overall performance. ---------------------------------- **RELATED WORK** The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and <cite>Das et al. (2018)</cite> , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer. 1 Other approaches consider this as a link prediction problem in which multi-hop reasoning can be used to learn relational paths that link two entities.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_2",
  "x": "We base our work on the recent reinforcement learning approaches introduced in <cite>Das et al. (2018)</cite> and Lin et al. (2018) . We denote the knowledge graph as G, the set of entities as E, the set of relations as R and the set of directed edges L between entities of the form l = (e 1 , r, e 2 ) with e 1 , e 2 \u2208 E and r \u2208 R. The goal is to find an answer entity e a given a question entity e q and the question relation r q , when (e q , r q , e a ) is not part of graph G. We formulate this problem as a Markov Decision Problem (MDP) (Sutton and Barto, 1998) with the following states, actions, transition function and rewards: States. At every timestep t, the state s t is defined by the current entity e t , the question entity e q and relation r q , for which e t , e q \u2208 E and r q \u2208 R. More formally, s t = (e t , e q , r q ).",
  "y": "uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_3",
  "x": "Rewards. The agent is rewarded based on the final state. For example, in <cite>Das et al. (2018)</cite> and Lin et al. (2018) the agent obtains a reward of 1 if the correct answer entity is reached as the final state and 0 otherwise (i.e., R(s T ) = I{e T = e a }). Figure 2a illustrates the LSTM which encodes history of the path taken. The output at timestep t is used as input to the policy network, illustrated in Figure 2b , to determine which action to take next.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_4",
  "x": "This is illustrated in Figure 3 . In the framework of <cite>Das et al. (2018)</cite> a binary reward is used which rewards the learner for the answer being wrong or correct. Following a similar protocol, we could award a score of 1 to return 'no answer' when there is no answer available in the KG. However, we cannot achieve reasonable training with such reward structure. This is because there is no specific pattern for 'no answer' that could be directly learned.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_5",
  "x": "Both the public dataset and the proprietary dataset are <cite>Das et al. (2018)</cite> , using the same train/val/test splits for FB15k-237. We extend the publicly available implementation of <cite>Das et al. (2018)</cite> for our experimentation.",
  "y": "similarities uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_6",
  "x": "Unlike <cite>Das et al. (2018)</cite> , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than <cite>Das et al. (2018)</cite> . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over <cite>Das et al. (2018)</cite> and Lin et al. (2018) .",
  "y": "extends differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_0",
  "x": "Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017) ), this paper -to the best of our knowledge -is the first attempt investigating regularization effects of AT in a joint setting for two related tasks. We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Sch\u00fctze (2017) ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; <cite>Bekoulis et al. (2018a)</cite> ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_1",
  "x": "Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training.",
  "y": "background"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_2",
  "x": "Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Sch\u00fctze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_3",
  "x": "Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training. Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017) , relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018) .",
  "y": "differences extends"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_4",
  "x": "We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) . We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in <cite>Bekoulis et al. (2018a)</cite> .",
  "y": "uses"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_5",
  "x": "Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to Miwa and Sasaki (2014) , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of \u223c3% on both tasks compared to <cite>Bekoulis et al. (2018a)</cite> , whose quadratic scoring layer complicates NER.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_0",
  "x": "Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010) . In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_1",
  "x": "Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., and verb-noun idioms (e.g., Salton et al., 2016) .",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_2",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) .",
  "y": "background motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_3",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality.",
  "y": "motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_4",
  "x": "2 Once vector representations of an MWE and its component words are obtained, following <cite>Salehi et al. (2015)</cite> , the following equations are then used to compute the compositionality of an MWE: where MWE is the vector representation of the MWE, and C 1 and C 2 are vector representations for the first and second components of the MWE, respectively. 3 In both cases, we use cosine as the similarity measure. comp 1 is based on Reddy et al. (2011) . As shown in equation (1), the compositionality of an MWE is computed based on measuring the similarity of the MWE and each of its component words, and then combining these two similarities into an overall compositionality score.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_5",
  "x": "**TRAINING CORPUS** We train language models over a portion of English and German Wikipedia dumps -following <cite>Salehi et al. (2015)</cite> -from 20 January 2018. The raw dumps are preprocessed using WP2TXT 6 to remove wikimarkup, metadata, and XML and HTML tags. The text from Wikipedia contains many characters that are not typically found in MWEs, for example, non-ASCII characters. Such characters drastically increase the size of the vocabulary of the language model, which leads to very long training times.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_6",
  "x": "We therefore instead carry out experiments training on a 1% sample of the English dump, and a 2% sample of the German dump (to give a corpus of similar size to the English one). Details of the resulting training corpora are provided in table 1. ---------------------------------- **EVALUATION DATA** The proposed model is evaluated over the same three datasets as <cite>Salehi et al. (2015)</cite> , <cite>which</cite> cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions).",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_7",
  "x": "ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) . (Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements. GNC This dataset contains 244 German noun compounds (e.g., Ahornblatt 'maple leaf', Knoblauch 'garlic') which are annotated on a scale of [1, 7] for their overall compositionality, and the compositionality of each component word (von der Heide and Borgwaldt, 2009).",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_8",
  "x": "---------------------------------- **EVALUATION METHODOLOGY** We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided. In this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_9",
  "x": "We begin by considering results using the default settings (described in section \u00a73.1) using both comp 1 and comp 2 . For comp 1 , we set \u03b1 to 0.7 for ENC and GNC following <cite>Salehi et al. (2015)</cite> ; for EVPC we set \u03b1 to 0.5. Results are shown in table 2. For ENC, and the particle component of EVPC, both comp 1 and comp 2 achieve significant correlations (i.e., p < 0.05). However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_10",
  "x": "However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions. We now consider the compositionality of individual component words.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_11",
  "x": "For ENC, and the particle component of EVPC, both comp 1 and comp 2 achieve significant correlations (i.e., p < 0.05). However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_12",
  "x": "Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 For GNC, and the verb component of EVPC, in line with the previous results over the entire dataset, neither compositionality measure gives significant correlations, with the exception of the verb component of EVPC using comp 2 for unattested expressions, although again the number of expressions here is relatively small. In an effort to improve on the default setup we considered a range of model variations. In particular we considered an RNN and GRU (instead of an LSTM), character embeddings of size 25 and 50 (instead of a one-hot representation), increasing the batch size to 100 (from 20), using dropout between 0.2-0.6, and using a bi-directional LSTM.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_13",
  "x": "There are 71 such expressions. For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions.",
  "y": "differences"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_0",
  "x": "There have been quite a number of recent papers on parallel text: Brown et al ---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_1",
  "x": "---------------------------------- **MOTIVATION** There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.",
  "y": "background motivation"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_2",
  "x": "Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_3",
  "x": "In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets. The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese. It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences.",
  "y": "background motivation"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_4",
  "x": "1 1. These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992 show where the concordances were found in the texts. We want to know whether the distribution of numbers in Table 1 is similar to those in Table 2, and if so, we will suspect that fisheries and p~ches As can be seen in the concordances in Table 3 , for K=10, the vector is <1, 1, 0, 1, 1,0, 1, 0, 0, 0>. By almost any measure of similarity one could imagine, this vector will be found to be quite different from the one for fisheries, and therefore, we will correctly discover that fisheries is not the translation of lections. To make this argument a little more precise, it might help to compare the contingency matrices in Tables 5 and 6 . The contingency matrices show: (a) the number of pieces where both the English and French word were found, (b) the number of pieces where just the English word was found, (c) the number of pieces where just the French word was found, and (d) the number of peices where neither word was found.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_5",
  "x": "---------------------------------- **RESULTS** This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992) . The 30 significant pairs with the largest mutual information values are shown in Table 9 . As can be seen, the results provide a quick-anddirty estimate of a bilingual lexicon.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_6",
  "x": "This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align . In this way, we might be able to apply word_align to a broader class of language combinations including possibly English-Japanese and English-Chinese. Currently, word_align depends on charalign<cite> (Church, 1993)</cite> to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. ---------------------------------- **REFERENCES**",
  "y": "background uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_0",
  "x": "---------------------------------- **INTRODUCTION** Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016;<cite> Caliskan et al., 2017)</cite> . This finding is not particularly surprising given that (1) we are likely project our biases in the text that we produce and (2) these biases in text are bound to be encoded in word vectors due to the distributional nature (Harris, 1954) of the word embedding models (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) . For illustration, consider the famous analogy-based gender bias example from Bolukbasi et al. (2016) : \"Man is to computer programmer as woman is to homemaker\".",
  "y": "background motivation"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_1",
  "x": "For illustration, consider the famous analogy-based gender bias example from Bolukbasi et al. (2016) : \"Man is to computer programmer as woman is to homemaker\". This bias will be reflected in the text (i.e., the word man will cooccur more often with words like programmer or engineer, whereas woman will more often appear next to homemaker or nurse), and will, in turn, be captured by word embeddings built from such biased texts. While biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases (e.g., Garg et al., 2018) , they may cause ethical problems for many downstream applications and NLP models. In order to measure the extent to which various societal biases are captured by word embeddings,<cite> Caliskan et al. (2017)</cite> proposed the Word Embedding Association Test (WEAT). WEAT measures semantic similarity, computed through word embeddings, between two sets of target words (e.g., insects vs. flowers) and two sets of attribute words (e.g., pleasant vs. unpleasant words).",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_2",
  "x": "We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli.",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_3",
  "x": "---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) . Whereas IAT measures biases based on response times of human subjects to provided stimuli, WEAT quantifies the biases using semantic similarities between word embeddings of the same stimuli. For each bias test, WEAT specifies four stimuli sets: two sets of target words and two sets of attribute words.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_4",
  "x": "In summary, for languages other than EN and for cross-lingual settings, we execute six bias tests (T1, T2, T6-T9). ---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric.",
  "y": "uses extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_5",
  "x": "---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces.",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_6",
  "x": "**METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework.",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_7",
  "x": "Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> . Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1).",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_8",
  "x": "Let X and Y be two sets of targets, and A and B two sets of attributes (see \u00a72.1). The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> . The effect size, that is, the \"amount of bias\", is computed as the normalized measure of separation between association distributions:",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_9",
  "x": "Generally, we find these results to be important as they indicate that embedding models may accentuate or diminish biases expressed in text. Corpora. In Table 4 we compare the biases of embeddings trained with the same model (GLOVE) but on different corpora: Common Crawl (i.e., noisy web content), Wikipedia (i.e., encyclopedic the definition of A and vice versa (Tissier et al., 2017) . 5 This is consistent with the original results obtained by<cite> Caliskan et al. (2017)</cite> . Corpus T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 WIKI 1.4 1.5 1.2 1.4 1.4 1.8 1.2 1.3 1.3 1.2 CC 1.5 1.6 1.5 1.6 1.4 1.9 1.1 1.3 1.4 1.3 TWEETS 1.2 1.0 1.1 1.2 1.2 1.2 \u22120.2 * 0.6 * 0.7 * 0.8 * Table 6 : XWEAT bias effects (aggregated over all six tests) for cross-lingual word embedding spaces.",
  "y": "similarities"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_10",
  "x": "**MULTILINGUAL COMPARISON.** ---------------------------------- **CONCLUSION** In this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces. To this end, we have extended previous analyses based on the WEAT test<cite> (Caliskan et al., 2017</cite>; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text.",
  "y": "uses extends"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_0",
  "x": "2 However, discourse connectives do not always mark the presence of discourse relations. For example, while the word 'et' is not a discourse connective in (1) , it signals a continuation relation in (2). (2) La f\u00e9d\u00e9ration CGT des transports s'est \u00e9lev\u00e9e contre \"l'absence de concertation\" et estime que les salari\u00e9s \"n'ont rien de bon \u00e0 attendre de cette restructuration\". 1 The CGT transport federation have risen against \"the lack of consultation\" and consider that employees have \"nothing positive to expect from this restructuring.\" 2 While studies have shown that discourse usage of discourse connectives can be accurately identified for English [13, <cite>20]</cite> , only a few studies have focused on the disambiguation of discourse connectives in other languages. In this paper, we investigate the usefulness of features proposed in the literature for the disambiguation of English discourse connectives for French discourse connectives.",
  "y": "background motivation"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_1",
  "x": "It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_2",
  "x": "It contains articles from the Wall Street Journal, where discourse connectives that are used in discourse-usage have been annotated by the discourse relation that they signal. The same approach has been used in the French Discourse Treebank (FDTB) [7] , however to date, only discourse-usage and non-discourse-usage of French discourse connectives have been annotated in the FDTB. Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_3",
  "x": "One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_4",
  "x": "For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_5",
  "x": "For example, English discourse connectives include mostly subordinating conjunctions (e.g. when) or coordinating conjunctions (e.g. but). In addition, only a few connectives are disjoint (e.g. On the one hand ... On the other hand). This is not the case for Chinese which uses many more disjoint connectives [26] . Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_6",
  "x": "The disparity between the entropy of discourse connectives in the FDTB and the PDTB can be explained by the differences between the languages and the annotation methodology. Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_7",
  "x": "The disparity between the entropy of discourse connectives in the FDTB and the PDTB can be explained by the differences between the languages and the annotation methodology. Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. ---------------------------------- **FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives.",
  "y": "uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_8",
  "x": "In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_9",
  "x": "The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature). However, we created a new feature (called the Pos Feature) to explicitly indicate the position of the discourse connective within the sentence (i.e.",
  "y": "extends"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_10",
  "x": "**AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively. ---------------------------------- **DATA PREPARATION**",
  "y": "differences similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_11",
  "x": "**RESULTS AND ANALYSIS** ---------------------------------- **RESULTS** Similarly to Pitler and Nenkova<cite> [20]</cite> , we report results using a maximum entropy classifier using ten-fold cross-validation over the extracted datasets. We used the off-the-shelf implementation of the maximum entropy classifier available in WEKA [9] for our experiments.",
  "y": "similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_12",
  "x": "**CONCLUSION AND FUTURE WORK** In this paper, we have investigated the applicability of the syntactic and lexical features proposed by Pitler and Nenkova<cite> [20]</cite> for the disambiguation of English discourse connectives for French. Our experiments on the French Discourse Treebank (FDTB) show that even though the syntactic features are less informative for the disambiguation of French discourse connectives than for English discourse connectives, overall the features can effectively disambiguate French discourse connectives between discourse-usage and non-discourseusage as well in French as in English. The fact that the local syntactic features proposed for English can be used almost as effectively for French and Arabic [4] suggests that lexicalized discourse connectives share certain common structural features cross-linguistically and that these structures are potentially an important component in discourse processing. However, our analysis also shows that the features are not as effective for all connectives.",
  "y": "extends"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_0",
  "x": "We trained an image-caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input. As improvements over previous work we used a 3-layer GRU and employed importance sampling, cyclic learning rates, ensembling and vectorial self-attention. Our results on both MBN and MFCC features are significantly higher than the previous state-of-the-art. The largest improvement comes from using the learned MBN features but our approach also improves results for MFCCs, which are the same features as were used in <cite>[15]</cite> . The learned MBN features provide better performance whereas the MFCCs are more cognitively plausible input features.",
  "y": "similarities differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_1",
  "x": "We trained deep neural networks (DNNs) to create sentence embeddings without the use of prior knowledge of lexical semantics (see [7, 9, 10] for other studies on this task). The visually grounded sentence embeddings that arose capture semantic information about the sentence as measured by the Semantic Textual Similarity task (see [11] ), performing comparably to text-only methods that require word embeddings. In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> .",
  "y": "similarities"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_2",
  "x": "The visually grounded sentence embeddings that arose capture semantic information about the sentence as measured by the Semantic Textual Similarity task (see [11] ), performing comparably to text-only methods that require word embeddings. In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_3",
  "x": "Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_4",
  "x": "Our multimodal encoder maps images and their corresponding captions to a common embedding space. The idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space. Our model consists of two parts; an image encoder and a sentence encoder as depicted in Figure 1 . The approach is based on our own text-based model described in [8] and on the speech-based models described in [13,<cite> 15]</cite> and we refer to those studies for more details. Here, we focus on the differences with previous work.",
  "y": "background uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_5",
  "x": "We take the cosine similarity cos(x, y) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin \u03b1. We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8,<cite> 15]</cite> ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity). While [10] used only the single hardest example in the batch for text-captions, we found that this did not work for the spoken captions. Instead we found that using the hardest 25 percent worked well. The networks are trained using Adam [25] with a cyclic learning rate schedule based on [26] .",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_6",
  "x": "For ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings. The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] .",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_7",
  "x": "---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_8",
  "x": "While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] . Given a sentence embedding created by our model, a classifier has to decide which of the words in its vocabulary occur in the sentence. Based on the original written captions, our database contains 7,374 unique words with a combined occurrence frequency of 324,480.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_9",
  "x": "The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_10",
  "x": "The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_11",
  "x": "Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description).",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_12",
  "x": "Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_13",
  "x": "Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work. There is a large performance gap between the text-caption to image retrieval results and the spoken-caption to image results, showing there is still a lot of room for improvement. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_14",
  "x": "Our results improve significantly over previous approaches for both untrained and trained audio features. In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise.",
  "y": "future_work uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_15",
  "x": "In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise. In the current study, we only investigated whether the model learns to recognise words, but the potential benefit of our model is that it might learn multi-word statements or might even learn to look at sub-lexical level information.",
  "y": "future_work"
 },
 {
  "id": "e803782890224294066ce447671981_0",
  "x": "The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "e803782890224294066ce447671981_1",
  "x": "The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_2",
  "x": "Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_3",
  "x": "**INTRODUCTION** Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_4",
  "x": "Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_5",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_6",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_7",
  "x": "More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_8",
  "x": "This helps us to understand what information about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required. On the other hand, using these simplified patterns, we may loose some structural information important for recovery of non-local dependencies. To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching. The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in <cite>(Johnson, 2002)</cite> . However, additional work remains to be done for our algorithm to perform well on the output of a parser.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_9",
  "x": "As will be described below, this allows us to \"factor out\" tense and modality of finite clauses from our patterns, making the patterns more general. ---------------------------------- **PATTERN EXTRACTION AND MATCHING** After converting the Penn Treebank to a dependency treebank, we first extracted non-local dependency patterns. As in <cite>(Johnson, 2002)</cite> , our patterns are minimal connected fragments containing both nodes involved in a non-local dependency.",
  "y": "similarities uses"
 },
 {
  "id": "e803782890224294066ce447671981_10",
  "x": "When several patterns intersect, as may be the case, for example, when a word participates in more than one nonlocal dependency, these patterns are handled independently. Figure 2 shows examples of dependency graphs (above) and extracted patterns (below, with filled bullets corresponding to the nodes of a nonlocal dependency). As before, dotted lines denote non-local dependencies. The definition of a structure matching a pattern, and the algorithms for pattern matching and pattern extraction from a corpus are straightforward and similar to those described in <cite>(Johnson, 2002)</cite> . The total number of non-local dependencies found in the Penn WSJ is 57325.",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_12",
  "x": "The table also shows the number of times a pattern (together with a specific non-local dependency label) actually occurs in the whole Penn Treebank corpus (the column Dependency count). In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_13",
  "x": "This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> . Our results are presented in Table 3 . The first three columns show the results for those non-local dependencies that are actually covered by our 16 patterns (i.e., for 93.7% of all non-local dependencies).",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_17",
  "x": "On parser output P R f P R f Overall 0.80 0.70 0.75 0.73 0.63 0.68 Table 4 : Results from <cite>(Johnson, 2002)</cite> . It is difficult to make a strict comparison of our results and those in <cite>(Johnson, 2002)</cite> . The two algorithms are designed for slightly different purposes: while <cite>Johnson's approach</cite> allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes). ---------------------------------- **DISCUSSION**",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_18",
  "x": "The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns. One of the reasons for this improvement is that our approach allows us to discriminate between different syntactic phenomena involving non-local dependencies. In most cases our patterns correspond to linguistic phenomena. That helps to understand why a particular construction is easy or difficult for our approach, and in many cases to make the necessary modifications to the algorithm (e.g., adding other features to instances of patterns). For example, for patterns 11 and 12 (see Tables 1 and 2 ) our classifier distinguishes subject and object reasonably well, apparently, because the feature has a local object is explicitly present for all instances (for the examples 11 and 12 in Table 2 , expand has a local object, but do doesn't).",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_19",
  "x": "We think that the aproach allows us to identify those \"hard\" cases. The natural next step in evaluating our algorithm is to work with the output of a parser instead of the original local structures from the Penn Treebank. Obviously, because of parsing errors the performance drops significantly: e.g., in the experiments reported in <cite>(Johnson, 2002 )</cite> the overall fscore decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4 ). While experimenting with Collins' parser (Collins, 1999) , we found that for our algorithm the accuracy drops even more dramatically, when we train the classifier on Penn Treebank data and test it on parser output. One of the reasons is that, since we run our algorithm not on the parser's output itself but on the output automatically converted to dependency structures, conversion errors also contribute to the performance drop.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_20",
  "x": "We extend the pattern matching approach of <cite>Johnson (2002)</cite> with machine learning techniques, and use dependency structures instead of constituency trees. Evaluation on the Penn Treebank shows an increase in accuracy. However, we do not have yet satisfactory results when working on a parser output. The conversion algorithm and the dependency labels we use are largely based on the Penn Treebank annotation, and it seems difficult to use them with the output of a parser. A parsing accuracy evaluation scheme based on grammatical relations (GR), presented in (Briscoe et al., 2002) , provides a set of dependency labels (grammatical relations) and a manually annotated dependency corpus.",
  "y": "differences extends"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_0",
  "x": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] . These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_1",
  "x": "Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency. Recently, encoding the entire knowledge base into a lowdimensional vector space to learn latent representations of entity and relation has attracted widespread attention. These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_2",
  "x": "These knowledge embedding models yield better performance in terms of low complexity and high scalability compared with previous works. Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_3",
  "x": "TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities. These approaches do not explore the fact that relation paths, which are denoted as the sequences of relations, i.e., p=(r 1 , r 2 , . . ., r m ), play an important role in knowledge base completion. For example, the sequence of triples (J.K. Rowling, CreatedRole, Harry Potter), (Harry Potter, Describedin, Harry Potter and the Philosopher's Stone) can be used to infer the new fact (J.K. Rowling, WroteBook, Harry Potter and the Philosopher's Stone), which does not appear in the original KBs. Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> .",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_4",
  "x": "Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h but this path is meaningless for inferring additional relationships between h and t. Therefore, reliable relation paths are urgently needed. Moreover, their consistent semantics, which is essential for knowledge representation learning, is consistent with the semantics of relation r. Based on this intuition, we propose a compositional learning model of relation path embedding (RPE), which extends the projection and type constraints of the specific relation to the specific path.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_5",
  "x": "As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA. Figure 1 illustrates the basic idea for relation-specific and path-specific projections. Each entity is projected by M r and M p into the corresponding relation and path spaces. These different embedding spaces hold the following hypothesis: in the relation-specific space, relation r is regarded as a translation from head h r to tail t r ; likewise, p * , the path representation by the composition of relation embeddings, is regarded as a translation from head h p to tail t p in the path-specific space.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_6",
  "x": "Note that each relation should possess Domain and Range fields to indicate the subject and object types, respectively. For example, the relation haschildren's Domain and Range types both belong to a person. By exploiting these limited rules, the harmful influence of a merely data-driven pattern can be avoided. Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space. A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> .",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_7",
  "x": "A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> . PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings. PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> . PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations. In large-scale KBs, relation paths have great significance for enhancing the reasoning ability for more complicated situations.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_8",
  "x": "**EVALUATION PROTOCOL** We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014;<cite> Lin et al., 2015b]</cite> . First, for each test triple (h,r,t), we replace h or t with every entity in \u03b6. Second, each corrupted triple is calculated by the corresponding score function S(h,r,t). The final step is to rank the original correct entity with these scores in descending order.",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_9",
  "x": "If these corrupted triples are reserved, then we call this setting \"Raw\". In both settings, if the latent representations of entity and relation are better, then a lower mean rank and a higher Hits@10 should be achieved. Because we use the same dataset, the baseline results reported in<cite> [Lin et al., 2015b</cite>;<cite> Lin et al., 2015a</cite>; Ji et al., 2016] are directly used for comparison. ---------------------------------- **IMPLEMENTATION**",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_10",
  "x": "Table 3 presents the separated evaluation results by mapping properties of relations on FB15K. Mapping properties of relations follows the same rules in [Bordes et al., 2013] , and the metrics are Hits@10 on head and tail entities. From Table 3 , we can conclude that 1) RPE (ACOM) outperforms all baselines in all mapping properties of relations. In particular, for the 1-to-N, N-to-1, and N-to-N types of relations [Bordes et al., 2013] 75.9 70.9 77.8 TransE (bern) [Bordes et al., 2013] 75.9 81.5 85.3 TransH (unif) [Wang et al., 2014] 77.7 76.5 78.4 TransH (bern) [Wang et al., 2014] 78.8 83.3 85.8 TransR (unif)<cite> [Lin et al., 2015b]</cite> 85.5 74.7 79.2 TransR (bern)<cite> [Lin et al., 2015b]</cite> 85.9 82.5 87.0 PTransE (ADD, 2-hop)<cite> [Lin et al., 2015a]</cite> 80.9 73.5 83.4 PTransE (MUL, 2-hop)<cite> [Lin et al., 2015a]</cite> 79.4 73.6 79.3 PTransE (ADD, 3-hop)<cite> [Lin et al., 2015a]</cite> 80 that plague knowledge embedding models, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity's prediction and 6.9%, 7.0%, and 5.1% on tail entity's prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop). 2) RPE (MCOM) does not perform as well as RPE (ACOM), and we believe that this result is because RPE's path representation is not consistent with RPE (MCOM)'s composition of projections.",
  "y": "differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_11",
  "x": "For a test triple (h,r,t), if its score S(h,r,t) is below \u03b4 r , then we predict it as a positive one; otherwise, it is negative. {\u03b4 r } is obtained by maximizing the classification accuracies on the valid set. ---------------------------------- **IMPLEMENTATION** We directly compare our model with prior work using the results about knowledge embedding models reported in<cite> [Lin et al., 2015b]</cite> n=50, m=50, \u03b3 1 =5, \u03b3 2 =6, \u03b1=0.0001, B=1440, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on WN11; n=100, m=100, \u03b3 1 =3, \u03b3 2 =6, \u03b1=0.0001, B=960, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on FB13; and n=100, m=100, \u03b3 1 =4, \u03b3 2 =5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7 =0.05, taking the L 1 norm on FB15K.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_0",
  "x": "While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_1",
  "x": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_2",
  "x": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_3",
  "x": "We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation).",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_4",
  "x": "The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] .",
  "y": "motivation uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_5",
  "x": "This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_6",
  "x": "---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_7",
  "x": "---------------------------------- **RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_8",
  "x": "In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space. In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_9",
  "x": "In parallel, images are processed through a Convolutional Neural Network (CNN) pre-trained on ImageNet [15] , extracting the activations of the last fully connected layer to be used as a representation of the images. To solve the dimensionality matching between both representations (the output of the GRUs and the last fully-connected of the CNN) an affine transformation is applied on the image representation. Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation. The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task. FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM).",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_10",
  "x": "**MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_11",
  "x": "**MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer.",
  "y": "differences extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_12",
  "x": "The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences. To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair.",
  "y": "background differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_13",
  "x": "The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair. Assuming that a correct pair of elements should be closer in the multimodal space than a random pair. The loss L can be formally defined as follows: Where i is an image vector, c is its correct caption vector, and i k and c k are sets of random images and captions respectively.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_14",
  "x": "In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks. To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_15",
  "x": "**EXPERIMENTS** In this section we evaluate the impact of using the FNE in a multimodal pipeline (FN-MME) for both image annotation and image retrieval tasks. To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_16",
  "x": "It contains 31,783 photographs of everyday activities, events and scenes. Five correct captions are provided for each image. In our experiments 29,000 images are used for training, 1,014 conform the validation set and 1,000 are kept for test. These splits are the same ones used by <cite>Kiros et al. [1]</cite> and by Karpathy and Fei-Fei [22] . The MSCOCO dataset [13] includes images of everyday scenes containing common objects in their natural context.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_17",
  "x": "Using all the words present in the dataset is likely to produce overfitting problems when training on examples containing words that only occur a few times. This overfitting problem may not have a huge impact on performance, but it may add undesired noise in the multimodal representation. The original setup [<cite>1]</cite> limited the word embedding to the 300 most frequent words, while using 300 GRUs. The Bi-LSTM model [25] in contrast defines the vocabulary size to include words appearing more than 5 time in the dataset, leading to dictionaries of size 2,018 for Flickr8k, 7,400 for Flickr30k and 8,801 for MSCOCO. Our own preliminary experiments on the validation set showed that increasing multimodal space dimensionality and dictionary length slightly improved the performance of image retrieval, in detriment of image annotation.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_18",
  "x": "**RESULTS** For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA). Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> .",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_19",
  "x": "We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_20",
  "x": "Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_21",
  "x": "Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_22",
  "x": "---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_23",
  "x": "For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_24",
  "x": "For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding. These results suggest that the visual representation provided by the FNE is superior to the current standard for the construction of most multimodal embeddings. When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "future_work"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_0",
  "x": "In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_1",
  "x": "For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006) . For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005) . In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach.",
  "y": "motivation"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_2",
  "x": "In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses. We apply beam search to decoding instead of greedy search. The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.'s approach.",
  "y": "differences uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_3",
  "x": "The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> . The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue. A stack holds the partial parse trees that are built during the parsing process. A parse state is defined as a stack,queue pair.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_4",
  "x": "Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008) .",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_5",
  "x": "Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008) .",
  "y": "extends"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_6",
  "x": "The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of Figure 2 : the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 .",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_7",
  "x": "The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of Figure 2 : the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_8",
  "x": "**RESTRICTIONS ON THE SEQUENCE OF ACTIONS** Not all sequences of actions produce valid binarized trees. In the deterministic parser of <cite>Wang et al. (2006)</cite> , the highest scoring action predicted by the classifier may prevent a valid binary tree from being built. In this case, Wang et al. simply return a partial parse consisting of all the subtrees on the stack. In our parser a set of restrictions is applied which guarantees a valid parse tree.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_9",
  "x": "S0cS1cp, S0cS1cq Table 1 : Feature templates these templates by first instantiating a template with particular labels, words and tags, and then pairing the instantiated template with a particular action. In the table, the symbols S 0 , S 1 , S 2 , and S 3 represent the top four nodes on the stack, and the symbols N 0 , N 1 , N 2 and N 3 represent the first four words in the incoming queue. S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_10",
  "x": "The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\".",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_11",
  "x": "S 0 L, S 0 R and S 0 U represent the left and right child for binary branching S 0 , and the single child for unary branching S 0 , respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_12",
  "x": "The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack. It takes three different values: 1 (no matching right bracket has been pushed onto stack), 2 (a matching right bracket has been pushed onto stack) and 3 (a matching right bracket has been pushed onto stack, but then popped off). The \"Separator\" row shows features that include one of the separator punctuations (i.e. \"\uff0c\", \"\u3002\", \"\u3001\" and \"\uff1b\") between the head words of S 0 and S 1 .",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_13",
  "x": "Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_14",
  "x": "Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_15",
  "x": "The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5 Table 3 : Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002) . The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3 . The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 .",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_16",
  "x": "The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_17",
  "x": "The results of various models using automatically assigned POS-tags are shown in Table 4 . The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_18",
  "x": "Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, POS tagging accuracy is not very high, with the perceptron-based tagger achieving an accuracy of only 93%. The beam-search decoding framework we use could accommodate joint parsing and tagging, although the use of features based on the tags of incoming words complicates matters somewhat, since these features rely on tags having been assigned to all words in a pre-processing step.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_19",
  "x": "Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) . The main difference between our parser and Ratnaparkhi's is that we use a global discriminative model, whereas Ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_20",
  "x": "An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999) . The main difference between our parser and Ratnaparkhi's is that we use a global discriminative model, whereas Ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process.",
  "y": "differences"
 },
 {
  "id": "e9b2f32ed29589b4a6d49d3b30fc3a_0",
  "x": "By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation <cite>(Brown et al. 1993</cite> ) and information retrieval (Franz, M. and McCarley, S. 2002) . In this paper, we present a cross-lingual English-Arabic search engine combined with an on demand ArabicEnglish statistical machine translation system that relies on source language analysis for both improved search and translation. We developed novel statistical learning algorithms for performing Arabic word segmentation (Lee, Y. et al 2003) into morphemes and morphological source language (Arabic) analysis (Lee, Y. et al 2003b) . These components improve both monolingual (Arabic) search and cross-lingual (English-Arabic) search and machine translation. In addition, the system supports either document translation or convolutional models for cross-lingual search (Franz, M. and McCarley, S. 2002) .",
  "y": "motivation extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_0",
  "x": "Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. ---------------------------------- **INTRODUCTION** Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (<cite>Rush et al., 2015</cite>) . This paper also shares a similar goal and motivation to <cite>previous work</cite>: improving the encoderdecoder models for natural language generation.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_1",
  "x": "The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_2",
  "x": "However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences.",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_5",
  "x": "Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_6",
  "x": "We expect that the quality of headlines will improve with this reasonable combination (<cite>ABS</cite> and AMR). ---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_7",
  "x": "---------------------------------- **<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_8",
  "x": "Then, <cite>ABS</cite> outputs a summary\u0176 given an input sentence X as follows: where nnlm(Y C,i ) is a feed-forward neural network language model proposed in (Bengio et al., 2003) , and enc(X, Y C,i ) is an input sentence encoder with attention mechanism. This paper uses D and H as denoting sizes (dimensions) of vectors for word embedding and hidden layer, respectively. Let E \u2208 R D\u00d7|V | be an embedding matrix of output words. Moreover, let U \u2208 R H\u00d7(CD) and O \u2208 R |V |\u00d7H be weight matrices of hidden and output layers, respectively 1 .",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_9",
  "x": "Then, we define the attention-based AMR encoder 'encAMR(A, Y C,i )' as follows: Finally, we combine our attention-based AMR encoder shown in Equation 14 as an additional term of Equation 3 to build our headline generation system. ---------------------------------- **EXPERIMENTS** To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in <cite>Rush et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_11",
  "x": "---------------------------------- **DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_12",
  "x": "**DUC-2004** <cite>Gigaword</cite> test data used <cite>Gigaword</cite> in (<cite>Rush et al., 2015</cite>) Our sampled test data (<cite>Rush et al., 2015</cite>) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_13",
  "x": "For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder.",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_14",
  "x": "For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_15",
  "x": "In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters. We tried several values for the initial learning rate, and selected the value that achieved the best performance for each method. We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_17",
  "x": "We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_18",
  "x": "We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch. Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_19",
  "x": "The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_20",
  "x": "<cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_21",
  "x": "The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_22",
  "x": "Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_23",
  "x": "We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results. This means that the 7 https://github.com/facebook/NAMAS I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported .",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_26",
  "x": "<cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_27",
  "x": "To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus. \"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2. We can also observe that <cite>ABS</cite>+AMR achieved the best ROUGE-1 scores on all of the test data. According to this fact, <cite>ABS</cite>+AMR tends to successfully yield semantically important words.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_29",
  "x": "\"<cite>Gigaword</cite> (randomly sampled)\" in Table 1 shows the results of this setting. We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2. We can also observe that <cite>ABS</cite>+AMR achieved the best ROUGE-1 scores on all of the test data. According to this fact, <cite>ABS</cite>+AMR tends to successfully yield semantically important words. In other words, embeddings encoded through the AMR encoder are useful for capturing important concepts in input sentences.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_35",
  "x": "In other words, the encoder without the attention mechanism tends to be overfitting. ---------------------------------- **RELATED WORK** Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of <cite>Rush et al. (2015)</cite> : the combination of the feed-forward neural network language model and attention-based sentence encoder. also adapted the RNN encoder-decoder with attention for headline generation tasks.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_0",
  "x": "**ABSTRACT** This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi \u2192 Nepali direction in which we have examined the performance of a Recursive Neural Network (RNN) based Neural Machine Translation (NMT) system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by <cite>(Artetxe et al., 2017)</cite> and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_1",
  "x": "This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a) , utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016) , complete unsupervised architectures <cite>(Artetxe et al., 2017)</cite> (Lample et al., 2018 ) and many others have been proposed. ---------------------------------- **UTILIZING MONOLINGUAL DATA IN NMT**",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_2",
  "x": "In (Burlot and Yvon, 2018) , it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in <cite>(Artetxe et al., 2017)</cite> follows an architecture where encoder is shared and decoder is separate for each language. Encoder tries to map sentences from both languages in the same space, which is supported by cross-lingual word embeddings. They fix cross-lingual word embeddings in the encoder while training, which helps in generating cross-lingual sentence representations in the same space.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_3",
  "x": "Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized. We have utilized architecture proposed in <cite>(Artetxe et al., 2017)</cite> where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation. This architecture can also be utilized for completely unsupervised setting. Third system is also a pure RNN based NMT system where additional parallel data (synthetic data) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized. In this way the amount of available data becomes three times of the original data.",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_4",
  "x": "All the data is combined together, shuffled and then provided to the NMT system, there is no identification provided to distinguish between parallel data and copy data. To train all three systems we have utilized the implementation of <cite>(Artetxe et al., 2017)</cite> . ---------------------------------- **EXPERIMENTAL DETAILS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_0",
  "x": "Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings. ---------------------------------- **INTRODUCTION** Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; <cite>Conneau et al., 2018</cite>; , the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_2",
  "x": "Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_3",
  "x": "In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space. Seminal work in supervised alignment of word vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_4",
  "x": "In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in<cite> Conneau et al. (2018)</cite> . Contributions We show that Generalized Procrustes Analysis (GPA) (Gower, 1975) , a method that maps two vector spaces into a third, latent space, is superior to PA for BDI, e.g., improving the state-of-the-art on the widely used EnglishItalian dataset (Dinu et al., 2015) from a P@1 score of 66.2% to 67.6%. We compare GPA to PA 1 Two vector spaces are isomorphic if there is an invertible linear transformation from one to the other. on aligning English with five languages representing different language families (Arabic, German, Spanish, Finnish, and Russian) , showing that GPA consistently outperforms PA. GPA also allows for the use of additional support languages, aligning three or more languages at a time, which can boost performance even further.",
  "y": "differences"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_5",
  "x": "In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_6",
  "x": "In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_7",
  "x": "**EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_8",
  "x": "We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_9",
  "x": "When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000. We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_10",
  "x": "Results are listed in Table 2 . GPA improves over PA consistently for all five languages. Most notably, for Finnish it scores 2.5% higher than PA. Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_11",
  "x": "We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings. Results are reported in Table 3 . We observe that GPA outperforms PA consistently on Italian and German with the WaCky embeddings, and on all languages with the Wikipedia embeddings. Notice that once more, Finnish benefits the most from a switch to GPA in the Wikipedia embeddings setting, but it is also the only language to suffer from that switch in the WaCky setup. Interestingly, PA fails to learn a good alignment for Italian and Finnish when supervised with numerals, while GPA performs comparably with numerals as with other forms of supervision.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_13",
  "x": "This serves to show that the learned alignments are generally good, but they are not sufficiently precise. This issue can have two sources: a suboptimal method for learning the alignment and/or a ceiling effect on how good of an alignment can be obtained, within the space of orthogonal linear transformations. ---------------------------------- **PROCRUSTES FIT** To explore the latter issue and to further compare the capabilities of PA and GPA, we perform a Procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of<cite> Conneau et al. (2018)</cite> 9 for both training and evaluation 10 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_14",
  "x": "Bilingual embeddings Many diverse crosslingual word embedding models have been proposed . The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013) . In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017 Artetxe et al., , 2018 <cite>Conneau et al., 2018</cite>; Lu et al., 2015) . The approach most similar to ours, Faruqui and Dyer (2014) , uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t \u2212 1.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_0",
  "x": "---------------------------------- **INTRODUCTION** To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_1",
  "x": "To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010) .",
  "y": "motivation"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_2",
  "x": "**RELATED WORK** A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004) . These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, <cite>Primadhanty et al. (2015)</cite> explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_3",
  "x": "Here, parameter learning can be accomplished via Markov chain Monte Carlo or stochastic gradient descent. ---------------------------------- **EXPERIMENTS** As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization<cite> (Primadhanty et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_4",
  "x": "**SETTINGS** Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not. all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_5",
  "x": "This dataset contains five tags: person (PER), location (LOC), organization (ORG), miscellaneous (MISC), and non-entities (O). Features. We used a subset of features from experiments performed by <cite>Primadhanty et al. (2015)</cite> . Table 3 summarizes the features used in our experiment, including context and entity features. Tools.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_6",
  "x": "---------------------------------- **RESULTS** Table 2 presents results of our experiments. Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_7",
  "x": "**RESULTS** Table 2 presents results of our experiments. Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features.",
  "y": "similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_8",
  "x": "For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features. The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_9",
  "x": "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_10",
  "x": "Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION** In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification.",
  "y": "differences similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_11",
  "x": "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_12",
  "x": "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
  "y": "differences"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_0",
  "x": "Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj\u00f6rkelund et al., 2017) . Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by <cite>Tsarfaty et al. (2010</cite> Tsarfaty et al. ( , 2013 : \u2022 Can we represent words abstractly so as to reflect shared morphological aspects between them? \u2022 Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?",
  "y": "motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_1",
  "x": "For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task. These results suggest that neural dependency parsers do not adequately infer this crucial linguistic feature directly from the input text. ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_2",
  "x": "The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial.",
  "y": "background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_3",
  "x": "The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help. Pipeline model vs. Multi-task learning In general, MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial.",
  "y": "differences"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_0",
  "x": "This paper gives an Abstract Categorial Grammar (ACG) account of<cite> (Kallmeyer and Kuhlmann, 2012)</cite>'s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG derivation trees, we have a unified picture where derivation trees and dependency trees are related but independent equivalent ways to account for the same surface-meaning relation. ---------------------------------- **INTRODUCTION**",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_1",
  "x": "Derivation trees soon appeared as good candidates to encode semantic-like relations between the elementary trees they glue together. However, some mismatch between these trees and the relative scoping of logical connectives and relational symbols, or between these trees and the dependency relations, have been observed. Solving these problems often leads to modifications of derivation tree structures (Schabes and Shieber, 1994; Kallmeyer, 2002; Joshi et al., 2003; Rambow et al., 2001; Chen-Main and Joshi, To appear) . While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently<cite> (Kallmeyer and Kuhlmann, 2012)</cite> has proposed a transformation from standard derivation trees to dependency trees. This paper provides an ACG perspective on this transformation.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_2",
  "x": "The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define. ---------------------------------- **ABSTRACT CATEGORIAL GRAMMARS**",
  "y": "background motivation"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_3",
  "x": ") with the proviso that for any constant c : 2 We refer the reader to (Pogodalla, 2009 ) for the details. 3 The TAG literature typically uses this example, and<cite> (Kallmeyer and Kuhlmann, 2012)</cite> as well, to show the mismatch between the derivation trees and the expected se- This sentence is usually analyzed in TAG with a derivation tree where the to love component scopes over all the other arguments, and where claims and seems are unrelated, as Fig. 2(a) shows. The three higher-order signatures are: \u03a3 der\u03b8 : Its atomic types include s, vp, np, s A , vp A . . . where the X types stand for the categories X of the nodes where a substitution can occur while the X A types stand for the categories X of the nodes where an adjunction can occur. For each elementary tree \u03b3 lex.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_4",
  "x": "bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's process to translate derivation trees into dependency trees is a two-step process. The first one does the actual transformation, using macro-tree transduction, while the second one modifies the way to get the yield from the dependency trees rather than from the derivation ones. ---------------------------------- **FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b).",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_5",
  "x": "**FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_6",
  "x": "**FROM DERIVATION TO DEPENDENCY TREES** This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_7",
  "x": "We represent the dependency tree for (1) as In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once. (Yoshinaka, 2006) has shown how to encode such MTTs as the composition G \u2022 G \u22121 of two ACGs, and we will use a very similar construct.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_8",
  "x": "The yield of the derived tree resulting from the operations of the derivation tree \u03b3 of Fig. 3 defined in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , w 2 where x, y denotes a tuple of strings. Because of the adjunction, the corresponding dependency structure has a reverse order (\u03b3 = \u03b3 a (\u03b3 i )), the requirement on yield dep imposes that In the interpretation of derivation trees as strings, initial trees (with no substitution nodes) Abstract Indeed, an initial tree can have several adjunction sites. In this case, to be ready for another adjunction after a first one, the first result itself should be a tuple of strings.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_9",
  "x": "(2) a. John Bill claims Mary seems to love b. John Mary seems to love (3) John Bill seems to claim Mary seems to love Remark. Were we not interested in the yields but only in the dependency structures, we wouldn't have to manage this ambiguity. This is true both for<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's approach and ours. But as we have here a unified framework for the two-step process they propose, this lexical blow up will result in a multiplicity of types as Section 5 shows. ----------------------------------",
  "y": "background motivation similarities"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_10",
  "x": "In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love . C 10 to love and C 01 to love are used for sentences where only one adjunction at the s or at the vp node occurs respectively. C 00 to love : np np s is used when no adjunction occurs. 6 This really mimics (Yoshinaka, 2006) 's encoding of<cite> (Kallmeyer and Kuhlmann, 2012)</cite> MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it). The superscripts (2(n + 1))(2(n \u2212 1)) express that an adjunction that has n adjunctions above it is translated as a function that takes a 2(n + 1)-tuple as argument and returns a 2(n \u2212 1)-tuple.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_11",
  "x": "---------------------------------- **CONCLUSION** In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_12",
  "x": "Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_13",
  "x": "Dealing with typed trees to represent derivation trees allows us to provide a meaningful (wrt. the TAG formalism) abstract vocabulary \u03a3 der\u03b8 encoding this macro-tree transducer. The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_0",
  "x": "By contrast, information about the sender of an email message has always been explicitly represented in the message headers, starting with early standardization attempts (Bhushan et al., 1973) and including the two decade old current standard (Crocker, 1982) . Applications that aim to present voicemail messages through an email-like interface -take as an example the idea of a \"uniform inbox\" presentation of email, voicemail, and other kinds of messages 2 -must deal with the problem of how to obtain information analogous to what would be contained in email headers. Here we will discuss one way of addressing this problem, treating it exclusively as the task of extracting relevant information from voicemail transcripts. In practice, e.g. in the context of a sophisticated voicemail front-end ) that is tightly integrated with an organization-wide voicemail system and private branch exchange (PBX), additional sources of information may be available: the voicemail system or the PBX might provide information about the originating station of a call, and speaker identification can be used to match a caller's voice against models of known callers ). Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of <cite>Huang et al. (2001)</cite> , but the features and techniques we use are very different.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_1",
  "x": "While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names. <cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner .",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_2",
  "x": "While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates. This allows for more or less inde-pendent optimization of recall and precision, somewhat similar to the PNrule classifier learner . We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_3",
  "x": "While this is less of a problem when evaluating on manual transcriptions, the experience reported in<cite> (Huang et al., 2001)</cite> suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name extraction on automatically generated transcripts. We therefore avoid using anything but a small number of greetings and commonly occurring words like 'hi', 'this', 'is' etc. and a small number of common first names for extracting caller phrases and use positional information in addition to word-based features. We locate caller phrases by first identifying their start position in the message and then predicting the length of the phrase. The empirical distribution of caller phrase lengths in the development data is shown in Figure 2 . Most caller phrases are between two and four words long ('it's Pat', 'this is Pat Caller') and there are moderately good lexical indicators that signal the end of a caller phrase ('I', 'could', 'please', etc.).",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_4",
  "x": "Since we are dealing with discrete word indices in both cases, we treat this as a classification task, rather than a regression task. A large number of classifier learners can be used to automatically infer classifiers for the two subtasks at hand. We chose a decision tree learner for convenience and note that this choice does not affect the overall results nearly as much as modifying our feature inventory. Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_6",
  "x": "Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_7",
  "x": "While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_8",
  "x": "While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly. In fact, it seems rather likely that the log-linear models and the features they employ over-fit the training data.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_10",
  "x": "In fact, we count caller phrases as correct as long as they contain the full name of the caller, since this is the common denominator in the otherwise somewhat heterogeneous labeling of our training corpus; more on this issue in the next section. The difference between the approach in<cite> (Huang et al., 2001</cite> ) and ours may be partly due to the performance of the ASR components: <cite>Huang et al. (2001)</cite> report a word error rate of 'about 35%', whereas we used a recognizer (Bacchiani, 2001 ) with a word error rate of only 23%. Still, the reduced performance of the HZP model on ASR transcripts compared with manual transcripts is points toward overfitting, or reliance on features that do not generalize to ASR transcripts. Our main approach, on the other hand, uses classifiers that are extremely knowledgepoor in comparison with the many features of the log-linear models for the various named entity taggers, employing no more than a few dozen categorical features. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_11",
  "x": "The features we made available to the learner were the length of the phone number in numeric digits, its distance from the end of the message, and a small number of lexical cues in the surrounding context of a candidate number ('call', 'number', etc.) . This approach (which we call classify below) increases the precision of the combined two steps to acceptable levels without hurting recall too much. A comparison of performance results is presented in Table 4 . Rows HZP rules and HZP log-linear refer to the rule-based baseline and the best log-linear model of<cite> (Huang et al., 2001</cite> ) and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths. Our main results appear in the remaining rows.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_12",
  "x": "Finally, if we use a decision tree classifier in the second phase, we can achieve extremely high precision with a minimal impact on recall. Our two-phase procedure outperforms all other methods we considered. We evaluated the performance of our best models on the same 101 unseen ASR transcripts used above in the evaluation of the caller phrase extraction. The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_13",
  "x": "The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers. So the observed performance difference is most likely due to the difference in extraction methods. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_14",
  "x": "Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_15",
  "x": "\u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_16",
  "x": "This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ). \u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_0",
  "x": "Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005) , sentence compression<cite> (Sporleder and Lapata, 2005)</cite> , natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006) . These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure. For example, the tree in Figure 1 shows a sentence-level discourse tree for the string \"Prices have dropped but remain quite high, according to CEO Smith,\" which has three discourse segments, each labeled with either \"Nucleus\" or \"Satellite\" depending on how central the segment is to the coherence of the text. There are a number of corpora annotated with discourse structure, including the well-known RST Treebank (Carlson et al., 2002) ; the Discourse GraphBank (Wolf and Gibson, 2005) ; and the Penn Discourse Treebank (Miltsakaki et al., 2004) . While the annotation approaches differ across these corpora, the requirement of sentence segmentation into sub-sentential discourse units is shared across all approaches.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_1",
  "x": "<cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers. The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments. They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_2",
  "x": "Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features.",
  "y": "motivation"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_3",
  "x": "They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features. Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios. While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_4",
  "x": "<cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_5",
  "x": "<cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used.",
  "y": "differences"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_6",
  "x": "Soricut and Marcu (2003) omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing. By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_7",
  "x": "Then In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type. In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_8",
  "x": "In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2. ---------------------------------- **BASELINE SPADE SETUP**",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_9",
  "x": "The default script that comes with SPADE does not turn off tokenization inside of the parser, which leads to degraded performance when the input has already been tokenized in the Penn Treebank style. Secondly, Charniak and Johnson (2005) showed how reranking of the 50-best output of the Charniak (2000) parser gives substantial improvements in parsing accuracy. These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the <cite>Sporleder and Lapata (2005)</cite> 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence initial boundaries in the scoring, as discussed in the previous section.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_10",
  "x": "This feature set is very close to that used in <cite>Sporleder and Lapata (2005)</cite> , but not identical. Their n-gram feature definitions were different (though similar), and they made use of cue phrases from Knott (1996) . In addition, they used a rulebased clauser that we did not. Despite such differences, this feature set is quite close to what is described in that paper. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_11",
  "x": "This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence<cite> (Sporleder and Lapata, 2005</cite>; Marcu, 1998; Marcu, 1999; Cristea et al., 2005) . Once again, the finite-state system does not perform statistically significantly different from SPADE on either labeled or unlabeled discourse parsing. Using all features, however, results in greater than 5% absolute accuracy improvement over both of these systems, which is significant, in all cases, at p < 0.001. ---------------------------------- **DISCUSSION AND FUTURE DIRECTIONS**",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_0",
  "x": "Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available. There has been a resurgence of interest in unsupervised parsing during the last few years. Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings \u2264 10 words (WSJ10) using a constituentcontext model called CCM. Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10. While Klein and Manning's approach may be described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP <cite>(Bod 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_1",
  "x": "Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included. In this paper we will also test an alternative model for unsupervised all-subtrees 400 parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima'an (2005) , and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus. While we do not achieve as high an f-score as the UML-DOP model in<cite> Bod (2006)</cite> , we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in<cite> Bod (2006)</cite> . We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995) , showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing. Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition.",
  "y": "uses differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_2",
  "x": "Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows: (1) Divide a corpus into an EC and HC (2) Assign all unlabeled binary trees to the sentences in EC, and store them in a shared parse forest (3) Convert the subtrees from the parse forests into a compact PCFG reduction (see next section) (4) Compute the shortest derivations for the sentences in HC (as in DOP*) (5) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG (6) Use the STSG to compute the most probable parse trees for new test data by means of Viterbi n-best (see next section) We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?",
  "y": "extends"
 },
 {
  "id": "f54235664f013f0fec918222be9198_3",
  "x": "(1) Divide a treebank into an EC and HC (2) Convert the subtrees from EC into a PCFG reduction (3) Compute the shortest derivations for the sentences in HC (by simply assigning each subtree equal weight and applying Viterbi 1-best) (4) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG Zollmann and Sima'an show that the resulting estimator is consistent. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003) . Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in<cite> Bod (2006)</cite> is particularly time consuming and can only operate by randomly sampling trees. Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_4",
  "x": "The total number of nodes is cubic in sentence length n. This means that there are O(n 3 ) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied. This is a huge reduction compared to<cite> Bod (2006)</cite> where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2 , where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003) . Figure 2. PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG.",
  "y": "differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_5",
  "x": "PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG. The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima'an 2005, theorem 5.2) . This means that U-DOP* results in an STSG which is much more succinct than previous DOP-based STSGs. Moreover, as in Bod (1998 Bod ( , 2000 , we use an extension of Good-Turing to smooth the subtrees and to deal with 'unknown' subtrees. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_6",
  "x": "To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and<cite> Bod (2006)</cite> : Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation. As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings. The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Sch\u00fctze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5). Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa. We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Manning (2002, 2004) .",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_7",
  "x": "The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> . For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_8",
  "x": "For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998 Bod ( , 2000 to include 'unknown' subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_9",
  "x": "For these supervised parsers, we employed the standard training set, i.e. Penn's WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model. Table 5 . Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penn's WSJ As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set. While a similar result was obtained in<cite> Bod (2006)</cite> , the absolute difference between unsupervised parsing and the treebank grammar was extremely small in<cite> Bod (2006)</cite>: 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_0",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner).",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_1",
  "x": "**INTRODUCTION** BERT (Devlin et al., 2018) represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task (Peters et al., 2018; Radford et al., 2018) . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR.",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_2",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner).",
  "y": "extends"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_3",
  "x": "Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT. Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering.",
  "y": "differences uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_4",
  "x": "We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking. Each paragraph is then fed into the BERT reader along with the original natural language question for inference. Our reader is built using Google's reference implementation, but with a small tweak: to allow comparison and aggregation of results from different segments, we remove the final softmax layer over different answer spans; cf. . For each candidate paragraph, we apply inference over the entire paragraph, and the reader selects the best text span and provides a score.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_5",
  "x": "We then combine the reader score with the retriever score via linear interpolation: S = (1 \u2212 \u00b5) \u00b7 S Anserini + \u00b5 \u00b7 S BERT , where \u00b5 \u2208 [0, 1] is a hyperparameter (tuned on a training sample). One major shortcoming with BERTserini is that <cite>Yang et al. (2019)</cite> only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents. This contrasts with the diversity of paragraphs that the model will likely encounter at inference time, since they are selected from potentially millions of articles. The solution to this problem, of course, is to fine tune BERT with the types of paragraphs it is likely to see at inference time. Unfortunately, such data does not exist for modern QA test collections.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_6",
  "x": "For these, we use the 2018-12-01 dump of Chinese Wikipedia, tokenized with Lucene's CJKAnalyzer into overlapping bigrams. We apply hanziconv 1 to transform the corpus into simplified characters for CMRC and traditional characters for DRCD. Following <cite>Yang et al. (2019)</cite> , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans. As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_7",
  "x": "As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score. Statistics for the datasets are shown in Table 4. 2 For data augmentation, based on preliminary experiments, we find that examining n = 10 candidates from passage retrieval works well, and we further discover that effectiveness is insensitive to the amount of negative samples. Thus, we eliminate the need to tune d by simply using all passages that do not contain the answer as negative examples.",
  "y": "similarities"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_8",
  "x": "**RESULTS** Our main results on SQuAD are shown in Table 2 . The row marked \"SRC\" indicates fine tuning with SQuAD data only and matches the BERTserini condition of <cite>Yang et al. (2019)</cite> ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade). As expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using DS(+) definitely (Wang et al., 2017) 29.1 37.5 - Kratzwald and Feuerriegel (2018) 29.8 --Par. R. 28.5 -83.1 Par.",
  "y": "differences"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_0",
  "x": "Co-occurrence networks of words (or adjacency networks) have accounted for most of the models tackling textual applications. In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_1",
  "x": "Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ). However, when deep analyzes are performed, network-based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. In order to improve the performance of network-based applications, I suggest a twofold research line: (i) the introduction of measurements consistent with the nature of the problem; and (ii) the combination of topological strategies with other traditional natural language processing methods.",
  "y": "background motivation future_work"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_2",
  "x": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_3",
  "x": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "future_work motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_4",
  "x": "Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. For this reason, I believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_5",
  "x": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "uses"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_6",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_7",
  "x": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "uses"
 },
 {
  "id": "f856c4fb5e6e00729d33b15b24aff6_0",
  "x": "These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress <cite>(Cassell et al., 2000</cite>; Koller and Stone, 2007) shows that there's still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon. Modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings.",
  "y": "motivation future_work"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_0",
  "x": "In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_1",
  "x": "In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; . This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_2",
  "x": "This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_3",
  "x": "The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of \"out-of-vocabulary\" or \"unknown\" outputs. Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) .",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_4",
  "x": "Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) . The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et al., 2013) . Another focus of recent research has been ways of using monolingual corpus data, available in much larger quantities, to augment the limited parallel corpora used to train translation models.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_5",
  "x": "**MODEL DESCRIPTION** The model identified as metamind-single is based on the attention-based encoder-decoder framework described in <cite>Luong (2015)</cite> , using the attention mechanism referred to as \"Global attention (dot). \" The encoder is a four-layer stacked LSTM recurrent neural network whose inputs (at the bottom layer) are vectors w in t corresponding to the subword units in the input sentence and which saves the topmost output state at each timestep e t as the variable-length encoding matrix E. The decoder also contains a four-layer stacked LSTM whose states (c 0 and h 0 for each layer) are initialized to the last states for each layer of the encoder. At the first timestep, the decoder LSTM receives as input an initialization word vector w out 0 ; its topmost output state h t is concatenated with an encoder context vector \u03ba t computed as: score(h t , e s ) = h t e s \u03b1 st = softmax all s (score(h t , e s )) This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_6",
  "x": "This concatenated output is then fed through an additional neural network layer to produce a final attentional output vectorh, which serves as input to the output softmax:h output probabilities = softmax(W outh ) For subsequent timesteps, the decoder LSTM receives as input the previous word vector w out t\u22121 concatenated with the previous output vectorh. Decoding is performed using beam search, with beam width 16. The beam search decoder differs slightly from <cite>Luong (2015)</cite> in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences.",
  "y": "differences"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_7",
  "x": "Results for all three runs described above are presented in Table 1 . Only the ensemble was submitted to the human evaluation process, with a final ranking of second place (behind U. Edinburgh's ensemble of four independently initialized models). Our best single model matches the performance of the best model from U. Edinburgh, which applies a similar attentional framework, subword splitting, and back-translated augmentation. The Y-LSTM model underperformed relative to the model based on <cite>Luong (2015)</cite> , but provided a small additional boost to the ensemble. The primary contribution of this model is to demonstrate that purely attentional NMT is possible: the only inputs to the decoder are through the attention mechanism.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_0",
  "x": "In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different. In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>. The key idea is that assuming that the speech data is generated from two separate latent variable sets z1 and z2, where z1 contains segment-level (short-term) variables and z2 contains sequencelevel (long-term) variables (z2 that are further conditioned on an s-vector \u00b52). Leveraging the multi-scale nature that different factors affect speech at different time scales (e.g., speaker identity affects the fundamental frequency and volume of speech signal at the sequence level while the phonetic content affects the speech signal at the segment level), by training an autoencoder in a sequence-to-sequence manner, z1 can be forced to encode segment-level information (e.g., speech content), while z2 and \u00b52 can be forced to encode sequence-level information (e.g., speaker identity). In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information.",
  "y": "background uses"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_1",
  "x": "For example, in [5] , the authors show that learning disentangling latent factors corresponding to pose and identity in photos of human faces can improve the performance of both pose estimation and face verification. Learning disentangled representation from high-dimensional data is not a trivial task and multiple techniques, such as \u03b2-VAE [1] , Info-GAN [2] , and DC-IGN [3] , have been developed to address this problem. While disentangling natural image representation has been studied extensively, much less work has focused on natural speech, leaving a rather large void in the understanding of this problem. In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different. In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_2",
  "x": "Leveraging the multi-scale nature that different factors affect speech at different time scales (e.g., speaker identity affects the fundamental frequency and volume of speech signal at the sequence level while the phonetic content affects the speech signal at the segment level), by training an autoencoder in a sequence-to-sequence manner, z1 can be forced to encode segment-level information (e.g., speech content), while z2 and \u00b52 can be forced to encode sequence-level information (e.g., speaker identity). In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_3",
  "x": "In the experiments, by keeping z2 fixed and changing z1, speech of the same content, but by different speakers, can be synthesized naturally, demonstrating the clean separation between content and speaker information. Further, the learned s-vector is shown to be a stronger feature than the conventional i-vector in the speaker verification task, demonstrating that it encodes speaker-level information well. In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)).",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_4",
  "x": "The main insight of this work is how to use supervision to conduct representation disentanglement. In summary, although the implementation is very different, in order to learn disentangled representations, both works add constraints to the latent variables. Such a constraint can be a prior assumption (in the unsupervised case) or a regularization term (in the supervised case). While both efforts show good empirical performance in real tasks and lay the groundwork for future efforts, the learned disentangled representation is relatively coarse-grained. That is, in [<cite>6</cite>] , z1 and z2 are in fact corresponding to general fast-changing and slow-changing information, i.e., z1 may contain other fast-changing information such as emotion, while z2 may contain slow-changing factors such as background and channel noise.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_5",
  "x": "Further, this may support novel AI applications, such as speech style transfer and predicting the future voice of a given subject (similar technology has been adopted in computer vision, e.g., image style transfer [16] and face aging [17] ). In contrast, a coarse-grained disentangled representation [<cite>6</cite>, 7] may only support a simple voice speaker conversion task. Inference: Learning fine-grained disentangled representation can also help with more accurate inference and reasoning. When we attempt to predict one target variable, we usually want to eliminate the interference of other factors. For example, a speech recognition system is expected to be emotionindependent, while a speech emotion recognition system is expected to be text-independent.",
  "y": "differences"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_0",
  "x": "For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Ma and Hovy, 2016) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_1",
  "x": "\u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_2",
  "x": "Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018) , choose a different data split on the POS dataset.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_3",
  "x": "\u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing. \u2022 Features.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_4",
  "x": "Ma and Hovy (2016) do not use preprocessing. \u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_5",
  "x": "Collobert et al. (2011) and use neural features to represent external gazetteer information. Besides, Lample et al. (2016) and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. <cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_6",
  "x": "\u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly. \u2022 Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_7",
  "x": "It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features.",
  "y": "extends"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_8",
  "x": "Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. <cite>These models</cite> achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_9",
  "x": "2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with <cite>most previous work</cite> on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_10",
  "x": "Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively. Subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_11",
  "x": "Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; <cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) . In our experiments, we take the same structure as Ma and Hovy (2016) , using one layer CNN structure with max-pooling to capture character-level representations. Figure 2 (a) shows the CNN structure on representing word \"Mexico\". Character LSTM.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_12",
  "x": "LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; <cite>Chiu and Nichols, 2016;</cite> Liu et al., 2018) . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011; dos Santos et al., 2015; Strubell et al., 2017) . Word CNN. Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings. If a character sequence representation layer is used, then word embeddings and character sequence representations are concatenated for word representations.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_13",
  "x": "As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of Lample et al. (2016) can be reproduced by our CLSTM+WLSTM+CRF. In most cases, our \"Nochar\" based models underperform their corresponding prototypes Strubell et al., 2017) , which utilize the hand-crafted features.",
  "y": "uses"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_14",
  "x": "Our observation is consistent with most literature (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) . ---------------------------------- **ANALYSIS** Decoding speed. We test the decoding speeds of the twelve models on the NER dataset using a Nvidia GTX 1080 GPU.",
  "y": "similarities"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_0",
  "x": "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_1",
  "x": "**INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> .",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_2",
  "x": "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information.",
  "y": "background motivation"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_3",
  "x": "Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_4",
  "x": "This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ---------------------------------- **PROPOSED APPROACH**",
  "y": "background differences"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_5",
  "x": "The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data. ---------------------------------- **THE RULE-BASED MODEL** The motivations for developing a set of rules for this task are twofold.",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_0",
  "x": "In this paper, we outline a fundamentally different approach to online video popularity analysis that allows social media creators both to predict video popularity as well as to understand the impact of its headline or video frames on the future popularity. To that end, we propose to use an attention-based model and gradient-weighted class activation maps [9] , inspired by the recent successes of the attention mechanism in other domains [15, <cite>16]</cite> . Although some works focused on understanding the influence of image parts on its popularity [6, 1] , our method addresses videos, not images, and exploits the temporal characteristics of video clips through the attention mechanism. By extending the baseline popularity prediction method with the attention mechanism, we enable more intuitive visualization of the impact visual and textual features of the video have on its final popularity, while achieving state-of-the-art results on the popularity prediction task. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_1",
  "x": "For each frame feature vector we apply a learnable linear transformation followed by ReLU, obtaining a sequence of frame embeddings (q j ) N j=1 . The final video embedding is a weighted average of these embed- Weights \u03b1 i are computed with attention mechanism implemented as a two-layer neural network<cite> [16]</cite> : the first layer produces a hidden representation u i = tanh(W u q i + b u ) and the second layer outputs unnormalized importance a i = W a u i + b a . W a can be interpreted as a trainable high level representation of the most informative vector in u i space. Final weights are normalized with softmax:",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_2",
  "x": "This way we obtain a sequence-wide normalized heatmap of frame regions influencing the final popularity score. For visualizations in the text domain, we use attention weights \u03b2 t used to compute text representation d. These weights capture relative importance of words in their context to headline popularity, as shown in<cite> [16]</cite> in the context of sentiment analysis. ---------------------------------- **EXPERIMENTS** We use a dataset of 37k Facebook videos with 80/10/10 train/validation/test splits.",
  "y": "background uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_0",
  "x": "Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. This process is similar to abstractive summarization but instead of a summary the models generate keyphrases. Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases.",
  "y": "motivation"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_1",
  "x": "**PROPOSED ADVERSARIAL MODEL** As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> . Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i .",
  "y": "uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_2",
  "x": "All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i . We employ catSeq model<cite> (Yuan et al. 2018)</cite> for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU. To incorporate the out-of-vocabulary words, we use a copying mechanism (Gu et al. 2016). We also make use of attention mechanism to help the generator identify the relevant components of the source text.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_0",
  "x": "More recently, <cite>Wiegand and Klakow (2010)</cite> explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010) , a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007) , for example, use a list of manually compiled communication verbs. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_1",
  "x": "As a large unlabeled (training) corpus, we chose the North American News Text Corpus. As a labeled (test) corpus, we use the MPQA corpus. 2 We use the definition of OHs as described in<cite> (Wiegand and Klakow, 2010)</cite> . The instance space are all noun phrases (NP) in that corpus. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_2",
  "x": "With this definition we train a supervised classifier based on convolution kernels (Collins and Duffy, 2001 ) as this method has been shown to be quite effective for OH extraction<cite> (Wiegand and Klakow, 2010)</cite> . Convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part-of-speech sequences, that are directly provided to the learner. Thus a classifier can be built without the taking the burden of implementing an explicit feature extraction. We chose the best performing set of tree kernels (Collins and Duffy, 2001; Moschitti, 2006) from that work. It comprises two tree kernels based on constituency parse trees and a tree kernel based on semantic role trees.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_3",
  "x": "We exclude sequence and vector kernels in this work not only for reasons of simplicity but also since their addition to tree kernels only results in a marginal improvement. Moreover, the features in the vector kernel heavily rely on taskspecific resources, e.g. a sentiment lexicon, which are deliberately avoided in our low-resource classifier as our method should be applicable to any language (and for many languages sentiment resources are either sparse or do not exist at all). In addition to <cite>Wiegand and Klakow (2010)</cite> , we have to discard the content of candidate NPs (e.g. the candidate opinion holder NP [N P Cand [N N S advocates] ] is reduced to [N P Cand ]), the reason for this being that in our automatically generated training set, OHs will always be protoOHs. Retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns (our resulting classifier should detect any OH and not only protoOHs). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_4",
  "x": "We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree . Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_5",
  "x": "Again, we use the supervised learner based on tree kernels ( \u00a74.1). We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree .",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_6",
  "x": "Table 4 lists the most highly ranked verbs that are extracted. 6 As an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task-specific resources, i.e. communication verbs from the Appraisal Lexicon (AL) (Bloom et al., 2007) and opinion words from the Subjectivity Lexicon (SL) (Wilson et al., 2005) . Both resources have been found predictive for OH extraction (Bloom et al., 2007;<cite> Wiegand and Klakow, 2010)</cite> . Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_7",
  "x": "The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_8",
  "x": "The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_9",
  "x": "---------------------------------- **INCORPORATING KNOWLEDGE FROM PROTOOHS INTO SUPERVISED LEARNING** As a maximum amount of labeled training data we chose 60000 instances (i.e. NPs) which is even a bit more than used in<cite> (Wiegand and Klakow, 2010)</cite> . In addition, we also test 1%, 5%, 10%, 25% and 50% of the training set. From the remaining data instances, we use 25000 instances as test data.",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_10",
  "x": "This is why the different classifiers presented should primarily be compared to our own baseline (TKPlain) and not the numbers presented in previous work as they always use the maximal size of labeled training data and additionally task-specific resources (e.g. sentiment lexicons). The results show that using the information extracted from the unlabeled data can be usefully combined with the labeled training data. Tree augmentation causes both precision and recall to rise. This observation is consistent with<cite> (Wiegand and Klakow, 2010)</cite> where, however, AL and SL are considered for augmentation. When the vector kernel with the prediction of the rule-based classifier is also included, precision drops slightly but recall is notably boosted resulting in an even more increased F-Score.",
  "y": "similarities"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_0",
  "x": "Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996) ; Girju (2007) ; Sinha (2009); Tsvetkov and Wintner (2010) ; Ziering et al. (2013) . <cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_1",
  "x": "<cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. Although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. Moreover, the system needs part of speech (PoS) tags and splitting information for determining 2NCs and is therefore rather language-dependent.",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_2",
  "x": "The fact, that the alignment of the third noun, violations (violazioni), is separated from the rest, points us in the direction of LEFT-branching. Using less restricted forms of cross-lingual supervision, we achieve a much higher coverage than <cite>Ziering and Van der Plas (2014)</cite> . Furthermore, our results are more accurate. In contrast to previous unsupervised methods, our system is applicable in both token-and type-based modes. Token-based bracketing is context-dependent and allows for a better treatment of structural ambiguity (as in luxury cattle truck).",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_3",
  "x": "While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results. Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_4",
  "x": "Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_5",
  "x": "Moreover, in contrast to previous work (e.g., Vadas and Curran (2007b) ), we take only common nouns as components into account rather than named entities. We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_6",
  "x": "We inspected a subset of all 3NCs in the database and estimated the best filter quality to be with \u03b8 = 0.04. This threshold discards increasing land abandonment but keeps human rights abuse. Our final dataset contains 14,941 tokens and 8824 types. Systems in comparison. We compare AWDB with the bracketing approach of <cite>Ziering and Van der Plas (2014)</cite>.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_7",
  "x": "We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT. Human annotation. We observed that there is only a very small overlap between test sets of previous work on NP bracketing and the Europarl database. The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_8",
  "x": "We observed that there is only a very small overlap between test sets of previous work on NP bracketing and the Europarl database. The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained. Thus, we decided to create our own test set. 2 statmt.org/europarl 3 ims.uni-stuttgart.de/data/NCDatabase.html 4 opus.lingfil.uu.se 5 en.wikipedia.org 6 For a fair comparison, we leave systems that have access to external knowledge, such as web search engines, aside. A trained independent annotator classified a set of 1100 tokens in context with one of the following labels: LEFT, RIGHT, EXTRACTION (for extraction errors that survived the high-confidence noun filter P noun (word)), UNDECIDED (for 3NCs that cannot be disambiguated within the one-sentence context) and SEMANTIC INDETERMINATE (for 3NCs for which LEFT and RIGHT have the same meaning such as book price fixing (i.e., price fixing for books is equivalent to fixing of the book price)).",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_9",
  "x": "Our first result is that type-based cross-lingual bracketing outperforms token-based and achieves up to 91.2% in coverage. As expected, the system of <cite>Ziering and Van der Plas (2014)</cite> does not cover more than 48.1%. The \u03c7 2 method and the back-off models cover all 3NCs in our dataset. The fact that AWDB type misses 8.8% of the dataset is mainly due to equal distances between aligned words (e.g., crisis resolution mechanism is only aligned to closed compounds, such as the Swedish krisl\u00f6sningsmekanism or to nouns separated by one preposition, such as the Spanish mecanismo de resoluci\u00f3n de crisis). In future work, we will add more languages in the hope to find more variation and thus get an even higher coverage.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_10",
  "x": "---------------------------------- **SYSTEM** Acc Table 2 : Direct comparison on common test sets; \u2020 = significantly better than the systems in comparison Table 2 directly compares the systems on common subsets (com), i.e., on 3NCs for which all systems in the set provide a result. The main reason why cross-lingual systems make bracketing errors is the quality of automatic word alignment. AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 .",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_11",
  "x": "The last four lines of Table 2 show all systems with full coverage. AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy. The harmonic mean numbers for the system of <cite>Ziering and Van der Plas (2014)</cite> illustrate that coverage gain of types outweighs a higher accuracy of tokens. Our intuition that token-based approaches are superior in accuracy is hardly reflected in the present results.",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_0",
  "x": "Explanations can help formulate clear and accurate mental models of autonomous systems and robots. Mental models, in cognitive theory, provide one view on how humans reason either functionally (understanding what the robot does) or structurally (understanding how it works). Mental models are important as they strongly impact how and whether robots and systems are used. In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation <cite>[3]</cite> . However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission.",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_1",
  "x": "Types of explanations include why to provide a trace or reasoning and why not to elaborate on the system's control method or strategy [4] . Lim et al. (2009) [10] show that both why and why not explanations increase understanding but only why increases trust. We adopt here the 'speak-aloud' method whereby an expert provides rationalisation of the AxV behaviours while watching videos of missions on the SeeTrack software. This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models. Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play <cite>[3]</cite> .",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_0",
  "x": "---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g.,<cite> Dyer et al. (2016)</cite> with 92.4 F 1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_1",
  "x": "**PREVIOUS WORK** We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; <cite>Dyer et al., 2016)</cite> are parsing models that have the current best results in NN parsing. ---------------------------------- **LSTM-LM**",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_2",
  "x": "---------------------------------- **RNNG** Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree<cite> (Dyer et al., 2016)</cite> : where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2). RNNG and our model differ in how they compute the conditioning event (z 1 , \u00b7 \u00b7 \u00b7 , z t\u22121 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_3",
  "x": "To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011 ) with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010) . (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as<cite> Dyer et al. (2016)</cite> do with their discriminative and generative models.",
  "y": "similarities"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_5",
  "x": "**RESULTS** ---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG<cite> (Dyer et al., 2016)</cite> , both of which are trained on the WSJ only. ----------------------------------",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_6",
  "x": "In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models<cite> (Dyer et al., 2016)</cite> . We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of Vinyals et al. (2015) and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.",
  "y": "differences"
 }
]